{
  "sha": "890d7154e777d0673defcd11db3ef792473d68c9",
  "node_id": "MDY6Q29tbWl0MTE0ODI5NTAzOjg5MGQ3MTU0ZTc3N2QwNjczZGVmY2QxMWRiM2VmNzkyNDczZDY4Yzk=",
  "commit": {
    "author": {
      "name": "Dmitry Verkhoturov",
      "email": "paskal.07@gmail.com",
      "date": "2020-05-24T23:48:17Z"
    },
    "committer": {
      "name": "Umputun",
      "email": "umputun@gmail.com",
      "date": "2020-05-25T00:00:07Z"
    },
    "message": "bump go modules",
    "tree": {
      "sha": "4b000b21af73672e472d5a798a1dc66d60d057ff",
      "url": "https://api.github.com/repos/umputun/remark42/git/trees/4b000b21af73672e472d5a798a1dc66d60d057ff"
    },
    "url": "https://api.github.com/repos/umputun/remark42/git/commits/890d7154e777d0673defcd11db3ef792473d68c9",
    "comment_count": 0,
    "verification": {
      "verified": false,
      "reason": "unsigned",
      "signature": null,
      "payload": null
    }
  },
  "url": "https://api.github.com/repos/umputun/remark42/commits/890d7154e777d0673defcd11db3ef792473d68c9",
  "html_url": "https://github.com/umputun/remark42/commit/890d7154e777d0673defcd11db3ef792473d68c9",
  "comments_url": "https://api.github.com/repos/umputun/remark42/commits/890d7154e777d0673defcd11db3ef792473d68c9/comments",
  "author": {
    "login": "paskal",
    "id": 712534,
    "node_id": "MDQ6VXNlcjcxMjUzNA==",
    "avatar_url": "https://avatars1.githubusercontent.com/u/712534?v=4",
    "gravatar_id": "",
    "url": "https://api.github.com/users/paskal",
    "html_url": "https://github.com/paskal",
    "followers_url": "https://api.github.com/users/paskal/followers",
    "following_url": "https://api.github.com/users/paskal/following{/other_user}",
    "gists_url": "https://api.github.com/users/paskal/gists{/gist_id}",
    "starred_url": "https://api.github.com/users/paskal/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/paskal/subscriptions",
    "organizations_url": "https://api.github.com/users/paskal/orgs",
    "repos_url": "https://api.github.com/users/paskal/repos",
    "events_url": "https://api.github.com/users/paskal/events{/privacy}",
    "received_events_url": "https://api.github.com/users/paskal/received_events",
    "type": "User",
    "site_admin": false
  },
  "committer": {
    "login": "umputun",
    "id": 535880,
    "node_id": "MDQ6VXNlcjUzNTg4MA==",
    "avatar_url": "https://avatars0.githubusercontent.com/u/535880?v=4",
    "gravatar_id": "",
    "url": "https://api.github.com/users/umputun",
    "html_url": "https://github.com/umputun",
    "followers_url": "https://api.github.com/users/umputun/followers",
    "following_url": "https://api.github.com/users/umputun/following{/other_user}",
    "gists_url": "https://api.github.com/users/umputun/gists{/gist_id}",
    "starred_url": "https://api.github.com/users/umputun/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/umputun/subscriptions",
    "organizations_url": "https://api.github.com/users/umputun/orgs",
    "repos_url": "https://api.github.com/users/umputun/repos",
    "events_url": "https://api.github.com/users/umputun/events{/privacy}",
    "received_events_url": "https://api.github.com/users/umputun/received_events",
    "type": "User",
    "site_admin": false
  },
  "parents": [
    {
      "sha": "0d67f7e53d4c07b9d299d2dd2d8d33b75fc83a23",
      "url": "https://api.github.com/repos/umputun/remark42/commits/0d67f7e53d4c07b9d299d2dd2d8d33b75fc83a23",
      "html_url": "https://github.com/umputun/remark42/commit/0d67f7e53d4c07b9d299d2dd2d8d33b75fc83a23"
    }
  ],
  "stats": {
    "total": 134124,
    "additions": 39083,
    "deletions": 95041
  },
  "files": [
    {
      "sha": "703f7dd60c8aab1af611ca1ec921b1547320d8ab",
      "filename": "backend/_example/memory_store/go.mod",
      "status": "modified",
      "additions": 1,
      "deletions": 1,
      "changes": 2,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/_example/memory_store/go.mod",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/_example/memory_store/go.mod",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/_example/memory_store/go.mod?ref=890d7154e777d0673defcd11db3ef792473d68c9",
      "patch": "@@ -3,7 +3,7 @@ module github.com/umputun/remark42/memory_store\n go 1.14\n \n require (\n-\tgithub.com/go-pkgz/jrpc v0.1.0\n+\tgithub.com/go-pkgz/jrpc v0.2.0\n \tgithub.com/go-pkgz/lgr v0.7.0\n \tgithub.com/pkg/errors v0.9.1\n \tgithub.com/stretchr/testify v1.5.1"
    },
    {
      "sha": "8cdac7f238f96785370f22ded675734746534c1e",
      "filename": "backend/_example/memory_store/go.sum",
      "status": "modified",
      "additions": 105,
      "deletions": 123,
      "changes": 228,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/_example/memory_store/go.sum",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/_example/memory_store/go.sum",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/_example/memory_store/go.sum?ref=890d7154e777d0673defcd11db3ef792473d68c9",
      "patch": "@@ -1,16 +1,12 @@\n-cloud.google.com/go v0.26.0/go.mod h1:aQUYkXzVsufM+DwF1aE+0xfcU+56JwCaLick0ClmMTw=\n cloud.google.com/go v0.34.0/go.mod h1:aQUYkXzVsufM+DwF1aE+0xfcU+56JwCaLick0ClmMTw=\n-cloud.google.com/go v0.38.0/go.mod h1:990N+gfupTy94rShfmMCWGDn0LpTmnzTp2qbd1dvSRU=\n-cloud.google.com/go v0.40.0/go.mod h1:Tk58MuI9rbLMKlAjeO/bDnteAx7tX2gJIXw4T5Jwlro=\n github.com/BurntSushi/toml v0.3.1/go.mod h1:xHWCNGjB5oqiDr8zfno3MHue2Ht5sIBksp03qcyfWMU=\n github.com/Depado/bfchroma v1.2.0 h1:NyYPFVhWvq8S2ts6Ok4kwXVE3TEO5fof+9ZOKbBJQUo=\n github.com/Depado/bfchroma v1.2.0/go.mod h1:U3RJUYwWVJrZRaJQyfS+wuxBApSTR/BC37PhAI+Ydps=\n github.com/PuerkitoBio/goquery v1.5.1 h1:PSPBGne8NIUWw+/7vFBV+kG2J/5MOjbzc7154OaKCSE=\n github.com/PuerkitoBio/goquery v1.5.1/go.mod h1:GsLWisAFVj4WgDibEWF4pvYnkVQBpKBKeU+7zCJoLcc=\n-github.com/ajg/form v0.0.0-20160822230020-523a5da1a92f/go.mod h1:uL1WgH+h2mgNtvBq0339dVnzXdBETtL2LeUXaIv25UY=\n+github.com/ajg/form v1.5.1/go.mod h1:uL1WgH+h2mgNtvBq0339dVnzXdBETtL2LeUXaIv25UY=\n github.com/alecthomas/assert v0.0.0-20170929043011-405dbfeb8e38 h1:smF2tmSOzy2Mm+0dGI2AIUHY+w0BUc+4tn40djz7+6U=\n github.com/alecthomas/assert v0.0.0-20170929043011-405dbfeb8e38/go.mod h1:r7bzyVFMNntcxPZXK3/+KdruV1H5KSlyVY0gc+NgInI=\n-github.com/alecthomas/chroma v0.6.0 h1:gcvXlpe0/NoQP3BvneRfgcauLIJDw9VblkoFwZ5XGFs=\n github.com/alecthomas/chroma v0.6.0/go.mod h1:MmozekIi2rfQSzDcdEZ2BoJ9Pxs/7uc2Y4Boh+hIeZo=\n github.com/alecthomas/chroma v0.7.2 h1:B76NU/zbQYIUhUowbi4fmvREmDUJLsUzKWTZmQd3ABY=\n github.com/alecthomas/chroma v0.7.2/go.mod h1:fv5SzZPFJbwp2NXJWpFIX7DZS4HgV1K4ew4Pc2OZD9s=\n@@ -21,122 +17,136 @@ github.com/alecthomas/repr v0.0.0-20180818092828-117648cd9897/go.mod h1:xTS7Pm1p\n github.com/alecthomas/repr v0.0.0-20181024024818-d37bc2a10ba1 h1:GDQdwm/gAcJcLAKQQZGOJ4knlw+7rfEQQcmwTbt4p5E=\n github.com/alecthomas/repr v0.0.0-20181024024818-d37bc2a10ba1/go.mod h1:xTS7Pm1pD1mvyM075QCDSRqH6qRLXylzS24ZTpRiSzQ=\n github.com/alicebob/gopher-json v0.0.0-20180125190556-5a6b3ba71ee6/go.mod h1:SGnFV6hVsYE877CKEZ6tDNTjaSXYUk6QqoIK6PrAtcc=\n-github.com/alicebob/miniredis v2.5.0+incompatible/go.mod h1:8HZjEj4yU0dwhYHky+DxYx+6BMjkBbe5ONFIF1MXffk=\n github.com/alicebob/miniredis/v2 v2.11.4/go.mod h1:VL3UDEfAH59bSa7MuHMuFToxkqyHh69s/WUbYlOAuyg=\n github.com/andybalholm/cascadia v1.1.0 h1:BuuO6sSfQNFRu1LppgbD25Hr2vLYW25JvxHs5zzsLTo=\n github.com/andybalholm/cascadia v1.1.0/go.mod h1:GsXiBklL0woXo1j/WYWtSYYC4ouU9PqHO0sqidkEA4Y=\n github.com/chzyer/logex v1.1.10/go.mod h1:+Ywpsq7O8HXn0nuIou7OrIPyXbp3wmkHB+jjWRnGsAI=\n github.com/chzyer/readline v0.0.0-20180603132655-2972be24d48e/go.mod h1:nSuG5e5PlCu98SY8svDHJxuZscDgtXS6KTTbou5AhLI=\n github.com/chzyer/test v0.0.0-20180213035817-a1ea475d72b1/go.mod h1:Q3SI9o4m/ZMnBNeIyt5eFwwo7qiLfzFZmjNmxjkiQlU=\n-github.com/client9/misspell v0.3.4/go.mod h1:qj6jICC3Q7zFZvVWo7KLAzC3yx5G7kyvSDkc90ppPyw=\n github.com/danwakefield/fnmatch v0.0.0-20160403171240-cbb64ac3d964 h1:y5HC9v93H5EPKqaS1UYVg1uYah5Xf51mBfIoWehClUQ=\n github.com/danwakefield/fnmatch v0.0.0-20160403171240-cbb64ac3d964/go.mod h1:Xd9hchkHSWYkEqJwUGisez3G1QY8Ryz0sdWrLPMGjLk=\n github.com/davecgh/go-spew v1.1.0/go.mod h1:J7Y8YcW2NihsgmVo/mv3lAwl/skON4iLHjSsI+c5H38=\n github.com/davecgh/go-spew v1.1.1 h1:vj9j/u1bqnvCEfJOwUhtlOARqs3+rkHYY13jYWTU97c=\n github.com/davecgh/go-spew v1.1.1/go.mod h1:J7Y8YcW2NihsgmVo/mv3lAwl/skON4iLHjSsI+c5H38=\n github.com/dghubble/oauth1 v0.6.0/go.mod h1:8pFdfPkv/jr8mkChVbNVuJ0suiHe278BtWI4Tk1ujxk=\n github.com/dgrijalva/jwt-go v3.2.0+incompatible/go.mod h1:E3ru+11k8xSBh+hMPgOLZmtrrCbhqsmaPHjLKYnJCaQ=\n-github.com/didip/tollbooth v4.0.2+incompatible h1:fVSa33JzSz0hoh2NxpwZtksAzAgd7zjmGO20HCZtF4M=\n-github.com/didip/tollbooth v4.0.2+incompatible/go.mod h1:A9b0665CE6l1KmzpDws2++elm/CsuWBMa5Jv4WY0PEY=\n-github.com/didip/tollbooth_chi v0.0.0-20170928041846-6ab5f3083f3d h1:vs5Nf6IE0N/PwGJ8//zRed4gpCdcr99K2HzX7RuLOQ8=\n-github.com/didip/tollbooth_chi v0.0.0-20170928041846-6ab5f3083f3d/go.mod h1:YWyIfq3y4ArRfWZ9XksmuusP+7Mad+T0iFZ0kv0XG/M=\n+github.com/didip/tollbooth/v6 v6.0.1 h1:QvLvRpB1G2bzKvkRze0muMUBlGN9H1z7tJ4DH4ypWOU=\n+github.com/didip/tollbooth/v6 v6.0.1/go.mod h1:j2pKs+JQ5PvU/K4jFnrnwntrmfUbYLJE5oSdxR37FD0=\n+github.com/didip/tollbooth_chi v0.0.0-20200524181329-8b84cd7183d9 h1:gTh8fKuI/yLqQtZEPlDX3ZGsiTPZIe0ADHsxXSbwO1I=\n+github.com/didip/tollbooth_chi v0.0.0-20200524181329-8b84cd7183d9/go.mod h1:YWyIfq3y4ArRfWZ9XksmuusP+7Mad+T0iFZ0kv0XG/M=\n github.com/dlclark/regexp2 v1.1.6 h1:CqB4MjHw0MFCDj+PHHjiESmHX+N7t0tJzKvC6M97BRg=\n github.com/dlclark/regexp2 v1.1.6/go.mod h1:2pZnwuY/m+8K6iRw6wQdMtk+rH5tNGR1i55kozfMjCc=\n+github.com/fasthttp-contrib/websocket v0.0.0-20160511215533-1f3b11f56072/go.mod h1:duJ4Jxv5lDcvg4QuQr0oowTf7dz4/CR8NtyCooz9HL8=\n github.com/fatih/structs v1.1.0/go.mod h1:9NiDSp5zOcgEDl+j00MP/WkGVPOlPRLejGD8Ga6PJ7M=\n github.com/fsnotify/fsnotify v1.4.7/go.mod h1:jwhsz4b93w/PPRr/qN1Yymfu8t87LnFCMoQvtojpjFo=\n-github.com/gavv/httpexpect v0.0.0-20180803094507-bdde30871313/go.mod h1:x+9tiU1YnrOvnB725RkpoLv1M62hOWzwo5OXotisrKc=\n-github.com/gavv/monotime v0.0.0-20171021193802-6f8212e8d10d/go.mod h1:vmp8DIyckQMXOPl0AQVHt+7n5h7Gb7hS6CUydiV8QeA=\n-github.com/go-chi/chi v4.0.2+incompatible h1:maB6vn6FqCxrpz4FqWdh4+lwpyZIQS7YEAUcHlgXVRs=\n-github.com/go-chi/chi v4.0.2+incompatible/go.mod h1:eB3wogJHnLi3x/kFX2A+IbTBlXxmMeXJVKy9tTv1XzQ=\n-github.com/go-chi/chi v4.1.0+incompatible h1:ETj3cggsVIY2Xao5ExCu6YhEh5MD6JTfcBzS37R260w=\n-github.com/go-chi/chi v4.1.0+incompatible/go.mod h1:eB3wogJHnLi3x/kFX2A+IbTBlXxmMeXJVKy9tTv1XzQ=\n+github.com/gavv/httpexpect v2.0.0+incompatible/go.mod h1:x+9tiU1YnrOvnB725RkpoLv1M62hOWzwo5OXotisrKc=\n+github.com/go-chi/chi v4.1.1+incompatible h1:MmTgB0R8Bt/jccxp+t6S/1VGIKdJw5J74CK/c9tTfA4=\n+github.com/go-chi/chi v4.1.1+incompatible/go.mod h1:eB3wogJHnLi3x/kFX2A+IbTBlXxmMeXJVKy9tTv1XzQ=\n github.com/go-chi/cors v1.1.1/go.mod h1:K2Yje0VW/SJzxiyMYu6iPQYa7hMjQX2i/F491VChg1I=\n github.com/go-chi/render v1.0.1 h1:4/5tis2cKaNdnv9zFLfXzcquC9HbeZgCnxGnKrltBS8=\n github.com/go-chi/render v1.0.1/go.mod h1:pq4Rr7HbnsdaeHagklXub+p6Wd16Af5l9koip1OvJns=\n-github.com/go-pkgz/auth v0.10.1/go.mod h1:wxyQqc0UUP1jT4l6zk1r6XPcVdcgIzW2OiQ8hBEHd64=\n-github.com/go-pkgz/jrpc v0.1.0 h1:hNg/IyfEqJcSWOKkuHw0ZwcuGc9TDp7QZREsD2ycmiM=\n-github.com/go-pkgz/jrpc v0.1.0/go.mod h1:JxZsvoBklA50DNhELVJnJ567Rt+KrMH9rR3u515wvE8=\n-github.com/go-pkgz/lcw v0.5.0/go.mod h1:CSdQRQthxJQ4iDD4wTPPuWFbFdknJzwJ8WXu1nfxb10=\n-github.com/go-pkgz/lcw v0.5.1-0.20200509170726-dc283cfc28cf/go.mod h1:vovP88gZLeuIWn5cm0NlgPYFyGGkv3m2OcKMOOaHhj0=\n+github.com/go-pkgz/auth v0.10.2/go.mod h1:w4Z1qaYvuh2P3T2gNh0f8GcKCH0HHAoQtQ8iv+9+WGg=\n+github.com/go-pkgz/expirable-cache v0.0.3 h1:rTh6qNPp78z0bQE6HDhXBHUwqnV9i09Vm6dksJLXQDc=\n+github.com/go-pkgz/expirable-cache v0.0.3/go.mod h1:+IauqN00R2FqNRLCLA+X5YljQJrwB179PfiAoMPlTlQ=\n+github.com/go-pkgz/jrpc v0.2.0 h1:CLy/eZyekjraVrxZV18N2R1mYLMJ/nWrgdfyIOGPY/E=\n+github.com/go-pkgz/jrpc v0.2.0/go.mod h1:wd8vtQ4CgtCnuqua6x2b1SKIgv0VSOh5Dn0uUITbiUE=\n+github.com/go-pkgz/lcw v0.6.1/go.mod h1:vovP88gZLeuIWn5cm0NlgPYFyGGkv3m2OcKMOOaHhj0=\n github.com/go-pkgz/lgr v0.7.0 h1:S/AAPwt/RE9a5mNJskA7dGVp+Dq6SMIW6LYjG3ITxY8=\n github.com/go-pkgz/lgr v0.7.0/go.mod h1:yMgxU+GobMRJgIEbSzDKy/67W18S7qmGx/7BVL5AB8Q=\n github.com/go-pkgz/repeater v1.1.3/go.mod h1:hVTavuO5x3Gxnu8zW7d6sQBfAneKV8X2FjU48kGfpKw=\n-github.com/go-pkgz/rest v1.4.1 h1:DmaVLPH2O7yLehrWOW0uz01d2mVHz9fBR/iuTiPRzaw=\n-github.com/go-pkgz/rest v1.4.1/go.mod h1:COazNj35u3RXAgQNBr6neR599tYP3URiOpsu9p0rOtk=\n github.com/go-pkgz/rest v1.5.0 h1:C8SxXcXza4GiUUAn/95iCkvoIrGbS30qpwK19iqlrWQ=\n github.com/go-pkgz/rest v1.5.0/go.mod h1:nQaM3RhSTUAmbBZWY4hfe4buyeC9VckvhoCktiQXJxI=\n github.com/go-pkgz/syncs v1.1.1/go.mod h1:bt9lxWRRJ9vOCMGc8Big8ttjYHLKP88ofj1y38UlaHE=\n-github.com/go-redis/redis/v7 v7.0.0-beta.4/go.mod h1:xhhSbUMTsleRPur+Vgx9sUHtyN33bdjxY+9/0n9Ig8s=\n github.com/go-redis/redis/v7 v7.2.0/go.mod h1:JDNMw23GTyLNC4GZu9njt15ctBQVn7xjRfnwdHj/Dcg=\n github.com/go-session/session v3.1.2+incompatible/go.mod h1:8B3iivBQjrz/JtC68Np2T1yBBLxTan3mn/3OM0CyRt0=\n github.com/go-stack/stack v1.8.0/go.mod h1:v0f6uXyyMGvRgIKkXu+yp6POWl0qKG85gN/melR3HDY=\n-github.com/golang/glog v0.0.0-20160126235308-23def4e6c14b/go.mod h1:SBH7ygxi8pfUlaOkMMuAQtPIUF8ecWP5IEl/CR7VP2Q=\n-github.com/golang/mock v1.1.1/go.mod h1:oTYuIxOrZwtPieC+H1uAHpcLFnEyAGVDL/k47Jfbm0A=\n-github.com/golang/mock v1.2.0/go.mod h1:oTYuIxOrZwtPieC+H1uAHpcLFnEyAGVDL/k47Jfbm0A=\n+github.com/gobuffalo/attrs v0.0.0-20190224210810-a9411de4debd/go.mod h1:4duuawTqi2wkkpB4ePgWMaai6/Kc6WEz83bhFwpHzj0=\n+github.com/gobuffalo/depgen v0.0.0-20190329151759-d478694a28d3/go.mod h1:3STtPUQYuzV0gBVOY3vy6CfMm/ljR4pABfrTeHNLHUY=\n+github.com/gobuffalo/depgen v0.1.0/go.mod h1:+ifsuy7fhi15RWncXQQKjWS9JPkdah5sZvtHc2RXGlg=\n+github.com/gobuffalo/envy v1.6.15/go.mod h1:n7DRkBerg/aorDM8kbduw5dN3oXGswK5liaSCx4T5NI=\n+github.com/gobuffalo/envy v1.7.0/go.mod h1:n7DRkBerg/aorDM8kbduw5dN3oXGswK5liaSCx4T5NI=\n+github.com/gobuffalo/flect v0.1.0/go.mod h1:d2ehjJqGOH/Kjqcoz+F7jHTBbmDb38yXA598Hb50EGs=\n+github.com/gobuffalo/flect v0.1.1/go.mod h1:8JCgGVbRjJhVgD6399mQr4fx5rRfGKVzFjbj6RE/9UI=\n+github.com/gobuffalo/flect v0.1.3/go.mod h1:8JCgGVbRjJhVgD6399mQr4fx5rRfGKVzFjbj6RE/9UI=\n+github.com/gobuffalo/genny v0.0.0-20190329151137-27723ad26ef9/go.mod h1:rWs4Z12d1Zbf19rlsn0nurr75KqhYp52EAGGxTbBhNk=\n+github.com/gobuffalo/genny v0.0.0-20190403191548-3ca520ef0d9e/go.mod h1:80lIj3kVJWwOrXWWMRzzdhW3DsrdjILVil/SFKBzF28=\n+github.com/gobuffalo/genny v0.1.0/go.mod h1:XidbUqzak3lHdS//TPu2OgiFB+51Ur5f7CSnXZ/JDvo=\n+github.com/gobuffalo/genny v0.1.1/go.mod h1:5TExbEyY48pfunL4QSXxlDOmdsD44RRq4mVZ0Ex28Xk=\n+github.com/gobuffalo/gitgen v0.0.0-20190315122116-cc086187d211/go.mod h1:vEHJk/E9DmhejeLeNt7UVvlSGv3ziL+djtTr3yyzcOw=\n+github.com/gobuffalo/gogen v0.0.0-20190315121717-8f38393713f5/go.mod h1:V9QVDIxsgKNZs6L2IYiGR8datgMhB577vzTDqypH360=\n+github.com/gobuffalo/gogen v0.1.0/go.mod h1:8NTelM5qd8RZ15VjQTFkAW6qOMx5wBbW4dSCS3BY8gg=\n+github.com/gobuffalo/gogen v0.1.1/go.mod h1:y8iBtmHmGc4qa3urIyo1shvOD8JftTtfcKi+71xfDNE=\n+github.com/gobuffalo/logger v0.0.0-20190315122211-86e12af44bc2/go.mod h1:QdxcLw541hSGtBnhUc4gaNIXRjiDppFGaDqzbrBd3v8=\n+github.com/gobuffalo/mapi v1.0.1/go.mod h1:4VAGh89y6rVOvm5A8fKFxYG+wIW6LO1FMTG9hnKStFc=\n+github.com/gobuffalo/mapi v1.0.2/go.mod h1:4VAGh89y6rVOvm5A8fKFxYG+wIW6LO1FMTG9hnKStFc=\n+github.com/gobuffalo/packd v0.0.0-20190315124812-a385830c7fc0/go.mod h1:M2Juc+hhDXf/PnmBANFCqx4DM3wRbgDvnVWeG2RIxq4=\n+github.com/gobuffalo/packd v0.1.0/go.mod h1:M2Juc+hhDXf/PnmBANFCqx4DM3wRbgDvnVWeG2RIxq4=\n+github.com/gobuffalo/packr/v2 v2.0.9/go.mod h1:emmyGweYTm6Kdper+iywB6YK5YzuKchGtJQZ0Odn4pQ=\n+github.com/gobuffalo/packr/v2 v2.2.0/go.mod h1:CaAwI0GPIAv+5wKLtv8Afwl+Cm78K/I/VCm/3ptBN+0=\n+github.com/gobuffalo/syncx v0.0.0-20190224160051-33c29581e754/go.mod h1:HhnNqWY95UYwwW3uSASeV7vtgYkT2t16hJgV3AEPUpw=\n github.com/golang/protobuf v1.2.0/go.mod h1:6lQm79b+lXiMfvg/cZm0SGofjICqVBUtrP5yJMmIC1U=\n-github.com/golang/protobuf v1.3.1/go.mod h1:6lQm79b+lXiMfvg/cZm0SGofjICqVBUtrP5yJMmIC1U=\n github.com/golang/protobuf v1.3.2/go.mod h1:6lQm79b+lXiMfvg/cZm0SGofjICqVBUtrP5yJMmIC1U=\n github.com/golang/snappy v0.0.1/go.mod h1:/XxbfmMg8lxefKM7IXC3fBNl/7bRcc72aCRzEWrmP2Q=\n github.com/gomodule/redigo v1.7.1-0.20190322064113-39e2c31b7ca3/go.mod h1:B4C85qUVwatsJoIUNIfCRsp7qO0iAmpGFZ4EELWSbC4=\n-github.com/gomodule/redigo v2.0.0+incompatible/go.mod h1:B4C85qUVwatsJoIUNIfCRsp7qO0iAmpGFZ4EELWSbC4=\n-github.com/google/btree v0.0.0-20180813153112-4030bb1f1f0c/go.mod h1:lNA+9X1NB3Zf8V7Ke586lFgjr2dZNuvo3lPJSGZ5JPQ=\n github.com/google/go-cmp v0.2.0/go.mod h1:oXzfMopK8JAjlY9xF4vHSVASa0yLyX7SntLO5aqRK0M=\n-github.com/google/go-cmp v0.3.0/go.mod h1:8QqcDgzrUqlUb/G2PQTWiueGozuR1884gddMywk6iLU=\n github.com/google/go-querystring v1.0.0/go.mod h1:odCYkC5MyYFN7vkCjXpyrEuKhc/BUO6wN/zVPAxq5ck=\n-github.com/google/martian v2.1.0+incompatible/go.mod h1:9I4somxYTbIHy5NJKHRl3wXiIaQGbYVAs8BPL6v8lEs=\n-github.com/google/pprof v0.0.0-20181206194817-3ea8567a2e57/go.mod h1:zfwlbNMJ+OItoe0UupaVj+oy1omPYYDuagoSzA8v9mc=\n github.com/google/uuid v1.1.1/go.mod h1:TIyPZe4MgqvfeYDBFedMoGGpEw/LqOeaOT+nhxU+yHo=\n-github.com/googleapis/gax-go/v2 v2.0.4/go.mod h1:0Wqv26UfaUD9n4G6kQubkQ+KchISgw+vpHVxEJEs9eg=\n-github.com/gopherjs/gopherjs v0.0.0-20181103185306-d547d1d9531e/go.mod h1:wJfORRmW1u3UXTncJ5qlYoELFm8eSnnEO6hX4iZ3EWY=\n+github.com/gopherjs/gopherjs v0.0.0-20181017120253-0766667cb4d1/go.mod h1:wJfORRmW1u3UXTncJ5qlYoELFm8eSnnEO6hX4iZ3EWY=\n github.com/gorilla/feeds v1.1.1/go.mod h1:Nk0jZrvPFZX1OBe5NPiddPw7CfwF6Q9eqzaBbaightA=\n+github.com/gorilla/websocket v1.4.1/go.mod h1:YR8l580nyteQvAITg2hZ9XVh4b55+EU/adAjf1fMHhE=\n github.com/hashicorp/errwrap v1.0.0 h1:hLrqtEDnRye3+sgx6z4qVLNuviH3MR5aQ0ykNJa/UYA=\n github.com/hashicorp/errwrap v1.0.0/go.mod h1:YH+1FKiLXxHSkmPseP+kNlulaMuP3n2brvKWEqk/Jc4=\n-github.com/hashicorp/go-multierror v1.0.0 h1:iVjPR7a6H0tWELX5NxNe7bYopibicUzc7uPribsnS6o=\n-github.com/hashicorp/go-multierror v1.0.0/go.mod h1:dHtQlpGsu+cZNNAkkCN/P3hoUDHhCYQXV3UM06sGGrk=\n github.com/hashicorp/go-multierror v1.1.0 h1:B9UzwGQJehnUY1yNrnwREHc3fGbC2xefo8g4TbElacI=\n github.com/hashicorp/go-multierror v1.1.0/go.mod h1:spPvp8C1qA32ftKqdAHm4hHTbPw+vmowP0z+KUhOZdA=\n-github.com/hashicorp/golang-lru v0.5.0/go.mod h1:/m3WP610KZHVQ1SGc6re/UDhFvYD7pJ4Ao+sR/qLZy8=\n-github.com/hashicorp/golang-lru v0.5.1/go.mod h1:/m3WP610KZHVQ1SGc6re/UDhFvYD7pJ4Ao+sR/qLZy8=\n-github.com/hashicorp/golang-lru v0.5.3/go.mod h1:iADmTwqILo4mZ8BN3D2Q6+9jd8WM5uGBxy+E8yxSoD4=\n github.com/hashicorp/golang-lru v0.5.4/go.mod h1:iADmTwqILo4mZ8BN3D2Q6+9jd8WM5uGBxy+E8yxSoD4=\n github.com/hpcloud/tail v1.0.0/go.mod h1:ab1qPbhIpdTxEkNHXyeSf5vhxWSCs/tWer42PpOxQnU=\n github.com/imkira/go-interpol v1.1.0/go.mod h1:z0h2/2T3XF8kyEPpRgJ3kmNv+C43p+I/CoI+jC3w2iA=\n-github.com/jstemmer/go-junit-report v0.0.0-20190106144839-af01ea7f8024/go.mod h1:6v2b51hI/fHJwM22ozAgKL4VKDeJcHhJFhtBdhmNjmU=\n-github.com/jtolds/gls v4.2.1+incompatible/go.mod h1:QJZ7F/aHp+rZTRtaJ1ow/lLfFfVYBRgL+9YlvaHOwJU=\n+github.com/inconshreveable/mousetrap v1.0.0/go.mod h1:PxqpIevigyE2G7u3NXJIT2ANytuPF1OarO4DADm73n8=\n+github.com/joho/godotenv v1.3.0/go.mod h1:7hK45KPybAkOC6peb+G5yklZfMxEjkZhHbwpqxOKXbg=\n+github.com/jtolds/gls v4.20.0+incompatible/go.mod h1:QJZ7F/aHp+rZTRtaJ1ow/lLfFfVYBRgL+9YlvaHOwJU=\n github.com/k0kubun/colorstring v0.0.0-20150214042306-9440f1994b88/go.mod h1:3w7q1U84EfirKl04SVQ/s7nPm1ZPhiXd34z40TNz36k=\n-github.com/klauspost/compress v1.4.0/go.mod h1:RyIbtBH6LamlWaDj8nUwkbUhJ87Yi3uG0guNDohfE1A=\n-github.com/klauspost/cpuid v0.0.0-20180405133222-e7e905edc00e/go.mod h1:Pj4uuM528wm8OyEC2QMXAi2YiTZ96dNQPGgoMS4s3ek=\n+github.com/karrick/godirwalk v1.8.0/go.mod h1:H5KPZjojv4lE+QYImBI8xVtrBRgYrIVsaRPx4tDPEn4=\n+github.com/karrick/godirwalk v1.10.3/go.mod h1:RoGL9dQei4vP9ilrpETWE8CLOZ1kiN0LhBygSwrAsHA=\n+github.com/kisielk/errcheck v1.2.0/go.mod h1:/BMXB+zMLi60iA8Vv6Ksmxu/1UDYcXs4uQLJ+jE2L00=\n+github.com/klauspost/compress v1.8.2/go.mod h1:RyIbtBH6LamlWaDj8nUwkbUhJ87Yi3uG0guNDohfE1A=\n+github.com/klauspost/compress v1.9.5/go.mod h1:RyIbtBH6LamlWaDj8nUwkbUhJ87Yi3uG0guNDohfE1A=\n+github.com/klauspost/cpuid v1.2.1/go.mod h1:Pj4uuM528wm8OyEC2QMXAi2YiTZ96dNQPGgoMS4s3ek=\n+github.com/konsorten/go-windows-terminal-sequences v1.0.1/go.mod h1:T0+1ngSBFLxvqU3pZ+m/2kptfBszLMUkC4ZK/EgS/cQ=\n+github.com/konsorten/go-windows-terminal-sequences v1.0.2/go.mod h1:T0+1ngSBFLxvqU3pZ+m/2kptfBszLMUkC4ZK/EgS/cQ=\n github.com/kr/pretty v0.1.0 h1:L/CwN0zerZDmRFUapSPitk6f+Q3+0za1rQkzVuMiMFI=\n github.com/kr/pretty v0.1.0/go.mod h1:dAy3ld7l9f0ibDNOQOHHMYYIIbhfbHSm3C4ZsoJORNo=\n github.com/kr/pty v1.1.1/go.mod h1:pFQYn66WHrOpPYNljwOMqo10TkYh1fy3cYio2l3bCsQ=\n github.com/kr/text v0.1.0 h1:45sCR5RtlFHMR4UwH9sdQ5TC8v0qDQCHnXt+kaKSTVE=\n github.com/kr/text v0.1.0/go.mod h1:4Jbv+DJW3UT/LiOwJeYQe1efqtUx/iVham/4vfdArNI=\n github.com/kyokomi/emoji v2.2.1+incompatible/go.mod h1:mZ6aGCD7yk8j6QY6KICwnZ2pxoszVseX1DNoGtU2tBA=\n+github.com/markbates/oncer v0.0.0-20181203154359-bf2de49a0be2/go.mod h1:Ld9puTsIW75CHf65OeIOkyKbteujpZVXDpWK6YGZbxE=\n+github.com/markbates/safe v1.0.1/go.mod h1:nAqgmRi7cY2nqMc92/bSEeQA+R4OheNU2T1kNSCBdG0=\n github.com/mattn/go-colorable v0.0.9/go.mod h1:9vuHe8Xs5qXnSaW/c/ABM9alt+Vo+STaOChaDxuIBZU=\n-github.com/mattn/go-isatty v0.0.4 h1:bnP0vzxcAdeI1zdubAl5PjU6zsERjGZb7raWodagDYs=\n+github.com/mattn/go-colorable v0.1.4/go.mod h1:U0ppj6V5qS13XJ6of8GYAs25YV2eR4EVcfRqFIhoBtE=\n github.com/mattn/go-isatty v0.0.4/go.mod h1:M+lRXTBqGeGNdLjl/ufCoiOlB5xdOkqRJdNxMWT7Zi4=\n+github.com/mattn/go-isatty v0.0.8 h1:HLtExJ+uU2HOZ+wI0Tt5DtUDrx8yhUqDcp7fYERX4CE=\n+github.com/mattn/go-isatty v0.0.8/go.mod h1:Iq45c/XA43vh69/j3iqttzPXn0bhXyGjM0Hdxcsrc5s=\n github.com/microcosm-cc/bluemonday v1.0.2 h1:5lPfLTTAvAbtS0VqT+94yOtFnGfUWYyx0+iToC3Os3s=\n github.com/microcosm-cc/bluemonday v1.0.2/go.mod h1:iVP4YcDBq+n/5fb23BhYFvIMq/leAFZyRl6bYmGDlGc=\n github.com/mitchellh/mapstructure v1.1.2/go.mod h1:FVVH3fgwuzCH5S8UJGiWEs2h04kUh9fWfEaFds41c1Y=\n+github.com/montanaflynn/stats v0.0.0-20171201202039-1bf9dbcd8cbe/go.mod h1:wL8QJuTMNUDYhXwkmfOly8iTdp5TEcJFWZD2D7SIkUc=\n github.com/moul/http2curl v1.0.0/go.mod h1:8UbvGypXm98wA/IqH45anm5Y2Z6ep6O31QGOAZ3H0fQ=\n github.com/nullrocks/identicon v0.0.0-20180626043057-7875f45b0022/go.mod h1:x4NsS+uc7ecH/Cbm9xKQ6XzmJM57rWTkjywjfB2yQ18=\n github.com/onsi/ginkgo v1.6.0/go.mod h1:lLunBs/Ym6LB5Z9jYTR76FiuTmxDTDusOGeTQH+WWjE=\n-github.com/onsi/ginkgo v1.7.0/go.mod h1:lLunBs/Ym6LB5Z9jYTR76FiuTmxDTDusOGeTQH+WWjE=\n-github.com/onsi/ginkgo v1.8.0/go.mod h1:lLunBs/Ym6LB5Z9jYTR76FiuTmxDTDusOGeTQH+WWjE=\n github.com/onsi/ginkgo v1.10.1/go.mod h1:lLunBs/Ym6LB5Z9jYTR76FiuTmxDTDusOGeTQH+WWjE=\n-github.com/onsi/gomega v1.4.3/go.mod h1:ex+gbHU/CVuBBDIJjb2X0qEXbFg53c61hWP/1CpauHY=\n-github.com/onsi/gomega v1.5.0/go.mod h1:ex+gbHU/CVuBBDIJjb2X0qEXbFg53c61hWP/1CpauHY=\n+github.com/onsi/ginkgo v1.10.2/go.mod h1:lLunBs/Ym6LB5Z9jYTR76FiuTmxDTDusOGeTQH+WWjE=\n github.com/onsi/gomega v1.7.0/go.mod h1:ex+gbHU/CVuBBDIJjb2X0qEXbFg53c61hWP/1CpauHY=\n-github.com/patrickmn/go-cache v2.1.0+incompatible h1:HRMgzkcYKYpi3C8ajMPV8OFXaaRUnok+kx1WdO15EQc=\n-github.com/patrickmn/go-cache v2.1.0+incompatible/go.mod h1:3Qf8kWWT7OJRJbdiICTKqZju1ZixQ/KpMGzzAfe6+WQ=\n+github.com/pelletier/go-toml v1.4.0/go.mod h1:PN7xzY2wHTK0K9p34ErDQMlFxa51Fk0OUruD3k1mMwo=\n github.com/pkg/errors v0.8.0/go.mod h1:bwawxfHBFNV+L2hUp1rHADufV3IMtnDRdf1r5NINEl0=\n-github.com/pkg/errors v0.8.1 h1:iURUrRGxPUNPdy5/HRSm+Yj6okJ6UtLINN0Q9M4+h3I=\n github.com/pkg/errors v0.8.1/go.mod h1:bwawxfHBFNV+L2hUp1rHADufV3IMtnDRdf1r5NINEl0=\n github.com/pkg/errors v0.9.1 h1:FEBLx1zS214owpjy7qsBeixbURkuhQAwrK5UwLGTwt4=\n github.com/pkg/errors v0.9.1/go.mod h1:bwawxfHBFNV+L2hUp1rHADufV3IMtnDRdf1r5NINEl0=\n github.com/pmezard/go-difflib v1.0.0 h1:4DBwDE0NGyQoBHbLQYPwSUPoCMWR5BEzIk/f1lZbAQM=\n github.com/pmezard/go-difflib v1.0.0/go.mod h1:iKH77koFhYxTK1pcRnkKkqfTogsbg7gZNVY4sRDYZ/4=\n github.com/rakyll/statik v0.1.7/go.mod h1:AlZONWzMtEnMs7W4e/1LURLiI49pIMmp6V9Unghqrcc=\n+github.com/rogpeppe/go-internal v1.1.0/go.mod h1:M8bDsm7K2OlrFYOpmOWEs/qY81heoFRclV5y23lUDJ4=\n+github.com/rogpeppe/go-internal v1.2.2/go.mod h1:M8bDsm7K2OlrFYOpmOWEs/qY81heoFRclV5y23lUDJ4=\n+github.com/rogpeppe/go-internal v1.3.0/go.mod h1:M8bDsm7K2OlrFYOpmOWEs/qY81heoFRclV5y23lUDJ4=\n github.com/rs/xid v1.2.1 h1:mhH9Nq+C1fY2l1XIpgxIiUOfNpRBYH1kKcr+qfKgjRc=\n github.com/rs/xid v1.2.1/go.mod h1:+uKXf+4Djp6Md1KODXJxgGQPKngRmWyn10oCKFzNHOQ=\n github.com/russross/blackfriday/v2 v2.0.1 h1:lPqVAte+HuHNfhJ/0LC98ESWRz8afy9tM/0RK8m9o+Q=\n@@ -145,139 +155,111 @@ github.com/sergi/go-diff v1.0.0 h1:Kpca3qRNrduNnOQeazBd0ysaKrUJiIuISHxogkT9RPQ=\n github.com/sergi/go-diff v1.0.0/go.mod h1:0CfEIISq7TuYL3j771MWULgwwjU+GofnZX9QAmXWZgo=\n github.com/shurcooL/sanitized_anchor_name v1.0.0 h1:PdmoCO6wvbs+7yrJyMORt4/BmY5IYyJwS/kOiWx8mHo=\n github.com/shurcooL/sanitized_anchor_name v1.0.0/go.mod h1:1NzhyTcUVG4SuEtjjoZeVRXNmyL/1OwPU0+IJeTBvfc=\n+github.com/sirupsen/logrus v1.4.0/go.mod h1:LxeOpSwHxABJmUn/MG1IvRgCAasNZTLOkJPxbbu5VWo=\n+github.com/sirupsen/logrus v1.4.1/go.mod h1:ni0Sbl8bgC9z8RoU9G6nDWqqs/fq4eDPysMBDgk/93Q=\n+github.com/sirupsen/logrus v1.4.2/go.mod h1:tLMulIdttU9McNUspp0xgXVQah82FyeX6MwdIuYE2rE=\n github.com/smartystreets/assertions v0.0.0-20180927180507-b2de0cb4f26d/go.mod h1:OnSkiWE9lh6wB0YB77sQom3nweQdgAjqCqsofrRNTgc=\n-github.com/smartystreets/goconvey v0.0.0-20181108003508-044398e4856c/go.mod h1:XDJAKZRPZ1CvBcN2aX5YOUTYGHki24fSF0Iv48Ibg0s=\n-github.com/stretchr/objx v0.1.0 h1:4G4v2dO3VZwixGIRoQ5Lfboy6nUhCyYzaqnIAPPhYs4=\n+github.com/smartystreets/goconvey v1.6.4/go.mod h1:syvi0/a8iFYH4r/RixwvyeAJjdLS9QV7WQ/tjFTllLA=\n+github.com/spf13/cobra v0.0.3/go.mod h1:1l0Ry5zgKvJasoi3XT1TypsSe7PqH0Sj9dhYf7v3XqQ=\n+github.com/spf13/pflag v1.0.3/go.mod h1:DYY7MBk1bdzusC3SYhjObp+wFpr4gzcvqqNjLnInEg4=\n github.com/stretchr/objx v0.1.0/go.mod h1:HFkY916IF+rwdDfMAkV7OtwuqBVzrE8GR6GFx+wExME=\n+github.com/stretchr/objx v0.1.1 h1:2vfRuCMp5sSVIDSqO8oNnWJq7mPa6KVP3iPIwFBuy8A=\n+github.com/stretchr/objx v0.1.1/go.mod h1:HFkY916IF+rwdDfMAkV7OtwuqBVzrE8GR6GFx+wExME=\n github.com/stretchr/testify v1.2.2/go.mod h1:a8OnRcib4nhh0OaRAV+Yts87kKdq0PP7pXfy6kDkUVs=\n-github.com/stretchr/testify v1.3.0 h1:TivCn/peBQ7UY8ooIcPgZFpTNSz0Q2U6UrFlUfqbe0Q=\n github.com/stretchr/testify v1.3.0/go.mod h1:M5WIy9Dh21IEIfnGCwXGc5bZfKNJtfHm1UVUgZn+9EI=\n+github.com/stretchr/testify v1.4.0/go.mod h1:j7eGeouHqKxXV5pUuKE4zz7dFj8WfuZ+81PSLYec5m4=\n github.com/stretchr/testify v1.5.1 h1:nOGnQDM7FYENwehXlg/kFVnos3rEvtKTjRvOWSzb6H4=\n github.com/stretchr/testify v1.5.1/go.mod h1:5W2xD1RspED5o8YsWQXVCued0rvSQ+mT+I5cxcmMvtA=\n github.com/tidwall/btree v0.0.0-20170113224114-9876f1454cf0/go.mod h1:huei1BkDWJ3/sLXmO+bsCNELL+Bp2Kks9OLyQFkzvA8=\n-github.com/tidwall/buntdb v1.0.0/go.mod h1:Y39xhcDW10WlyYXeLgGftXVbjtM0QP+/kpz8xl9cbzE=\n-github.com/tidwall/gjson v1.1.3/go.mod h1:c/nTNbUr0E0OrXEhq1pwa8iEgc2DOt4ZZqAt1HtCkPA=\n+github.com/tidwall/buntdb v1.1.0/go.mod h1:Y39xhcDW10WlyYXeLgGftXVbjtM0QP+/kpz8xl9cbzE=\n+github.com/tidwall/gjson v1.3.2/go.mod h1:P256ACg0Mn+j1RXIDXoss50DeIABTYK1PULOJHhxOls=\n github.com/tidwall/grect v0.0.0-20161006141115-ba9a043346eb/go.mod h1:lKYYLFIr9OIgdgrtgkZ9zgRxRdvPYsExnYBsEAd8W5M=\n github.com/tidwall/match v1.0.1/go.mod h1:LujAq0jyVjBy028G1WhWfIzbpQfMO8bBZ6Tyb0+pL9E=\n github.com/tidwall/pretty v1.0.0/go.mod h1:XNkn88O1ChpSDQmQeStsy+sBenx6DDtFZJxhVysOjyk=\n github.com/tidwall/rtree v0.0.0-20180113144539-6cd427091e0e/go.mod h1:/h+UnNGt0IhNNJLkGikcdcJqm66zGD/uJGMRxK/9+Ao=\n github.com/tidwall/tinyqueue v0.0.0-20180302190814-1e39f5511563/go.mod h1:mLqSmt7Dv/CNneF2wfcChfN1rvapyQr01LGKnKex0DQ=\n github.com/umputun/go-flags v1.5.1 h1:vRauoXV3Ultt1HrxivSxowbintgZLJE+EcBy5ta3/mY=\n github.com/umputun/go-flags v1.5.1/go.mod h1:nTbvsO/hKqe7Utri/NoyN18GR3+EWf+9RrmsdwdhrEc=\n-github.com/umputun/remark42 v1.6.0 h1:MjonGcjKAYeMfWGhYhVDa8mol0rRg1IrhYmDoOTRhFI=\n github.com/valyala/bytebufferpool v1.0.0/go.mod h1:6bBcMArwyJ5K/AmCkWv1jt77kVWyCJ6HpOuEn7z0Csc=\n-github.com/valyala/fasthttp v1.0.0/go.mod h1:4vX61m6KN+xDduDNwXrhIAVZaZaZiQ1luJk8LWSxF3s=\n+github.com/valyala/fasthttp v1.6.0/go.mod h1:FstJa9V+Pj9vQ7OJie2qMHdwemEDaDiSdBnvPM1Su9w=\n github.com/valyala/tcplisten v0.0.0-20161114210144-ceec8f93295a/go.mod h1:v3UYOV9WzVtRmSR+PDvWpU/qWl4Wa5LApYYX4ZtKbio=\n github.com/xdg/scram v0.0.0-20180814205039-7eeb5667e42c/go.mod h1:lB8K/P019DLNhemzwFU4jHLhdvlE6uDZjXFejJXr49I=\n-github.com/xdg/stringprep v1.0.0/go.mod h1:Jhud4/sHMO4oL310DaZAKk9ZaJ08SJfe+sJh0HrGL1Y=\n+github.com/xdg/stringprep v0.0.0-20180714160509-73f8eece6fdc/go.mod h1:Jhud4/sHMO4oL310DaZAKk9ZaJ08SJfe+sJh0HrGL1Y=\n github.com/xeipuuv/gojsonpointer v0.0.0-20180127040702-4e3ac2762d5f/go.mod h1:N2zxlSyiKSe5eX1tZViRH5QA0qijqEDrYZiPEAiq3wU=\n github.com/xeipuuv/gojsonreference v0.0.0-20180127040603-bd5ef7bd5415/go.mod h1:GwrjFmJcFw6At/Gs6z4yjiIwzuJ1/+UwLxMQDVQXShQ=\n-github.com/xeipuuv/gojsonschema v0.0.0-20181112162635-ac52e6811b56/go.mod h1:5yf86TLmAcydyeJq5YvxkGPE2fm/u4myDekKRoLuqhs=\n+github.com/xeipuuv/gojsonschema v1.2.0/go.mod h1:anYRn/JVcOK2ZgGU+IjEV4nwlhoK5sQluxsYJ78Id3Y=\n github.com/yalp/jsonpath v0.0.0-20180802001716-5cc68e5049a0/go.mod h1:/LWChgwKmvncFJFHJ7Gvn9wZArjbV5/FppcK2fKk/tI=\n github.com/yudai/gojsondiff v1.0.0/go.mod h1:AY32+k2cwILAkW1fbgxQ5mUmMiZFgLIV+FBNExI05xg=\n github.com/yudai/golcs v0.0.0-20170316035057-ecda9a501e82/go.mod h1:lgjkn3NuSvDfVJdfcVVdX+jpBxNmX4rDAzaS45IcYoM=\n github.com/yudai/pp v2.0.1+incompatible/go.mod h1:PuxR/8QJ7cyCkFp/aUDS+JY727OFEZkTdatxwunjIkc=\n-github.com/yuin/gopher-lua v0.0.0-20190514113301-1cd887cd7036/go.mod h1:gqRgreBUhTSL0GeU64rtZ3Uq3wtjOa/TB2YfrtkCbVQ=\n github.com/yuin/gopher-lua v0.0.0-20191220021717-ab39c6098bdb/go.mod h1:gqRgreBUhTSL0GeU64rtZ3Uq3wtjOa/TB2YfrtkCbVQ=\n go.etcd.io/bbolt v1.3.4 h1:hi1bXHMVrlQh6WwxAy+qZCV/SYIlqo+Ushwdpa4tAKg=\n go.etcd.io/bbolt v1.3.4/go.mod h1:G5EMThwa9y8QZGBClrRx5EY+Yw9kAhnjy3bSjsnlVTQ=\n-go.mongodb.org/mongo-driver v1.1.1/go.mod h1:u7ryQJ+DOzQmeO7zB6MHyr8jkEQvC8vH7qLUO4lqsUM=\n-go.opencensus.io v0.21.0/go.mod h1:mSImk1erAIZhrmZN+AvHh14ztQfjbGwt4TtuofqLduU=\n+go.mongodb.org/mongo-driver v1.3.2/go.mod h1:MSWZXKOynuguX+JSvwP8i+58jYCXxbia8HS3gZBapIE=\n+golang.org/x/crypto v0.0.0-20180904163835-0709b304e793/go.mod h1:6SG95UA2DQfeDnfUPMdvaQW0Q7yPrPDi9nlGo2tz2b4=\n golang.org/x/crypto v0.0.0-20190308221718-c2843e01d9a2/go.mod h1:djNgcEr1/C05ACkg1iLfiJU5Ep61QUkGW8qpdssI0+w=\n-golang.org/x/crypto v0.0.0-20190605123033-f99c8df09eb5/go.mod h1:yigFU9vqHzYiE8UmvKecakEJjdnWj3jj499lnFckfCI=\n+golang.org/x/crypto v0.0.0-20190422162423-af44ce270edf/go.mod h1:WFFai1msRO1wXaEeE5yQxYXgSfI8pQAWXbQop6sCtWE=\n+golang.org/x/crypto v0.0.0-20190530122614-20be4c3c3ed5/go.mod h1:yigFU9vqHzYiE8UmvKecakEJjdnWj3jj499lnFckfCI=\n golang.org/x/crypto v0.0.0-20200406173513-056763e48d71/go.mod h1:LzIPMQfyMNhhGPhUkYOs5KpL4U8rLKemX1yGLhDgUto=\n-golang.org/x/exp v0.0.0-20190121172915-509febef88a4/go.mod h1:CJ0aWSM057203Lf6IL+f9T1iT9GByDxfZKAQTCR3kQA=\n-golang.org/x/image v0.0.0-20190523035834-f03afa92d3ff/go.mod h1:kZ7UVZpmo3dzQBMxlp+ypCbDeSB+sBbTgSJuh5dn5js=\n golang.org/x/image v0.0.0-20200119044424-58c23975cae1 h1:5h3ngYt7+vXCDZCup/HkCQgW5XwmSvR/nA2JmJ0RErg=\n golang.org/x/image v0.0.0-20200119044424-58c23975cae1/go.mod h1:FeLwcggjj3mMvU+oOTbSwawSJRM1uh48EjtB4UJZlP0=\n-golang.org/x/lint v0.0.0-20181026193005-c67002cb31c3/go.mod h1:UVdnD1Gm6xHRNCYTkRU2/jEulfH38KcIWyp/GAMgvoE=\n-golang.org/x/lint v0.0.0-20190227174305-5b3e6a55c961/go.mod h1:wehouNa3lNwaWXcvxsM5YxQ5yQlVC4a0KAMCusXpPoU=\n-golang.org/x/lint v0.0.0-20190301231843-5614ed5bae6f/go.mod h1:UVdnD1Gm6xHRNCYTkRU2/jEulfH38KcIWyp/GAMgvoE=\n-golang.org/x/lint v0.0.0-20190313153728-d0100b6bd8b3/go.mod h1:6SW0HCj/g11FgYtHlgUYUwCkIfeOF89ocIRzGO/8vkc=\n-golang.org/x/lint v0.0.0-20190409202823-959b441ac422/go.mod h1:6SW0HCj/g11FgYtHlgUYUwCkIfeOF89ocIRzGO/8vkc=\n golang.org/x/net v0.0.0-20180218175443-cbe0f9307d01/go.mod h1:mL1N/T3taQHkDXs73rZJwtUhF3w3ftmwwsq0BUmARs4=\n golang.org/x/net v0.0.0-20180724234803-3673e40ba225/go.mod h1:mL1N/T3taQHkDXs73rZJwtUhF3w3ftmwwsq0BUmARs4=\n-golang.org/x/net v0.0.0-20180826012351-8a410e7b638d/go.mod h1:mL1N/T3taQHkDXs73rZJwtUhF3w3ftmwwsq0BUmARs4=\n golang.org/x/net v0.0.0-20180906233101-161cd47e91fd/go.mod h1:mL1N/T3taQHkDXs73rZJwtUhF3w3ftmwwsq0BUmARs4=\n-golang.org/x/net v0.0.0-20180911220305-26e67e76b6c3/go.mod h1:mL1N/T3taQHkDXs73rZJwtUhF3w3ftmwwsq0BUmARs4=\n-golang.org/x/net v0.0.0-20181217023233-e147a9138326/go.mod h1:mL1N/T3taQHkDXs73rZJwtUhF3w3ftmwwsq0BUmARs4=\n golang.org/x/net v0.0.0-20181220203305-927f97764cc3/go.mod h1:mL1N/T3taQHkDXs73rZJwtUhF3w3ftmwwsq0BUmARs4=\n golang.org/x/net v0.0.0-20190108225652-1e06a53dbb7e/go.mod h1:mL1N/T3taQHkDXs73rZJwtUhF3w3ftmwwsq0BUmARs4=\n-golang.org/x/net v0.0.0-20190213061140-3a22650c66bd/go.mod h1:mL1N/T3taQHkDXs73rZJwtUhF3w3ftmwwsq0BUmARs4=\n golang.org/x/net v0.0.0-20190311183353-d8887717615a/go.mod h1:t9HGtf8HONx5eT2rtn7q6eTqICYqUVnKs3thJo3Qplg=\n golang.org/x/net v0.0.0-20190404232315-eb5bcb51f2a3/go.mod h1:t9HGtf8HONx5eT2rtn7q6eTqICYqUVnKs3thJo3Qplg=\n-golang.org/x/net v0.0.0-20190503192946-f4e77d36d62c/go.mod h1:t9HGtf8HONx5eT2rtn7q6eTqICYqUVnKs3thJo3Qplg=\n-golang.org/x/net v0.0.0-20190603091049-60506f45cf65/go.mod h1:HSz+uSET+XFnRR8LxR5pz3Of3rY3CfYBVs4xY44aLks=\n-golang.org/x/net v0.0.0-20190611141213-3f473d35a33a/go.mod h1:z5CRVTTTmAJ677TzLLGU+0bjPO0LkuOLi4/5GtJWs/s=\n-golang.org/x/net v0.0.0-20190620200207-3b0461eec859/go.mod h1:z5CRVTTTmAJ677TzLLGU+0bjPO0LkuOLi4/5GtJWs/s=\n-golang.org/x/net v0.0.0-20190724013045-ca1201d0de80 h1:Ao/3l156eZf2AW5wK8a7/smtodRU+gha3+BeqJ69lRk=\n-golang.org/x/net v0.0.0-20190724013045-ca1201d0de80/go.mod h1:z5CRVTTTmAJ677TzLLGU+0bjPO0LkuOLi4/5GtJWs/s=\n+golang.org/x/net v0.0.0-20190827160401-ba9fcec4b297/go.mod h1:z5CRVTTTmAJ677TzLLGU+0bjPO0LkuOLi4/5GtJWs/s=\n golang.org/x/net v0.0.0-20190923162816-aa69164e4478/go.mod h1:z5CRVTTTmAJ677TzLLGU+0bjPO0LkuOLi4/5GtJWs/s=\n-golang.org/x/net v0.0.0-20200202094626-16171245cfb2 h1:CCH4IOTTfewWjGOlSp+zGcjutRKlBEZQ6wTn8ozI/nI=\n golang.org/x/net v0.0.0-20200202094626-16171245cfb2/go.mod h1:z5CRVTTTmAJ677TzLLGU+0bjPO0LkuOLi4/5GtJWs/s=\n-golang.org/x/oauth2 v0.0.0-20180821212333-d2e6202438be/go.mod h1:N/0e6XlmueqKjAGxoOufVs8QHGRruUQn6yWY3a++T0U=\n-golang.org/x/oauth2 v0.0.0-20190226205417-e64efc72b421/go.mod h1:gOpvHmFTYa4IltrdGE7lF6nIHvwfUNPOp7c8zoXwtLw=\n+golang.org/x/net v0.0.0-20200520182314-0ba52f642ac2 h1:eDrdRpKgkcCqKZQwyZRyeFZgfqt37SL7Kv3tok06cKE=\n+golang.org/x/net v0.0.0-20200520182314-0ba52f642ac2/go.mod h1:qpuaurCH72eLCgpAm/N6yyVIVM9cpaDIP3A8BGJEC5A=\n golang.org/x/oauth2 v0.0.0-20190604053449-0f29369cfe45/go.mod h1:gOpvHmFTYa4IltrdGE7lF6nIHvwfUNPOp7c8zoXwtLw=\n+golang.org/x/oauth2 v0.0.0-20200107190931-bf48bf16ab8d/go.mod h1:gOpvHmFTYa4IltrdGE7lF6nIHvwfUNPOp7c8zoXwtLw=\n golang.org/x/sync v0.0.0-20180314180146-1d60e4601c6f/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=\n-golang.org/x/sync v0.0.0-20181108010431-42b317875d0f/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=\n golang.org/x/sync v0.0.0-20181221193216-37e7f081c4d4/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=\n golang.org/x/sync v0.0.0-20190227155943-e225da77a7e6/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=\n+golang.org/x/sync v0.0.0-20190412183630-56d357773e84/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=\n golang.org/x/sync v0.0.0-20190423024810-112230192c58/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=\n-golang.org/x/sys v0.0.0-20180830151530-49385e6e1522/go.mod h1:STP8DvDyc/dI5b8T5hshtkjS+E42TnysNCUPdjciGhY=\n+golang.org/x/sys v0.0.0-20180905080454-ebe1bf3edb33/go.mod h1:STP8DvDyc/dI5b8T5hshtkjS+E42TnysNCUPdjciGhY=\n golang.org/x/sys v0.0.0-20180909124046-d0be0721c37e/go.mod h1:STP8DvDyc/dI5b8T5hshtkjS+E42TnysNCUPdjciGhY=\n golang.org/x/sys v0.0.0-20181107165924-66b7b1311ac8/go.mod h1:STP8DvDyc/dI5b8T5hshtkjS+E42TnysNCUPdjciGhY=\n golang.org/x/sys v0.0.0-20181128092732-4ed8d59d0b35/go.mod h1:STP8DvDyc/dI5b8T5hshtkjS+E42TnysNCUPdjciGhY=\n golang.org/x/sys v0.0.0-20190204203706-41f3e6584952/go.mod h1:STP8DvDyc/dI5b8T5hshtkjS+E42TnysNCUPdjciGhY=\n golang.org/x/sys v0.0.0-20190215142949-d0b11bdaac8a/go.mod h1:STP8DvDyc/dI5b8T5hshtkjS+E42TnysNCUPdjciGhY=\n+golang.org/x/sys v0.0.0-20190222072716-a9d3bda3a223/go.mod h1:STP8DvDyc/dI5b8T5hshtkjS+E42TnysNCUPdjciGhY=\n+golang.org/x/sys v0.0.0-20190403152447-81d4e9dc473e/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\n golang.org/x/sys v0.0.0-20190412213103-97732733099d/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\n-golang.org/x/sys v0.0.0-20190507160741-ecd444e8653b/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\n-golang.org/x/sys v0.0.0-20190606165138-5da285871e9c/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\n-golang.org/x/sys v0.0.0-20190624142023-c5567b49c5d0/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\n+golang.org/x/sys v0.0.0-20190419153524-e8e3143a4f4a/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\n+golang.org/x/sys v0.0.0-20190422165155-953cdadca894/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\n+golang.org/x/sys v0.0.0-20190531175056-4c3a928424d2/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\n golang.org/x/sys v0.0.0-20191010194322-b09406accb47/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\n-golang.org/x/sys v0.0.0-20200202164722-d101bd2416d5 h1:LfCXLvNmTYH9kEmVgqbnsWfruoXZIrh4YBgqVHtDvw0=\n golang.org/x/sys v0.0.0-20200202164722-d101bd2416d5/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\n-golang.org/x/text v0.3.0 h1:g61tztE5qeGQ89tm6NTjjM9VPIm088od1l6aSorWRWg=\n+golang.org/x/sys v0.0.0-20200323222414-85ca7c5b95cd h1:xhmwyvizuTgC2qz7ZlMluP20uW+C3Rm0FD/WLDX8884=\n+golang.org/x/sys v0.0.0-20200323222414-85ca7c5b95cd/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\n golang.org/x/text v0.3.0/go.mod h1:NqM8EUOU14njkJ3fqMW+pc6Ldnwhi/IjpwHt7yyuwOQ=\n-golang.org/x/text v0.3.1-0.20180807135948-17ff2d5776d2/go.mod h1:NqM8EUOU14njkJ3fqMW+pc6Ldnwhi/IjpwHt7yyuwOQ=\n golang.org/x/text v0.3.2 h1:tW2bmiBqwgJj/UpqtC8EpXEZVYOwU0yG4iWbprSVAcs=\n golang.org/x/text v0.3.2/go.mod h1:bEr9sfX3Q8Zfm5fL9x+3itogRgK3+ptLWKqgva+5dAk=\n-golang.org/x/time v0.0.0-20181108054448-85acf8d2951c/go.mod h1:tRJNPiyCQ0inRvYxbN9jk5I+vvW/OXSQhTDSoE431IQ=\n-golang.org/x/time v0.0.0-20190308202827-9d24e82272b4 h1:SvFZT6jyqRaOeXpc5h/JSfZenJ2O330aBsf7JfSUXmQ=\n-golang.org/x/time v0.0.0-20190308202827-9d24e82272b4/go.mod h1:tRJNPiyCQ0inRvYxbN9jk5I+vvW/OXSQhTDSoE431IQ=\n+golang.org/x/time v0.0.0-20200416051211-89c76fbcd5d1 h1:NusfzzA6yGQ+ua51ck7E3omNUX/JuqbFSaRGqU8CcLI=\n+golang.org/x/time v0.0.0-20200416051211-89c76fbcd5d1/go.mod h1:tRJNPiyCQ0inRvYxbN9jk5I+vvW/OXSQhTDSoE431IQ=\n golang.org/x/tools v0.0.0-20180917221912-90fa682c2a6e/go.mod h1:n7NCudcB/nEzxVGmLbDWY5pfWTLqBcC2KZ6jyYvM4mQ=\n-golang.org/x/tools v0.0.0-20190114222345-bf090417da8b/go.mod h1:n7NCudcB/nEzxVGmLbDWY5pfWTLqBcC2KZ6jyYvM4mQ=\n-golang.org/x/tools v0.0.0-20190226205152-f727befe758c/go.mod h1:9Yl7xja0Znq3iFh3HoIrodX9oNMXvdceNzlUR8zjMvY=\n-golang.org/x/tools v0.0.0-20190311212946-11955173bddd/go.mod h1:LCzVGOaR6xXOjkQ3onu1FJEFr0SW1gC7cKk1uF8kGRs=\n-golang.org/x/tools v0.0.0-20190312170243-e65039ee4138/go.mod h1:LCzVGOaR6xXOjkQ3onu1FJEFr0SW1gC7cKk1uF8kGRs=\n-golang.org/x/tools v0.0.0-20190506145303-2d16b83fe98c/go.mod h1:RgjU9mgBXZiqYHBnxXauZ1Gv1EHHAz9KjViQ78xBX0Q=\n-golang.org/x/tools v0.0.0-20190606124116-d0a3d012864b/go.mod h1:/rFqwRUd4F7ZHNgwSSTFct+R/Kf4OFW1sUzUTQQTgfc=\n-google.golang.org/api v0.4.0/go.mod h1:8k5glujaEP+g9n7WNsDg8QP6cUVNI86fCNMcbazEtwE=\n-google.golang.org/api v0.6.0/go.mod h1:btoxGiFvQNVUZQ8W08zLtrVS08CNpINPEfxXxgJL1Q4=\n-google.golang.org/appengine v1.1.0/go.mod h1:EbEs0AVv82hx2wNQdGPgUI5lhzA/G0D9YwlJXL52JkM=\n+golang.org/x/tools v0.0.0-20181030221726-6c7e314b6563/go.mod h1:n7NCudcB/nEzxVGmLbDWY5pfWTLqBcC2KZ6jyYvM4mQ=\n+golang.org/x/tools v0.0.0-20190328211700-ab21143f2384/go.mod h1:LCzVGOaR6xXOjkQ3onu1FJEFr0SW1gC7cKk1uF8kGRs=\n+golang.org/x/tools v0.0.0-20190329151228-23e29df326fe/go.mod h1:LCzVGOaR6xXOjkQ3onu1FJEFr0SW1gC7cKk1uF8kGRs=\n+golang.org/x/tools v0.0.0-20190416151739-9c9e1878f421/go.mod h1:LCzVGOaR6xXOjkQ3onu1FJEFr0SW1gC7cKk1uF8kGRs=\n+golang.org/x/tools v0.0.0-20190420181800-aa740d480789/go.mod h1:LCzVGOaR6xXOjkQ3onu1FJEFr0SW1gC7cKk1uF8kGRs=\n+golang.org/x/tools v0.0.0-20190531172133-b3315ee88b7d/go.mod h1:/rFqwRUd4F7ZHNgwSSTFct+R/Kf4OFW1sUzUTQQTgfc=\n google.golang.org/appengine v1.4.0/go.mod h1:xpcJRLb0r/rnEns0DIKYYv+WjYCduHsrkT7/EB5XEv4=\n-google.golang.org/appengine v1.5.0/go.mod h1:xpcJRLb0r/rnEns0DIKYYv+WjYCduHsrkT7/EB5XEv4=\n-google.golang.org/appengine v1.6.1/go.mod h1:i06prIuMbXzDqacNJfV5OdTW448YApPu5ww/cMBSeb0=\n-google.golang.org/genproto v0.0.0-20180817151627-c66870c02cf8/go.mod h1:JiN7NxoALGmiZfu7CAH4rXhgtRTLTxftemlI0sWmxmc=\n-google.golang.org/genproto v0.0.0-20190307195333-5fe7a883aa19/go.mod h1:VzzqZJRnGkLBvHegQrXjBqPurQTc5/KpmUdxsrq26oE=\n-google.golang.org/genproto v0.0.0-20190418145605-e7d98fc518a7/go.mod h1:VzzqZJRnGkLBvHegQrXjBqPurQTc5/KpmUdxsrq26oE=\n-google.golang.org/genproto v0.0.0-20190502173448-54afdca5d873/go.mod h1:VzzqZJRnGkLBvHegQrXjBqPurQTc5/KpmUdxsrq26oE=\n-google.golang.org/genproto v0.0.0-20190530194941-fb225487d101/go.mod h1:z3L6/3dTEVtUr6QSP8miRzeRqwQOioJ9I66odjN4I7s=\n-google.golang.org/grpc v1.19.0/go.mod h1:mqu4LbDTu4XGKhr4mRzUsmM4RtVoemTSY81AxZiDr8c=\n-google.golang.org/grpc v1.20.1/go.mod h1:10oTOabMzJvdu6/UiuZezV6QK5dSlG84ov/aaiqXj38=\n-gopkg.in/check.v1 v0.0.0-20161208181325-20d25e280405 h1:yhCVgyC4o1eVCa2tZl7eS0r+SDo693bJlVdllGtEeKM=\n gopkg.in/check.v1 v0.0.0-20161208181325-20d25e280405/go.mod h1:Co6ibVJAznAaIkqp8huTwlJQCZ016jof/cbN4VW5Yz0=\n-gopkg.in/check.v1 v1.0.0-20180628173108-788fd7840127 h1:qIbj1fsPNlZgppZ+VLlY7N33q108Sa+fhmuc+sWQYwY=\n gopkg.in/check.v1 v1.0.0-20180628173108-788fd7840127/go.mod h1:Co6ibVJAznAaIkqp8huTwlJQCZ016jof/cbN4VW5Yz0=\n+gopkg.in/check.v1 v1.0.0-20190902080502-41f04d3bba15 h1:YR8cESwS4TdDjEe65xsg0ogRM/Nc3DYOhEAlW+xobZo=\n gopkg.in/check.v1 v1.0.0-20190902080502-41f04d3bba15/go.mod h1:Co6ibVJAznAaIkqp8huTwlJQCZ016jof/cbN4VW5Yz0=\n+gopkg.in/errgo.v2 v2.1.0/go.mod h1:hNsd1EY+bozCKY1Ytp96fpM3vjJbqLJn88ws8XvfDNI=\n gopkg.in/fsnotify.v1 v1.4.7/go.mod h1:Tz8NjZHkW78fSQdbUxIjBTcgA1z1m8ZHf0WmKUhAMys=\n-gopkg.in/oauth2.v3 v3.10.1/go.mod h1:nTG+m2PRcHR9jzGNrGdxSsUKz7vvwkqSlhFrstgZcRU=\n+gopkg.in/oauth2.v3 v3.12.0/go.mod h1:XEYgKqWX095YiPT+Aw5y3tCn+7/FMnlTFKrupgSiJ3I=\n gopkg.in/tomb.v1 v1.0.0-20141024135613-dd632973f1e7/go.mod h1:dt/ZhP58zS4L8KSrWDmTeBkI65Dw0HsyUHuEVlX15mw=\n gopkg.in/yaml.v2 v2.2.1/go.mod h1:hI93XBmqTisBFMUTm0b8Fm+jr3Dg1NNxqwp+5A1VGuI=\n-gopkg.in/yaml.v2 v2.2.2 h1:ZCJp+EgiOT7lHqUV2J862kp8Qj64Jo6az82+3Td9dZw=\n gopkg.in/yaml.v2 v2.2.2/go.mod h1:hI93XBmqTisBFMUTm0b8Fm+jr3Dg1NNxqwp+5A1VGuI=\n gopkg.in/yaml.v2 v2.2.4 h1:/eiJrUcujPVeJ3xlSWaiNi3uSVmDGBK1pDHUHAnao1I=\n gopkg.in/yaml.v2 v2.2.4/go.mod h1:hI93XBmqTisBFMUTm0b8Fm+jr3Dg1NNxqwp+5A1VGuI=\n-honnef.co/go/tools v0.0.0-20190102054323-c2f93a96b099/go.mod h1:rf3lG4BRIbNafJWhAfAdb/ePZxsR/4RtNHQocxwk9r4=\n-honnef.co/go/tools v0.0.0-20190106161140-3f1c8253044a/go.mod h1:rf3lG4BRIbNafJWhAfAdb/ePZxsR/4RtNHQocxwk9r4=\n-honnef.co/go/tools v0.0.0-20190418001031-e561f6794a2a/go.mod h1:rf3lG4BRIbNafJWhAfAdb/ePZxsR/4RtNHQocxwk9r4=\n-rsc.io/binaryregexp v0.2.0/go.mod h1:qTv7/COck+e2FymRvadv62gMdZztPaShugOCi3I+8D8="
    },
    {
      "sha": "53c6b405976aa9f8e04c66190491587ac2d0566c",
      "filename": "backend/app/rest/api/rest.go",
      "status": "modified",
      "additions": 1,
      "deletions": 1,
      "changes": 2,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/app/rest/api/rest.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/app/rest/api/rest.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/app/rest/api/rest.go?ref=890d7154e777d0673defcd11db3ef792473d68c9",
      "patch": "@@ -11,7 +11,7 @@ import (\n \t\"sync\"\n \t\"time\"\n \n-\t\"github.com/didip/tollbooth\"\n+\t\"github.com/didip/tollbooth/v6\"\n \t\"github.com/didip/tollbooth_chi\"\n \t\"github.com/go-chi/chi\"\n \t\"github.com/go-chi/chi/middleware\""
    },
    {
      "sha": "c1376e10fd505b4184c9d2d5cb0cfac653473664",
      "filename": "backend/app/rest/api/rest_public_test.go",
      "status": "modified",
      "additions": 0,
      "deletions": 1,
      "changes": 1,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/app/rest/api/rest_public_test.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/app/rest/api/rest_public_test.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/app/rest/api/rest_public_test.go?ref=890d7154e777d0673defcd11db3ef792473d68c9",
      "patch": "@@ -727,7 +727,6 @@ func TestRest_Robots(t *testing.T) {\n }\n \n func TestRest_LastCommentsStream(t *testing.T) {\n-\tt.Skip() // TODO: enable after cache is migrated to https://github.com/dgraph-io/ristretto\n \tts, srv, teardown := startupT(t)\n \tsrv.pubRest.readOnlyAge = 10000000 // make sure we don't hit read-only\n \tsrv.pubRest.streamer.Refresh = 50 * time.Millisecond"
    },
    {
      "sha": "267d9a6926802f894d0cd1d6a35330eef77e37ab",
      "filename": "backend/go.mod",
      "status": "modified",
      "additions": 7,
      "deletions": 7,
      "changes": 14,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/go.mod",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/go.mod",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/go.mod?ref=890d7154e777d0673defcd11db3ef792473d68c9",
      "patch": "@@ -7,14 +7,14 @@ require (\n \tgithub.com/PuerkitoBio/goquery v1.5.1\n \tgithub.com/alecthomas/chroma v0.7.2\n \tgithub.com/dgrijalva/jwt-go v3.2.0+incompatible\n-\tgithub.com/didip/tollbooth v4.0.2+incompatible\n-\tgithub.com/didip/tollbooth_chi v0.0.0-20170928041846-6ab5f3083f3d\n-\tgithub.com/go-chi/chi v4.1.0+incompatible\n+\tgithub.com/didip/tollbooth/v6 v6.0.1\n+\tgithub.com/didip/tollbooth_chi v0.0.0-20200524181329-8b84cd7183d9\n+\tgithub.com/go-chi/chi v4.1.1+incompatible\n \tgithub.com/go-chi/cors v1.1.1\n \tgithub.com/go-chi/render v1.0.1\n-\tgithub.com/go-pkgz/auth v0.10.1\n-\tgithub.com/go-pkgz/jrpc v0.1.0\n-\tgithub.com/go-pkgz/lcw v0.6.0\n+\tgithub.com/go-pkgz/auth v0.10.2\n+\tgithub.com/go-pkgz/jrpc v0.2.0\n+\tgithub.com/go-pkgz/lcw v0.6.1\n \tgithub.com/go-pkgz/lgr v0.7.0\n \tgithub.com/go-pkgz/repeater v1.1.3\n \tgithub.com/go-pkgz/rest v1.5.0\n@@ -33,5 +33,5 @@ require (\n \tgo.etcd.io/bbolt v1.3.4\n \tgolang.org/x/crypto v0.0.0-20200406173513-056763e48d71\n \tgolang.org/x/image v0.0.0-20200119044424-58c23975cae1\n-\tgolang.org/x/net v0.0.0-20200202094626-16171245cfb2\n+\tgolang.org/x/net v0.0.0-20200520182314-0ba52f642ac2\n )"
    },
    {
      "sha": "8d223cd97eee869646ddb78956c301779e202591",
      "filename": "backend/go.sum",
      "status": "modified",
      "additions": 127,
      "deletions": 125,
      "changes": 252,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/go.sum",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/go.sum",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/go.sum?ref=890d7154e777d0673defcd11db3ef792473d68c9",
      "patch": "@@ -1,15 +1,12 @@\n-cloud.google.com/go v0.26.0/go.mod h1:aQUYkXzVsufM+DwF1aE+0xfcU+56JwCaLick0ClmMTw=\n+cloud.google.com/go v0.34.0 h1:eOI3/cP2VTU6uZLDYAoic+eyzzB9YyGmJ7eIjl8rOPg=\n cloud.google.com/go v0.34.0/go.mod h1:aQUYkXzVsufM+DwF1aE+0xfcU+56JwCaLick0ClmMTw=\n-cloud.google.com/go v0.38.0/go.mod h1:990N+gfupTy94rShfmMCWGDn0LpTmnzTp2qbd1dvSRU=\n-cloud.google.com/go v0.40.0 h1:FjSY7bOj+WzJe6TZRVtXI2b9kAYvtNg4lMbcH2+MUkk=\n-cloud.google.com/go v0.40.0/go.mod h1:Tk58MuI9rbLMKlAjeO/bDnteAx7tX2gJIXw4T5Jwlro=\n github.com/BurntSushi/toml v0.3.1/go.mod h1:xHWCNGjB5oqiDr8zfno3MHue2Ht5sIBksp03qcyfWMU=\n github.com/Depado/bfchroma v1.2.0 h1:NyYPFVhWvq8S2ts6Ok4kwXVE3TEO5fof+9ZOKbBJQUo=\n github.com/Depado/bfchroma v1.2.0/go.mod h1:U3RJUYwWVJrZRaJQyfS+wuxBApSTR/BC37PhAI+Ydps=\n github.com/PuerkitoBio/goquery v1.5.1 h1:PSPBGne8NIUWw+/7vFBV+kG2J/5MOjbzc7154OaKCSE=\n github.com/PuerkitoBio/goquery v1.5.1/go.mod h1:GsLWisAFVj4WgDibEWF4pvYnkVQBpKBKeU+7zCJoLcc=\n-github.com/ajg/form v0.0.0-20160822230020-523a5da1a92f h1:zvClvFQwU++UpIUBGC8YmDlfhUrweEy1R1Fj1gu5iIM=\n-github.com/ajg/form v0.0.0-20160822230020-523a5da1a92f/go.mod h1:uL1WgH+h2mgNtvBq0339dVnzXdBETtL2LeUXaIv25UY=\n+github.com/ajg/form v1.5.1 h1:t9c7v8JUKu/XxOGBU0yjNpaMloxGEJhUkqFRq0ibGeU=\n+github.com/ajg/form v1.5.1/go.mod h1:uL1WgH+h2mgNtvBq0339dVnzXdBETtL2LeUXaIv25UY=\n github.com/alecthomas/assert v0.0.0-20170929043011-405dbfeb8e38 h1:smF2tmSOzy2Mm+0dGI2AIUHY+w0BUc+4tn40djz7+6U=\n github.com/alecthomas/assert v0.0.0-20170929043011-405dbfeb8e38/go.mod h1:r7bzyVFMNntcxPZXK3/+KdruV1H5KSlyVY0gc+NgInI=\n github.com/alecthomas/chroma v0.6.0/go.mod h1:MmozekIi2rfQSzDcdEZ2BoJ9Pxs/7uc2Y4Boh+hIeZo=\n@@ -30,7 +27,6 @@ github.com/andybalholm/cascadia v1.1.0/go.mod h1:GsXiBklL0woXo1j/WYWtSYYC4ouU9Pq\n github.com/chzyer/logex v1.1.10/go.mod h1:+Ywpsq7O8HXn0nuIou7OrIPyXbp3wmkHB+jjWRnGsAI=\n github.com/chzyer/readline v0.0.0-20180603132655-2972be24d48e/go.mod h1:nSuG5e5PlCu98SY8svDHJxuZscDgtXS6KTTbou5AhLI=\n github.com/chzyer/test v0.0.0-20180213035817-a1ea475d72b1/go.mod h1:Q3SI9o4m/ZMnBNeIyt5eFwwo7qiLfzFZmjNmxjkiQlU=\n-github.com/client9/misspell v0.3.4/go.mod h1:qj6jICC3Q7zFZvVWo7KLAzC3yx5G7kyvSDkc90ppPyw=\n github.com/danwakefield/fnmatch v0.0.0-20160403171240-cbb64ac3d964 h1:y5HC9v93H5EPKqaS1UYVg1uYah5Xf51mBfIoWehClUQ=\n github.com/danwakefield/fnmatch v0.0.0-20160403171240-cbb64ac3d964/go.mod h1:Xd9hchkHSWYkEqJwUGisez3G1QY8Ryz0sdWrLPMGjLk=\n github.com/davecgh/go-spew v1.1.0/go.mod h1:J7Y8YcW2NihsgmVo/mv3lAwl/skON4iLHjSsI+c5H38=\n@@ -40,38 +36,38 @@ github.com/dghubble/oauth1 v0.6.0 h1:m1yC01Ohc/eF38jwZ8JUjL1a+XHHXtGQgK+MxQbmSx0\n github.com/dghubble/oauth1 v0.6.0/go.mod h1:8pFdfPkv/jr8mkChVbNVuJ0suiHe278BtWI4Tk1ujxk=\n github.com/dgrijalva/jwt-go v3.2.0+incompatible h1:7qlOGliEKZXTDg6OTjfoBKDXWrumCAMpl/TFQ4/5kLM=\n github.com/dgrijalva/jwt-go v3.2.0+incompatible/go.mod h1:E3ru+11k8xSBh+hMPgOLZmtrrCbhqsmaPHjLKYnJCaQ=\n-github.com/didip/tollbooth v4.0.2+incompatible h1:fVSa33JzSz0hoh2NxpwZtksAzAgd7zjmGO20HCZtF4M=\n-github.com/didip/tollbooth v4.0.2+incompatible/go.mod h1:A9b0665CE6l1KmzpDws2++elm/CsuWBMa5Jv4WY0PEY=\n-github.com/didip/tollbooth_chi v0.0.0-20170928041846-6ab5f3083f3d h1:vs5Nf6IE0N/PwGJ8//zRed4gpCdcr99K2HzX7RuLOQ8=\n-github.com/didip/tollbooth_chi v0.0.0-20170928041846-6ab5f3083f3d/go.mod h1:YWyIfq3y4ArRfWZ9XksmuusP+7Mad+T0iFZ0kv0XG/M=\n+github.com/didip/tollbooth/v6 v6.0.1 h1:QvLvRpB1G2bzKvkRze0muMUBlGN9H1z7tJ4DH4ypWOU=\n+github.com/didip/tollbooth/v6 v6.0.1/go.mod h1:j2pKs+JQ5PvU/K4jFnrnwntrmfUbYLJE5oSdxR37FD0=\n+github.com/didip/tollbooth_chi v0.0.0-20200524181329-8b84cd7183d9 h1:gTh8fKuI/yLqQtZEPlDX3ZGsiTPZIe0ADHsxXSbwO1I=\n+github.com/didip/tollbooth_chi v0.0.0-20200524181329-8b84cd7183d9/go.mod h1:YWyIfq3y4ArRfWZ9XksmuusP+7Mad+T0iFZ0kv0XG/M=\n github.com/dlclark/regexp2 v1.1.6 h1:CqB4MjHw0MFCDj+PHHjiESmHX+N7t0tJzKvC6M97BRg=\n github.com/dlclark/regexp2 v1.1.6/go.mod h1:2pZnwuY/m+8K6iRw6wQdMtk+rH5tNGR1i55kozfMjCc=\n+github.com/fasthttp-contrib/websocket v0.0.0-20160511215533-1f3b11f56072 h1:DddqAaWDpywytcG8w/qoQ5sAN8X12d3Z3koB0C3Rxsc=\n+github.com/fasthttp-contrib/websocket v0.0.0-20160511215533-1f3b11f56072/go.mod h1:duJ4Jxv5lDcvg4QuQr0oowTf7dz4/CR8NtyCooz9HL8=\n github.com/fatih/structs v1.1.0 h1:Q7juDM0QtcnhCpeyLGQKyg4TOIghuNXrkL32pHAUMxo=\n github.com/fatih/structs v1.1.0/go.mod h1:9NiDSp5zOcgEDl+j00MP/WkGVPOlPRLejGD8Ga6PJ7M=\n github.com/fsnotify/fsnotify v1.4.7 h1:IXs+QLmnXW2CcXuY+8Mzv/fWEsPGWxqefPtCP5CnV9I=\n github.com/fsnotify/fsnotify v1.4.7/go.mod h1:jwhsz4b93w/PPRr/qN1Yymfu8t87LnFCMoQvtojpjFo=\n-github.com/gavv/httpexpect v0.0.0-20180803094507-bdde30871313 h1:GSPjYG49Uqn3S1oeFgJtlGI3ykTavl/yvYgZlz6wsoI=\n-github.com/gavv/httpexpect v0.0.0-20180803094507-bdde30871313/go.mod h1:x+9tiU1YnrOvnB725RkpoLv1M62hOWzwo5OXotisrKc=\n-github.com/gavv/monotime v0.0.0-20171021193802-6f8212e8d10d h1:oYXrtNhqNKL1dVtKdv8XUq5zqdGVFNQ0/4tvccXZOLM=\n-github.com/gavv/monotime v0.0.0-20171021193802-6f8212e8d10d/go.mod h1:vmp8DIyckQMXOPl0AQVHt+7n5h7Gb7hS6CUydiV8QeA=\n-github.com/go-chi/chi v4.0.2+incompatible/go.mod h1:eB3wogJHnLi3x/kFX2A+IbTBlXxmMeXJVKy9tTv1XzQ=\n-github.com/go-chi/chi v4.1.0+incompatible h1:ETj3cggsVIY2Xao5ExCu6YhEh5MD6JTfcBzS37R260w=\n-github.com/go-chi/chi v4.1.0+incompatible/go.mod h1:eB3wogJHnLi3x/kFX2A+IbTBlXxmMeXJVKy9tTv1XzQ=\n+github.com/gavv/httpexpect v2.0.0+incompatible h1:1X9kcRshkSKEjNJJxX9Y9mQ5BRfbxU5kORdjhlA1yX8=\n+github.com/gavv/httpexpect v2.0.0+incompatible/go.mod h1:x+9tiU1YnrOvnB725RkpoLv1M62hOWzwo5OXotisrKc=\n+github.com/go-chi/chi v4.1.1+incompatible h1:MmTgB0R8Bt/jccxp+t6S/1VGIKdJw5J74CK/c9tTfA4=\n+github.com/go-chi/chi v4.1.1+incompatible/go.mod h1:eB3wogJHnLi3x/kFX2A+IbTBlXxmMeXJVKy9tTv1XzQ=\n github.com/go-chi/cors v1.1.1 h1:eHuqxsIw89iXcWnWUN8R72JMibABJTN/4IOYI5WERvw=\n github.com/go-chi/cors v1.1.1/go.mod h1:K2Yje0VW/SJzxiyMYu6iPQYa7hMjQX2i/F491VChg1I=\n github.com/go-chi/render v1.0.1 h1:4/5tis2cKaNdnv9zFLfXzcquC9HbeZgCnxGnKrltBS8=\n github.com/go-chi/render v1.0.1/go.mod h1:pq4Rr7HbnsdaeHagklXub+p6Wd16Af5l9koip1OvJns=\n-github.com/go-pkgz/auth v0.10.1 h1:GYf64js5n/oVEPXhRShyDDiR/oahdZOMLbxmnnGX2IU=\n-github.com/go-pkgz/auth v0.10.1/go.mod h1:wxyQqc0UUP1jT4l6zk1r6XPcVdcgIzW2OiQ8hBEHd64=\n-github.com/go-pkgz/jrpc v0.1.0 h1:hNg/IyfEqJcSWOKkuHw0ZwcuGc9TDp7QZREsD2ycmiM=\n-github.com/go-pkgz/jrpc v0.1.0/go.mod h1:JxZsvoBklA50DNhELVJnJ567Rt+KrMH9rR3u515wvE8=\n-github.com/go-pkgz/lcw v0.6.0 h1:BVEZDhVMM2W719tRED3j14lGRqNmGIlnVKVeJkPRVKs=\n-github.com/go-pkgz/lcw v0.6.0/go.mod h1:vovP88gZLeuIWn5cm0NlgPYFyGGkv3m2OcKMOOaHhj0=\n+github.com/go-pkgz/auth v0.10.2 h1:aYTEu0sxBi+UJXm3IIn9/cgW7bTXZkiykY23rdYZjAc=\n+github.com/go-pkgz/auth v0.10.2/go.mod h1:w4Z1qaYvuh2P3T2gNh0f8GcKCH0HHAoQtQ8iv+9+WGg=\n+github.com/go-pkgz/expirable-cache v0.0.3 h1:rTh6qNPp78z0bQE6HDhXBHUwqnV9i09Vm6dksJLXQDc=\n+github.com/go-pkgz/expirable-cache v0.0.3/go.mod h1:+IauqN00R2FqNRLCLA+X5YljQJrwB179PfiAoMPlTlQ=\n+github.com/go-pkgz/jrpc v0.2.0 h1:CLy/eZyekjraVrxZV18N2R1mYLMJ/nWrgdfyIOGPY/E=\n+github.com/go-pkgz/jrpc v0.2.0/go.mod h1:wd8vtQ4CgtCnuqua6x2b1SKIgv0VSOh5Dn0uUITbiUE=\n+github.com/go-pkgz/lcw v0.6.1 h1:hb1v9oFaP6MiGSvROoVTiTmcyU/qHzGebUOhA74l4xU=\n+github.com/go-pkgz/lcw v0.6.1/go.mod h1:vovP88gZLeuIWn5cm0NlgPYFyGGkv3m2OcKMOOaHhj0=\n github.com/go-pkgz/lgr v0.7.0 h1:S/AAPwt/RE9a5mNJskA7dGVp+Dq6SMIW6LYjG3ITxY8=\n github.com/go-pkgz/lgr v0.7.0/go.mod h1:yMgxU+GobMRJgIEbSzDKy/67W18S7qmGx/7BVL5AB8Q=\n github.com/go-pkgz/repeater v1.1.3 h1:q6+JQF14ESSy28Dd7F+wRelY4F+41HJ0LEy/szNnMiE=\n github.com/go-pkgz/repeater v1.1.3/go.mod h1:hVTavuO5x3Gxnu8zW7d6sQBfAneKV8X2FjU48kGfpKw=\n-github.com/go-pkgz/rest v1.4.1/go.mod h1:COazNj35u3RXAgQNBr6neR599tYP3URiOpsu9p0rOtk=\n github.com/go-pkgz/rest v1.5.0 h1:C8SxXcXza4GiUUAn/95iCkvoIrGbS30qpwK19iqlrWQ=\n github.com/go-pkgz/rest v1.5.0/go.mod h1:nQaM3RhSTUAmbBZWY4hfe4buyeC9VckvhoCktiQXJxI=\n github.com/go-pkgz/syncs v1.1.1 h1:jWN+y6FS/Xe+8z4l3QMbSnODGyaxDHGojIS+wyKIjxg=\n@@ -81,80 +77,104 @@ github.com/go-redis/redis/v7 v7.2.0/go.mod h1:JDNMw23GTyLNC4GZu9njt15ctBQVn7xjRf\n github.com/go-session/session v3.1.2+incompatible/go.mod h1:8B3iivBQjrz/JtC68Np2T1yBBLxTan3mn/3OM0CyRt0=\n github.com/go-stack/stack v1.8.0 h1:5SgMzNM5HxrEjV0ww2lTmX6E2Izsfxas4+YHWRs3Lsk=\n github.com/go-stack/stack v1.8.0/go.mod h1:v0f6uXyyMGvRgIKkXu+yp6POWl0qKG85gN/melR3HDY=\n-github.com/golang/glog v0.0.0-20160126235308-23def4e6c14b/go.mod h1:SBH7ygxi8pfUlaOkMMuAQtPIUF8ecWP5IEl/CR7VP2Q=\n-github.com/golang/mock v1.1.1/go.mod h1:oTYuIxOrZwtPieC+H1uAHpcLFnEyAGVDL/k47Jfbm0A=\n-github.com/golang/mock v1.2.0/go.mod h1:oTYuIxOrZwtPieC+H1uAHpcLFnEyAGVDL/k47Jfbm0A=\n+github.com/gobuffalo/attrs v0.0.0-20190224210810-a9411de4debd/go.mod h1:4duuawTqi2wkkpB4ePgWMaai6/Kc6WEz83bhFwpHzj0=\n+github.com/gobuffalo/depgen v0.0.0-20190329151759-d478694a28d3/go.mod h1:3STtPUQYuzV0gBVOY3vy6CfMm/ljR4pABfrTeHNLHUY=\n+github.com/gobuffalo/depgen v0.1.0/go.mod h1:+ifsuy7fhi15RWncXQQKjWS9JPkdah5sZvtHc2RXGlg=\n+github.com/gobuffalo/envy v1.6.15/go.mod h1:n7DRkBerg/aorDM8kbduw5dN3oXGswK5liaSCx4T5NI=\n+github.com/gobuffalo/envy v1.7.0/go.mod h1:n7DRkBerg/aorDM8kbduw5dN3oXGswK5liaSCx4T5NI=\n+github.com/gobuffalo/flect v0.1.0/go.mod h1:d2ehjJqGOH/Kjqcoz+F7jHTBbmDb38yXA598Hb50EGs=\n+github.com/gobuffalo/flect v0.1.1/go.mod h1:8JCgGVbRjJhVgD6399mQr4fx5rRfGKVzFjbj6RE/9UI=\n+github.com/gobuffalo/flect v0.1.3/go.mod h1:8JCgGVbRjJhVgD6399mQr4fx5rRfGKVzFjbj6RE/9UI=\n+github.com/gobuffalo/genny v0.0.0-20190329151137-27723ad26ef9/go.mod h1:rWs4Z12d1Zbf19rlsn0nurr75KqhYp52EAGGxTbBhNk=\n+github.com/gobuffalo/genny v0.0.0-20190403191548-3ca520ef0d9e/go.mod h1:80lIj3kVJWwOrXWWMRzzdhW3DsrdjILVil/SFKBzF28=\n+github.com/gobuffalo/genny v0.1.0/go.mod h1:XidbUqzak3lHdS//TPu2OgiFB+51Ur5f7CSnXZ/JDvo=\n+github.com/gobuffalo/genny v0.1.1/go.mod h1:5TExbEyY48pfunL4QSXxlDOmdsD44RRq4mVZ0Ex28Xk=\n+github.com/gobuffalo/gitgen v0.0.0-20190315122116-cc086187d211/go.mod h1:vEHJk/E9DmhejeLeNt7UVvlSGv3ziL+djtTr3yyzcOw=\n+github.com/gobuffalo/gogen v0.0.0-20190315121717-8f38393713f5/go.mod h1:V9QVDIxsgKNZs6L2IYiGR8datgMhB577vzTDqypH360=\n+github.com/gobuffalo/gogen v0.1.0/go.mod h1:8NTelM5qd8RZ15VjQTFkAW6qOMx5wBbW4dSCS3BY8gg=\n+github.com/gobuffalo/gogen v0.1.1/go.mod h1:y8iBtmHmGc4qa3urIyo1shvOD8JftTtfcKi+71xfDNE=\n+github.com/gobuffalo/logger v0.0.0-20190315122211-86e12af44bc2/go.mod h1:QdxcLw541hSGtBnhUc4gaNIXRjiDppFGaDqzbrBd3v8=\n+github.com/gobuffalo/mapi v1.0.1/go.mod h1:4VAGh89y6rVOvm5A8fKFxYG+wIW6LO1FMTG9hnKStFc=\n+github.com/gobuffalo/mapi v1.0.2/go.mod h1:4VAGh89y6rVOvm5A8fKFxYG+wIW6LO1FMTG9hnKStFc=\n+github.com/gobuffalo/packd v0.0.0-20190315124812-a385830c7fc0/go.mod h1:M2Juc+hhDXf/PnmBANFCqx4DM3wRbgDvnVWeG2RIxq4=\n+github.com/gobuffalo/packd v0.1.0/go.mod h1:M2Juc+hhDXf/PnmBANFCqx4DM3wRbgDvnVWeG2RIxq4=\n+github.com/gobuffalo/packr/v2 v2.0.9/go.mod h1:emmyGweYTm6Kdper+iywB6YK5YzuKchGtJQZ0Odn4pQ=\n+github.com/gobuffalo/packr/v2 v2.2.0/go.mod h1:CaAwI0GPIAv+5wKLtv8Afwl+Cm78K/I/VCm/3ptBN+0=\n+github.com/gobuffalo/syncx v0.0.0-20190224160051-33c29581e754/go.mod h1:HhnNqWY95UYwwW3uSASeV7vtgYkT2t16hJgV3AEPUpw=\n github.com/golang/protobuf v1.2.0/go.mod h1:6lQm79b+lXiMfvg/cZm0SGofjICqVBUtrP5yJMmIC1U=\n-github.com/golang/protobuf v1.3.1/go.mod h1:6lQm79b+lXiMfvg/cZm0SGofjICqVBUtrP5yJMmIC1U=\n github.com/golang/protobuf v1.3.2 h1:6nsPYzhq5kReh6QImI3k5qWzO4PEbvbIW2cwSfR/6xs=\n github.com/golang/protobuf v1.3.2/go.mod h1:6lQm79b+lXiMfvg/cZm0SGofjICqVBUtrP5yJMmIC1U=\n github.com/golang/snappy v0.0.1 h1:Qgr9rKW7uDUkrbSmQeiDsGa8SjGyCOGtuasMWwvp2P4=\n github.com/golang/snappy v0.0.1/go.mod h1:/XxbfmMg8lxefKM7IXC3fBNl/7bRcc72aCRzEWrmP2Q=\n github.com/gomodule/redigo v1.7.1-0.20190322064113-39e2c31b7ca3 h1:6amM4HsNPOvMLVc2ZnyqrjeQ92YAVWn7T4WBKK87inY=\n github.com/gomodule/redigo v1.7.1-0.20190322064113-39e2c31b7ca3/go.mod h1:B4C85qUVwatsJoIUNIfCRsp7qO0iAmpGFZ4EELWSbC4=\n-github.com/google/btree v0.0.0-20180813153112-4030bb1f1f0c/go.mod h1:lNA+9X1NB3Zf8V7Ke586lFgjr2dZNuvo3lPJSGZ5JPQ=\n+github.com/google/go-cmp v0.2.0 h1:+dTQ8DZQJz0Mb/HjFlkptS1FeQ4cWSnN941F8aEG4SQ=\n github.com/google/go-cmp v0.2.0/go.mod h1:oXzfMopK8JAjlY9xF4vHSVASa0yLyX7SntLO5aqRK0M=\n-github.com/google/go-cmp v0.3.0 h1:crn/baboCvb5fXaQ0IJ1SGTsTVrWpDsCWC8EGETZijY=\n-github.com/google/go-cmp v0.3.0/go.mod h1:8QqcDgzrUqlUb/G2PQTWiueGozuR1884gddMywk6iLU=\n github.com/google/go-querystring v1.0.0 h1:Xkwi/a1rcvNg1PPYe5vI8GbeBY/jrVuDX5ASuANWTrk=\n github.com/google/go-querystring v1.0.0/go.mod h1:odCYkC5MyYFN7vkCjXpyrEuKhc/BUO6wN/zVPAxq5ck=\n-github.com/google/martian v2.1.0+incompatible/go.mod h1:9I4somxYTbIHy5NJKHRl3wXiIaQGbYVAs8BPL6v8lEs=\n-github.com/google/pprof v0.0.0-20181206194817-3ea8567a2e57/go.mod h1:zfwlbNMJ+OItoe0UupaVj+oy1omPYYDuagoSzA8v9mc=\n github.com/google/uuid v1.1.1 h1:Gkbcsh/GbpXz7lPftLA3P6TYMwjCLYm83jiFQZF/3gY=\n github.com/google/uuid v1.1.1/go.mod h1:TIyPZe4MgqvfeYDBFedMoGGpEw/LqOeaOT+nhxU+yHo=\n-github.com/googleapis/gax-go/v2 v2.0.4/go.mod h1:0Wqv26UfaUD9n4G6kQubkQ+KchISgw+vpHVxEJEs9eg=\n-github.com/gopherjs/gopherjs v0.0.0-20181103185306-d547d1d9531e h1:JKmoR8x90Iww1ks85zJ1lfDGgIiMDuIptTOhJq+zKyg=\n-github.com/gopherjs/gopherjs v0.0.0-20181103185306-d547d1d9531e/go.mod h1:wJfORRmW1u3UXTncJ5qlYoELFm8eSnnEO6hX4iZ3EWY=\n+github.com/gopherjs/gopherjs v0.0.0-20181017120253-0766667cb4d1 h1:EGx4pi6eqNxGaHF6qqu48+N2wcFQ5qg5FXgOdqsJ5d8=\n+github.com/gopherjs/gopherjs v0.0.0-20181017120253-0766667cb4d1/go.mod h1:wJfORRmW1u3UXTncJ5qlYoELFm8eSnnEO6hX4iZ3EWY=\n github.com/gorilla/feeds v1.1.1 h1:HwKXxqzcRNg9to+BbvJog4+f3s/xzvtZXICcQGutYfY=\n github.com/gorilla/feeds v1.1.1/go.mod h1:Nk0jZrvPFZX1OBe5NPiddPw7CfwF6Q9eqzaBbaightA=\n+github.com/gorilla/websocket v1.4.1 h1:q7AeDBpnBk8AogcD4DSag/Ukw/KV+YhzLj2bP5HvKCM=\n+github.com/gorilla/websocket v1.4.1/go.mod h1:YR8l580nyteQvAITg2hZ9XVh4b55+EU/adAjf1fMHhE=\n github.com/hashicorp/errwrap v1.0.0 h1:hLrqtEDnRye3+sgx6z4qVLNuviH3MR5aQ0ykNJa/UYA=\n github.com/hashicorp/errwrap v1.0.0/go.mod h1:YH+1FKiLXxHSkmPseP+kNlulaMuP3n2brvKWEqk/Jc4=\n github.com/hashicorp/go-multierror v1.1.0 h1:B9UzwGQJehnUY1yNrnwREHc3fGbC2xefo8g4TbElacI=\n github.com/hashicorp/go-multierror v1.1.0/go.mod h1:spPvp8C1qA32ftKqdAHm4hHTbPw+vmowP0z+KUhOZdA=\n-github.com/hashicorp/golang-lru v0.5.0/go.mod h1:/m3WP610KZHVQ1SGc6re/UDhFvYD7pJ4Ao+sR/qLZy8=\n-github.com/hashicorp/golang-lru v0.5.1/go.mod h1:/m3WP610KZHVQ1SGc6re/UDhFvYD7pJ4Ao+sR/qLZy8=\n github.com/hashicorp/golang-lru v0.5.4 h1:YDjusn29QI/Das2iO9M0BHnIbxPeyuCHsjMW+lJfyTc=\n github.com/hashicorp/golang-lru v0.5.4/go.mod h1:iADmTwqILo4mZ8BN3D2Q6+9jd8WM5uGBxy+E8yxSoD4=\n github.com/hpcloud/tail v1.0.0 h1:nfCOvKYfkgYP8hkirhJocXT2+zOD8yUNjXaWfTlyFKI=\n github.com/hpcloud/tail v1.0.0/go.mod h1:ab1qPbhIpdTxEkNHXyeSf5vhxWSCs/tWer42PpOxQnU=\n github.com/imkira/go-interpol v1.1.0 h1:KIiKr0VSG2CUW1hl1jpiyuzuJeKUUpC8iM1AIE7N1Vk=\n github.com/imkira/go-interpol v1.1.0/go.mod h1:z0h2/2T3XF8kyEPpRgJ3kmNv+C43p+I/CoI+jC3w2iA=\n-github.com/jstemmer/go-junit-report v0.0.0-20190106144839-af01ea7f8024/go.mod h1:6v2b51hI/fHJwM22ozAgKL4VKDeJcHhJFhtBdhmNjmU=\n-github.com/jtolds/gls v4.2.1+incompatible h1:fSuqC+Gmlu6l/ZYAoZzx2pyucC8Xza35fpRVWLVmUEE=\n-github.com/jtolds/gls v4.2.1+incompatible/go.mod h1:QJZ7F/aHp+rZTRtaJ1ow/lLfFfVYBRgL+9YlvaHOwJU=\n+github.com/inconshreveable/mousetrap v1.0.0/go.mod h1:PxqpIevigyE2G7u3NXJIT2ANytuPF1OarO4DADm73n8=\n+github.com/joho/godotenv v1.3.0/go.mod h1:7hK45KPybAkOC6peb+G5yklZfMxEjkZhHbwpqxOKXbg=\n+github.com/jtolds/gls v4.20.0+incompatible h1:xdiiI2gbIgH/gLH7ADydsJ1uDOEzR8yvV7C0MuV77Wo=\n+github.com/jtolds/gls v4.20.0+incompatible/go.mod h1:QJZ7F/aHp+rZTRtaJ1ow/lLfFfVYBRgL+9YlvaHOwJU=\n github.com/k0kubun/colorstring v0.0.0-20150214042306-9440f1994b88 h1:uC1QfSlInpQF+M0ao65imhwqKnz3Q2z/d8PWZRMQvDM=\n github.com/k0kubun/colorstring v0.0.0-20150214042306-9440f1994b88/go.mod h1:3w7q1U84EfirKl04SVQ/s7nPm1ZPhiXd34z40TNz36k=\n-github.com/klauspost/compress v1.4.0 h1:8nsMz3tWa9SWWPL60G1V6CUsf4lLjWLTNEtibhe8gh8=\n-github.com/klauspost/compress v1.4.0/go.mod h1:RyIbtBH6LamlWaDj8nUwkbUhJ87Yi3uG0guNDohfE1A=\n-github.com/klauspost/cpuid v0.0.0-20180405133222-e7e905edc00e h1:+lIPJOWl+jSiJOc70QXJ07+2eg2Jy2EC7Mi11BWujeM=\n-github.com/klauspost/cpuid v0.0.0-20180405133222-e7e905edc00e/go.mod h1:Pj4uuM528wm8OyEC2QMXAi2YiTZ96dNQPGgoMS4s3ek=\n+github.com/karrick/godirwalk v1.8.0/go.mod h1:H5KPZjojv4lE+QYImBI8xVtrBRgYrIVsaRPx4tDPEn4=\n+github.com/karrick/godirwalk v1.10.3/go.mod h1:RoGL9dQei4vP9ilrpETWE8CLOZ1kiN0LhBygSwrAsHA=\n+github.com/kisielk/errcheck v1.2.0/go.mod h1:/BMXB+zMLi60iA8Vv6Ksmxu/1UDYcXs4uQLJ+jE2L00=\n+github.com/klauspost/compress v1.8.2/go.mod h1:RyIbtBH6LamlWaDj8nUwkbUhJ87Yi3uG0guNDohfE1A=\n+github.com/klauspost/compress v1.9.5 h1:U+CaK85mrNNb4k8BNOfgJtJ/gr6kswUCFj6miSzVC6M=\n+github.com/klauspost/compress v1.9.5/go.mod h1:RyIbtBH6LamlWaDj8nUwkbUhJ87Yi3uG0guNDohfE1A=\n+github.com/klauspost/cpuid v1.2.1/go.mod h1:Pj4uuM528wm8OyEC2QMXAi2YiTZ96dNQPGgoMS4s3ek=\n+github.com/konsorten/go-windows-terminal-sequences v1.0.1/go.mod h1:T0+1ngSBFLxvqU3pZ+m/2kptfBszLMUkC4ZK/EgS/cQ=\n+github.com/konsorten/go-windows-terminal-sequences v1.0.2/go.mod h1:T0+1ngSBFLxvqU3pZ+m/2kptfBszLMUkC4ZK/EgS/cQ=\n github.com/kr/pretty v0.1.0 h1:L/CwN0zerZDmRFUapSPitk6f+Q3+0za1rQkzVuMiMFI=\n github.com/kr/pretty v0.1.0/go.mod h1:dAy3ld7l9f0ibDNOQOHHMYYIIbhfbHSm3C4ZsoJORNo=\n github.com/kr/pty v1.1.1/go.mod h1:pFQYn66WHrOpPYNljwOMqo10TkYh1fy3cYio2l3bCsQ=\n github.com/kr/text v0.1.0 h1:45sCR5RtlFHMR4UwH9sdQ5TC8v0qDQCHnXt+kaKSTVE=\n github.com/kr/text v0.1.0/go.mod h1:4Jbv+DJW3UT/LiOwJeYQe1efqtUx/iVham/4vfdArNI=\n github.com/kyokomi/emoji v2.2.1+incompatible h1:uP/6J5y5U0XxPh6fv8YximpVD1uMrshXG78I1+uF5SA=\n github.com/kyokomi/emoji v2.2.1+incompatible/go.mod h1:mZ6aGCD7yk8j6QY6KICwnZ2pxoszVseX1DNoGtU2tBA=\n-github.com/mattn/go-colorable v0.0.9 h1:UVL0vNpWh04HeJXV0KLcaT7r06gOH2l4OW6ddYRUIY4=\n+github.com/markbates/oncer v0.0.0-20181203154359-bf2de49a0be2/go.mod h1:Ld9puTsIW75CHf65OeIOkyKbteujpZVXDpWK6YGZbxE=\n+github.com/markbates/safe v1.0.1/go.mod h1:nAqgmRi7cY2nqMc92/bSEeQA+R4OheNU2T1kNSCBdG0=\n github.com/mattn/go-colorable v0.0.9/go.mod h1:9vuHe8Xs5qXnSaW/c/ABM9alt+Vo+STaOChaDxuIBZU=\n-github.com/mattn/go-isatty v0.0.4 h1:bnP0vzxcAdeI1zdubAl5PjU6zsERjGZb7raWodagDYs=\n+github.com/mattn/go-colorable v0.1.4 h1:snbPLB8fVfU9iwbbo30TPtbLRzwWu6aJS6Xh4eaaviA=\n+github.com/mattn/go-colorable v0.1.4/go.mod h1:U0ppj6V5qS13XJ6of8GYAs25YV2eR4EVcfRqFIhoBtE=\n github.com/mattn/go-isatty v0.0.4/go.mod h1:M+lRXTBqGeGNdLjl/ufCoiOlB5xdOkqRJdNxMWT7Zi4=\n+github.com/mattn/go-isatty v0.0.8 h1:HLtExJ+uU2HOZ+wI0Tt5DtUDrx8yhUqDcp7fYERX4CE=\n+github.com/mattn/go-isatty v0.0.8/go.mod h1:Iq45c/XA43vh69/j3iqttzPXn0bhXyGjM0Hdxcsrc5s=\n github.com/microcosm-cc/bluemonday v1.0.2 h1:5lPfLTTAvAbtS0VqT+94yOtFnGfUWYyx0+iToC3Os3s=\n github.com/microcosm-cc/bluemonday v1.0.2/go.mod h1:iVP4YcDBq+n/5fb23BhYFvIMq/leAFZyRl6bYmGDlGc=\n github.com/mitchellh/mapstructure v1.1.2/go.mod h1:FVVH3fgwuzCH5S8UJGiWEs2h04kUh9fWfEaFds41c1Y=\n+github.com/montanaflynn/stats v0.0.0-20171201202039-1bf9dbcd8cbe/go.mod h1:wL8QJuTMNUDYhXwkmfOly8iTdp5TEcJFWZD2D7SIkUc=\n github.com/moul/http2curl v1.0.0 h1:dRMWoAtb+ePxMlLkrCbAqh4TlPHXvoGUSQ323/9Zahs=\n github.com/moul/http2curl v1.0.0/go.mod h1:8UbvGypXm98wA/IqH45anm5Y2Z6ep6O31QGOAZ3H0fQ=\n github.com/nullrocks/identicon v0.0.0-20180626043057-7875f45b0022 h1:Ys0rDzh8s4UMlGaDa1UTA0sfKgvF0hQZzTYX8ktjiDc=\n github.com/nullrocks/identicon v0.0.0-20180626043057-7875f45b0022/go.mod h1:x4NsS+uc7ecH/Cbm9xKQ6XzmJM57rWTkjywjfB2yQ18=\n github.com/onsi/ginkgo v1.6.0/go.mod h1:lLunBs/Ym6LB5Z9jYTR76FiuTmxDTDusOGeTQH+WWjE=\n-github.com/onsi/ginkgo v1.7.0/go.mod h1:lLunBs/Ym6LB5Z9jYTR76FiuTmxDTDusOGeTQH+WWjE=\n-github.com/onsi/ginkgo v1.10.1 h1:q/mM8GF/n0shIN8SaAZ0V+jnLPzen6WIVZdiwrRlMlo=\n github.com/onsi/ginkgo v1.10.1/go.mod h1:lLunBs/Ym6LB5Z9jYTR76FiuTmxDTDusOGeTQH+WWjE=\n-github.com/onsi/gomega v1.4.3/go.mod h1:ex+gbHU/CVuBBDIJjb2X0qEXbFg53c61hWP/1CpauHY=\n+github.com/onsi/ginkgo v1.10.2 h1:uqH7bpe+ERSiDa34FDOF7RikN6RzXgduUF8yarlZp94=\n+github.com/onsi/ginkgo v1.10.2/go.mod h1:lLunBs/Ym6LB5Z9jYTR76FiuTmxDTDusOGeTQH+WWjE=\n github.com/onsi/gomega v1.7.0 h1:XPnZz8VVBHjVsy1vzJmRwIcSwiUO+JFfrv/xGiigmME=\n github.com/onsi/gomega v1.7.0/go.mod h1:ex+gbHU/CVuBBDIJjb2X0qEXbFg53c61hWP/1CpauHY=\n-github.com/patrickmn/go-cache v2.1.0+incompatible h1:HRMgzkcYKYpi3C8ajMPV8OFXaaRUnok+kx1WdO15EQc=\n-github.com/patrickmn/go-cache v2.1.0+incompatible/go.mod h1:3Qf8kWWT7OJRJbdiICTKqZju1ZixQ/KpMGzzAfe6+WQ=\n+github.com/pelletier/go-toml v1.4.0/go.mod h1:PN7xzY2wHTK0K9p34ErDQMlFxa51Fk0OUruD3k1mMwo=\n github.com/pkg/errors v0.8.0/go.mod h1:bwawxfHBFNV+L2hUp1rHADufV3IMtnDRdf1r5NINEl0=\n github.com/pkg/errors v0.8.1/go.mod h1:bwawxfHBFNV+L2hUp1rHADufV3IMtnDRdf1r5NINEl0=\n github.com/pkg/errors v0.9.1 h1:FEBLx1zS214owpjy7qsBeixbURkuhQAwrK5UwLGTwt4=\n@@ -163,6 +183,9 @@ github.com/pmezard/go-difflib v1.0.0 h1:4DBwDE0NGyQoBHbLQYPwSUPoCMWR5BEzIk/f1lZb\n github.com/pmezard/go-difflib v1.0.0/go.mod h1:iKH77koFhYxTK1pcRnkKkqfTogsbg7gZNVY4sRDYZ/4=\n github.com/rakyll/statik v0.1.7 h1:OF3QCZUuyPxuGEP7B4ypUa7sB/iHtqOTDYZXGM8KOdQ=\n github.com/rakyll/statik v0.1.7/go.mod h1:AlZONWzMtEnMs7W4e/1LURLiI49pIMmp6V9Unghqrcc=\n+github.com/rogpeppe/go-internal v1.1.0/go.mod h1:M8bDsm7K2OlrFYOpmOWEs/qY81heoFRclV5y23lUDJ4=\n+github.com/rogpeppe/go-internal v1.2.2/go.mod h1:M8bDsm7K2OlrFYOpmOWEs/qY81heoFRclV5y23lUDJ4=\n+github.com/rogpeppe/go-internal v1.3.0/go.mod h1:M8bDsm7K2OlrFYOpmOWEs/qY81heoFRclV5y23lUDJ4=\n github.com/rs/xid v1.2.1 h1:mhH9Nq+C1fY2l1XIpgxIiUOfNpRBYH1kKcr+qfKgjRc=\n github.com/rs/xid v1.2.1/go.mod h1:+uKXf+4Djp6Md1KODXJxgGQPKngRmWyn10oCKFzNHOQ=\n github.com/russross/blackfriday/v2 v2.0.1 h1:lPqVAte+HuHNfhJ/0LC98ESWRz8afy9tM/0RK8m9o+Q=\n@@ -171,22 +194,29 @@ github.com/sergi/go-diff v1.0.0 h1:Kpca3qRNrduNnOQeazBd0ysaKrUJiIuISHxogkT9RPQ=\n github.com/sergi/go-diff v1.0.0/go.mod h1:0CfEIISq7TuYL3j771MWULgwwjU+GofnZX9QAmXWZgo=\n github.com/shurcooL/sanitized_anchor_name v1.0.0 h1:PdmoCO6wvbs+7yrJyMORt4/BmY5IYyJwS/kOiWx8mHo=\n github.com/shurcooL/sanitized_anchor_name v1.0.0/go.mod h1:1NzhyTcUVG4SuEtjjoZeVRXNmyL/1OwPU0+IJeTBvfc=\n+github.com/sirupsen/logrus v1.4.0/go.mod h1:LxeOpSwHxABJmUn/MG1IvRgCAasNZTLOkJPxbbu5VWo=\n+github.com/sirupsen/logrus v1.4.1/go.mod h1:ni0Sbl8bgC9z8RoU9G6nDWqqs/fq4eDPysMBDgk/93Q=\n+github.com/sirupsen/logrus v1.4.2/go.mod h1:tLMulIdttU9McNUspp0xgXVQah82FyeX6MwdIuYE2rE=\n github.com/smartystreets/assertions v0.0.0-20180927180507-b2de0cb4f26d h1:zE9ykElWQ6/NYmHa3jpm/yHnI4xSofP+UP6SpjHcSeM=\n github.com/smartystreets/assertions v0.0.0-20180927180507-b2de0cb4f26d/go.mod h1:OnSkiWE9lh6wB0YB77sQom3nweQdgAjqCqsofrRNTgc=\n-github.com/smartystreets/goconvey v0.0.0-20181108003508-044398e4856c h1:Ho+uVpkel/udgjbwB5Lktg9BtvJSh2DT0Hi6LPSyI2w=\n-github.com/smartystreets/goconvey v0.0.0-20181108003508-044398e4856c/go.mod h1:XDJAKZRPZ1CvBcN2aX5YOUTYGHki24fSF0Iv48Ibg0s=\n-github.com/stretchr/objx v0.1.0 h1:4G4v2dO3VZwixGIRoQ5Lfboy6nUhCyYzaqnIAPPhYs4=\n+github.com/smartystreets/goconvey v1.6.4 h1:fv0U8FUIMPNf1L9lnHLvLhgicrIVChEkdzIKYqbNC9s=\n+github.com/smartystreets/goconvey v1.6.4/go.mod h1:syvi0/a8iFYH4r/RixwvyeAJjdLS9QV7WQ/tjFTllLA=\n+github.com/spf13/cobra v0.0.3/go.mod h1:1l0Ry5zgKvJasoi3XT1TypsSe7PqH0Sj9dhYf7v3XqQ=\n+github.com/spf13/pflag v1.0.3/go.mod h1:DYY7MBk1bdzusC3SYhjObp+wFpr4gzcvqqNjLnInEg4=\n github.com/stretchr/objx v0.1.0/go.mod h1:HFkY916IF+rwdDfMAkV7OtwuqBVzrE8GR6GFx+wExME=\n+github.com/stretchr/objx v0.1.1 h1:2vfRuCMp5sSVIDSqO8oNnWJq7mPa6KVP3iPIwFBuy8A=\n+github.com/stretchr/objx v0.1.1/go.mod h1:HFkY916IF+rwdDfMAkV7OtwuqBVzrE8GR6GFx+wExME=\n github.com/stretchr/testify v1.2.2/go.mod h1:a8OnRcib4nhh0OaRAV+Yts87kKdq0PP7pXfy6kDkUVs=\n github.com/stretchr/testify v1.3.0/go.mod h1:M5WIy9Dh21IEIfnGCwXGc5bZfKNJtfHm1UVUgZn+9EI=\n+github.com/stretchr/testify v1.4.0/go.mod h1:j7eGeouHqKxXV5pUuKE4zz7dFj8WfuZ+81PSLYec5m4=\n github.com/stretchr/testify v1.5.1 h1:nOGnQDM7FYENwehXlg/kFVnos3rEvtKTjRvOWSzb6H4=\n github.com/stretchr/testify v1.5.1/go.mod h1:5W2xD1RspED5o8YsWQXVCued0rvSQ+mT+I5cxcmMvtA=\n github.com/tidwall/btree v0.0.0-20170113224114-9876f1454cf0 h1:QnyrPZZvPmR0AtJCxxfCtI1qN+fYpKTKJ/5opWmZ34k=\n github.com/tidwall/btree v0.0.0-20170113224114-9876f1454cf0/go.mod h1:huei1BkDWJ3/sLXmO+bsCNELL+Bp2Kks9OLyQFkzvA8=\n-github.com/tidwall/buntdb v1.0.0 h1:urIJqQ8OR9fibXXtFSu8sR5arMqZK8ZnNq22yWl+A+8=\n-github.com/tidwall/buntdb v1.0.0/go.mod h1:Y39xhcDW10WlyYXeLgGftXVbjtM0QP+/kpz8xl9cbzE=\n-github.com/tidwall/gjson v1.1.3 h1:u4mspaByxY+Qk4U1QYYVzGFI8qxN/3jtEV0ZDb2vRic=\n-github.com/tidwall/gjson v1.1.3/go.mod h1:c/nTNbUr0E0OrXEhq1pwa8iEgc2DOt4ZZqAt1HtCkPA=\n+github.com/tidwall/buntdb v1.1.0 h1:H6LzK59KiNjf1nHVPFrYj4Qnl8d8YLBsYamdL8N+Bao=\n+github.com/tidwall/buntdb v1.1.0/go.mod h1:Y39xhcDW10WlyYXeLgGftXVbjtM0QP+/kpz8xl9cbzE=\n+github.com/tidwall/gjson v1.3.2 h1:+7p3qQFaH3fOMXAJSrdZwGKcOO/lYdGS0HqGhPqDdTI=\n+github.com/tidwall/gjson v1.3.2/go.mod h1:P256ACg0Mn+j1RXIDXoss50DeIABTYK1PULOJHhxOls=\n github.com/tidwall/grect v0.0.0-20161006141115-ba9a043346eb h1:5NSYaAdrnblKByzd7XByQEJVT8+9v0W/tIY0Oo4OwrE=\n github.com/tidwall/grect v0.0.0-20161006141115-ba9a043346eb/go.mod h1:lKYYLFIr9OIgdgrtgkZ9zgRxRdvPYsExnYBsEAd8W5M=\n github.com/tidwall/match v1.0.1 h1:PnKP62LPNxHKTwvHHZZzdOAOCtsJTjo6dZLCwpKm5xc=\n@@ -201,19 +231,19 @@ github.com/umputun/go-flags v1.5.1 h1:vRauoXV3Ultt1HrxivSxowbintgZLJE+EcBy5ta3/m\n github.com/umputun/go-flags v1.5.1/go.mod h1:nTbvsO/hKqe7Utri/NoyN18GR3+EWf+9RrmsdwdhrEc=\n github.com/valyala/bytebufferpool v1.0.0 h1:GqA5TC/0021Y/b9FG4Oi9Mr3q7XYx6KllzawFIhcdPw=\n github.com/valyala/bytebufferpool v1.0.0/go.mod h1:6bBcMArwyJ5K/AmCkWv1jt77kVWyCJ6HpOuEn7z0Csc=\n-github.com/valyala/fasthttp v1.0.0 h1:BwIoZQbBsTo3v2F5lz5Oy3TlTq4wLKTLV260EVTEWco=\n-github.com/valyala/fasthttp v1.0.0/go.mod h1:4vX61m6KN+xDduDNwXrhIAVZaZaZiQ1luJk8LWSxF3s=\n+github.com/valyala/fasthttp v1.6.0 h1:uWF8lgKmeaIewWVPwi4GRq2P6+R46IgYZdxWtM+GtEY=\n+github.com/valyala/fasthttp v1.6.0/go.mod h1:FstJa9V+Pj9vQ7OJie2qMHdwemEDaDiSdBnvPM1Su9w=\n github.com/valyala/tcplisten v0.0.0-20161114210144-ceec8f93295a/go.mod h1:v3UYOV9WzVtRmSR+PDvWpU/qWl4Wa5LApYYX4ZtKbio=\n github.com/xdg/scram v0.0.0-20180814205039-7eeb5667e42c h1:u40Z8hqBAAQyv+vATcGgV0YCnDjqSL7/q/JyPhhJSPk=\n github.com/xdg/scram v0.0.0-20180814205039-7eeb5667e42c/go.mod h1:lB8K/P019DLNhemzwFU4jHLhdvlE6uDZjXFejJXr49I=\n-github.com/xdg/stringprep v1.0.0 h1:d9X0esnoa3dFsV0FG35rAT0RIhYFlPq7MiP+DW89La0=\n-github.com/xdg/stringprep v1.0.0/go.mod h1:Jhud4/sHMO4oL310DaZAKk9ZaJ08SJfe+sJh0HrGL1Y=\n+github.com/xdg/stringprep v0.0.0-20180714160509-73f8eece6fdc h1:n+nNi93yXLkJvKwXNP9d55HC7lGK4H/SRcwB5IaUZLo=\n+github.com/xdg/stringprep v0.0.0-20180714160509-73f8eece6fdc/go.mod h1:Jhud4/sHMO4oL310DaZAKk9ZaJ08SJfe+sJh0HrGL1Y=\n github.com/xeipuuv/gojsonpointer v0.0.0-20180127040702-4e3ac2762d5f h1:J9EGpcZtP0E/raorCMxlFGSTBrsSlaDGf3jU/qvAE2c=\n github.com/xeipuuv/gojsonpointer v0.0.0-20180127040702-4e3ac2762d5f/go.mod h1:N2zxlSyiKSe5eX1tZViRH5QA0qijqEDrYZiPEAiq3wU=\n github.com/xeipuuv/gojsonreference v0.0.0-20180127040603-bd5ef7bd5415 h1:EzJWgHovont7NscjpAxXsDA8S8BMYve8Y5+7cuRE7R0=\n github.com/xeipuuv/gojsonreference v0.0.0-20180127040603-bd5ef7bd5415/go.mod h1:GwrjFmJcFw6At/Gs6z4yjiIwzuJ1/+UwLxMQDVQXShQ=\n-github.com/xeipuuv/gojsonschema v0.0.0-20181112162635-ac52e6811b56 h1:yhqBHs09SmmUoNOHc9jgK4a60T3XFRtPAkYxVnqgY50=\n-github.com/xeipuuv/gojsonschema v0.0.0-20181112162635-ac52e6811b56/go.mod h1:5yf86TLmAcydyeJq5YvxkGPE2fm/u4myDekKRoLuqhs=\n+github.com/xeipuuv/gojsonschema v1.2.0 h1:LhYJRs+L4fBtjZUfuSZIKGeVu0QRy8e5Xi7D17UxZ74=\n+github.com/xeipuuv/gojsonschema v1.2.0/go.mod h1:anYRn/JVcOK2ZgGU+IjEV4nwlhoK5sQluxsYJ78Id3Y=\n github.com/yalp/jsonpath v0.0.0-20180802001716-5cc68e5049a0 h1:6fRhSjgLCkTD3JnJxvaJ4Sj+TYblw757bqYgZaOq5ZY=\n github.com/yalp/jsonpath v0.0.0-20180802001716-5cc68e5049a0/go.mod h1:/LWChgwKmvncFJFHJ7Gvn9wZArjbV5/FppcK2fKk/tI=\n github.com/yudai/gojsondiff v1.0.0 h1:27cbfqXLVEJ1o8I6v3y9lg8Ydm53EKqHXAOMxEGlCOA=\n@@ -226,107 +256,79 @@ github.com/yuin/gopher-lua v0.0.0-20191220021717-ab39c6098bdb h1:ZkM6LRnq40pR1Ox\n github.com/yuin/gopher-lua v0.0.0-20191220021717-ab39c6098bdb/go.mod h1:gqRgreBUhTSL0GeU64rtZ3Uq3wtjOa/TB2YfrtkCbVQ=\n go.etcd.io/bbolt v1.3.4 h1:hi1bXHMVrlQh6WwxAy+qZCV/SYIlqo+Ushwdpa4tAKg=\n go.etcd.io/bbolt v1.3.4/go.mod h1:G5EMThwa9y8QZGBClrRx5EY+Yw9kAhnjy3bSjsnlVTQ=\n-go.mongodb.org/mongo-driver v1.1.1 h1:Sq1fR+0c58RME5EoqKdjkiQAmPjmfHlZOoRI6fTUOcs=\n-go.mongodb.org/mongo-driver v1.1.1/go.mod h1:u7ryQJ+DOzQmeO7zB6MHyr8jkEQvC8vH7qLUO4lqsUM=\n-go.opencensus.io v0.21.0/go.mod h1:mSImk1erAIZhrmZN+AvHh14ztQfjbGwt4TtuofqLduU=\n+go.mongodb.org/mongo-driver v1.3.2 h1:IYppNjEV/C+/3VPbhHVxQ4t04eVW0cLp0/pNdW++6Ug=\n+go.mongodb.org/mongo-driver v1.3.2/go.mod h1:MSWZXKOynuguX+JSvwP8i+58jYCXxbia8HS3gZBapIE=\n+golang.org/x/crypto v0.0.0-20180904163835-0709b304e793/go.mod h1:6SG95UA2DQfeDnfUPMdvaQW0Q7yPrPDi9nlGo2tz2b4=\n golang.org/x/crypto v0.0.0-20190308221718-c2843e01d9a2/go.mod h1:djNgcEr1/C05ACkg1iLfiJU5Ep61QUkGW8qpdssI0+w=\n-golang.org/x/crypto v0.0.0-20190605123033-f99c8df09eb5 h1:58fnuSXlxZmFdJyvtTFVmVhcMLU6v5fEb/ok4wyqtNU=\n-golang.org/x/crypto v0.0.0-20190605123033-f99c8df09eb5/go.mod h1:yigFU9vqHzYiE8UmvKecakEJjdnWj3jj499lnFckfCI=\n+golang.org/x/crypto v0.0.0-20190422162423-af44ce270edf/go.mod h1:WFFai1msRO1wXaEeE5yQxYXgSfI8pQAWXbQop6sCtWE=\n+golang.org/x/crypto v0.0.0-20190530122614-20be4c3c3ed5/go.mod h1:yigFU9vqHzYiE8UmvKecakEJjdnWj3jj499lnFckfCI=\n golang.org/x/crypto v0.0.0-20200406173513-056763e48d71 h1:DOmugCavvUtnUD114C1Wh+UgTgQZ4pMLzXxi1pSt+/Y=\n golang.org/x/crypto v0.0.0-20200406173513-056763e48d71/go.mod h1:LzIPMQfyMNhhGPhUkYOs5KpL4U8rLKemX1yGLhDgUto=\n-golang.org/x/exp v0.0.0-20190121172915-509febef88a4/go.mod h1:CJ0aWSM057203Lf6IL+f9T1iT9GByDxfZKAQTCR3kQA=\n-golang.org/x/image v0.0.0-20190523035834-f03afa92d3ff h1:+2zgJKVDVAz/BWSsuniCmU1kLCjL88Z8/kv39xCI9NQ=\n-golang.org/x/image v0.0.0-20190523035834-f03afa92d3ff/go.mod h1:kZ7UVZpmo3dzQBMxlp+ypCbDeSB+sBbTgSJuh5dn5js=\n golang.org/x/image v0.0.0-20200119044424-58c23975cae1 h1:5h3ngYt7+vXCDZCup/HkCQgW5XwmSvR/nA2JmJ0RErg=\n golang.org/x/image v0.0.0-20200119044424-58c23975cae1/go.mod h1:FeLwcggjj3mMvU+oOTbSwawSJRM1uh48EjtB4UJZlP0=\n-golang.org/x/lint v0.0.0-20181026193005-c67002cb31c3/go.mod h1:UVdnD1Gm6xHRNCYTkRU2/jEulfH38KcIWyp/GAMgvoE=\n-golang.org/x/lint v0.0.0-20190227174305-5b3e6a55c961/go.mod h1:wehouNa3lNwaWXcvxsM5YxQ5yQlVC4a0KAMCusXpPoU=\n-golang.org/x/lint v0.0.0-20190301231843-5614ed5bae6f/go.mod h1:UVdnD1Gm6xHRNCYTkRU2/jEulfH38KcIWyp/GAMgvoE=\n-golang.org/x/lint v0.0.0-20190313153728-d0100b6bd8b3/go.mod h1:6SW0HCj/g11FgYtHlgUYUwCkIfeOF89ocIRzGO/8vkc=\n-golang.org/x/lint v0.0.0-20190409202823-959b441ac422/go.mod h1:6SW0HCj/g11FgYtHlgUYUwCkIfeOF89ocIRzGO/8vkc=\n golang.org/x/net v0.0.0-20180218175443-cbe0f9307d01/go.mod h1:mL1N/T3taQHkDXs73rZJwtUhF3w3ftmwwsq0BUmARs4=\n golang.org/x/net v0.0.0-20180724234803-3673e40ba225/go.mod h1:mL1N/T3taQHkDXs73rZJwtUhF3w3ftmwwsq0BUmARs4=\n-golang.org/x/net v0.0.0-20180826012351-8a410e7b638d/go.mod h1:mL1N/T3taQHkDXs73rZJwtUhF3w3ftmwwsq0BUmARs4=\n golang.org/x/net v0.0.0-20180906233101-161cd47e91fd/go.mod h1:mL1N/T3taQHkDXs73rZJwtUhF3w3ftmwwsq0BUmARs4=\n-golang.org/x/net v0.0.0-20180911220305-26e67e76b6c3/go.mod h1:mL1N/T3taQHkDXs73rZJwtUhF3w3ftmwwsq0BUmARs4=\n-golang.org/x/net v0.0.0-20181217023233-e147a9138326/go.mod h1:mL1N/T3taQHkDXs73rZJwtUhF3w3ftmwwsq0BUmARs4=\n golang.org/x/net v0.0.0-20181220203305-927f97764cc3/go.mod h1:mL1N/T3taQHkDXs73rZJwtUhF3w3ftmwwsq0BUmARs4=\n golang.org/x/net v0.0.0-20190108225652-1e06a53dbb7e/go.mod h1:mL1N/T3taQHkDXs73rZJwtUhF3w3ftmwwsq0BUmARs4=\n-golang.org/x/net v0.0.0-20190213061140-3a22650c66bd/go.mod h1:mL1N/T3taQHkDXs73rZJwtUhF3w3ftmwwsq0BUmARs4=\n golang.org/x/net v0.0.0-20190311183353-d8887717615a/go.mod h1:t9HGtf8HONx5eT2rtn7q6eTqICYqUVnKs3thJo3Qplg=\n golang.org/x/net v0.0.0-20190404232315-eb5bcb51f2a3/go.mod h1:t9HGtf8HONx5eT2rtn7q6eTqICYqUVnKs3thJo3Qplg=\n-golang.org/x/net v0.0.0-20190503192946-f4e77d36d62c/go.mod h1:t9HGtf8HONx5eT2rtn7q6eTqICYqUVnKs3thJo3Qplg=\n-golang.org/x/net v0.0.0-20190603091049-60506f45cf65/go.mod h1:HSz+uSET+XFnRR8LxR5pz3Of3rY3CfYBVs4xY44aLks=\n-golang.org/x/net v0.0.0-20190611141213-3f473d35a33a/go.mod h1:z5CRVTTTmAJ677TzLLGU+0bjPO0LkuOLi4/5GtJWs/s=\n-golang.org/x/net v0.0.0-20190724013045-ca1201d0de80/go.mod h1:z5CRVTTTmAJ677TzLLGU+0bjPO0LkuOLi4/5GtJWs/s=\n+golang.org/x/net v0.0.0-20190827160401-ba9fcec4b297/go.mod h1:z5CRVTTTmAJ677TzLLGU+0bjPO0LkuOLi4/5GtJWs/s=\n golang.org/x/net v0.0.0-20190923162816-aa69164e4478/go.mod h1:z5CRVTTTmAJ677TzLLGU+0bjPO0LkuOLi4/5GtJWs/s=\n-golang.org/x/net v0.0.0-20200202094626-16171245cfb2 h1:CCH4IOTTfewWjGOlSp+zGcjutRKlBEZQ6wTn8ozI/nI=\n golang.org/x/net v0.0.0-20200202094626-16171245cfb2/go.mod h1:z5CRVTTTmAJ677TzLLGU+0bjPO0LkuOLi4/5GtJWs/s=\n-golang.org/x/oauth2 v0.0.0-20180821212333-d2e6202438be/go.mod h1:N/0e6XlmueqKjAGxoOufVs8QHGRruUQn6yWY3a++T0U=\n-golang.org/x/oauth2 v0.0.0-20190226205417-e64efc72b421/go.mod h1:gOpvHmFTYa4IltrdGE7lF6nIHvwfUNPOp7c8zoXwtLw=\n-golang.org/x/oauth2 v0.0.0-20190604053449-0f29369cfe45 h1:SVwTIAaPC2U/AvvLNZ2a7OVsmBpC8L5BlwK1whH3hm0=\n+golang.org/x/net v0.0.0-20200520182314-0ba52f642ac2 h1:eDrdRpKgkcCqKZQwyZRyeFZgfqt37SL7Kv3tok06cKE=\n+golang.org/x/net v0.0.0-20200520182314-0ba52f642ac2/go.mod h1:qpuaurCH72eLCgpAm/N6yyVIVM9cpaDIP3A8BGJEC5A=\n golang.org/x/oauth2 v0.0.0-20190604053449-0f29369cfe45/go.mod h1:gOpvHmFTYa4IltrdGE7lF6nIHvwfUNPOp7c8zoXwtLw=\n+golang.org/x/oauth2 v0.0.0-20200107190931-bf48bf16ab8d h1:TzXSXBo42m9gQenoE3b9BGiEpg5IG2JkU5FkPIawgtw=\n+golang.org/x/oauth2 v0.0.0-20200107190931-bf48bf16ab8d/go.mod h1:gOpvHmFTYa4IltrdGE7lF6nIHvwfUNPOp7c8zoXwtLw=\n golang.org/x/sync v0.0.0-20180314180146-1d60e4601c6f/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=\n-golang.org/x/sync v0.0.0-20181108010431-42b317875d0f/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=\n golang.org/x/sync v0.0.0-20181221193216-37e7f081c4d4/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=\n golang.org/x/sync v0.0.0-20190227155943-e225da77a7e6/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=\n+golang.org/x/sync v0.0.0-20190412183630-56d357773e84/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=\n golang.org/x/sync v0.0.0-20190423024810-112230192c58 h1:8gQV6CLnAEikrhgkHFbMAEhagSSnXWGV915qUMm9mrU=\n golang.org/x/sync v0.0.0-20190423024810-112230192c58/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=\n-golang.org/x/sys v0.0.0-20180830151530-49385e6e1522/go.mod h1:STP8DvDyc/dI5b8T5hshtkjS+E42TnysNCUPdjciGhY=\n+golang.org/x/sys v0.0.0-20180905080454-ebe1bf3edb33/go.mod h1:STP8DvDyc/dI5b8T5hshtkjS+E42TnysNCUPdjciGhY=\n golang.org/x/sys v0.0.0-20180909124046-d0be0721c37e/go.mod h1:STP8DvDyc/dI5b8T5hshtkjS+E42TnysNCUPdjciGhY=\n golang.org/x/sys v0.0.0-20181107165924-66b7b1311ac8/go.mod h1:STP8DvDyc/dI5b8T5hshtkjS+E42TnysNCUPdjciGhY=\n golang.org/x/sys v0.0.0-20181128092732-4ed8d59d0b35/go.mod h1:STP8DvDyc/dI5b8T5hshtkjS+E42TnysNCUPdjciGhY=\n golang.org/x/sys v0.0.0-20190204203706-41f3e6584952/go.mod h1:STP8DvDyc/dI5b8T5hshtkjS+E42TnysNCUPdjciGhY=\n golang.org/x/sys v0.0.0-20190215142949-d0b11bdaac8a/go.mod h1:STP8DvDyc/dI5b8T5hshtkjS+E42TnysNCUPdjciGhY=\n+golang.org/x/sys v0.0.0-20190222072716-a9d3bda3a223/go.mod h1:STP8DvDyc/dI5b8T5hshtkjS+E42TnysNCUPdjciGhY=\n+golang.org/x/sys v0.0.0-20190403152447-81d4e9dc473e/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\n golang.org/x/sys v0.0.0-20190412213103-97732733099d/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\n-golang.org/x/sys v0.0.0-20190507160741-ecd444e8653b/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\n-golang.org/x/sys v0.0.0-20190606165138-5da285871e9c/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\n+golang.org/x/sys v0.0.0-20190419153524-e8e3143a4f4a/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\n+golang.org/x/sys v0.0.0-20190422165155-953cdadca894/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\n+golang.org/x/sys v0.0.0-20190531175056-4c3a928424d2/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\n golang.org/x/sys v0.0.0-20191010194322-b09406accb47/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\n-golang.org/x/sys v0.0.0-20200202164722-d101bd2416d5 h1:LfCXLvNmTYH9kEmVgqbnsWfruoXZIrh4YBgqVHtDvw0=\n golang.org/x/sys v0.0.0-20200202164722-d101bd2416d5/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\n+golang.org/x/sys v0.0.0-20200323222414-85ca7c5b95cd h1:xhmwyvizuTgC2qz7ZlMluP20uW+C3Rm0FD/WLDX8884=\n+golang.org/x/sys v0.0.0-20200323222414-85ca7c5b95cd/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\n golang.org/x/text v0.3.0/go.mod h1:NqM8EUOU14njkJ3fqMW+pc6Ldnwhi/IjpwHt7yyuwOQ=\n-golang.org/x/text v0.3.1-0.20180807135948-17ff2d5776d2/go.mod h1:NqM8EUOU14njkJ3fqMW+pc6Ldnwhi/IjpwHt7yyuwOQ=\n golang.org/x/text v0.3.2 h1:tW2bmiBqwgJj/UpqtC8EpXEZVYOwU0yG4iWbprSVAcs=\n golang.org/x/text v0.3.2/go.mod h1:bEr9sfX3Q8Zfm5fL9x+3itogRgK3+ptLWKqgva+5dAk=\n-golang.org/x/time v0.0.0-20181108054448-85acf8d2951c/go.mod h1:tRJNPiyCQ0inRvYxbN9jk5I+vvW/OXSQhTDSoE431IQ=\n-golang.org/x/time v0.0.0-20190308202827-9d24e82272b4 h1:SvFZT6jyqRaOeXpc5h/JSfZenJ2O330aBsf7JfSUXmQ=\n-golang.org/x/time v0.0.0-20190308202827-9d24e82272b4/go.mod h1:tRJNPiyCQ0inRvYxbN9jk5I+vvW/OXSQhTDSoE431IQ=\n+golang.org/x/time v0.0.0-20200416051211-89c76fbcd5d1 h1:NusfzzA6yGQ+ua51ck7E3omNUX/JuqbFSaRGqU8CcLI=\n+golang.org/x/time v0.0.0-20200416051211-89c76fbcd5d1/go.mod h1:tRJNPiyCQ0inRvYxbN9jk5I+vvW/OXSQhTDSoE431IQ=\n golang.org/x/tools v0.0.0-20180917221912-90fa682c2a6e/go.mod h1:n7NCudcB/nEzxVGmLbDWY5pfWTLqBcC2KZ6jyYvM4mQ=\n-golang.org/x/tools v0.0.0-20190114222345-bf090417da8b/go.mod h1:n7NCudcB/nEzxVGmLbDWY5pfWTLqBcC2KZ6jyYvM4mQ=\n-golang.org/x/tools v0.0.0-20190226205152-f727befe758c/go.mod h1:9Yl7xja0Znq3iFh3HoIrodX9oNMXvdceNzlUR8zjMvY=\n-golang.org/x/tools v0.0.0-20190311212946-11955173bddd/go.mod h1:LCzVGOaR6xXOjkQ3onu1FJEFr0SW1gC7cKk1uF8kGRs=\n-golang.org/x/tools v0.0.0-20190312170243-e65039ee4138/go.mod h1:LCzVGOaR6xXOjkQ3onu1FJEFr0SW1gC7cKk1uF8kGRs=\n-golang.org/x/tools v0.0.0-20190506145303-2d16b83fe98c/go.mod h1:RgjU9mgBXZiqYHBnxXauZ1Gv1EHHAz9KjViQ78xBX0Q=\n-golang.org/x/tools v0.0.0-20190606124116-d0a3d012864b/go.mod h1:/rFqwRUd4F7ZHNgwSSTFct+R/Kf4OFW1sUzUTQQTgfc=\n-google.golang.org/api v0.4.0/go.mod h1:8k5glujaEP+g9n7WNsDg8QP6cUVNI86fCNMcbazEtwE=\n-google.golang.org/api v0.6.0/go.mod h1:btoxGiFvQNVUZQ8W08zLtrVS08CNpINPEfxXxgJL1Q4=\n-google.golang.org/appengine v1.1.0/go.mod h1:EbEs0AVv82hx2wNQdGPgUI5lhzA/G0D9YwlJXL52JkM=\n+golang.org/x/tools v0.0.0-20181030221726-6c7e314b6563/go.mod h1:n7NCudcB/nEzxVGmLbDWY5pfWTLqBcC2KZ6jyYvM4mQ=\n+golang.org/x/tools v0.0.0-20190328211700-ab21143f2384/go.mod h1:LCzVGOaR6xXOjkQ3onu1FJEFr0SW1gC7cKk1uF8kGRs=\n+golang.org/x/tools v0.0.0-20190329151228-23e29df326fe/go.mod h1:LCzVGOaR6xXOjkQ3onu1FJEFr0SW1gC7cKk1uF8kGRs=\n+golang.org/x/tools v0.0.0-20190416151739-9c9e1878f421/go.mod h1:LCzVGOaR6xXOjkQ3onu1FJEFr0SW1gC7cKk1uF8kGRs=\n+golang.org/x/tools v0.0.0-20190420181800-aa740d480789/go.mod h1:LCzVGOaR6xXOjkQ3onu1FJEFr0SW1gC7cKk1uF8kGRs=\n+golang.org/x/tools v0.0.0-20190531172133-b3315ee88b7d/go.mod h1:/rFqwRUd4F7ZHNgwSSTFct+R/Kf4OFW1sUzUTQQTgfc=\n+google.golang.org/appengine v1.4.0 h1:/wp5JvzpHIxhs/dumFmF7BXTf3Z+dd4uXta4kVyO508=\n google.golang.org/appengine v1.4.0/go.mod h1:xpcJRLb0r/rnEns0DIKYYv+WjYCduHsrkT7/EB5XEv4=\n-google.golang.org/appengine v1.5.0/go.mod h1:xpcJRLb0r/rnEns0DIKYYv+WjYCduHsrkT7/EB5XEv4=\n-google.golang.org/appengine v1.6.1 h1:QzqyMA1tlu6CgqCDUtU9V+ZKhLFT2dkJuANu5QaxI3I=\n-google.golang.org/appengine v1.6.1/go.mod h1:i06prIuMbXzDqacNJfV5OdTW448YApPu5ww/cMBSeb0=\n-google.golang.org/genproto v0.0.0-20180817151627-c66870c02cf8/go.mod h1:JiN7NxoALGmiZfu7CAH4rXhgtRTLTxftemlI0sWmxmc=\n-google.golang.org/genproto v0.0.0-20190307195333-5fe7a883aa19/go.mod h1:VzzqZJRnGkLBvHegQrXjBqPurQTc5/KpmUdxsrq26oE=\n-google.golang.org/genproto v0.0.0-20190418145605-e7d98fc518a7/go.mod h1:VzzqZJRnGkLBvHegQrXjBqPurQTc5/KpmUdxsrq26oE=\n-google.golang.org/genproto v0.0.0-20190502173448-54afdca5d873/go.mod h1:VzzqZJRnGkLBvHegQrXjBqPurQTc5/KpmUdxsrq26oE=\n-google.golang.org/genproto v0.0.0-20190530194941-fb225487d101/go.mod h1:z3L6/3dTEVtUr6QSP8miRzeRqwQOioJ9I66odjN4I7s=\n-google.golang.org/grpc v1.19.0/go.mod h1:mqu4LbDTu4XGKhr4mRzUsmM4RtVoemTSY81AxZiDr8c=\n-google.golang.org/grpc v1.20.1/go.mod h1:10oTOabMzJvdu6/UiuZezV6QK5dSlG84ov/aaiqXj38=\n gopkg.in/check.v1 v0.0.0-20161208181325-20d25e280405/go.mod h1:Co6ibVJAznAaIkqp8huTwlJQCZ016jof/cbN4VW5Yz0=\n gopkg.in/check.v1 v1.0.0-20180628173108-788fd7840127/go.mod h1:Co6ibVJAznAaIkqp8huTwlJQCZ016jof/cbN4VW5Yz0=\n gopkg.in/check.v1 v1.0.0-20190902080502-41f04d3bba15 h1:YR8cESwS4TdDjEe65xsg0ogRM/Nc3DYOhEAlW+xobZo=\n gopkg.in/check.v1 v1.0.0-20190902080502-41f04d3bba15/go.mod h1:Co6ibVJAznAaIkqp8huTwlJQCZ016jof/cbN4VW5Yz0=\n+gopkg.in/errgo.v2 v2.1.0/go.mod h1:hNsd1EY+bozCKY1Ytp96fpM3vjJbqLJn88ws8XvfDNI=\n gopkg.in/fsnotify.v1 v1.4.7 h1:xOHLXZwVvI9hhs+cLKq5+I5onOuwQLhQwiu63xxlHs4=\n gopkg.in/fsnotify.v1 v1.4.7/go.mod h1:Tz8NjZHkW78fSQdbUxIjBTcgA1z1m8ZHf0WmKUhAMys=\n-gopkg.in/oauth2.v3 v3.10.1 h1:/abis3O6tZFizY1/FgKGoDOmmu2ddvscBSSPdHVa6OI=\n-gopkg.in/oauth2.v3 v3.10.1/go.mod h1:nTG+m2PRcHR9jzGNrGdxSsUKz7vvwkqSlhFrstgZcRU=\n+gopkg.in/oauth2.v3 v3.12.0 h1:yOffAPoolH/i2JxwmC+pgtnY3362iPahsDpLXfDFvNg=\n+gopkg.in/oauth2.v3 v3.12.0/go.mod h1:XEYgKqWX095YiPT+Aw5y3tCn+7/FMnlTFKrupgSiJ3I=\n gopkg.in/tomb.v1 v1.0.0-20141024135613-dd632973f1e7 h1:uRGJdciOHaEIrze2W8Q3AKkepLTh2hOroT7a+7czfdQ=\n gopkg.in/tomb.v1 v1.0.0-20141024135613-dd632973f1e7/go.mod h1:dt/ZhP58zS4L8KSrWDmTeBkI65Dw0HsyUHuEVlX15mw=\n gopkg.in/yaml.v2 v2.2.1/go.mod h1:hI93XBmqTisBFMUTm0b8Fm+jr3Dg1NNxqwp+5A1VGuI=\n gopkg.in/yaml.v2 v2.2.2/go.mod h1:hI93XBmqTisBFMUTm0b8Fm+jr3Dg1NNxqwp+5A1VGuI=\n gopkg.in/yaml.v2 v2.2.4 h1:/eiJrUcujPVeJ3xlSWaiNi3uSVmDGBK1pDHUHAnao1I=\n gopkg.in/yaml.v2 v2.2.4/go.mod h1:hI93XBmqTisBFMUTm0b8Fm+jr3Dg1NNxqwp+5A1VGuI=\n-honnef.co/go/tools v0.0.0-20190102054323-c2f93a96b099/go.mod h1:rf3lG4BRIbNafJWhAfAdb/ePZxsR/4RtNHQocxwk9r4=\n-honnef.co/go/tools v0.0.0-20190106161140-3f1c8253044a/go.mod h1:rf3lG4BRIbNafJWhAfAdb/ePZxsR/4RtNHQocxwk9r4=\n-honnef.co/go/tools v0.0.0-20190418001031-e561f6794a2a/go.mod h1:rf3lG4BRIbNafJWhAfAdb/ePZxsR/4RtNHQocxwk9r4=\n-rsc.io/binaryregexp v0.2.0/go.mod h1:qTv7/COck+e2FymRvadv62gMdZztPaShugOCi3I+8D8="
    },
    {
      "sha": "c364af1da0953639e0614778e565876eb0b29e04",
      "filename": "backend/vendor/cloud.google.com/go/AUTHORS",
      "status": "added",
      "additions": 15,
      "deletions": 0,
      "changes": 15,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/cloud.google.com/go/AUTHORS",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/cloud.google.com/go/AUTHORS",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/cloud.google.com/go/AUTHORS?ref=890d7154e777d0673defcd11db3ef792473d68c9",
      "patch": "@@ -0,0 +1,15 @@\n+# This is the official list of cloud authors for copyright purposes.\n+# This file is distinct from the CONTRIBUTORS files.\n+# See the latter for an explanation.\n+\n+# Names should be added to this file as:\n+# Name or Organization <email address>\n+# The email address is not required for organizations.\n+\n+Filippo Valsorda <hi@filippo.io>\n+Google Inc.\n+Ingo Oeser <nightlyone@googlemail.com>\n+Palm Stone Games, Inc.\n+Paweł Knap <pawelknap88@gmail.com>\n+Péter Szilágyi <peterke@gmail.com>\n+Tyler Treat <ttreat31@gmail.com>"
    },
    {
      "sha": "3b3cbed98e9a9e0ab3052b98359d43a82a9c1c8c",
      "filename": "backend/vendor/cloud.google.com/go/CONTRIBUTORS",
      "status": "added",
      "additions": 40,
      "deletions": 0,
      "changes": 40,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/cloud.google.com/go/CONTRIBUTORS",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/cloud.google.com/go/CONTRIBUTORS",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/cloud.google.com/go/CONTRIBUTORS?ref=890d7154e777d0673defcd11db3ef792473d68c9",
      "patch": "@@ -0,0 +1,40 @@\n+# People who have agreed to one of the CLAs and can contribute patches.\n+# The AUTHORS file lists the copyright holders; this file\n+# lists people.  For example, Google employees are listed here\n+# but not in AUTHORS, because Google holds the copyright.\n+#\n+# https://developers.google.com/open-source/cla/individual\n+# https://developers.google.com/open-source/cla/corporate\n+#\n+# Names should be added to this file as:\n+#     Name <email address>\n+\n+# Keep the list alphabetically sorted.\n+\n+Alexis Hunt <lexer@google.com>\n+Andreas Litt <andreas.litt@gmail.com>\n+Andrew Gerrand <adg@golang.org>\n+Brad Fitzpatrick <bradfitz@golang.org>\n+Burcu Dogan <jbd@google.com>\n+Dave Day <djd@golang.org>\n+David Sansome <me@davidsansome.com>\n+David Symonds <dsymonds@golang.org>\n+Filippo Valsorda <hi@filippo.io>\n+Glenn Lewis <gmlewis@google.com>\n+Ingo Oeser <nightlyone@googlemail.com>\n+James Hall <james.hall@shopify.com>\n+Johan Euphrosine <proppy@google.com>\n+Jonathan Amsterdam <jba@google.com>\n+Kunpei Sakai <namusyaka@gmail.com>\n+Luna Duclos <luna.duclos@palmstonegames.com>\n+Magnus Hiie <magnus.hiie@gmail.com>\n+Mario Castro <mariocaster@gmail.com>\n+Michael McGreevy <mcgreevy@golang.org>\n+Omar Jarjur <ojarjur@google.com>\n+Paweł Knap <pawelknap88@gmail.com>\n+Péter Szilágyi <peterke@gmail.com>\n+Sarah Adams <shadams@google.com>\n+Thanatat Tamtan <acoshift@gmail.com>\n+Toby Burress <kurin@google.com>\n+Tuo Shan <shantuo@google.com>\n+Tyler Treat <ttreat31@gmail.com>"
    },
    {
      "sha": "0d929a6192111b46a33242a1eada37a38caf2f25",
      "filename": "backend/vendor/cloud.google.com/go/compute/metadata/metadata.go",
      "status": "modified",
      "additions": 6,
      "deletions": 18,
      "changes": 24,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/cloud.google.com/go/compute/metadata/metadata.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/cloud.google.com/go/compute/metadata/metadata.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/cloud.google.com/go/compute/metadata/metadata.go?ref=890d7154e777d0673defcd11db3ef792473d68c9",
      "patch": "@@ -137,7 +137,7 @@ func testOnGCE() bool {\n \tresc := make(chan bool, 2)\n \n \t// Try two strategies in parallel.\n-\t// See https://github.com/googleapis/google-cloud-go/issues/194\n+\t// See https://github.com/GoogleCloudPlatform/google-cloud-go/issues/194\n \tgo func() {\n \t\treq, _ := http.NewRequest(\"GET\", \"http://\"+metadataIP, nil)\n \t\treq.Header.Set(\"User-Agent\", userAgent)\n@@ -300,8 +300,8 @@ func (c *Client) getETag(suffix string) (value, etag string, err error) {\n \t\t// being stable anyway.\n \t\thost = metadataIP\n \t}\n-\tu := \"http://\" + host + \"/computeMetadata/v1/\" + suffix\n-\treq, _ := http.NewRequest(\"GET\", u, nil)\n+\turl := \"http://\" + host + \"/computeMetadata/v1/\" + suffix\n+\treq, _ := http.NewRequest(\"GET\", url, nil)\n \treq.Header.Set(\"Metadata-Flavor\", \"Google\")\n \treq.Header.Set(\"User-Agent\", userAgent)\n \tres, err := c.hc.Do(req)\n@@ -312,13 +312,13 @@ func (c *Client) getETag(suffix string) (value, etag string, err error) {\n \tif res.StatusCode == http.StatusNotFound {\n \t\treturn \"\", \"\", NotDefinedError(suffix)\n \t}\n+\tif res.StatusCode != 200 {\n+\t\treturn \"\", \"\", fmt.Errorf(\"status code %d trying to fetch %s\", res.StatusCode, url)\n+\t}\n \tall, err := ioutil.ReadAll(res.Body)\n \tif err != nil {\n \t\treturn \"\", \"\", err\n \t}\n-\tif res.StatusCode != 200 {\n-\t\treturn \"\", \"\", &Error{Code: res.StatusCode, Message: string(all)}\n-\t}\n \treturn string(all), res.Header.Get(\"Etag\"), nil\n }\n \n@@ -499,15 +499,3 @@ func (c *Client) Subscribe(suffix string, fn func(v string, ok bool) error) erro\n \t\t}\n \t}\n }\n-\n-// Error contains an error response from the server.\n-type Error struct {\n-\t// Code is the HTTP response status code.\n-\tCode int\n-\t// Message is the server response message.\n-\tMessage string\n-}\n-\n-func (e *Error) Error() string {\n-\treturn fmt.Sprintf(\"compute: Received %d `%s`\", e.Code, e.Message)\n-}"
    },
    {
      "sha": "1043f539b1a99d3512b6b97cf83d5312a0c29394",
      "filename": "backend/vendor/github.com/didip/tollbooth/v6/.gitignore",
      "status": "renamed",
      "additions": 0,
      "deletions": 0,
      "changes": 0,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/didip/tollbooth/v6/.gitignore",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/didip/tollbooth/v6/.gitignore",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/github.com/didip/tollbooth/v6/.gitignore?ref=890d7154e777d0673defcd11db3ef792473d68c9",
      "previous_filename": "backend/vendor/github.com/didip/tollbooth/.gitignore"
    },
    {
      "sha": "98b2ab1ce3a9cf95381b0fb6812e2e2cb9763e15",
      "filename": "backend/vendor/github.com/didip/tollbooth/v6/.golangci.yml",
      "status": "added",
      "additions": 37,
      "deletions": 0,
      "changes": 37,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/didip/tollbooth/v6/.golangci.yml",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/didip/tollbooth/v6/.golangci.yml",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/github.com/didip/tollbooth/v6/.golangci.yml?ref=890d7154e777d0673defcd11db3ef792473d68c9",
      "patch": "@@ -0,0 +1,37 @@\n+linters:\n+  enable:\n+    - megacheck\n+    - golint\n+    - govet\n+    - unconvert\n+    - megacheck\n+    - structcheck\n+    - gas\n+    - gocyclo\n+    - dupl\n+    - misspell\n+    - unparam\n+    - varcheck\n+    - deadcode\n+    - typecheck\n+    - ineffassign\n+    - varcheck\n+    - stylecheck\n+    - gochecknoinits\n+    - scopelint\n+    - gocritic\n+    - nakedret\n+    - gosimple\n+    - prealloc\n+  fast: false\n+  disable-all: true\n+\n+issues:\n+  exclude-rules:\n+    - path: _test\\.go\n+      linters:\n+        - dupl\n+    - text: \"Errors unhandled\"\n+      linters:\n+        - gosec\n+  exclude-use-default: false"
    },
    {
      "sha": "349ee1c29e8906a6d8ebb65a671636abdbb99564",
      "filename": "backend/vendor/github.com/didip/tollbooth/v6/LICENSE",
      "status": "renamed",
      "additions": 0,
      "deletions": 0,
      "changes": 0,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/didip/tollbooth/v6/LICENSE",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/didip/tollbooth/v6/LICENSE",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/github.com/didip/tollbooth/v6/LICENSE?ref=890d7154e777d0673defcd11db3ef792473d68c9",
      "previous_filename": "backend/vendor/github.com/didip/tollbooth/LICENSE"
    },
    {
      "sha": "302704a5b56a39f51675a8423088285ec8c4d74a",
      "filename": "backend/vendor/github.com/didip/tollbooth/v6/README.md",
      "status": "renamed",
      "additions": 16,
      "deletions": 5,
      "changes": 21,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/didip/tollbooth/v6/README.md",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/didip/tollbooth/v6/README.md",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/github.com/didip/tollbooth/v6/README.md?ref=890d7154e777d0673defcd11db3ef792473d68c9",
      "patch": "@@ -12,11 +12,17 @@ This is a generic middleware to rate-limit HTTP requests.\n \n ## Versions\n \n-**v1.0.0:** This version maintains the old API but all of the thirdparty modules are moved to their own repo.\n+**v1.0.0:** This version maintains the old API but all the thirdparty modules are moved to their own repo.\n \n-**v2.x.x:** Brand new API for the sake of code cleanup, thread safety, & auto-expiring data structures.\n+**v2.x.x:** Brand-new API for the sake of code cleanup, thread safety, & auto-expiring data structures.\n \n-**v3.x.x:** Apparently we have been using golang.org/x/time/rate incorrectly. See issue #48. It always limit X number per 1 second. The time duration is not changeable, so it does not make sense to pass TTL to tollbooth.\n+**v3.x.x:** Apparently we have been using golang.org/x/time/rate incorrectly. See issue #48. It always limits X number per 1 second. The time duration is not changeable, so it does not make sense to pass TTL to tollbooth.\n+\n+**v4.x.x:** Float64 for max requests per second\n+\n+**v5.x.x:** go.mod and go.sum\n+\n+**v6.x.x:** Replaced `go-cache` with `github.com/go-pkgz/expirable-cache` because `go-cache` leaks goroutines.\n \n \n ## Five Minute Tutorial\n@@ -26,7 +32,6 @@ package main\n import (\n     \"github.com/didip/tollbooth\"\n     \"net/http\"\n-    \"time\"\n )\n \n func HelloHandler(w http.ResponseWriter, req *http.Request) {\n@@ -131,7 +136,7 @@ func main() {\n     lmt.SetOnLimitReached(func(w http.ResponseWriter, r *http.Request) { fmt.Println(\"A request was rejected\") })\n     ```\n \n-6. Tollbooth does not require external storage since it uses an algorithm called [Token Bucket](http://en.wikipedia.org/wiki/Token_bucket) [(Go library: golang.org/x/time/rate)](//godoc.org/golang.org/x/time/rate).\n+6. Tollbooth does not require external storage since it uses an algorithm called [Token Bucket](http://en.wikipedia.org/wiki/Token_bucket) [(Go library: golang.org/x/time/rate)](https://godoc.org/golang.org/x/time/rate).\n \n \n ## Other Web Frameworks\n@@ -162,3 +167,9 @@ Sometimes, other frameworks require a little bit of shim to use Tollbooth. These\n * [LaborUnion](https://github.com/didip/laborunion): A dynamic worker pool library.\n \n * [Gomet](https://github.com/didip/gomet): Simple HTTP client & server long poll library for Go. Useful for receiving live updates without needing Websocket.\n+\n+## Contributions\n+\n+Before sending a PR with code changes, please make sure altered code is covered with tests which are passing, and that golangci-lint shows no errors.\n+\n+To check the linter output, [install it](https://golangci-lint.run/usage/install/#local-installation) and then run `golangci-lint run` in the root directory of the repository.",
      "previous_filename": "backend/vendor/github.com/didip/tollbooth/README.md"
    },
    {
      "sha": "149bc5a161e26ef25a28bdcfe4c56a1a6ec1b4b3",
      "filename": "backend/vendor/github.com/didip/tollbooth/v6/errors/errors.go",
      "status": "renamed",
      "additions": 0,
      "deletions": 0,
      "changes": 0,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/didip/tollbooth/v6/errors/errors.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/didip/tollbooth/v6/errors/errors.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/github.com/didip/tollbooth/v6/errors/errors.go?ref=890d7154e777d0673defcd11db3ef792473d68c9",
      "previous_filename": "backend/vendor/github.com/didip/tollbooth/errors/errors.go"
    },
    {
      "sha": "9420f2279b746c29f7545bd5257f7472e9ee87fb",
      "filename": "backend/vendor/github.com/didip/tollbooth/v6/go.mod",
      "status": "added",
      "additions": 8,
      "deletions": 0,
      "changes": 8,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/didip/tollbooth/v6/go.mod",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/didip/tollbooth/v6/go.mod",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/github.com/didip/tollbooth/v6/go.mod?ref=890d7154e777d0673defcd11db3ef792473d68c9",
      "patch": "@@ -0,0 +1,8 @@\n+module github.com/didip/tollbooth/v6\n+\n+go 1.12\n+\n+require (\n+\tgithub.com/go-pkgz/expirable-cache v0.0.3\n+\tgolang.org/x/time v0.0.0-20200416051211-89c76fbcd5d1\n+)"
    },
    {
      "sha": "5b83bac94245403fe333ab172d0b3cf89478b9fe",
      "filename": "backend/vendor/github.com/didip/tollbooth/v6/go.sum",
      "status": "added",
      "additions": 17,
      "deletions": 0,
      "changes": 17,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/didip/tollbooth/v6/go.sum",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/didip/tollbooth/v6/go.sum",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/github.com/didip/tollbooth/v6/go.sum?ref=890d7154e777d0673defcd11db3ef792473d68c9",
      "patch": "@@ -0,0 +1,17 @@\n+github.com/davecgh/go-spew v1.1.0 h1:ZDRjVQ15GmhC3fiQ8ni8+OwkZQO4DARzQgrnXU1Liz8=\n+github.com/davecgh/go-spew v1.1.0/go.mod h1:J7Y8YcW2NihsgmVo/mv3lAwl/skON4iLHjSsI+c5H38=\n+github.com/go-pkgz/expirable-cache v0.0.3 h1:rTh6qNPp78z0bQE6HDhXBHUwqnV9i09Vm6dksJLXQDc=\n+github.com/go-pkgz/expirable-cache v0.0.3/go.mod h1:+IauqN00R2FqNRLCLA+X5YljQJrwB179PfiAoMPlTlQ=\n+github.com/pkg/errors v0.9.1 h1:FEBLx1zS214owpjy7qsBeixbURkuhQAwrK5UwLGTwt4=\n+github.com/pkg/errors v0.9.1/go.mod h1:bwawxfHBFNV+L2hUp1rHADufV3IMtnDRdf1r5NINEl0=\n+github.com/pmezard/go-difflib v1.0.0 h1:4DBwDE0NGyQoBHbLQYPwSUPoCMWR5BEzIk/f1lZbAQM=\n+github.com/pmezard/go-difflib v1.0.0/go.mod h1:iKH77koFhYxTK1pcRnkKkqfTogsbg7gZNVY4sRDYZ/4=\n+github.com/stretchr/objx v0.1.0/go.mod h1:HFkY916IF+rwdDfMAkV7OtwuqBVzrE8GR6GFx+wExME=\n+github.com/stretchr/testify v1.5.1 h1:nOGnQDM7FYENwehXlg/kFVnos3rEvtKTjRvOWSzb6H4=\n+github.com/stretchr/testify v1.5.1/go.mod h1:5W2xD1RspED5o8YsWQXVCued0rvSQ+mT+I5cxcmMvtA=\n+golang.org/x/time v0.0.0-20200416051211-89c76fbcd5d1 h1:NusfzzA6yGQ+ua51ck7E3omNUX/JuqbFSaRGqU8CcLI=\n+golang.org/x/time v0.0.0-20200416051211-89c76fbcd5d1/go.mod h1:tRJNPiyCQ0inRvYxbN9jk5I+vvW/OXSQhTDSoE431IQ=\n+gopkg.in/check.v1 v0.0.0-20161208181325-20d25e280405 h1:yhCVgyC4o1eVCa2tZl7eS0r+SDo693bJlVdllGtEeKM=\n+gopkg.in/check.v1 v0.0.0-20161208181325-20d25e280405/go.mod h1:Co6ibVJAznAaIkqp8huTwlJQCZ016jof/cbN4VW5Yz0=\n+gopkg.in/yaml.v2 v2.2.2 h1:ZCJp+EgiOT7lHqUV2J862kp8Qj64Jo6az82+3Td9dZw=\n+gopkg.in/yaml.v2 v2.2.2/go.mod h1:hI93XBmqTisBFMUTm0b8Fm+jr3Dg1NNxqwp+5A1VGuI="
    },
    {
      "sha": "fef29afe6d223b2f85b5ea93af19c9ee15ce413c",
      "filename": "backend/vendor/github.com/didip/tollbooth/v6/libstring/libstring.go",
      "status": "renamed",
      "additions": 0,
      "deletions": 0,
      "changes": 0,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/didip/tollbooth/v6/libstring/libstring.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/didip/tollbooth/v6/libstring/libstring.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/github.com/didip/tollbooth/v6/libstring/libstring.go?ref=890d7154e777d0673defcd11db3ef792473d68c9",
      "previous_filename": "backend/vendor/github.com/didip/tollbooth/libstring/libstring.go"
    },
    {
      "sha": "c01f4345934ec762bdbc195832b34d9fd3597e9e",
      "filename": "backend/vendor/github.com/didip/tollbooth/v6/limiter/limiter.go",
      "status": "renamed",
      "additions": 136,
      "deletions": 53,
      "changes": 189,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/didip/tollbooth/v6/limiter/limiter.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/didip/tollbooth/v6/limiter/limiter.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/github.com/didip/tollbooth/v6/limiter/limiter.go?ref=890d7154e777d0673defcd11db3ef792473d68c9",
      "patch": "@@ -6,7 +6,7 @@ import (\n \t\"sync\"\n \t\"time\"\n \n-\tgocache \"github.com/patrickmn/go-cache\"\n+\t\"github.com/go-pkgz/expirable-cache\"\n \t\"golang.org/x/time/rate\"\n )\n \n@@ -20,33 +20,23 @@ func New(generalExpirableOptions *ExpirableOptions) *Limiter {\n \t\tSetOnLimitReached(nil).\n \t\tSetIPLookups([]string{\"RemoteAddr\", \"X-Forwarded-For\", \"X-Real-IP\"}).\n \t\tSetForwardedForIndexFromBehind(0).\n-\t\tSetHeaders(make(map[string][]string))\n+\t\tSetHeaders(make(map[string][]string)).\n+\t\tSetContextValues(make(map[string][]string))\n \n \tif generalExpirableOptions != nil {\n \t\tlmt.generalExpirableOptions = generalExpirableOptions\n \t} else {\n \t\tlmt.generalExpirableOptions = &ExpirableOptions{}\n \t}\n \n-\t// Default for ExpireJobInterval is every minute.\n-\tif lmt.generalExpirableOptions.ExpireJobInterval <= 0 {\n-\t\tlmt.generalExpirableOptions.ExpireJobInterval = time.Minute\n-\t}\n-\n \t// Default for DefaultExpirationTTL is 10 years.\n \tif lmt.generalExpirableOptions.DefaultExpirationTTL <= 0 {\n \t\tlmt.generalExpirableOptions.DefaultExpirationTTL = 87600 * time.Hour\n \t}\n \n-\tlmt.tokenBuckets = gocache.New(\n-\t\tlmt.generalExpirableOptions.DefaultExpirationTTL,\n-\t\tlmt.generalExpirableOptions.ExpireJobInterval,\n-\t)\n+\tlmt.tokenBuckets, _ = cache.NewCache(cache.TTL(lmt.generalExpirableOptions.DefaultExpirationTTL))\n \n-\tlmt.basicAuthUsers = gocache.New(\n-\t\tlmt.generalExpirableOptions.DefaultExpirationTTL,\n-\t\tlmt.generalExpirableOptions.ExpireJobInterval,\n-\t)\n+\tlmt.basicAuthUsers, _ = cache.NewCache(cache.TTL(lmt.generalExpirableOptions.DefaultExpirationTTL))\n \n \treturn lmt\n }\n@@ -86,18 +76,22 @@ type Limiter struct {\n \tgeneralExpirableOptions *ExpirableOptions\n \n \t// List of basic auth usernames to limit.\n-\tbasicAuthUsers *gocache.Cache\n+\tbasicAuthUsers cache.Cache\n \n \t// Map of HTTP headers to limit.\n \t// Empty means skip headers checking.\n-\theaders map[string]*gocache.Cache\n+\theaders map[string]cache.Cache\n+\n+\t// Map of Context values to limit.\n+\tcontextValues map[string]cache.Cache\n \n \t// Map of limiters with TTL\n-\ttokenBuckets *gocache.Cache\n+\ttokenBuckets cache.Cache\n \n-\ttokenBucketExpirationTTL time.Duration\n-\tbasicAuthExpirationTTL   time.Duration\n-\theaderEntryExpirationTTL time.Duration\n+\ttokenBucketExpirationTTL  time.Duration\n+\tbasicAuthExpirationTTL    time.Duration\n+\theaderEntryExpirationTTL  time.Duration\n+\tcontextEntryExpirationTTL time.Duration\n \n \tsync.RWMutex\n }\n@@ -111,7 +105,7 @@ func (l *Limiter) SetTokenBucketExpirationTTL(ttl time.Duration) *Limiter {\n \treturn l\n }\n \n-// GettokenBucketExpirationTTL is thread-safe way of getting custom token bucket expiration TTL.\n+// GetTokenBucketExpirationTTL is thread-safe way of getting custom token bucket expiration TTL.\n func (l *Limiter) GetTokenBucketExpirationTTL() time.Duration {\n \tl.RLock()\n \tdefer l.RUnlock()\n@@ -150,7 +144,23 @@ func (l *Limiter) GetHeaderEntryExpirationTTL() time.Duration {\n \treturn l.headerEntryExpirationTTL\n }\n \n-// SetMax is thread-safe way of setting maximum number of requests to limit per duration.\n+// SetContextValueEntryExpirationTTL is thread-safe way of setting custom Context value expiration TTL.\n+func (l *Limiter) SetContextValueEntryExpirationTTL(ttl time.Duration) *Limiter {\n+\tl.Lock()\n+\tl.contextEntryExpirationTTL = ttl\n+\tl.Unlock()\n+\n+\treturn l\n+}\n+\n+// GetContextValueEntryExpirationTTL is thread-safe way of getting custom Context value expiration TTL.\n+func (l *Limiter) GetContextValueEntryExpirationTTL() time.Duration {\n+\tl.RLock()\n+\tdefer l.RUnlock()\n+\treturn l.contextEntryExpirationTTL\n+}\n+\n+// SetMax is thread-safe way of setting maximum number of requests to limit per second.\n func (l *Limiter) SetMax(max float64) *Limiter {\n \tl.Lock()\n \tl.max = max\n@@ -159,7 +169,7 @@ func (l *Limiter) SetMax(max float64) *Limiter {\n \treturn l\n }\n \n-// GetMax is thread-safe way of getting maximum number of requests to limit per duration.\n+// GetMax is thread-safe way of getting maximum number of requests to limit per second.\n func (l *Limiter) GetMax() float64 {\n \tl.RLock()\n \tdefer l.RUnlock()\n@@ -315,20 +325,13 @@ func (l *Limiter) SetBasicAuthUsers(basicAuthUsers []string) *Limiter {\n \n // GetBasicAuthUsers is thread-safe way of getting list of basic auth usernames to limit.\n func (l *Limiter) GetBasicAuthUsers() []string {\n-\tasMap := l.basicAuthUsers.Items()\n-\n-\tvar basicAuthUsers []string\n-\tfor basicAuthUser, _ := range asMap {\n-\t\tbasicAuthUsers = append(basicAuthUsers, basicAuthUser)\n-\t}\n-\n-\treturn basicAuthUsers\n+\treturn l.basicAuthUsers.Keys()\n }\n \n // RemoveBasicAuthUsers is thread-safe way of removing basic auth usernames from existing list.\n func (l *Limiter) RemoveBasicAuthUsers(basicAuthUsers []string) *Limiter {\n \tfor _, toBeRemoved := range basicAuthUsers {\n-\t\tl.basicAuthUsers.Delete(toBeRemoved)\n+\t\tl.basicAuthUsers.Invalidate(toBeRemoved)\n \t}\n \n \treturn l\n@@ -337,7 +340,7 @@ func (l *Limiter) RemoveBasicAuthUsers(basicAuthUsers []string) *Limiter {\n // SetHeaders is thread-safe way of setting map of HTTP headers to limit.\n func (l *Limiter) SetHeaders(headers map[string][]string) *Limiter {\n \tif l.headers == nil {\n-\t\tl.headers = make(map[string]*gocache.Cache)\n+\t\tl.headers = make(map[string]cache.Cache)\n \t}\n \n \tfor header, entries := range headers {\n@@ -355,13 +358,7 @@ func (l *Limiter) GetHeaders() map[string][]string {\n \tdefer l.RUnlock()\n \n \tfor header, entriesAsGoCache := range l.headers {\n-\t\tentries := make([]string, 0)\n-\n-\t\tfor entry, _ := range entriesAsGoCache.Items() {\n-\t\t\tentries = append(entries, entry)\n-\t\t}\n-\n-\t\tresults[header] = entries\n+\t\tresults[header] = entriesAsGoCache.Keys()\n \t}\n \n \treturn results\n@@ -379,7 +376,7 @@ func (l *Limiter) SetHeader(header string, entries []string) *Limiter {\n \t}\n \n \tif !found {\n-\t\texisting = gocache.New(ttl, l.generalExpirableOptions.ExpireJobInterval)\n+\t\texisting, _ = cache.NewCache(cache.TTL(ttl))\n \t}\n \n \tfor _, entry := range entries {\n@@ -399,14 +396,7 @@ func (l *Limiter) GetHeader(header string) []string {\n \tentriesAsGoCache := l.headers[header]\n \tl.RUnlock()\n \n-\tentriesAsMap := entriesAsGoCache.Items()\n-\tentries := make([]string, 0)\n-\n-\tfor entry, _ := range entriesAsMap {\n-\t\tentries = append(entries, entry)\n-\t}\n-\n-\treturn entries\n+\treturn entriesAsGoCache.Keys()\n }\n \n // RemoveHeader is thread-safe way of removing entries of 1 HTTP header.\n@@ -417,13 +407,13 @@ func (l *Limiter) RemoveHeader(header string) *Limiter {\n \t}\n \n \tl.Lock()\n-\tl.headers[header] = gocache.New(ttl, l.generalExpirableOptions.ExpireJobInterval)\n+\tl.headers[header], _ = cache.NewCache(cache.TTL(ttl))\n \tl.Unlock()\n \n \treturn l\n }\n \n-// RemoveHeaderEntries is thread-safe way of adding new entries to 1 HTTP header rule.\n+// RemoveHeaderEntries is thread-safe way of removing new entries to 1 HTTP header rule.\n func (l *Limiter) RemoveHeaderEntries(header string, entriesForRemoval []string) *Limiter {\n \tl.RLock()\n \tentries, found := l.headers[header]\n@@ -434,7 +424,100 @@ func (l *Limiter) RemoveHeaderEntries(header string, entriesForRemoval []string)\n \t}\n \n \tfor _, toBeRemoved := range entriesForRemoval {\n-\t\tentries.Delete(toBeRemoved)\n+\t\tentries.Invalidate(toBeRemoved)\n+\t}\n+\n+\treturn l\n+}\n+\n+// SetContextValues is thread-safe way of setting map of HTTP headers to limit.\n+func (l *Limiter) SetContextValues(contextValues map[string][]string) *Limiter {\n+\tif l.contextValues == nil {\n+\t\tl.contextValues = make(map[string]cache.Cache)\n+\t}\n+\n+\tfor contextValue, entries := range contextValues {\n+\t\tl.SetContextValue(contextValue, entries)\n+\t}\n+\n+\treturn l\n+}\n+\n+// GetContextValues is thread-safe way of getting a map of Context values to limit.\n+func (l *Limiter) GetContextValues() map[string][]string {\n+\tresults := make(map[string][]string)\n+\n+\tl.RLock()\n+\tdefer l.RUnlock()\n+\n+\tfor contextValue, entriesAsGoCache := range l.contextValues {\n+\t\tresults[contextValue] = entriesAsGoCache.Keys()\n+\t}\n+\n+\treturn results\n+}\n+\n+// SetContextValue is thread-safe way of setting entries of 1 Context value.\n+func (l *Limiter) SetContextValue(contextValue string, entries []string) *Limiter {\n+\tl.RLock()\n+\texisting, found := l.contextValues[contextValue]\n+\tl.RUnlock()\n+\n+\tttl := l.GetContextValueEntryExpirationTTL()\n+\tif ttl <= 0 {\n+\t\tttl = l.generalExpirableOptions.DefaultExpirationTTL\n+\t}\n+\n+\tif !found {\n+\t\texisting, _ = cache.NewCache(cache.TTL(ttl))\n+\t}\n+\n+\tfor _, entry := range entries {\n+\t\texisting.Set(entry, true, ttl)\n+\t}\n+\n+\tl.Lock()\n+\tl.contextValues[contextValue] = existing\n+\tl.Unlock()\n+\n+\treturn l\n+}\n+\n+// GetContextValue is thread-safe way of getting 1 Context value entry.\n+func (l *Limiter) GetContextValue(contextValue string) []string {\n+\tl.RLock()\n+\tentriesAsGoCache := l.contextValues[contextValue]\n+\tl.RUnlock()\n+\n+\treturn entriesAsGoCache.Keys()\n+}\n+\n+// RemoveContextValue is thread-safe way of removing entries of 1 Context value.\n+func (l *Limiter) RemoveContextValue(contextValue string) *Limiter {\n+\tttl := l.GetContextValueEntryExpirationTTL()\n+\tif ttl <= 0 {\n+\t\tttl = l.generalExpirableOptions.DefaultExpirationTTL\n+\t}\n+\n+\tl.Lock()\n+\tl.contextValues[contextValue], _ = cache.NewCache(cache.TTL(ttl))\n+\tl.Unlock()\n+\n+\treturn l\n+}\n+\n+// RemoveContextValuesEntries is thread-safe way of removing entries to a ContextValue.\n+func (l *Limiter) RemoveContextValuesEntries(contextValue string, entriesForRemoval []string) *Limiter {\n+\tl.RLock()\n+\tentries, found := l.contextValues[contextValue]\n+\tl.RUnlock()\n+\n+\tif !found {\n+\t\treturn l\n+\t}\n+\n+\tfor _, toBeRemoved := range entriesForRemoval {\n+\t\tentries.Invalidate(toBeRemoved)\n \t}\n \n \treturn l",
      "previous_filename": "backend/vendor/github.com/didip/tollbooth/limiter/limiter.go"
    },
    {
      "sha": "e5f537b5317628c0e7cee08064d46498285c357c",
      "filename": "backend/vendor/github.com/didip/tollbooth/v6/limiter/limiter_options.go",
      "status": "renamed",
      "additions": 2,
      "deletions": 0,
      "changes": 2,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/didip/tollbooth/v6/limiter/limiter_options.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/didip/tollbooth/v6/limiter/limiter_options.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/github.com/didip/tollbooth/v6/limiter/limiter_options.go?ref=890d7154e777d0673defcd11db3ef792473d68c9",
      "patch": "@@ -4,9 +4,11 @@ import (\n \t\"time\"\n )\n \n+// ExpirableOptions are options used for new limiter creation\n type ExpirableOptions struct {\n \tDefaultExpirationTTL time.Duration\n \n \t// How frequently expire job triggers\n+\t// Deprecated: not used anymore\n \tExpireJobInterval time.Duration\n }",
      "previous_filename": "backend/vendor/github.com/didip/tollbooth/limiter/limiter_options.go"
    },
    {
      "sha": "38ce09d1cd0923d998c96a0536d74025835a6253",
      "filename": "backend/vendor/github.com/didip/tollbooth/v6/tollbooth.go",
      "status": "renamed",
      "additions": 61,
      "deletions": 76,
      "changes": 137,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/didip/tollbooth/v6/tollbooth.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/didip/tollbooth/v6/tollbooth.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/github.com/didip/tollbooth/v6/tollbooth.go?ref=890d7154e777d0673defcd11db3ef792473d68c9",
      "patch": "@@ -2,15 +2,14 @@\n package tollbooth\n \n import (\n-\t\"net/http\"\n-\t\"strings\"\n-\n \t\"fmt\"\n \t\"math\"\n+\t\"net/http\"\n+\t\"strings\"\n \n-\t\"github.com/didip/tollbooth/errors\"\n-\t\"github.com/didip/tollbooth/libstring\"\n-\t\"github.com/didip/tollbooth/limiter\"\n+\t\"github.com/didip/tollbooth/v6/errors\"\n+\t\"github.com/didip/tollbooth/v6/libstring\"\n+\t\"github.com/didip/tollbooth/v6/limiter\"\n )\n \n // setResponseHeaders configures X-Rate-Limit-Limit and X-Rate-Limit-Duration\n@@ -23,7 +22,10 @@ func setResponseHeaders(lmt *limiter.Limiter, w http.ResponseWriter, r *http.Req\n \n // NewLimiter is a convenience function to limiter.New.\n func NewLimiter(max float64, tbOptions *limiter.ExpirableOptions) *limiter.Limiter {\n-\treturn limiter.New(tbOptions).SetMax(max).SetBurst(int(math.Max(1, max)))\n+\treturn limiter.New(tbOptions).\n+\t\tSetMax(max).\n+\t\tSetBurst(int(math.Max(1, max))).\n+\t\tSetIPLookups([]string{\"X-Forwarded-For\", \"X-Real-IP\", \"RemoteAddr\"})\n }\n \n // LimitByKeys keeps track number of request made by keys separated by pipe.\n@@ -49,99 +51,82 @@ func BuildKeys(lmt *limiter.Limiter, r *http.Request) [][]string {\n \n \tlmtMethods := lmt.GetMethods()\n \tlmtHeaders := lmt.GetHeaders()\n+\tlmtContextValues := lmt.GetContextValues()\n \tlmtBasicAuthUsers := lmt.GetBasicAuthUsers()\n \n \tlmtHeadersIsSet := len(lmtHeaders) > 0\n+\tlmtContextValuesIsSet := len(lmtContextValues) > 0\n \tlmtBasicAuthUsersIsSet := len(lmtBasicAuthUsers) > 0\n \n-\tif lmtMethods != nil && lmtHeadersIsSet && lmtBasicAuthUsersIsSet {\n-\t\t// Limit by HTTP methods and HTTP headers+values and Basic Auth credentials.\n-\t\tif libstring.StringInSlice(lmtMethods, r.Method) {\n-\t\t\tfor headerKey, headerValues := range lmtHeaders {\n-\t\t\t\tif (headerValues == nil || len(headerValues) <= 0) && r.Header.Get(headerKey) != \"\" {\n-\t\t\t\t\t// If header values are empty, rate-limit all request containing headerKey.\n-\t\t\t\t\tusername, _, ok := r.BasicAuth()\n-\t\t\t\t\tif ok && libstring.StringInSlice(lmtBasicAuthUsers, username) {\n-\t\t\t\t\t\tsliceKeys = append(sliceKeys, []string{remoteIP, path, r.Method, headerKey, r.Header.Get(headerKey), username})\n-\t\t\t\t\t}\n+\tmethod := \"\"\n+\tif lmtMethods != nil && libstring.StringInSlice(lmtMethods, r.Method) {\n+\t\tmethod = r.Method\n+\t}\n \n-\t\t\t\t} else if len(headerValues) > 0 && r.Header.Get(headerKey) != \"\" {\n-\t\t\t\t\t// If header values are not empty, rate-limit all request with headerKey and headerValues.\n-\t\t\t\t\tfor _, headerValue := range headerValues {\n-\t\t\t\t\t\tif r.Header.Get(headerKey) == headerValue {\n-\t\t\t\t\t\t\tusername, _, ok := r.BasicAuth()\n-\t\t\t\t\t\t\tif ok && libstring.StringInSlice(lmtBasicAuthUsers, username) {\n-\t\t\t\t\t\t\t\tsliceKeys = append(sliceKeys, []string{remoteIP, path, r.Method, headerKey, headerValue, username})\n-\t\t\t\t\t\t\t}\n-\t\t\t\t\t\t\tbreak\n-\t\t\t\t\t\t}\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t}\n+\tusernameToLimit := \"\"\n+\tif lmtBasicAuthUsersIsSet {\n+\t\tusername, _, ok := r.BasicAuth()\n+\t\tif ok && libstring.StringInSlice(lmtBasicAuthUsers, username) {\n+\t\t\tusernameToLimit = username\n \t\t}\n+\t}\n+\n+\theaderValuesToLimit := [][]string{}\n+\tif lmtHeadersIsSet {\n+\t\tfor headerKey, headerValues := range lmtHeaders {\n+\t\t\treqHeaderValue := r.Header.Get(headerKey)\n+\t\t\tif reqHeaderValue == \"\" {\n+\t\t\t\tcontinue\n+\t\t\t}\n+\n+\t\t\tif len(headerValues) == 0 {\n+\t\t\t\t// If header values are empty, rate-limit all request containing headerKey.\n+\t\t\t\theaderValuesToLimit = append(headerValuesToLimit, []string{headerKey, reqHeaderValue})\n \n-\t} else if lmtMethods != nil && lmtHeadersIsSet {\n-\t\t// Limit by HTTP methods and HTTP headers+values.\n-\t\tif libstring.StringInSlice(lmtMethods, r.Method) {\n-\t\t\tfor headerKey, headerValues := range lmtHeaders {\n-\t\t\t\tif (headerValues == nil || len(headerValues) <= 0) && r.Header.Get(headerKey) != \"\" {\n-\t\t\t\t\t// If header values are empty, rate-limit all request with headerKey.\n-\t\t\t\t\tsliceKeys = append(sliceKeys, []string{remoteIP, path, r.Method, headerKey, r.Header.Get(headerKey)})\n-\n-\t\t\t\t} else if len(headerValues) > 0 && r.Header.Get(headerKey) != \"\" {\n-\t\t\t\t\t// We are only limiting if request's header value is defined inside `headerValues`.\n-\t\t\t\t\tfor _, headerValue := range headerValues {\n-\t\t\t\t\t\tif r.Header.Get(headerKey) == headerValue {\n-\t\t\t\t\t\t\tsliceKeys = append(sliceKeys, []string{remoteIP, path, r.Method, headerKey, headerValue})\n-\t\t\t\t\t\t\tbreak\n-\t\t\t\t\t\t}\n+\t\t\t} else {\n+\t\t\t\t// If header values are not empty, rate-limit all request with headerKey and headerValues.\n+\t\t\t\tfor _, headerValue := range headerValues {\n+\t\t\t\t\tif r.Header.Get(headerKey) == headerValue {\n+\t\t\t\t\t\theaderValuesToLimit = append(headerValuesToLimit, []string{headerKey, headerValue})\n+\t\t\t\t\t\tbreak\n \t\t\t\t\t}\n \t\t\t\t}\n \t\t\t}\n \t\t}\n+\t} else {\n+\t\theaderValuesToLimit = append(headerValuesToLimit, []string{\"\", \"\"})\n+\t}\n \n-\t} else if lmtMethods != nil && lmtBasicAuthUsersIsSet {\n-\t\t// Limit by HTTP methods and Basic Auth credentials.\n-\t\tif libstring.StringInSlice(lmtMethods, r.Method) {\n-\t\t\tusername, _, ok := r.BasicAuth()\n-\t\t\tif ok && libstring.StringInSlice(lmtBasicAuthUsers, username) {\n-\t\t\t\tsliceKeys = append(sliceKeys, []string{remoteIP, path, r.Method, username})\n+\tcontextValuesToLimit := [][]string{}\n+\tif lmtContextValuesIsSet {\n+\t\tfor contextKey, contextValues := range lmtContextValues {\n+\t\t\treqContextValue := fmt.Sprintf(\"%v\", r.Context().Value(contextKey))\n+\t\t\tif reqContextValue == \"\" {\n+\t\t\t\tcontinue\n \t\t\t}\n-\t\t}\n \n-\t} else if lmtMethods != nil {\n-\t\t// Limit by HTTP methods.\n-\t\tif libstring.StringInSlice(lmtMethods, r.Method) {\n-\t\t\tsliceKeys = append(sliceKeys, []string{remoteIP, path, r.Method})\n-\t\t}\n+\t\t\tif len(contextValues) == 0 {\n+\t\t\t\t// If header values are empty, rate-limit all request containing headerKey.\n+\t\t\t\tcontextValuesToLimit = append(contextValuesToLimit, []string{contextKey, reqContextValue})\n \n-\t} else if lmtHeadersIsSet {\n-\t\t// Limit by HTTP headers+values.\n-\t\tfor headerKey, headerValues := range lmtHeaders {\n-\t\t\tif (headerValues == nil || len(headerValues) <= 0) && r.Header.Get(headerKey) != \"\" {\n-\t\t\t\t// If header values are empty, rate-limit all request with headerKey.\n-\t\t\t\tsliceKeys = append(sliceKeys, []string{remoteIP, path, headerKey, r.Header.Get(headerKey)})\n-\n-\t\t\t} else if len(headerValues) > 0 && r.Header.Get(headerKey) != \"\" {\n+\t\t\t} else {\n \t\t\t\t// If header values are not empty, rate-limit all request with headerKey and headerValues.\n-\t\t\t\tfor _, headerValue := range headerValues {\n-\t\t\t\t\tif r.Header.Get(headerKey) == headerValue {\n-\t\t\t\t\t\tsliceKeys = append(sliceKeys, []string{remoteIP, path, headerKey, headerValue})\n+\t\t\t\tfor _, contextValue := range contextValues {\n+\t\t\t\t\tif reqContextValue == contextValue {\n+\t\t\t\t\t\tcontextValuesToLimit = append(contextValuesToLimit, []string{contextKey, contextValue})\n \t\t\t\t\t\tbreak\n \t\t\t\t\t}\n \t\t\t\t}\n \t\t\t}\n \t\t}\n+\t} else {\n+\t\tcontextValuesToLimit = append(contextValuesToLimit, []string{\"\", \"\"})\n+\t}\n \n-\t} else if lmtBasicAuthUsersIsSet {\n-\t\t// Limit by Basic Auth credentials.\n-\t\tusername, _, ok := r.BasicAuth()\n-\t\tif ok && libstring.StringInSlice(lmtBasicAuthUsers, username) {\n-\t\t\tsliceKeys = append(sliceKeys, []string{remoteIP, path, username})\n+\tfor _, header := range headerValuesToLimit {\n+\t\tfor _, contextValue := range contextValuesToLimit {\n+\t\t\tsliceKeys = append(sliceKeys, []string{remoteIP, path, method, header[0], header[1], contextValue[0], contextValue[1], usernameToLimit})\n \t\t}\n-\t} else {\n-\t\t// Default: Limit by remoteIP and path.\n-\t\tsliceKeys = append(sliceKeys, []string{remoteIP, path})\n \t}\n \n \treturn sliceKeys",
      "previous_filename": "backend/vendor/github.com/didip/tollbooth/tollbooth.go"
    },
    {
      "sha": "d1ba02b10138d6ad6082c95d2375333929cbb7bf",
      "filename": "backend/vendor/github.com/didip/tollbooth_chi/README.md",
      "status": "modified",
      "additions": 1,
      "deletions": 1,
      "changes": 2,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/didip/tollbooth_chi/README.md",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/didip/tollbooth_chi/README.md",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/github.com/didip/tollbooth_chi/README.md?ref=890d7154e777d0673defcd11db3ef792473d68c9",
      "patch": "@@ -18,7 +18,7 @@ import (\n \n func main() {\n     // Create a limiter struct.\n-    limiter := tollbooth.NewLimiter(1, time.Second, nil)\n+    limiter := tollbooth.NewLimiter(1, nil)\n \n     r := chi.NewRouter()\n "
    },
    {
      "sha": "d4333801022f5ea2182c1dd84719e88235943c9d",
      "filename": "backend/vendor/github.com/didip/tollbooth_chi/tollbooth_chi.go",
      "status": "modified",
      "additions": 2,
      "deletions": 2,
      "changes": 4,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/didip/tollbooth_chi/tollbooth_chi.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/didip/tollbooth_chi/tollbooth_chi.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/github.com/didip/tollbooth_chi/tollbooth_chi.go?ref=890d7154e777d0673defcd11db3ef792473d68c9",
      "patch": "@@ -1,8 +1,8 @@\n package tollbooth_chi\n \n import (\n-\t\"github.com/didip/tollbooth\"\n-\t\"github.com/didip/tollbooth/limiter\"\n+\t\"github.com/didip/tollbooth/v6\"\n+\t\"github.com/didip/tollbooth/v6/limiter\"\n \t\"net/http\"\n )\n "
    },
    {
      "sha": "f1e277aa596529951cabf2a18747f8b1cedf0f26",
      "filename": "backend/vendor/github.com/go-chi/chi/CHANGELOG.md",
      "status": "modified",
      "additions": 9,
      "deletions": 1,
      "changes": 10,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/go-chi/chi/CHANGELOG.md",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/go-chi/chi/CHANGELOG.md",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/github.com/go-chi/chi/CHANGELOG.md?ref=890d7154e777d0673defcd11db3ef792473d68c9",
      "patch": "@@ -1,12 +1,20 @@\n # Changelog\n \n+## v4.1.1 (2020-04-16)\n+\n+- fix for issue https://github.com/go-chi/chi/issues/411 which allows for overlapping regexp\n+  route to the correct handler through a recursive tree search, thanks to @Jahaja for the PR/fix!\n+- new middleware.RouteHeaders as a simple router for request headers with wildcard support\n+- History of changes: see https://github.com/go-chi/chi/compare/v4.1.0...v4.1.1\n+\n+\n ## v4.1.0 (2020-04-1)\n \n - middleware.LogEntry: Write method on interface now passes the response header\n   and an extra interface type useful for custom logger implementations.\n - middleware.WrapResponseWriter: minor fix\n - middleware.Recoverer: a bit prettier\n-- History of changes: see https://github.com/go-chi/chi/compare/v4.0.3...v4.1.0\n+- History of changes: see https://github.com/go-chi/chi/compare/v4.0.4...v4.1.0\n \n \n ## v4.0.4 (2020-03-24)"
    },
    {
      "sha": "15a36fe5bd7520af8e91cd3dc7b42515f822e294",
      "filename": "backend/vendor/github.com/go-chi/chi/README.md",
      "status": "modified",
      "additions": 4,
      "deletions": 2,
      "changes": 6,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/go-chi/chi/README.md",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/go-chi/chi/README.md",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/github.com/go-chi/chi/README.md?ref=890d7154e777d0673defcd11db3ef792473d68c9",
      "patch": "@@ -46,11 +46,14 @@ package main\n \n import (\n \t\"net/http\"\n+\n \t\"github.com/go-chi/chi\"\n+\t\"github.com/go-chi/chi/middleware\"\n )\n \n func main() {\n \tr := chi.NewRouter()\n+\tr.Use(middleware.Logger)\n \tr.Get(\"/\", func(w http.ResponseWriter, r *http.Request) {\n \t\tw.Write([]byte(\"welcome\"))\n \t})\n@@ -346,8 +349,7 @@ Please see https://github.com/go-chi for additional packages.\n | [jwtauth](https://github.com/go-chi/jwtauth)       | JWT authentication                                          |\n | [hostrouter](https://github.com/go-chi/hostrouter) | Domain/host based request routing                           |\n | [httpcoala](https://github.com/go-chi/httpcoala)   | HTTP request coalescer                                      |\n-| [chi-authz](https://github.com/casbin/chi-authz)   | Request ACL via https://github.com/hsluoyz/casbin           |\n-| [phi](https://github.com/fate-lovely/phi)          | Port chi to [fasthttp](https://github.com/valyala/fasthttp) |\n+| [httplog](https://github.com/goware/httplog)       | Small but powerful structured HTTP request logging          |\n --------------------------------------------------------------------------------------------------------------------\n \n please [submit a PR](./CONTRIBUTING.md) if you'd like to include a link to a chi-compatible middleware"
    },
    {
      "sha": "cc371e00a82e84bfb38982796b736ca42fd9608d",
      "filename": "backend/vendor/github.com/go-chi/chi/middleware/middleware.go",
      "status": "modified",
      "additions": 11,
      "deletions": 0,
      "changes": 11,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/go-chi/chi/middleware/middleware.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/go-chi/chi/middleware/middleware.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/github.com/go-chi/chi/middleware/middleware.go?ref=890d7154e777d0673defcd11db3ef792473d68c9",
      "patch": "@@ -1,5 +1,16 @@\n package middleware\n \n+import \"net/http\"\n+\n+// New will create a new middleware handler from a http.Handler.\n+func New(h http.Handler) func(next http.Handler) http.Handler {\n+\treturn func(next http.Handler) http.Handler {\n+\t\treturn http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {\n+\t\t\th.ServeHTTP(w, r)\n+\t\t})\n+\t}\n+}\n+\n // contextKey is a value for use with context.WithValue. It's used as\n // a pointer so it fits in an interface{} without allocation. This technique\n // for defining context keys was copied from Go 1.7's new use of context in net/http."
    },
    {
      "sha": "7ee30c877325a2fd84ca37ae22cf91bfc8f4e192",
      "filename": "backend/vendor/github.com/go-chi/chi/middleware/route_headers.go",
      "status": "added",
      "additions": 160,
      "deletions": 0,
      "changes": 160,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/go-chi/chi/middleware/route_headers.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/go-chi/chi/middleware/route_headers.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/github.com/go-chi/chi/middleware/route_headers.go?ref=890d7154e777d0673defcd11db3ef792473d68c9",
      "patch": "@@ -0,0 +1,160 @@\n+package middleware\n+\n+import (\n+\t\"net/http\"\n+\t\"strings\"\n+)\n+\n+// RouteHeaders is a neat little header-based router that allows you to direct\n+// the flow of a request through a middleware stack based on a request header.\n+//\n+// For example, lets say you'd like to setup multiple routers depending on the\n+// request Host header, you could then do something as so:\n+//\n+// r := chi.NewRouter()\n+// rSubdomain := chi.NewRouter()\n+//\n+// r.Use(middleware.RouteHeaders().\n+//   Route(\"Host\", \"example.com\", middleware.New(r)).\n+//   Route(\"Host\", \"*.example.com\", middleware.New(rSubdomain)).\n+//   Handler)\n+//\n+// r.Get(\"/\", h)\n+// rSubdomain.Get(\"/\", h2)\n+//\n+//\n+// Another example, imagine you want to setup multiple CORS handlers, where for\n+// your origin servers you allow authorized requests, but for third-party public\n+// requests, authorization is disabled.\n+//\n+// r := chi.NewRouter()\n+//\n+// r.Use(middleware.RouteHeaders().\n+//   Route(\"Origin\", \"https://app.skyweaver.net\", cors.Handler(cors.Options{\n+// \t   AllowedOrigins:   []string{\"https://api.skyweaver.net\"},\n+// \t   AllowedMethods:   []string{\"GET\", \"POST\", \"PUT\", \"DELETE\", \"OPTIONS\"},\n+// \t   AllowedHeaders:   []string{\"Accept\", \"Authorization\", \"Content-Type\"},\n+// \t   AllowCredentials: true, // <----------<<< allow credentials\n+//   })).\n+//   Route(\"Origin\", \"*\", cors.Handler(cors.Options{\n+// \t   AllowedOrigins:   []string{\"*\"},\n+// \t   AllowedMethods:   []string{\"GET\", \"POST\", \"PUT\", \"DELETE\", \"OPTIONS\"},\n+// \t   AllowedHeaders:   []string{\"Accept\", \"Content-Type\"},\n+// \t   AllowCredentials: false, // <----------<<< do not allow credentials\n+//   })).\n+//   Handler)\n+//\n+func RouteHeaders() HeaderRouter {\n+\treturn HeaderRouter{}\n+}\n+\n+type HeaderRouter map[string][]HeaderRoute\n+\n+func (hr HeaderRouter) Route(header string, match string, middlewareHandler func(next http.Handler) http.Handler) HeaderRouter {\n+\theader = strings.ToLower(header)\n+\tk := hr[header]\n+\tif k == nil {\n+\t\thr[header] = []HeaderRoute{}\n+\t}\n+\thr[header] = append(hr[header], HeaderRoute{MatchOne: NewPattern(match), Middleware: middlewareHandler})\n+\treturn hr\n+}\n+\n+func (hr HeaderRouter) RouteAny(header string, match []string, middlewareHandler func(next http.Handler) http.Handler) HeaderRouter {\n+\theader = strings.ToLower(header)\n+\tk := hr[header]\n+\tif k == nil {\n+\t\thr[header] = []HeaderRoute{}\n+\t}\n+\tpatterns := []Pattern{}\n+\tfor _, m := range match {\n+\t\tpatterns = append(patterns, NewPattern(m))\n+\t}\n+\thr[header] = append(hr[header], HeaderRoute{MatchAny: patterns, Middleware: middlewareHandler})\n+\treturn hr\n+}\n+\n+func (hr HeaderRouter) RouteDefault(handler func(next http.Handler) http.Handler) HeaderRouter {\n+\thr[\"*\"] = []HeaderRoute{{Middleware: handler}}\n+\treturn hr\n+}\n+\n+func (hr HeaderRouter) Handler(next http.Handler) http.Handler {\n+\treturn http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {\n+\t\tif len(hr) == 0 {\n+\t\t\t// skip if no routes set\n+\t\t\tnext.ServeHTTP(w, r)\n+\t\t}\n+\n+\t\t// find first matching header route, and continue\n+\t\tfor header, matchers := range hr {\n+\t\t\theaderValue := r.Header.Get(header)\n+\t\t\tif headerValue == \"\" {\n+\t\t\t\tcontinue\n+\t\t\t}\n+\t\t\theaderValue = strings.ToLower(headerValue)\n+\t\t\tfor _, matcher := range matchers {\n+\t\t\t\tif matcher.IsMatch(headerValue) {\n+\t\t\t\t\tmatcher.Middleware(next).ServeHTTP(w, r)\n+\t\t\t\t\treturn\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\n+\t\t// if no match, check for \"*\" default route\n+\t\tmatcher, ok := hr[\"*\"]\n+\t\tif !ok || matcher[0].Middleware == nil {\n+\t\t\tnext.ServeHTTP(w, r)\n+\t\t\treturn\n+\t\t}\n+\t\tmatcher[0].Middleware(next).ServeHTTP(w, r)\n+\t})\n+}\n+\n+type HeaderRoute struct {\n+\tMatchAny   []Pattern\n+\tMatchOne   Pattern\n+\tMiddleware func(next http.Handler) http.Handler\n+}\n+\n+func (r HeaderRoute) IsMatch(value string) bool {\n+\tif len(r.MatchAny) > 0 {\n+\t\tfor _, m := range r.MatchAny {\n+\t\t\tif m.Match(value) {\n+\t\t\t\treturn true\n+\t\t\t}\n+\t\t}\n+\t} else if r.MatchOne.Match(value) {\n+\t\treturn true\n+\t}\n+\treturn false\n+}\n+\n+type Pattern struct {\n+\tprefix   string\n+\tsuffix   string\n+\twildcard bool\n+}\n+\n+func NewPattern(value string) Pattern {\n+\tp := Pattern{}\n+\tif i := strings.IndexByte(value, '*'); i >= 0 {\n+\t\tp.wildcard = true\n+\t\tp.prefix = value[0:i]\n+\t\tp.suffix = value[i+1:]\n+\t} else {\n+\t\tp.prefix = value\n+\t}\n+\treturn p\n+}\n+\n+func (p Pattern) Match(v string) bool {\n+\tif !p.wildcard {\n+\t\tif p.prefix == v {\n+\t\t\treturn true\n+\t\t} else {\n+\t\t\treturn false\n+\t\t}\n+\t}\n+\treturn len(v) >= len(p.prefix+p.suffix) && strings.HasPrefix(v, p.prefix) && strings.HasSuffix(v, p.suffix)\n+}"
    },
    {
      "sha": "a7e29c5b0b852022643bb67694637724b66ef010",
      "filename": "backend/vendor/github.com/go-chi/chi/tree.go",
      "status": "modified",
      "additions": 22,
      "deletions": 7,
      "changes": 29,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/go-chi/chi/tree.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/go-chi/chi/tree.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/github.com/go-chi/chi/tree.go?ref=890d7154e777d0673defcd11db3ef792473d68c9",
      "patch": "@@ -417,8 +417,6 @@ func (n *node) findRoute(rctx *Context, method methodTyp, path string) *node {\n \t\t\t\tcontinue\n \t\t\t}\n \n-\t\t\tfound := false\n-\n \t\t\t// serially loop through each node grouped by the tail delimiter\n \t\t\tfor idx := 0; idx < len(nds); idx++ {\n \t\t\t\txn = nds[idx]\n@@ -443,16 +441,33 @@ func (n *node) findRoute(rctx *Context, method methodTyp, path string) *node {\n \t\t\t\t\tcontinue\n \t\t\t\t}\n \n+\t\t\t\tprevlen := len(rctx.routeParams.Values)\n \t\t\t\trctx.routeParams.Values = append(rctx.routeParams.Values, xsearch[:p])\n \t\t\t\txsearch = xsearch[p:]\n-\t\t\t\tfound = true\n-\t\t\t\tbreak\n-\t\t\t}\n \n-\t\t\tif !found {\n-\t\t\t\trctx.routeParams.Values = append(rctx.routeParams.Values, \"\")\n+\t\t\t\tif len(xsearch) == 0 {\n+\t\t\t\t\tif xn.isLeaf() {\n+\t\t\t\t\t\th := xn.endpoints[method]\n+\t\t\t\t\t\tif h != nil && h.handler != nil {\n+\t\t\t\t\t\t\trctx.routeParams.Keys = append(rctx.routeParams.Keys, h.paramKeys...)\n+\t\t\t\t\t\t\treturn xn\n+\t\t\t\t\t\t}\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\n+\t\t\t\t// recursively find the next node on this branch\n+\t\t\t\tfin := xn.findRoute(rctx, method, xsearch)\n+\t\t\t\tif fin != nil {\n+\t\t\t\t\treturn fin\n+\t\t\t\t}\n+\n+\t\t\t\t// not found on this branch, reset vars\n+\t\t\t\trctx.routeParams.Values = rctx.routeParams.Values[:prevlen]\n+\t\t\t\txsearch = search\n \t\t\t}\n \n+\t\t\trctx.routeParams.Values = append(rctx.routeParams.Values, \"\")\n+\n \t\tdefault:\n \t\t\t// catch-all nodes\n \t\t\trctx.routeParams.Values = append(rctx.routeParams.Values, search)"
    },
    {
      "sha": "1164958a0cdaa5ecc56ad89ccd537f672dbcbb7b",
      "filename": "backend/vendor/github.com/go-pkgz/auth/.golangci.yml",
      "status": "modified",
      "additions": 16,
      "deletions": 14,
      "changes": 30,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/go-pkgz/auth/.golangci.yml",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/go-pkgz/auth/.golangci.yml",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/github.com/go-pkgz/auth/.golangci.yml?ref=890d7154e777d0673defcd11db3ef792473d68c9",
      "patch": "@@ -2,13 +2,11 @@ linters-settings:\n   govet:\n     check-shadowing: true\n   golint:\n-    min-confidence: 0.8\n+    min-confidence: 0\n   gocyclo:\n     min-complexity: 15\n   maligned:\n     suggest-new: true\n-  dupl:\n-    threshold: 100\n   goconst:\n     min-len: 2\n     min-occurrences: 2\n@@ -24,10 +22,8 @@ linters-settings:\n     disabled-checks:\n       - wrapperFunc\n       - hugeParam\n-      - rangeValCopy\n \n linters:\n-  disable-all: true\n   enable:\n     - megacheck\n     - golint\n@@ -37,6 +33,7 @@ linters:\n     - structcheck\n     - gas\n     - gocyclo\n+    - dupl\n     - misspell\n     - unparam\n     - varcheck\n@@ -46,27 +43,32 @@ linters:\n     - varcheck\n     - stylecheck\n     - gochecknoinits\n+    - scopelint\n     - gocritic\n-    - golint\n     - nakedret\n     - gosimple\n     - prealloc\n-\n   fast: false\n-\n+  disable-all: true\n \n run:\n-  # modules-download-mode: vendor\n+  output:\n+    format: tab\n   skip-dirs:\n     - vendor\n-  concurrency: 4\n \n issues:\n   exclude-rules:\n-    - text: \"weak cryptographic primitive\"\n+    - text: \"should have a package comment, unless it's in another file for this package\"\n+      linters:\n+        - golint\n+    - text: \"Blacklisted import `crypto/md5`: weak cryptographic primitive\"\n+      linters:\n+        - gosec\n+    - text: \"Blacklisted import `crypto/sha1`: weak cryptographic primitive\"\n+      linters:\n+        - gosec\n+    - text: \" Use of weak cryptographic primitive\"\n       linters:\n         - gosec\n   exclude-use-default: false\n-\n-service:\n-  golangci-lint-version: 1.19.x"
    },
    {
      "sha": "dfffb8cb94d59a3330f262933056562ef5a99312",
      "filename": "backend/vendor/github.com/go-pkgz/auth/README.md",
      "status": "modified",
      "additions": 27,
      "deletions": 32,
      "changes": 59,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/go-pkgz/auth/README.md",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/go-pkgz/auth/README.md",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/github.com/go-pkgz/auth/README.md?ref=890d7154e777d0673defcd11db3ef792473d68c9",
      "patch": "@@ -1,9 +1,7 @@\n-# auth - authentication via oauth2, direct and email \n+# auth - authentication via oauth2, direct and email\n [![Build Status](https://github.com/go-pkgz/auth/workflows/build/badge.svg)](https://github.com/go-pkgz/auth/actions) [![Coverage Status](https://coveralls.io/repos/github/go-pkgz/auth/badge.svg?branch=master)](https://coveralls.io/github/go-pkgz/auth?branch=master) [![godoc](https://godoc.org/github.com/go-pkgz/auth?status.svg)](https://pkg.go.dev/github.com/go-pkgz/auth?tab=doc)\n \n-\n-\n-This library provides \"social login\" with Github, Google, Facebook, Twitter and Yandex as well as custom auth providers and email verification.  \n+This library provides \"social login\" with Github, Google, Facebook, Twitter and Yandex as well as custom auth providers and email verification.\n \n - Multiple oauth2 providers can be used at the same time\n - Special `dev` provider allows local testing and development\n@@ -12,14 +10,14 @@ This library provides \"social login\" with Github, Google, Facebook, Twitter and\n - Direct authentication with user's provided credential checker\n - Verified authentication with user's provided sender (email, im, etc)\n - Custom oauth2 server and ability to use any third party provider\n-- Integrated avatar proxy with FS, boltdb and gridfs storage\n+- Integrated avatar proxy with an FS, boltdb and gridfs storage\n - Support of user-defined storage for avatars\n-- Identicon for default avatars  \n+- Identicon for default avatars\n - Black list with user-defined validator\n - Multiple aud (audience) supported\n - Secure key with customizable `SecretReader`\n - Ability to store an extra information to token and retrieve on login\n-- Pre-auth and post-auth hooks to handle custom use cases. \n+- Pre-auth and post-auth hooks to handle custom use cases.\n - Middleware for easy integration into http routers\n - Wrappers to extract user info from the request\n \n@@ -77,7 +75,7 @@ func main() {\n `github.com/go-pkgz/auth/middleware` provides ready-to-use middleware.\n \n - `middleware.Auth` - requires authenticated user\n-- `middleware.Admin` - requires authenticated and admin user\n+- `middleware.Admin` - requires authenticated admin user\n - `middleware.Trace` - doesn't require authenticated user, but adds user info to request\n \n ## Details\n@@ -86,7 +84,7 @@ Generally, adding support of `auth` includes a few relatively simple steps:\n \n 1. Setup `auth.Opts` structure with all parameters. Each of them [documented](https://github.com/go-pkgz/auth/blob/master/auth.go#L29) and most of parameters are optional and have sane defaults.\n 2. [Create](https://github.com/go-pkgz/auth/blob/master/auth.go#L56) the new `auth.Service` with provided options.\n-3. [Add all](https://github.com/go-pkgz/auth/blob/master/auth.go#L149) desirable authentication providers. Currently supported Github, Google, Facebook and Yandex \n+3. [Add all](https://github.com/go-pkgz/auth/blob/master/auth.go#L149) desirable authentication providers. Currently supported Github, Google, Facebook and Yandex\n 4. Retrieve [middleware](https://github.com/go-pkgz/auth/blob/master/auth.go#L144) and [http handlers](https://github.com/go-pkgz/auth/blob/master/auth.go#L105) from `auth.Service`\n 5. Wire auth and avatar handlers into http router as sub–routes.\n \n@@ -97,7 +95,7 @@ For the example above authentication handlers wired as `/auth` and provides:\n - `/auth/<provider>/login?site=<site_id>&from=<redirect_url>` - site_id used as `aud` claim for the token and can be processed by `SecretReader` to load/retrieve/define different secrets. redirect_url is the url to redirect after successful login.\n - `/avatar/<avatar_id>` - returns the avatar (image). Links to those pictures added into user info automatically, for details see \"Avatar proxy\"\n - `/auth/<provider>/logout` and `/auth/logout` - invalidate \"session\" by removing JWT cookie\n-- `/auth/list` - gives a json list of active providers \n+- `/auth/list` - gives a json list of active providers\n - `/auth/user` - returns `token.User` (json)\n \n ### User info\n@@ -114,8 +112,7 @@ It also has placeholders for fields application can populate with custom `token.\n - `IP`  - hash of user's IP address\n - `Email` - user's email\n - `Attributes` - map of string:any-value. To simplify management of this map some setters and getters provided, for example `users.StrAttr`, `user.SetBoolAttr` and so on. See [user.go](https://github.com/go-pkgz/auth/blob/master/token/user.go) for more details.\n- \n-   \n+\n ### Avatar proxy\n \n Direct links to avatars won't survive any real-life usage if they linked from a public page. For example, page [like this](https://remark42.com/demo/) may have hundreds of avatars and, most likely, will trigger throttling on provider's side. To eliminate such restriction `auth` library provides an automatic proxy\n@@ -134,30 +131,30 @@ Direct links to avatars won't survive any real-life usage if they linked from a\n         - boltdb - `bolt://tmp/avatars.bdb`\n         - mongo - `\"mongodb://127.0.0.1:27017/test?ava_db=db1&ava_coll=coll1`\n     - `AvatarRoutePath` - route prefix for direct links to proxied avatar. For example `/api/v1/avatars` will make full links like this - `http://example.com/api/v1/avatars/1234567890123.image`. The url will be stored in user's token and retrieved by middleware (see \"User Info\")\n-    - `AvatarResizeLimit` - size (in pixels) used to resize the avatar. Pls note - resize happens once as a part of `Put` call, i.e. on login. 0 size (default) disables resizing.      \n+    - `AvatarResizeLimit` - size (in pixels) used to resize the avatar. Pls note - resize happens once as a part of `Put` call, i.e. on login. 0 size (default) disables resizing.\n \n ### Direct authentication\n \n-In addition to oauth2 providers `auth.Service` allows to use direct user-defined authentication. This is done by adding direct provider with `auth.AddDirectProvider`. \n+In addition to oauth2 providers `auth.Service` allows to use direct user-defined authentication. This is done by adding direct provider with `auth.AddDirectProvider`.\n \n ```go\n \tservice.AddDirectProvider(\"local\", provider.CredCheckerFunc(func(user, password string) (ok bool, err error) {\n-\t\tok, err := checkUserSomehow(user, password) \n+\t\tok, err := checkUserSomehow(user, password)\n \t\treturn ok, err\n \t}))\n ```\n \n-Such provider acts like any other, i.e. will be registered as `/auth/local/login`. \n+Such provider acts like any other, i.e. will be registered as `/auth/local/login`.\n \n The API for this provider - `GET /auth/<name>/login?user=<user>&passwd=<password>&aud=<site_id>&session=[1|0]`\n \n-_note: password parameter doesn't have to be naked/real password and can be any kind of password hash prepared by caller._ \n+_note: password parameter doesn't have to be naked/real password and can be any kind of password hash prepared by caller._\n \n ### Verified authentication\n \n-Another non-oauth2 provider allowing user-confirmed authentication, for example by email or slack or telegram. This is \n+Another non-oauth2 provider allowing user-confirmed authentication, for example by email or slack or telegram. This is\n done by adding confirmed provider with `auth.AddVerifProvider`.\n- \n+\n ```go\n     msgTemplate := \"Confirmation email, token: {{.Token}}\"\n \tservice.AddVerifProvider(\"email\", msgTemplate, sender)\n@@ -178,7 +175,7 @@ type Sender interface {\n }\n ```\n \n-For convenience a functional wrapper `SenderFunc` provided. Email sender provided in `provider/sender` package and can be \n+For convenience a functional wrapper `SenderFunc` provided. Email sender provided in `provider/sender` package and can be\n used as `Sender`.\n \n The API for this provider:\n@@ -187,7 +184,7 @@ The API for this provider:\n  - `GET /auth/<name>/login?token=<conf.token>&sess=[1|0]` - authorize with confirmation token\n \n The provider acts like any other, i.e. will be registered as `/auth/email/login`.\n-  \n+\n ### Custom oauth2\n \n This provider brings two extra functions:\n@@ -224,7 +221,7 @@ In order to add a new oauth2 provider following input is required:\n \t})\n \t```\n 2.  Adds local oauth2 server user can fully customize. It uses [`gopkg.in/oauth2.v3`](https://github.com/go-oauth2/oauth2) library and example shows how [to initialize](https://github.com/go-pkgz/auth/blob/master/_example/main.go#L227) the server and [setup a provider](https://github.com/go-pkgz/auth/blob/master/_example/main.go#L100).\n-\t*  to start local oauth2 server following options are required: \n+\t*  to start local oauth2 server following options are required:\n \t\t* `URL` - url of oauth2 server with port\n \t\t* `WithLoginPage` - flag to define whether login page should be shown\n \t\t* `LoginPageHandler` - function to handle login request. If not specified default login page will be shown\n@@ -262,16 +259,16 @@ All of the interfaces above have corresponding Func adapters - `SecretFunc`, `Cl\n Restricting some users or some tokens is two step process:\n \n - `ClaimsUpdater` sets an attribute, like `blocked` (or `allowed`)\n-- `Validator` checks the attribute and returns true/false \n+- `Validator` checks the attribute and returns true/false\n \n-_This technic used in the [example](https://github.com/go-pkgz/auth/blob/master/_example/backend/main.go#L36) code_\n+_This technique used in the [example](https://github.com/go-pkgz/auth/blob/master/_example/main.go#L56) code_\n \n The process can be simplified by doing all checks directly in `Validator`, but depends on particular case such solution\n can be too expensive because `Validator` runs on each request as a part of auth middleware. In contrast, `ClaimsUpdater` called on token creation/refresh only.\n \n-### Multi-tenant services and support for different audiences \n+### Multi-tenant services and support for different audiences\n \n-For complex systems a single authenticator may serve multiple distinct subsystems or multiple set of independent users. For example some SaaS offerings may need to provide different authentications for different customers and prevent use of tokens/cookies made by another customer. \n+For complex systems a single authenticator may serve multiple distinct subsystems or multiple set of independent users. For example some SaaS offerings may need to provide different authentications for different customers and prevent use of tokens/cookies made by another customer.\n \n Such functionality can be implemented in 3 different ways:\n \n@@ -281,7 +278,6 @@ Such functionality can be implemented in 3 different ways:\n \n In order to allow `aud` support the list of allowed audiences should be passed in as `opts.Audiences` parameter. Non-empty value will trigger internal checks for token generation (will reject token creation for alien `aud`) as well as `Auth` middleware.\n \n-\n ### Dev provider\n \n Working with oauth2 providers can be a pain, especially during development phase. A special, development-only provider `dev` can make it less painful. This one can be registered directly, i.e. `service.AddProvider(\"dev\", \"\", \"\")` and should be activated like this:\n@@ -297,7 +293,7 @@ Working with oauth2 providers can be a pain, especially during development phase\n \t}()\n ```\n \n-It will run fake aouth2 \"server\" on port :8084 and user could login with any user name. See [example](https://github.com/go-pkgz/auth/blob/master/_example/backend/main.go) for more details. \n+It will run fake aouth2 \"server\" on port :8084 and user could login with any user name. See [example](https://github.com/go-pkgz/auth/blob/master/_example/main.go) for more details.\n \n _Warning: this is not the real oauth2 server but just a small fake thing for development and testing only. Don't use `dev` provider with any production code._\n \n@@ -311,8 +307,7 @@ In addition to the primary method (i.e. JWT cookie with XSRF header) there are t\n \n ### Logging\n \n-By default this library doesn't print anything to stdout/stderr, however user can pass a logger implementing `logger.L` interface with a single method `Logf(format string, args ...interface{})`. Functional adapter for this interface included as `logger.Func`. There are two predefined implementations in the `logger` package - `NoOp` (prints nothing, default) and `Std` wrapping `log.Printf` from stdlib.\n-\n+By default, this library doesn't print anything to stdout/stderr, however user can pass a logger implementing `logger.L` interface with a single method `Logf(format string, args ...interface{})`. Functional adapter for this interface included as `logger.Func`. There are two predefined implementations in the `logger` package - `NoOp` (prints nothing, default) and `Std` wrapping `log.Printf` from stdlib.\n \n ## Register oauth2 providers\n \n@@ -371,8 +366,8 @@ For more details refer to [Yandex OAuth](https://tech.yandex.com/oauth/doc/dg/co\n 1.\tFill **App name**  and **Description** and **URL** of your site\n 1.\tIn the field **Callback URLs** enter the correct url of your callback handler e.g. https://example.mysite.com/{route}/twitter/callback\n 1.\tUnder **Key and tokens** take note of the **Consumer API Key** and **Consumer API Secret key**. Those will be used as `cid` and `csecret`\n-## Status \n+## Status\n \n-The library extracted from [remark42](https://github.com/umputun/remark) project. The original code in production use on multiple sites and seems to work fine. \n+The library extracted from [remark42](https://github.com/umputun/remark) project. The original code in production use on multiple sites and seems to work fine.\n \n `go-pkgz/auth` library still in development and until version 1 released some breaking changes possible."
    },
    {
      "sha": "59a5a9593d0876fe72585545782f7cc4db552b75",
      "filename": "backend/vendor/github.com/go-pkgz/auth/go.mod",
      "status": "modified",
      "additions": 7,
      "deletions": 17,
      "changes": 24,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/go-pkgz/auth/go.mod",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/go-pkgz/auth/go.mod",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/github.com/go-pkgz/auth/go.mod?ref=890d7154e777d0673defcd11db3ef792473d68c9",
      "patch": "@@ -1,28 +1,18 @@\n module github.com/go-pkgz/auth\n \n require (\n-\tcloud.google.com/go v0.40.0 // indirect\n \tgithub.com/dghubble/oauth1 v0.6.0\n \tgithub.com/dgrijalva/jwt-go v3.2.0+incompatible\n-\tgithub.com/go-pkgz/rest v1.4.1\n-\tgithub.com/go-stack/stack v1.8.0 // indirect\n-\tgithub.com/golang/snappy v0.0.1 // indirect\n-\tgithub.com/kr/pretty v0.1.0 // indirect\n+\tgithub.com/go-pkgz/rest v1.5.0\n \tgithub.com/microcosm-cc/bluemonday v1.0.2\n \tgithub.com/nullrocks/identicon v0.0.0-20180626043057-7875f45b0022\n-\tgithub.com/pkg/errors v0.8.1\n-\tgithub.com/stretchr/testify v1.3.0\n-\tgithub.com/tidwall/pretty v1.0.0 // indirect\n-\tgithub.com/xdg/scram v0.0.0-20180814205039-7eeb5667e42c // indirect\n-\tgithub.com/xdg/stringprep v1.0.0 // indirect\n+\tgithub.com/pkg/errors v0.9.1\n+\tgithub.com/stretchr/testify v1.5.1\n \tgo.etcd.io/bbolt v1.3.4\n-\tgo.mongodb.org/mongo-driver v1.1.1\n-\tgolang.org/x/image v0.0.0-20190523035834-f03afa92d3ff\n-\tgolang.org/x/net v0.0.0-20190611141213-3f473d35a33a // indirect\n-\tgolang.org/x/oauth2 v0.0.0-20190604053449-0f29369cfe45\n-\tgoogle.golang.org/appengine v1.6.1 // indirect\n-\tgopkg.in/check.v1 v1.0.0-20180628173108-788fd7840127 // indirect\n-\tgopkg.in/oauth2.v3 v3.10.1\n+\tgo.mongodb.org/mongo-driver v1.3.2\n+\tgolang.org/x/image v0.0.0-20200119044424-58c23975cae1\n+\tgolang.org/x/oauth2 v0.0.0-20200107190931-bf48bf16ab8d\n+\tgopkg.in/oauth2.v3 v3.12.0\n )\n \n go 1.13"
    },
    {
      "sha": "79e556df58a5962222bbc098fa65d9e2b9bc2fb0",
      "filename": "backend/vendor/github.com/go-pkgz/auth/go.sum",
      "status": "modified",
      "additions": 118,
      "deletions": 110,
      "changes": 228,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/go-pkgz/auth/go.sum",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/go-pkgz/auth/go.sum",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/github.com/go-pkgz/auth/go.sum?ref=890d7154e777d0673defcd11db3ef792473d68c9",
      "patch": "@@ -1,111 +1,143 @@\n-cloud.google.com/go v0.26.0/go.mod h1:aQUYkXzVsufM+DwF1aE+0xfcU+56JwCaLick0ClmMTw=\n cloud.google.com/go v0.34.0 h1:eOI3/cP2VTU6uZLDYAoic+eyzzB9YyGmJ7eIjl8rOPg=\n cloud.google.com/go v0.34.0/go.mod h1:aQUYkXzVsufM+DwF1aE+0xfcU+56JwCaLick0ClmMTw=\n-cloud.google.com/go v0.38.0/go.mod h1:990N+gfupTy94rShfmMCWGDn0LpTmnzTp2qbd1dvSRU=\n-cloud.google.com/go v0.40.0 h1:FjSY7bOj+WzJe6TZRVtXI2b9kAYvtNg4lMbcH2+MUkk=\n-cloud.google.com/go v0.40.0/go.mod h1:Tk58MuI9rbLMKlAjeO/bDnteAx7tX2gJIXw4T5Jwlro=\n github.com/BurntSushi/toml v0.3.1/go.mod h1:xHWCNGjB5oqiDr8zfno3MHue2Ht5sIBksp03qcyfWMU=\n-github.com/ajg/form v0.0.0-20160822230020-523a5da1a92f h1:zvClvFQwU++UpIUBGC8YmDlfhUrweEy1R1Fj1gu5iIM=\n-github.com/ajg/form v0.0.0-20160822230020-523a5da1a92f/go.mod h1:uL1WgH+h2mgNtvBq0339dVnzXdBETtL2LeUXaIv25UY=\n-github.com/client9/misspell v0.3.4/go.mod h1:qj6jICC3Q7zFZvVWo7KLAzC3yx5G7kyvSDkc90ppPyw=\n+github.com/ajg/form v1.5.1 h1:t9c7v8JUKu/XxOGBU0yjNpaMloxGEJhUkqFRq0ibGeU=\n+github.com/ajg/form v1.5.1/go.mod h1:uL1WgH+h2mgNtvBq0339dVnzXdBETtL2LeUXaIv25UY=\n github.com/davecgh/go-spew v1.1.0/go.mod h1:J7Y8YcW2NihsgmVo/mv3lAwl/skON4iLHjSsI+c5H38=\n github.com/davecgh/go-spew v1.1.1 h1:vj9j/u1bqnvCEfJOwUhtlOARqs3+rkHYY13jYWTU97c=\n github.com/davecgh/go-spew v1.1.1/go.mod h1:J7Y8YcW2NihsgmVo/mv3lAwl/skON4iLHjSsI+c5H38=\n github.com/dghubble/oauth1 v0.6.0 h1:m1yC01Ohc/eF38jwZ8JUjL1a+XHHXtGQgK+MxQbmSx0=\n github.com/dghubble/oauth1 v0.6.0/go.mod h1:8pFdfPkv/jr8mkChVbNVuJ0suiHe278BtWI4Tk1ujxk=\n github.com/dgrijalva/jwt-go v3.2.0+incompatible h1:7qlOGliEKZXTDg6OTjfoBKDXWrumCAMpl/TFQ4/5kLM=\n github.com/dgrijalva/jwt-go v3.2.0+incompatible/go.mod h1:E3ru+11k8xSBh+hMPgOLZmtrrCbhqsmaPHjLKYnJCaQ=\n+github.com/fasthttp-contrib/websocket v0.0.0-20160511215533-1f3b11f56072 h1:DddqAaWDpywytcG8w/qoQ5sAN8X12d3Z3koB0C3Rxsc=\n+github.com/fasthttp-contrib/websocket v0.0.0-20160511215533-1f3b11f56072/go.mod h1:duJ4Jxv5lDcvg4QuQr0oowTf7dz4/CR8NtyCooz9HL8=\n github.com/fatih/structs v1.1.0 h1:Q7juDM0QtcnhCpeyLGQKyg4TOIghuNXrkL32pHAUMxo=\n github.com/fatih/structs v1.1.0/go.mod h1:9NiDSp5zOcgEDl+j00MP/WkGVPOlPRLejGD8Ga6PJ7M=\n github.com/fsnotify/fsnotify v1.4.7 h1:IXs+QLmnXW2CcXuY+8Mzv/fWEsPGWxqefPtCP5CnV9I=\n github.com/fsnotify/fsnotify v1.4.7/go.mod h1:jwhsz4b93w/PPRr/qN1Yymfu8t87LnFCMoQvtojpjFo=\n-github.com/gavv/httpexpect v0.0.0-20180803094507-bdde30871313 h1:GSPjYG49Uqn3S1oeFgJtlGI3ykTavl/yvYgZlz6wsoI=\n-github.com/gavv/httpexpect v0.0.0-20180803094507-bdde30871313/go.mod h1:x+9tiU1YnrOvnB725RkpoLv1M62hOWzwo5OXotisrKc=\n-github.com/gavv/monotime v0.0.0-20171021193802-6f8212e8d10d h1:oYXrtNhqNKL1dVtKdv8XUq5zqdGVFNQ0/4tvccXZOLM=\n-github.com/gavv/monotime v0.0.0-20171021193802-6f8212e8d10d/go.mod h1:vmp8DIyckQMXOPl0AQVHt+7n5h7Gb7hS6CUydiV8QeA=\n-github.com/go-pkgz/rest v1.4.1 h1:DmaVLPH2O7yLehrWOW0uz01d2mVHz9fBR/iuTiPRzaw=\n-github.com/go-pkgz/rest v1.4.1/go.mod h1:COazNj35u3RXAgQNBr6neR599tYP3URiOpsu9p0rOtk=\n+github.com/gavv/httpexpect v2.0.0+incompatible h1:1X9kcRshkSKEjNJJxX9Y9mQ5BRfbxU5kORdjhlA1yX8=\n+github.com/gavv/httpexpect v2.0.0+incompatible/go.mod h1:x+9tiU1YnrOvnB725RkpoLv1M62hOWzwo5OXotisrKc=\n+github.com/go-pkgz/rest v1.5.0 h1:C8SxXcXza4GiUUAn/95iCkvoIrGbS30qpwK19iqlrWQ=\n+github.com/go-pkgz/rest v1.5.0/go.mod h1:nQaM3RhSTUAmbBZWY4hfe4buyeC9VckvhoCktiQXJxI=\n github.com/go-session/session v3.1.2+incompatible/go.mod h1:8B3iivBQjrz/JtC68Np2T1yBBLxTan3mn/3OM0CyRt0=\n github.com/go-stack/stack v1.8.0 h1:5SgMzNM5HxrEjV0ww2lTmX6E2Izsfxas4+YHWRs3Lsk=\n github.com/go-stack/stack v1.8.0/go.mod h1:v0f6uXyyMGvRgIKkXu+yp6POWl0qKG85gN/melR3HDY=\n-github.com/golang/glog v0.0.0-20160126235308-23def4e6c14b/go.mod h1:SBH7ygxi8pfUlaOkMMuAQtPIUF8ecWP5IEl/CR7VP2Q=\n-github.com/golang/mock v1.1.1/go.mod h1:oTYuIxOrZwtPieC+H1uAHpcLFnEyAGVDL/k47Jfbm0A=\n-github.com/golang/mock v1.2.0/go.mod h1:oTYuIxOrZwtPieC+H1uAHpcLFnEyAGVDL/k47Jfbm0A=\n+github.com/gobuffalo/attrs v0.0.0-20190224210810-a9411de4debd/go.mod h1:4duuawTqi2wkkpB4ePgWMaai6/Kc6WEz83bhFwpHzj0=\n+github.com/gobuffalo/depgen v0.0.0-20190329151759-d478694a28d3/go.mod h1:3STtPUQYuzV0gBVOY3vy6CfMm/ljR4pABfrTeHNLHUY=\n+github.com/gobuffalo/depgen v0.1.0/go.mod h1:+ifsuy7fhi15RWncXQQKjWS9JPkdah5sZvtHc2RXGlg=\n+github.com/gobuffalo/envy v1.6.15/go.mod h1:n7DRkBerg/aorDM8kbduw5dN3oXGswK5liaSCx4T5NI=\n+github.com/gobuffalo/envy v1.7.0/go.mod h1:n7DRkBerg/aorDM8kbduw5dN3oXGswK5liaSCx4T5NI=\n+github.com/gobuffalo/flect v0.1.0/go.mod h1:d2ehjJqGOH/Kjqcoz+F7jHTBbmDb38yXA598Hb50EGs=\n+github.com/gobuffalo/flect v0.1.1/go.mod h1:8JCgGVbRjJhVgD6399mQr4fx5rRfGKVzFjbj6RE/9UI=\n+github.com/gobuffalo/flect v0.1.3/go.mod h1:8JCgGVbRjJhVgD6399mQr4fx5rRfGKVzFjbj6RE/9UI=\n+github.com/gobuffalo/genny v0.0.0-20190329151137-27723ad26ef9/go.mod h1:rWs4Z12d1Zbf19rlsn0nurr75KqhYp52EAGGxTbBhNk=\n+github.com/gobuffalo/genny v0.0.0-20190403191548-3ca520ef0d9e/go.mod h1:80lIj3kVJWwOrXWWMRzzdhW3DsrdjILVil/SFKBzF28=\n+github.com/gobuffalo/genny v0.1.0/go.mod h1:XidbUqzak3lHdS//TPu2OgiFB+51Ur5f7CSnXZ/JDvo=\n+github.com/gobuffalo/genny v0.1.1/go.mod h1:5TExbEyY48pfunL4QSXxlDOmdsD44RRq4mVZ0Ex28Xk=\n+github.com/gobuffalo/gitgen v0.0.0-20190315122116-cc086187d211/go.mod h1:vEHJk/E9DmhejeLeNt7UVvlSGv3ziL+djtTr3yyzcOw=\n+github.com/gobuffalo/gogen v0.0.0-20190315121717-8f38393713f5/go.mod h1:V9QVDIxsgKNZs6L2IYiGR8datgMhB577vzTDqypH360=\n+github.com/gobuffalo/gogen v0.1.0/go.mod h1:8NTelM5qd8RZ15VjQTFkAW6qOMx5wBbW4dSCS3BY8gg=\n+github.com/gobuffalo/gogen v0.1.1/go.mod h1:y8iBtmHmGc4qa3urIyo1shvOD8JftTtfcKi+71xfDNE=\n+github.com/gobuffalo/logger v0.0.0-20190315122211-86e12af44bc2/go.mod h1:QdxcLw541hSGtBnhUc4gaNIXRjiDppFGaDqzbrBd3v8=\n+github.com/gobuffalo/mapi v1.0.1/go.mod h1:4VAGh89y6rVOvm5A8fKFxYG+wIW6LO1FMTG9hnKStFc=\n+github.com/gobuffalo/mapi v1.0.2/go.mod h1:4VAGh89y6rVOvm5A8fKFxYG+wIW6LO1FMTG9hnKStFc=\n+github.com/gobuffalo/packd v0.0.0-20190315124812-a385830c7fc0/go.mod h1:M2Juc+hhDXf/PnmBANFCqx4DM3wRbgDvnVWeG2RIxq4=\n+github.com/gobuffalo/packd v0.1.0/go.mod h1:M2Juc+hhDXf/PnmBANFCqx4DM3wRbgDvnVWeG2RIxq4=\n+github.com/gobuffalo/packr/v2 v2.0.9/go.mod h1:emmyGweYTm6Kdper+iywB6YK5YzuKchGtJQZ0Odn4pQ=\n+github.com/gobuffalo/packr/v2 v2.2.0/go.mod h1:CaAwI0GPIAv+5wKLtv8Afwl+Cm78K/I/VCm/3ptBN+0=\n+github.com/gobuffalo/syncx v0.0.0-20190224160051-33c29581e754/go.mod h1:HhnNqWY95UYwwW3uSASeV7vtgYkT2t16hJgV3AEPUpw=\n github.com/golang/protobuf v1.2.0 h1:P3YflyNX/ehuJFLhxviNdFxQPkGK5cDcApsge1SqnvM=\n github.com/golang/protobuf v1.2.0/go.mod h1:6lQm79b+lXiMfvg/cZm0SGofjICqVBUtrP5yJMmIC1U=\n-github.com/golang/protobuf v1.3.1 h1:YF8+flBXS5eO826T4nzqPrxfhQThhXl0YzfuUPu4SBg=\n-github.com/golang/protobuf v1.3.1/go.mod h1:6lQm79b+lXiMfvg/cZm0SGofjICqVBUtrP5yJMmIC1U=\n github.com/golang/snappy v0.0.1 h1:Qgr9rKW7uDUkrbSmQeiDsGa8SjGyCOGtuasMWwvp2P4=\n github.com/golang/snappy v0.0.1/go.mod h1:/XxbfmMg8lxefKM7IXC3fBNl/7bRcc72aCRzEWrmP2Q=\n-github.com/google/btree v0.0.0-20180813153112-4030bb1f1f0c/go.mod h1:lNA+9X1NB3Zf8V7Ke586lFgjr2dZNuvo3lPJSGZ5JPQ=\n+github.com/google/go-cmp v0.2.0 h1:+dTQ8DZQJz0Mb/HjFlkptS1FeQ4cWSnN941F8aEG4SQ=\n github.com/google/go-cmp v0.2.0/go.mod h1:oXzfMopK8JAjlY9xF4vHSVASa0yLyX7SntLO5aqRK0M=\n-github.com/google/go-cmp v0.3.0 h1:crn/baboCvb5fXaQ0IJ1SGTsTVrWpDsCWC8EGETZijY=\n-github.com/google/go-cmp v0.3.0/go.mod h1:8QqcDgzrUqlUb/G2PQTWiueGozuR1884gddMywk6iLU=\n github.com/google/go-querystring v1.0.0 h1:Xkwi/a1rcvNg1PPYe5vI8GbeBY/jrVuDX5ASuANWTrk=\n github.com/google/go-querystring v1.0.0/go.mod h1:odCYkC5MyYFN7vkCjXpyrEuKhc/BUO6wN/zVPAxq5ck=\n-github.com/google/martian v2.1.0+incompatible/go.mod h1:9I4somxYTbIHy5NJKHRl3wXiIaQGbYVAs8BPL6v8lEs=\n-github.com/google/pprof v0.0.0-20181206194817-3ea8567a2e57/go.mod h1:zfwlbNMJ+OItoe0UupaVj+oy1omPYYDuagoSzA8v9mc=\n-github.com/googleapis/gax-go/v2 v2.0.4/go.mod h1:0Wqv26UfaUD9n4G6kQubkQ+KchISgw+vpHVxEJEs9eg=\n-github.com/gopherjs/gopherjs v0.0.0-20181103185306-d547d1d9531e h1:JKmoR8x90Iww1ks85zJ1lfDGgIiMDuIptTOhJq+zKyg=\n-github.com/gopherjs/gopherjs v0.0.0-20181103185306-d547d1d9531e/go.mod h1:wJfORRmW1u3UXTncJ5qlYoELFm8eSnnEO6hX4iZ3EWY=\n-github.com/hashicorp/golang-lru v0.5.0 h1:CL2msUPvZTLb5O648aiLNJw3hnBxN2+1Jq8rCOH9wdo=\n-github.com/hashicorp/golang-lru v0.5.0/go.mod h1:/m3WP610KZHVQ1SGc6re/UDhFvYD7pJ4Ao+sR/qLZy8=\n-github.com/hashicorp/golang-lru v0.5.1/go.mod h1:/m3WP610KZHVQ1SGc6re/UDhFvYD7pJ4Ao+sR/qLZy8=\n+github.com/gopherjs/gopherjs v0.0.0-20181017120253-0766667cb4d1 h1:EGx4pi6eqNxGaHF6qqu48+N2wcFQ5qg5FXgOdqsJ5d8=\n+github.com/gopherjs/gopherjs v0.0.0-20181017120253-0766667cb4d1/go.mod h1:wJfORRmW1u3UXTncJ5qlYoELFm8eSnnEO6hX4iZ3EWY=\n+github.com/gorilla/websocket v1.4.1 h1:q7AeDBpnBk8AogcD4DSag/Ukw/KV+YhzLj2bP5HvKCM=\n+github.com/gorilla/websocket v1.4.1/go.mod h1:YR8l580nyteQvAITg2hZ9XVh4b55+EU/adAjf1fMHhE=\n github.com/hpcloud/tail v1.0.0 h1:nfCOvKYfkgYP8hkirhJocXT2+zOD8yUNjXaWfTlyFKI=\n github.com/hpcloud/tail v1.0.0/go.mod h1:ab1qPbhIpdTxEkNHXyeSf5vhxWSCs/tWer42PpOxQnU=\n github.com/imkira/go-interpol v1.1.0 h1:KIiKr0VSG2CUW1hl1jpiyuzuJeKUUpC8iM1AIE7N1Vk=\n github.com/imkira/go-interpol v1.1.0/go.mod h1:z0h2/2T3XF8kyEPpRgJ3kmNv+C43p+I/CoI+jC3w2iA=\n-github.com/jstemmer/go-junit-report v0.0.0-20190106144839-af01ea7f8024/go.mod h1:6v2b51hI/fHJwM22ozAgKL4VKDeJcHhJFhtBdhmNjmU=\n-github.com/jtolds/gls v4.2.1+incompatible h1:fSuqC+Gmlu6l/ZYAoZzx2pyucC8Xza35fpRVWLVmUEE=\n-github.com/jtolds/gls v4.2.1+incompatible/go.mod h1:QJZ7F/aHp+rZTRtaJ1ow/lLfFfVYBRgL+9YlvaHOwJU=\n+github.com/inconshreveable/mousetrap v1.0.0/go.mod h1:PxqpIevigyE2G7u3NXJIT2ANytuPF1OarO4DADm73n8=\n+github.com/joho/godotenv v1.3.0/go.mod h1:7hK45KPybAkOC6peb+G5yklZfMxEjkZhHbwpqxOKXbg=\n+github.com/jtolds/gls v4.20.0+incompatible h1:xdiiI2gbIgH/gLH7ADydsJ1uDOEzR8yvV7C0MuV77Wo=\n+github.com/jtolds/gls v4.20.0+incompatible/go.mod h1:QJZ7F/aHp+rZTRtaJ1ow/lLfFfVYBRgL+9YlvaHOwJU=\n github.com/k0kubun/colorstring v0.0.0-20150214042306-9440f1994b88 h1:uC1QfSlInpQF+M0ao65imhwqKnz3Q2z/d8PWZRMQvDM=\n github.com/k0kubun/colorstring v0.0.0-20150214042306-9440f1994b88/go.mod h1:3w7q1U84EfirKl04SVQ/s7nPm1ZPhiXd34z40TNz36k=\n-github.com/klauspost/compress v1.4.0 h1:8nsMz3tWa9SWWPL60G1V6CUsf4lLjWLTNEtibhe8gh8=\n-github.com/klauspost/compress v1.4.0/go.mod h1:RyIbtBH6LamlWaDj8nUwkbUhJ87Yi3uG0guNDohfE1A=\n-github.com/klauspost/cpuid v0.0.0-20180405133222-e7e905edc00e h1:+lIPJOWl+jSiJOc70QXJ07+2eg2Jy2EC7Mi11BWujeM=\n-github.com/klauspost/cpuid v0.0.0-20180405133222-e7e905edc00e/go.mod h1:Pj4uuM528wm8OyEC2QMXAi2YiTZ96dNQPGgoMS4s3ek=\n+github.com/karrick/godirwalk v1.8.0/go.mod h1:H5KPZjojv4lE+QYImBI8xVtrBRgYrIVsaRPx4tDPEn4=\n+github.com/karrick/godirwalk v1.10.3/go.mod h1:RoGL9dQei4vP9ilrpETWE8CLOZ1kiN0LhBygSwrAsHA=\n+github.com/kisielk/errcheck v1.2.0/go.mod h1:/BMXB+zMLi60iA8Vv6Ksmxu/1UDYcXs4uQLJ+jE2L00=\n+github.com/klauspost/compress v1.8.2/go.mod h1:RyIbtBH6LamlWaDj8nUwkbUhJ87Yi3uG0guNDohfE1A=\n+github.com/klauspost/compress v1.9.5 h1:U+CaK85mrNNb4k8BNOfgJtJ/gr6kswUCFj6miSzVC6M=\n+github.com/klauspost/compress v1.9.5/go.mod h1:RyIbtBH6LamlWaDj8nUwkbUhJ87Yi3uG0guNDohfE1A=\n+github.com/klauspost/cpuid v1.2.1/go.mod h1:Pj4uuM528wm8OyEC2QMXAi2YiTZ96dNQPGgoMS4s3ek=\n+github.com/konsorten/go-windows-terminal-sequences v1.0.1/go.mod h1:T0+1ngSBFLxvqU3pZ+m/2kptfBszLMUkC4ZK/EgS/cQ=\n+github.com/konsorten/go-windows-terminal-sequences v1.0.2/go.mod h1:T0+1ngSBFLxvqU3pZ+m/2kptfBszLMUkC4ZK/EgS/cQ=\n github.com/kr/pretty v0.1.0 h1:L/CwN0zerZDmRFUapSPitk6f+Q3+0za1rQkzVuMiMFI=\n github.com/kr/pretty v0.1.0/go.mod h1:dAy3ld7l9f0ibDNOQOHHMYYIIbhfbHSm3C4ZsoJORNo=\n github.com/kr/pty v1.1.1/go.mod h1:pFQYn66WHrOpPYNljwOMqo10TkYh1fy3cYio2l3bCsQ=\n github.com/kr/text v0.1.0 h1:45sCR5RtlFHMR4UwH9sdQ5TC8v0qDQCHnXt+kaKSTVE=\n github.com/kr/text v0.1.0/go.mod h1:4Jbv+DJW3UT/LiOwJeYQe1efqtUx/iVham/4vfdArNI=\n-github.com/mattn/go-colorable v0.0.9 h1:UVL0vNpWh04HeJXV0KLcaT7r06gOH2l4OW6ddYRUIY4=\n-github.com/mattn/go-colorable v0.0.9/go.mod h1:9vuHe8Xs5qXnSaW/c/ABM9alt+Vo+STaOChaDxuIBZU=\n-github.com/mattn/go-isatty v0.0.4 h1:bnP0vzxcAdeI1zdubAl5PjU6zsERjGZb7raWodagDYs=\n-github.com/mattn/go-isatty v0.0.4/go.mod h1:M+lRXTBqGeGNdLjl/ufCoiOlB5xdOkqRJdNxMWT7Zi4=\n+github.com/markbates/oncer v0.0.0-20181203154359-bf2de49a0be2/go.mod h1:Ld9puTsIW75CHf65OeIOkyKbteujpZVXDpWK6YGZbxE=\n+github.com/markbates/safe v1.0.1/go.mod h1:nAqgmRi7cY2nqMc92/bSEeQA+R4OheNU2T1kNSCBdG0=\n+github.com/mattn/go-colorable v0.1.4 h1:snbPLB8fVfU9iwbbo30TPtbLRzwWu6aJS6Xh4eaaviA=\n+github.com/mattn/go-colorable v0.1.4/go.mod h1:U0ppj6V5qS13XJ6of8GYAs25YV2eR4EVcfRqFIhoBtE=\n+github.com/mattn/go-isatty v0.0.8 h1:HLtExJ+uU2HOZ+wI0Tt5DtUDrx8yhUqDcp7fYERX4CE=\n+github.com/mattn/go-isatty v0.0.8/go.mod h1:Iq45c/XA43vh69/j3iqttzPXn0bhXyGjM0Hdxcsrc5s=\n github.com/microcosm-cc/bluemonday v1.0.2 h1:5lPfLTTAvAbtS0VqT+94yOtFnGfUWYyx0+iToC3Os3s=\n github.com/microcosm-cc/bluemonday v1.0.2/go.mod h1:iVP4YcDBq+n/5fb23BhYFvIMq/leAFZyRl6bYmGDlGc=\n+github.com/montanaflynn/stats v0.0.0-20171201202039-1bf9dbcd8cbe/go.mod h1:wL8QJuTMNUDYhXwkmfOly8iTdp5TEcJFWZD2D7SIkUc=\n github.com/moul/http2curl v1.0.0 h1:dRMWoAtb+ePxMlLkrCbAqh4TlPHXvoGUSQ323/9Zahs=\n github.com/moul/http2curl v1.0.0/go.mod h1:8UbvGypXm98wA/IqH45anm5Y2Z6ep6O31QGOAZ3H0fQ=\n github.com/nullrocks/identicon v0.0.0-20180626043057-7875f45b0022 h1:Ys0rDzh8s4UMlGaDa1UTA0sfKgvF0hQZzTYX8ktjiDc=\n github.com/nullrocks/identicon v0.0.0-20180626043057-7875f45b0022/go.mod h1:x4NsS+uc7ecH/Cbm9xKQ6XzmJM57rWTkjywjfB2yQ18=\n github.com/onsi/ginkgo v1.6.0/go.mod h1:lLunBs/Ym6LB5Z9jYTR76FiuTmxDTDusOGeTQH+WWjE=\n-github.com/onsi/ginkgo v1.7.0 h1:WSHQ+IS43OoUrWtD1/bbclrwK8TTH5hzp+umCiuxHgs=\n-github.com/onsi/ginkgo v1.7.0/go.mod h1:lLunBs/Ym6LB5Z9jYTR76FiuTmxDTDusOGeTQH+WWjE=\n-github.com/onsi/gomega v1.4.3 h1:RE1xgDvH7imwFD45h+u2SgIfERHlS2yNG4DObb5BSKU=\n-github.com/onsi/gomega v1.4.3/go.mod h1:ex+gbHU/CVuBBDIJjb2X0qEXbFg53c61hWP/1CpauHY=\n+github.com/onsi/ginkgo v1.10.2 h1:uqH7bpe+ERSiDa34FDOF7RikN6RzXgduUF8yarlZp94=\n+github.com/onsi/ginkgo v1.10.2/go.mod h1:lLunBs/Ym6LB5Z9jYTR76FiuTmxDTDusOGeTQH+WWjE=\n+github.com/onsi/gomega v1.7.0 h1:XPnZz8VVBHjVsy1vzJmRwIcSwiUO+JFfrv/xGiigmME=\n+github.com/onsi/gomega v1.7.0/go.mod h1:ex+gbHU/CVuBBDIJjb2X0qEXbFg53c61hWP/1CpauHY=\n+github.com/pelletier/go-toml v1.4.0/go.mod h1:PN7xzY2wHTK0K9p34ErDQMlFxa51Fk0OUruD3k1mMwo=\n github.com/pkg/errors v0.8.0 h1:WdK/asTD0HN+q6hsWO3/vpuAkAr+tw6aNJNDFFf0+qw=\n github.com/pkg/errors v0.8.0/go.mod h1:bwawxfHBFNV+L2hUp1rHADufV3IMtnDRdf1r5NINEl0=\n github.com/pkg/errors v0.8.1 h1:iURUrRGxPUNPdy5/HRSm+Yj6okJ6UtLINN0Q9M4+h3I=\n github.com/pkg/errors v0.8.1/go.mod h1:bwawxfHBFNV+L2hUp1rHADufV3IMtnDRdf1r5NINEl0=\n+github.com/pkg/errors v0.9.1 h1:FEBLx1zS214owpjy7qsBeixbURkuhQAwrK5UwLGTwt4=\n+github.com/pkg/errors v0.9.1/go.mod h1:bwawxfHBFNV+L2hUp1rHADufV3IMtnDRdf1r5NINEl0=\n github.com/pmezard/go-difflib v1.0.0 h1:4DBwDE0NGyQoBHbLQYPwSUPoCMWR5BEzIk/f1lZbAQM=\n github.com/pmezard/go-difflib v1.0.0/go.mod h1:iKH77koFhYxTK1pcRnkKkqfTogsbg7gZNVY4sRDYZ/4=\n+github.com/rogpeppe/go-internal v1.1.0/go.mod h1:M8bDsm7K2OlrFYOpmOWEs/qY81heoFRclV5y23lUDJ4=\n+github.com/rogpeppe/go-internal v1.2.2/go.mod h1:M8bDsm7K2OlrFYOpmOWEs/qY81heoFRclV5y23lUDJ4=\n+github.com/rogpeppe/go-internal v1.3.0/go.mod h1:M8bDsm7K2OlrFYOpmOWEs/qY81heoFRclV5y23lUDJ4=\n github.com/sergi/go-diff v1.0.0 h1:Kpca3qRNrduNnOQeazBd0ysaKrUJiIuISHxogkT9RPQ=\n github.com/sergi/go-diff v1.0.0/go.mod h1:0CfEIISq7TuYL3j771MWULgwwjU+GofnZX9QAmXWZgo=\n+github.com/sirupsen/logrus v1.4.0/go.mod h1:LxeOpSwHxABJmUn/MG1IvRgCAasNZTLOkJPxbbu5VWo=\n+github.com/sirupsen/logrus v1.4.1/go.mod h1:ni0Sbl8bgC9z8RoU9G6nDWqqs/fq4eDPysMBDgk/93Q=\n+github.com/sirupsen/logrus v1.4.2/go.mod h1:tLMulIdttU9McNUspp0xgXVQah82FyeX6MwdIuYE2rE=\n github.com/smartystreets/assertions v0.0.0-20180927180507-b2de0cb4f26d h1:zE9ykElWQ6/NYmHa3jpm/yHnI4xSofP+UP6SpjHcSeM=\n github.com/smartystreets/assertions v0.0.0-20180927180507-b2de0cb4f26d/go.mod h1:OnSkiWE9lh6wB0YB77sQom3nweQdgAjqCqsofrRNTgc=\n-github.com/smartystreets/goconvey v0.0.0-20181108003508-044398e4856c h1:Ho+uVpkel/udgjbwB5Lktg9BtvJSh2DT0Hi6LPSyI2w=\n-github.com/smartystreets/goconvey v0.0.0-20181108003508-044398e4856c/go.mod h1:XDJAKZRPZ1CvBcN2aX5YOUTYGHki24fSF0Iv48Ibg0s=\n+github.com/smartystreets/goconvey v1.6.4 h1:fv0U8FUIMPNf1L9lnHLvLhgicrIVChEkdzIKYqbNC9s=\n+github.com/smartystreets/goconvey v1.6.4/go.mod h1:syvi0/a8iFYH4r/RixwvyeAJjdLS9QV7WQ/tjFTllLA=\n+github.com/spf13/cobra v0.0.3/go.mod h1:1l0Ry5zgKvJasoi3XT1TypsSe7PqH0Sj9dhYf7v3XqQ=\n+github.com/spf13/pflag v1.0.3/go.mod h1:DYY7MBk1bdzusC3SYhjObp+wFpr4gzcvqqNjLnInEg4=\n github.com/stretchr/objx v0.1.0/go.mod h1:HFkY916IF+rwdDfMAkV7OtwuqBVzrE8GR6GFx+wExME=\n+github.com/stretchr/objx v0.1.1/go.mod h1:HFkY916IF+rwdDfMAkV7OtwuqBVzrE8GR6GFx+wExME=\n github.com/stretchr/testify v1.2.2/go.mod h1:a8OnRcib4nhh0OaRAV+Yts87kKdq0PP7pXfy6kDkUVs=\n github.com/stretchr/testify v1.3.0 h1:TivCn/peBQ7UY8ooIcPgZFpTNSz0Q2U6UrFlUfqbe0Q=\n github.com/stretchr/testify v1.3.0/go.mod h1:M5WIy9Dh21IEIfnGCwXGc5bZfKNJtfHm1UVUgZn+9EI=\n+github.com/stretchr/testify v1.4.0/go.mod h1:j7eGeouHqKxXV5pUuKE4zz7dFj8WfuZ+81PSLYec5m4=\n+github.com/stretchr/testify v1.5.1 h1:nOGnQDM7FYENwehXlg/kFVnos3rEvtKTjRvOWSzb6H4=\n+github.com/stretchr/testify v1.5.1/go.mod h1:5W2xD1RspED5o8YsWQXVCued0rvSQ+mT+I5cxcmMvtA=\n github.com/tidwall/btree v0.0.0-20170113224114-9876f1454cf0 h1:QnyrPZZvPmR0AtJCxxfCtI1qN+fYpKTKJ/5opWmZ34k=\n github.com/tidwall/btree v0.0.0-20170113224114-9876f1454cf0/go.mod h1:huei1BkDWJ3/sLXmO+bsCNELL+Bp2Kks9OLyQFkzvA8=\n-github.com/tidwall/buntdb v1.0.0 h1:urIJqQ8OR9fibXXtFSu8sR5arMqZK8ZnNq22yWl+A+8=\n-github.com/tidwall/buntdb v1.0.0/go.mod h1:Y39xhcDW10WlyYXeLgGftXVbjtM0QP+/kpz8xl9cbzE=\n-github.com/tidwall/gjson v1.1.3 h1:u4mspaByxY+Qk4U1QYYVzGFI8qxN/3jtEV0ZDb2vRic=\n-github.com/tidwall/gjson v1.1.3/go.mod h1:c/nTNbUr0E0OrXEhq1pwa8iEgc2DOt4ZZqAt1HtCkPA=\n+github.com/tidwall/buntdb v1.1.0 h1:H6LzK59KiNjf1nHVPFrYj4Qnl8d8YLBsYamdL8N+Bao=\n+github.com/tidwall/buntdb v1.1.0/go.mod h1:Y39xhcDW10WlyYXeLgGftXVbjtM0QP+/kpz8xl9cbzE=\n+github.com/tidwall/gjson v1.3.2 h1:+7p3qQFaH3fOMXAJSrdZwGKcOO/lYdGS0HqGhPqDdTI=\n+github.com/tidwall/gjson v1.3.2/go.mod h1:P256ACg0Mn+j1RXIDXoss50DeIABTYK1PULOJHhxOls=\n github.com/tidwall/grect v0.0.0-20161006141115-ba9a043346eb h1:5NSYaAdrnblKByzd7XByQEJVT8+9v0W/tIY0Oo4OwrE=\n github.com/tidwall/grect v0.0.0-20161006141115-ba9a043346eb/go.mod h1:lKYYLFIr9OIgdgrtgkZ9zgRxRdvPYsExnYBsEAd8W5M=\n github.com/tidwall/match v1.0.1 h1:PnKP62LPNxHKTwvHHZZzdOAOCtsJTjo6dZLCwpKm5xc=\n@@ -118,19 +150,19 @@ github.com/tidwall/tinyqueue v0.0.0-20180302190814-1e39f5511563 h1:Otn9S136ELckZ\n github.com/tidwall/tinyqueue v0.0.0-20180302190814-1e39f5511563/go.mod h1:mLqSmt7Dv/CNneF2wfcChfN1rvapyQr01LGKnKex0DQ=\n github.com/valyala/bytebufferpool v1.0.0 h1:GqA5TC/0021Y/b9FG4Oi9Mr3q7XYx6KllzawFIhcdPw=\n github.com/valyala/bytebufferpool v1.0.0/go.mod h1:6bBcMArwyJ5K/AmCkWv1jt77kVWyCJ6HpOuEn7z0Csc=\n-github.com/valyala/fasthttp v1.0.0 h1:BwIoZQbBsTo3v2F5lz5Oy3TlTq4wLKTLV260EVTEWco=\n-github.com/valyala/fasthttp v1.0.0/go.mod h1:4vX61m6KN+xDduDNwXrhIAVZaZaZiQ1luJk8LWSxF3s=\n+github.com/valyala/fasthttp v1.6.0 h1:uWF8lgKmeaIewWVPwi4GRq2P6+R46IgYZdxWtM+GtEY=\n+github.com/valyala/fasthttp v1.6.0/go.mod h1:FstJa9V+Pj9vQ7OJie2qMHdwemEDaDiSdBnvPM1Su9w=\n github.com/valyala/tcplisten v0.0.0-20161114210144-ceec8f93295a/go.mod h1:v3UYOV9WzVtRmSR+PDvWpU/qWl4Wa5LApYYX4ZtKbio=\n github.com/xdg/scram v0.0.0-20180814205039-7eeb5667e42c h1:u40Z8hqBAAQyv+vATcGgV0YCnDjqSL7/q/JyPhhJSPk=\n github.com/xdg/scram v0.0.0-20180814205039-7eeb5667e42c/go.mod h1:lB8K/P019DLNhemzwFU4jHLhdvlE6uDZjXFejJXr49I=\n-github.com/xdg/stringprep v1.0.0 h1:d9X0esnoa3dFsV0FG35rAT0RIhYFlPq7MiP+DW89La0=\n-github.com/xdg/stringprep v1.0.0/go.mod h1:Jhud4/sHMO4oL310DaZAKk9ZaJ08SJfe+sJh0HrGL1Y=\n+github.com/xdg/stringprep v0.0.0-20180714160509-73f8eece6fdc h1:n+nNi93yXLkJvKwXNP9d55HC7lGK4H/SRcwB5IaUZLo=\n+github.com/xdg/stringprep v0.0.0-20180714160509-73f8eece6fdc/go.mod h1:Jhud4/sHMO4oL310DaZAKk9ZaJ08SJfe+sJh0HrGL1Y=\n github.com/xeipuuv/gojsonpointer v0.0.0-20180127040702-4e3ac2762d5f h1:J9EGpcZtP0E/raorCMxlFGSTBrsSlaDGf3jU/qvAE2c=\n github.com/xeipuuv/gojsonpointer v0.0.0-20180127040702-4e3ac2762d5f/go.mod h1:N2zxlSyiKSe5eX1tZViRH5QA0qijqEDrYZiPEAiq3wU=\n github.com/xeipuuv/gojsonreference v0.0.0-20180127040603-bd5ef7bd5415 h1:EzJWgHovont7NscjpAxXsDA8S8BMYve8Y5+7cuRE7R0=\n github.com/xeipuuv/gojsonreference v0.0.0-20180127040603-bd5ef7bd5415/go.mod h1:GwrjFmJcFw6At/Gs6z4yjiIwzuJ1/+UwLxMQDVQXShQ=\n-github.com/xeipuuv/gojsonschema v0.0.0-20181112162635-ac52e6811b56 h1:yhqBHs09SmmUoNOHc9jgK4a60T3XFRtPAkYxVnqgY50=\n-github.com/xeipuuv/gojsonschema v0.0.0-20181112162635-ac52e6811b56/go.mod h1:5yf86TLmAcydyeJq5YvxkGPE2fm/u4myDekKRoLuqhs=\n+github.com/xeipuuv/gojsonschema v1.2.0 h1:LhYJRs+L4fBtjZUfuSZIKGeVu0QRy8e5Xi7D17UxZ74=\n+github.com/xeipuuv/gojsonschema v1.2.0/go.mod h1:anYRn/JVcOK2ZgGU+IjEV4nwlhoK5sQluxsYJ78Id3Y=\n github.com/yalp/jsonpath v0.0.0-20180802001716-5cc68e5049a0 h1:6fRhSjgLCkTD3JnJxvaJ4Sj+TYblw757bqYgZaOq5ZY=\n github.com/yalp/jsonpath v0.0.0-20180802001716-5cc68e5049a0/go.mod h1:/LWChgwKmvncFJFHJ7Gvn9wZArjbV5/FppcK2fKk/tI=\n github.com/yudai/gojsondiff v1.0.0 h1:27cbfqXLVEJ1o8I6v3y9lg8Ydm53EKqHXAOMxEGlCOA=\n@@ -141,92 +173,68 @@ github.com/yudai/pp v2.0.1+incompatible h1:Q4//iY4pNF6yPLZIigmvcl7k/bPgrcTPIFIcm\n github.com/yudai/pp v2.0.1+incompatible/go.mod h1:PuxR/8QJ7cyCkFp/aUDS+JY727OFEZkTdatxwunjIkc=\n go.etcd.io/bbolt v1.3.4 h1:hi1bXHMVrlQh6WwxAy+qZCV/SYIlqo+Ushwdpa4tAKg=\n go.etcd.io/bbolt v1.3.4/go.mod h1:G5EMThwa9y8QZGBClrRx5EY+Yw9kAhnjy3bSjsnlVTQ=\n-go.mongodb.org/mongo-driver v1.1.1 h1:Sq1fR+0c58RME5EoqKdjkiQAmPjmfHlZOoRI6fTUOcs=\n-go.mongodb.org/mongo-driver v1.1.1/go.mod h1:u7ryQJ+DOzQmeO7zB6MHyr8jkEQvC8vH7qLUO4lqsUM=\n-go.opencensus.io v0.21.0/go.mod h1:mSImk1erAIZhrmZN+AvHh14ztQfjbGwt4TtuofqLduU=\n+go.mongodb.org/mongo-driver v1.3.2 h1:IYppNjEV/C+/3VPbhHVxQ4t04eVW0cLp0/pNdW++6Ug=\n+go.mongodb.org/mongo-driver v1.3.2/go.mod h1:MSWZXKOynuguX+JSvwP8i+58jYCXxbia8HS3gZBapIE=\n+golang.org/x/crypto v0.0.0-20180904163835-0709b304e793/go.mod h1:6SG95UA2DQfeDnfUPMdvaQW0Q7yPrPDi9nlGo2tz2b4=\n golang.org/x/crypto v0.0.0-20190308221718-c2843e01d9a2/go.mod h1:djNgcEr1/C05ACkg1iLfiJU5Ep61QUkGW8qpdssI0+w=\n-golang.org/x/crypto v0.0.0-20190605123033-f99c8df09eb5 h1:58fnuSXlxZmFdJyvtTFVmVhcMLU6v5fEb/ok4wyqtNU=\n-golang.org/x/crypto v0.0.0-20190605123033-f99c8df09eb5/go.mod h1:yigFU9vqHzYiE8UmvKecakEJjdnWj3jj499lnFckfCI=\n-golang.org/x/exp v0.0.0-20190121172915-509febef88a4/go.mod h1:CJ0aWSM057203Lf6IL+f9T1iT9GByDxfZKAQTCR3kQA=\n-golang.org/x/image v0.0.0-20190523035834-f03afa92d3ff h1:+2zgJKVDVAz/BWSsuniCmU1kLCjL88Z8/kv39xCI9NQ=\n-golang.org/x/image v0.0.0-20190523035834-f03afa92d3ff/go.mod h1:kZ7UVZpmo3dzQBMxlp+ypCbDeSB+sBbTgSJuh5dn5js=\n-golang.org/x/lint v0.0.0-20181026193005-c67002cb31c3/go.mod h1:UVdnD1Gm6xHRNCYTkRU2/jEulfH38KcIWyp/GAMgvoE=\n-golang.org/x/lint v0.0.0-20190227174305-5b3e6a55c961/go.mod h1:wehouNa3lNwaWXcvxsM5YxQ5yQlVC4a0KAMCusXpPoU=\n-golang.org/x/lint v0.0.0-20190301231843-5614ed5bae6f/go.mod h1:UVdnD1Gm6xHRNCYTkRU2/jEulfH38KcIWyp/GAMgvoE=\n-golang.org/x/lint v0.0.0-20190313153728-d0100b6bd8b3/go.mod h1:6SW0HCj/g11FgYtHlgUYUwCkIfeOF89ocIRzGO/8vkc=\n-golang.org/x/lint v0.0.0-20190409202823-959b441ac422/go.mod h1:6SW0HCj/g11FgYtHlgUYUwCkIfeOF89ocIRzGO/8vkc=\n+golang.org/x/crypto v0.0.0-20190422162423-af44ce270edf/go.mod h1:WFFai1msRO1wXaEeE5yQxYXgSfI8pQAWXbQop6sCtWE=\n+golang.org/x/crypto v0.0.0-20190530122614-20be4c3c3ed5 h1:8dUaAV7K4uHsF56JQWkprecIQKdPHtR9jCHF5nB8uzc=\n+golang.org/x/crypto v0.0.0-20190530122614-20be4c3c3ed5/go.mod h1:yigFU9vqHzYiE8UmvKecakEJjdnWj3jj499lnFckfCI=\n+golang.org/x/image v0.0.0-20200119044424-58c23975cae1 h1:5h3ngYt7+vXCDZCup/HkCQgW5XwmSvR/nA2JmJ0RErg=\n+golang.org/x/image v0.0.0-20200119044424-58c23975cae1/go.mod h1:FeLwcggjj3mMvU+oOTbSwawSJRM1uh48EjtB4UJZlP0=\n golang.org/x/net v0.0.0-20180724234803-3673e40ba225/go.mod h1:mL1N/T3taQHkDXs73rZJwtUhF3w3ftmwwsq0BUmARs4=\n-golang.org/x/net v0.0.0-20180826012351-8a410e7b638d/go.mod h1:mL1N/T3taQHkDXs73rZJwtUhF3w3ftmwwsq0BUmARs4=\n golang.org/x/net v0.0.0-20180906233101-161cd47e91fd/go.mod h1:mL1N/T3taQHkDXs73rZJwtUhF3w3ftmwwsq0BUmARs4=\n-golang.org/x/net v0.0.0-20180911220305-26e67e76b6c3/go.mod h1:mL1N/T3taQHkDXs73rZJwtUhF3w3ftmwwsq0BUmARs4=\n-golang.org/x/net v0.0.0-20181217023233-e147a9138326/go.mod h1:mL1N/T3taQHkDXs73rZJwtUhF3w3ftmwwsq0BUmARs4=\n golang.org/x/net v0.0.0-20181220203305-927f97764cc3/go.mod h1:mL1N/T3taQHkDXs73rZJwtUhF3w3ftmwwsq0BUmARs4=\n golang.org/x/net v0.0.0-20190108225652-1e06a53dbb7e/go.mod h1:mL1N/T3taQHkDXs73rZJwtUhF3w3ftmwwsq0BUmARs4=\n-golang.org/x/net v0.0.0-20190213061140-3a22650c66bd/go.mod h1:mL1N/T3taQHkDXs73rZJwtUhF3w3ftmwwsq0BUmARs4=\n golang.org/x/net v0.0.0-20190311183353-d8887717615a/go.mod h1:t9HGtf8HONx5eT2rtn7q6eTqICYqUVnKs3thJo3Qplg=\n golang.org/x/net v0.0.0-20190404232315-eb5bcb51f2a3/go.mod h1:t9HGtf8HONx5eT2rtn7q6eTqICYqUVnKs3thJo3Qplg=\n-golang.org/x/net v0.0.0-20190503192946-f4e77d36d62c/go.mod h1:t9HGtf8HONx5eT2rtn7q6eTqICYqUVnKs3thJo3Qplg=\n-golang.org/x/net v0.0.0-20190603091049-60506f45cf65/go.mod h1:HSz+uSET+XFnRR8LxR5pz3Of3rY3CfYBVs4xY44aLks=\n-golang.org/x/net v0.0.0-20190611141213-3f473d35a33a h1:+KkCgOMgnKSgenxTBoiwkMqTiouMIy/3o8RLdmSbGoY=\n-golang.org/x/net v0.0.0-20190611141213-3f473d35a33a/go.mod h1:z5CRVTTTmAJ677TzLLGU+0bjPO0LkuOLi4/5GtJWs/s=\n-golang.org/x/oauth2 v0.0.0-20180821212333-d2e6202438be/go.mod h1:N/0e6XlmueqKjAGxoOufVs8QHGRruUQn6yWY3a++T0U=\n-golang.org/x/oauth2 v0.0.0-20190226205417-e64efc72b421/go.mod h1:gOpvHmFTYa4IltrdGE7lF6nIHvwfUNPOp7c8zoXwtLw=\n+golang.org/x/net v0.0.0-20190827160401-ba9fcec4b297 h1:k7pJ2yAPLPgbskkFdhRCsA77k2fySZ1zf2zCjvQCiIM=\n+golang.org/x/net v0.0.0-20190827160401-ba9fcec4b297/go.mod h1:z5CRVTTTmAJ677TzLLGU+0bjPO0LkuOLi4/5GtJWs/s=\n golang.org/x/oauth2 v0.0.0-20190604053449-0f29369cfe45 h1:SVwTIAaPC2U/AvvLNZ2a7OVsmBpC8L5BlwK1whH3hm0=\n golang.org/x/oauth2 v0.0.0-20190604053449-0f29369cfe45/go.mod h1:gOpvHmFTYa4IltrdGE7lF6nIHvwfUNPOp7c8zoXwtLw=\n+golang.org/x/oauth2 v0.0.0-20200107190931-bf48bf16ab8d h1:TzXSXBo42m9gQenoE3b9BGiEpg5IG2JkU5FkPIawgtw=\n+golang.org/x/oauth2 v0.0.0-20200107190931-bf48bf16ab8d/go.mod h1:gOpvHmFTYa4IltrdGE7lF6nIHvwfUNPOp7c8zoXwtLw=\n golang.org/x/sync v0.0.0-20180314180146-1d60e4601c6f/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=\n-golang.org/x/sync v0.0.0-20181108010431-42b317875d0f/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=\n golang.org/x/sync v0.0.0-20181221193216-37e7f081c4d4 h1:YUO/7uOKsKeq9UokNS62b8FYywz3ker1l1vDZRCRefw=\n golang.org/x/sync v0.0.0-20181221193216-37e7f081c4d4/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=\n golang.org/x/sync v0.0.0-20190227155943-e225da77a7e6/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=\n+golang.org/x/sync v0.0.0-20190412183630-56d357773e84/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=\n golang.org/x/sync v0.0.0-20190423024810-112230192c58 h1:8gQV6CLnAEikrhgkHFbMAEhagSSnXWGV915qUMm9mrU=\n golang.org/x/sync v0.0.0-20190423024810-112230192c58/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=\n-golang.org/x/sys v0.0.0-20180830151530-49385e6e1522/go.mod h1:STP8DvDyc/dI5b8T5hshtkjS+E42TnysNCUPdjciGhY=\n+golang.org/x/sys v0.0.0-20180905080454-ebe1bf3edb33/go.mod h1:STP8DvDyc/dI5b8T5hshtkjS+E42TnysNCUPdjciGhY=\n golang.org/x/sys v0.0.0-20180909124046-d0be0721c37e/go.mod h1:STP8DvDyc/dI5b8T5hshtkjS+E42TnysNCUPdjciGhY=\n golang.org/x/sys v0.0.0-20190215142949-d0b11bdaac8a/go.mod h1:STP8DvDyc/dI5b8T5hshtkjS+E42TnysNCUPdjciGhY=\n+golang.org/x/sys v0.0.0-20190222072716-a9d3bda3a223/go.mod h1:STP8DvDyc/dI5b8T5hshtkjS+E42TnysNCUPdjciGhY=\n+golang.org/x/sys v0.0.0-20190403152447-81d4e9dc473e/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\n golang.org/x/sys v0.0.0-20190412213103-97732733099d/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\n-golang.org/x/sys v0.0.0-20190507160741-ecd444e8653b/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\n-golang.org/x/sys v0.0.0-20190606165138-5da285871e9c/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\n+golang.org/x/sys v0.0.0-20190419153524-e8e3143a4f4a/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\n+golang.org/x/sys v0.0.0-20190422165155-953cdadca894/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\n+golang.org/x/sys v0.0.0-20190531175056-4c3a928424d2/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\n golang.org/x/sys v0.0.0-20200202164722-d101bd2416d5 h1:LfCXLvNmTYH9kEmVgqbnsWfruoXZIrh4YBgqVHtDvw0=\n golang.org/x/sys v0.0.0-20200202164722-d101bd2416d5/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\n golang.org/x/text v0.3.0/go.mod h1:NqM8EUOU14njkJ3fqMW+pc6Ldnwhi/IjpwHt7yyuwOQ=\n-golang.org/x/text v0.3.1-0.20180807135948-17ff2d5776d2/go.mod h1:NqM8EUOU14njkJ3fqMW+pc6Ldnwhi/IjpwHt7yyuwOQ=\n golang.org/x/text v0.3.2 h1:tW2bmiBqwgJj/UpqtC8EpXEZVYOwU0yG4iWbprSVAcs=\n golang.org/x/text v0.3.2/go.mod h1:bEr9sfX3Q8Zfm5fL9x+3itogRgK3+ptLWKqgva+5dAk=\n-golang.org/x/time v0.0.0-20181108054448-85acf8d2951c/go.mod h1:tRJNPiyCQ0inRvYxbN9jk5I+vvW/OXSQhTDSoE431IQ=\n golang.org/x/tools v0.0.0-20180917221912-90fa682c2a6e/go.mod h1:n7NCudcB/nEzxVGmLbDWY5pfWTLqBcC2KZ6jyYvM4mQ=\n-golang.org/x/tools v0.0.0-20190114222345-bf090417da8b/go.mod h1:n7NCudcB/nEzxVGmLbDWY5pfWTLqBcC2KZ6jyYvM4mQ=\n-golang.org/x/tools v0.0.0-20190226205152-f727befe758c/go.mod h1:9Yl7xja0Znq3iFh3HoIrodX9oNMXvdceNzlUR8zjMvY=\n-golang.org/x/tools v0.0.0-20190311212946-11955173bddd/go.mod h1:LCzVGOaR6xXOjkQ3onu1FJEFr0SW1gC7cKk1uF8kGRs=\n-golang.org/x/tools v0.0.0-20190312170243-e65039ee4138/go.mod h1:LCzVGOaR6xXOjkQ3onu1FJEFr0SW1gC7cKk1uF8kGRs=\n-golang.org/x/tools v0.0.0-20190506145303-2d16b83fe98c/go.mod h1:RgjU9mgBXZiqYHBnxXauZ1Gv1EHHAz9KjViQ78xBX0Q=\n-golang.org/x/tools v0.0.0-20190606124116-d0a3d012864b/go.mod h1:/rFqwRUd4F7ZHNgwSSTFct+R/Kf4OFW1sUzUTQQTgfc=\n-google.golang.org/api v0.4.0/go.mod h1:8k5glujaEP+g9n7WNsDg8QP6cUVNI86fCNMcbazEtwE=\n-google.golang.org/api v0.6.0/go.mod h1:btoxGiFvQNVUZQ8W08zLtrVS08CNpINPEfxXxgJL1Q4=\n-google.golang.org/appengine v1.1.0/go.mod h1:EbEs0AVv82hx2wNQdGPgUI5lhzA/G0D9YwlJXL52JkM=\n+golang.org/x/tools v0.0.0-20181030221726-6c7e314b6563/go.mod h1:n7NCudcB/nEzxVGmLbDWY5pfWTLqBcC2KZ6jyYvM4mQ=\n+golang.org/x/tools v0.0.0-20190328211700-ab21143f2384/go.mod h1:LCzVGOaR6xXOjkQ3onu1FJEFr0SW1gC7cKk1uF8kGRs=\n+golang.org/x/tools v0.0.0-20190329151228-23e29df326fe/go.mod h1:LCzVGOaR6xXOjkQ3onu1FJEFr0SW1gC7cKk1uF8kGRs=\n+golang.org/x/tools v0.0.0-20190416151739-9c9e1878f421/go.mod h1:LCzVGOaR6xXOjkQ3onu1FJEFr0SW1gC7cKk1uF8kGRs=\n+golang.org/x/tools v0.0.0-20190420181800-aa740d480789/go.mod h1:LCzVGOaR6xXOjkQ3onu1FJEFr0SW1gC7cKk1uF8kGRs=\n+golang.org/x/tools v0.0.0-20190531172133-b3315ee88b7d/go.mod h1:/rFqwRUd4F7ZHNgwSSTFct+R/Kf4OFW1sUzUTQQTgfc=\n google.golang.org/appengine v1.4.0 h1:/wp5JvzpHIxhs/dumFmF7BXTf3Z+dd4uXta4kVyO508=\n google.golang.org/appengine v1.4.0/go.mod h1:xpcJRLb0r/rnEns0DIKYYv+WjYCduHsrkT7/EB5XEv4=\n-google.golang.org/appengine v1.5.0/go.mod h1:xpcJRLb0r/rnEns0DIKYYv+WjYCduHsrkT7/EB5XEv4=\n-google.golang.org/appengine v1.6.1 h1:QzqyMA1tlu6CgqCDUtU9V+ZKhLFT2dkJuANu5QaxI3I=\n-google.golang.org/appengine v1.6.1/go.mod h1:i06prIuMbXzDqacNJfV5OdTW448YApPu5ww/cMBSeb0=\n-google.golang.org/genproto v0.0.0-20180817151627-c66870c02cf8/go.mod h1:JiN7NxoALGmiZfu7CAH4rXhgtRTLTxftemlI0sWmxmc=\n-google.golang.org/genproto v0.0.0-20190307195333-5fe7a883aa19/go.mod h1:VzzqZJRnGkLBvHegQrXjBqPurQTc5/KpmUdxsrq26oE=\n-google.golang.org/genproto v0.0.0-20190418145605-e7d98fc518a7/go.mod h1:VzzqZJRnGkLBvHegQrXjBqPurQTc5/KpmUdxsrq26oE=\n-google.golang.org/genproto v0.0.0-20190502173448-54afdca5d873/go.mod h1:VzzqZJRnGkLBvHegQrXjBqPurQTc5/KpmUdxsrq26oE=\n-google.golang.org/genproto v0.0.0-20190530194941-fb225487d101/go.mod h1:z3L6/3dTEVtUr6QSP8miRzeRqwQOioJ9I66odjN4I7s=\n-google.golang.org/grpc v1.19.0/go.mod h1:mqu4LbDTu4XGKhr4mRzUsmM4RtVoemTSY81AxZiDr8c=\n-google.golang.org/grpc v1.20.1/go.mod h1:10oTOabMzJvdu6/UiuZezV6QK5dSlG84ov/aaiqXj38=\n gopkg.in/check.v1 v0.0.0-20161208181325-20d25e280405/go.mod h1:Co6ibVJAznAaIkqp8huTwlJQCZ016jof/cbN4VW5Yz0=\n gopkg.in/check.v1 v1.0.0-20180628173108-788fd7840127 h1:qIbj1fsPNlZgppZ+VLlY7N33q108Sa+fhmuc+sWQYwY=\n gopkg.in/check.v1 v1.0.0-20180628173108-788fd7840127/go.mod h1:Co6ibVJAznAaIkqp8huTwlJQCZ016jof/cbN4VW5Yz0=\n+gopkg.in/errgo.v2 v2.1.0/go.mod h1:hNsd1EY+bozCKY1Ytp96fpM3vjJbqLJn88ws8XvfDNI=\n gopkg.in/fsnotify.v1 v1.4.7 h1:xOHLXZwVvI9hhs+cLKq5+I5onOuwQLhQwiu63xxlHs4=\n gopkg.in/fsnotify.v1 v1.4.7/go.mod h1:Tz8NjZHkW78fSQdbUxIjBTcgA1z1m8ZHf0WmKUhAMys=\n-gopkg.in/oauth2.v3 v3.10.1 h1:/abis3O6tZFizY1/FgKGoDOmmu2ddvscBSSPdHVa6OI=\n-gopkg.in/oauth2.v3 v3.10.1/go.mod h1:nTG+m2PRcHR9jzGNrGdxSsUKz7vvwkqSlhFrstgZcRU=\n+gopkg.in/oauth2.v3 v3.12.0 h1:yOffAPoolH/i2JxwmC+pgtnY3362iPahsDpLXfDFvNg=\n+gopkg.in/oauth2.v3 v3.12.0/go.mod h1:XEYgKqWX095YiPT+Aw5y3tCn+7/FMnlTFKrupgSiJ3I=\n gopkg.in/tomb.v1 v1.0.0-20141024135613-dd632973f1e7 h1:uRGJdciOHaEIrze2W8Q3AKkepLTh2hOroT7a+7czfdQ=\n gopkg.in/tomb.v1 v1.0.0-20141024135613-dd632973f1e7/go.mod h1:dt/ZhP58zS4L8KSrWDmTeBkI65Dw0HsyUHuEVlX15mw=\n gopkg.in/yaml.v2 v2.2.1 h1:mUhvW9EsL+naU5Q3cakzfE91YhliOondGd6ZrsDBHQE=\n gopkg.in/yaml.v2 v2.2.1/go.mod h1:hI93XBmqTisBFMUTm0b8Fm+jr3Dg1NNxqwp+5A1VGuI=\n-honnef.co/go/tools v0.0.0-20190102054323-c2f93a96b099/go.mod h1:rf3lG4BRIbNafJWhAfAdb/ePZxsR/4RtNHQocxwk9r4=\n-honnef.co/go/tools v0.0.0-20190106161140-3f1c8253044a/go.mod h1:rf3lG4BRIbNafJWhAfAdb/ePZxsR/4RtNHQocxwk9r4=\n-honnef.co/go/tools v0.0.0-20190418001031-e561f6794a2a/go.mod h1:rf3lG4BRIbNafJWhAfAdb/ePZxsR/4RtNHQocxwk9r4=\n-rsc.io/binaryregexp v0.2.0/go.mod h1:qTv7/COck+e2FymRvadv62gMdZztPaShugOCi3I+8D8=\n+gopkg.in/yaml.v2 v2.2.2 h1:ZCJp+EgiOT7lHqUV2J862kp8Qj64Jo6az82+3Td9dZw=\n+gopkg.in/yaml.v2 v2.2.2/go.mod h1:hI93XBmqTisBFMUTm0b8Fm+jr3Dg1NNxqwp+5A1VGuI="
    },
    {
      "sha": "7e3be662ce7f6f3a9199d4da54ecf50bfb10caeb",
      "filename": "backend/vendor/github.com/go-pkgz/auth/middleware/auth.go",
      "status": "modified",
      "additions": 3,
      "deletions": 1,
      "changes": 4,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/go-pkgz/auth/middleware/auth.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/go-pkgz/auth/middleware/auth.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/github.com/go-pkgz/auth/middleware/auth.go?ref=890d7154e777d0673defcd11db3ef792473d68c9",
      "patch": "@@ -5,6 +5,7 @@\n package middleware\n \n import (\n+\t\"crypto/subtle\"\n \t\"net/http\"\n \n \t\"github.com/pkg/errors\"\n@@ -178,7 +179,8 @@ func (a *Authenticator) basicAdminUser(r *http.Request) bool {\n \t\treturn false\n \t}\n \n-\tif user != \"admin\" || passwd != a.AdminPasswd {\n+\t// using ConstantTimeCompare to avoid timing attack\n+\tif user != \"admin\" || subtle.ConstantTimeCompare([]byte(passwd), []byte(a.AdminPasswd)) != 1 {\n \t\ta.Logf(\"[WARN] admin basic auth failed, user/passwd mismatch, %s:%s\", user, passwd)\n \t\treturn false\n \t}"
    },
    {
      "sha": "02c932a436ea5b8190577bd0a4856c68ad027b81",
      "filename": "backend/vendor/github.com/go-pkgz/auth/provider/dev_provider.go",
      "status": "modified",
      "additions": 1,
      "deletions": 1,
      "changes": 2,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/go-pkgz/auth/provider/dev_provider.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/go-pkgz/auth/provider/dev_provider.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/github.com/go-pkgz/auth/provider/dev_provider.go?ref=890d7154e777d0673defcd11db3ef792473d68c9",
      "patch": "@@ -36,7 +36,7 @@ type DevAuthServer struct {\n // Run oauth2 dev server on port devAuthPort\n func (d *DevAuthServer) Run(ctx context.Context) { //nolint (gocyclo)\n \td.username = \"dev_user\"\n-\td.Logf(\"[INFO] run local oauth2 dev server on %d, redir url=%s\", devAuthPort, d.Provider.conf.RedirectURL)\n+\td.Logf(\"[INFO] run local oauth2 dev server on %d, redirect url=%s\", devAuthPort, d.Provider.conf.RedirectURL)\n \td.lock.Lock()\n \tvar err error\n "
    },
    {
      "sha": "2990879ffdfaa3b2dc77cfcf0c8a97ccd0c340e8",
      "filename": "backend/vendor/github.com/go-pkgz/auth/provider/sender/email.go",
      "status": "modified",
      "additions": 5,
      "deletions": 2,
      "changes": 7,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/go-pkgz/auth/provider/sender/email.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/go-pkgz/auth/provider/sender/email.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/github.com/go-pkgz/auth/provider/sender/email.go?ref=890d7154e777d0673defcd11db3ef792473d68c9",
      "patch": "@@ -138,7 +138,7 @@ func (em *Email) client() (c *smtp.Client, err error) {\n \t\treturn nil, errors.Wrapf(err, \"timeout connecting to %s\", srvAddress)\n \t}\n \n-\tc, err = smtp.NewClient(conn, srvAddress)\n+\tc, err = smtp.NewClient(conn, em.Host)\n \tif err != nil {\n \t\treturn nil, errors.Wrap(err, \"failed to dial\")\n \t}\n@@ -166,7 +166,10 @@ func (em *Email) buildMessage(msg, to string) (message string, err error) {\n \tif _, err := qp.Write([]byte(msg)); err != nil {\n \t\treturn \"\", err\n \t}\n-\tdefer qp.Close()\n+\t// flush now, must NOT use defer, for small body, defer may cause buff.String() got empty body\n+\tif err := qp.Close(); err != nil {\n+\t\treturn \"\", errors.Wrapf(err, \"quotedprintable Write failed\")\n+\t}\n \tm := buff.String()\n \tmessage += \"\\n\" + m\n \treturn message, nil"
    },
    {
      "sha": "66fd13c903cac02eb9657cd53fb227823484401d",
      "filename": "backend/vendor/github.com/go-pkgz/expirable-cache/.gitignore",
      "status": "added",
      "additions": 15,
      "deletions": 0,
      "changes": 15,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/go-pkgz/expirable-cache/.gitignore",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/go-pkgz/expirable-cache/.gitignore",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/github.com/go-pkgz/expirable-cache/.gitignore?ref=890d7154e777d0673defcd11db3ef792473d68c9",
      "patch": "@@ -0,0 +1,15 @@\n+# Binaries for programs and plugins\n+*.exe\n+*.exe~\n+*.dll\n+*.so\n+*.dylib\n+\n+# Test binary, built with `go test -c`\n+*.test\n+\n+# Output of the go coverage tool, specifically when used with LiteIDE\n+*.out\n+\n+# Dependency directories (remove the comment below to include it)\n+# vendor/"
    },
    {
      "sha": "7442cd4087080b86164278b5a16aa02f6755e537",
      "filename": "backend/vendor/github.com/go-pkgz/expirable-cache/.golangci.yml",
      "status": "added",
      "additions": 64,
      "deletions": 0,
      "changes": 64,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/go-pkgz/expirable-cache/.golangci.yml",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/go-pkgz/expirable-cache/.golangci.yml",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/github.com/go-pkgz/expirable-cache/.golangci.yml?ref=890d7154e777d0673defcd11db3ef792473d68c9",
      "patch": "@@ -0,0 +1,64 @@\n+linters-settings:\n+  govet:\n+    check-shadowing: true\n+  golint:\n+    min-confidence: 0\n+  gocyclo:\n+    min-complexity: 15\n+  maligned:\n+    suggest-new: true\n+  goconst:\n+    min-len: 2\n+    min-occurrences: 2\n+  misspell:\n+    locale: US\n+  lll:\n+    line-length: 140\n+  gocritic:\n+    enabled-tags:\n+      - performance\n+      - style\n+      - experimental\n+    disabled-checks:\n+      - wrapperFunc\n+\n+linters:\n+  enable:\n+    - megacheck\n+    - golint\n+    - govet\n+    - unconvert\n+    - megacheck\n+    - structcheck\n+    - gas\n+    - gocyclo\n+    - dupl\n+    - misspell\n+    - unparam\n+    - varcheck\n+    - deadcode\n+    - typecheck\n+    - ineffassign\n+    - varcheck\n+    - stylecheck\n+    - gochecknoinits\n+    - scopelint\n+    - gocritic\n+    - nakedret\n+    - gosimple\n+    - prealloc\n+  fast: false\n+  disable-all: true\n+\n+run:\n+  output:\n+    format: tab\n+  skip-dirs:\n+    - vendor\n+\n+issues:\n+  exclude-rules:\n+    - text: \"should have a package comment, unless it's in another file for this package\"\n+      linters:\n+        - golint\n+  exclude-use-default: false"
    },
    {
      "sha": "d4771b094b6d23cda153c32ebb091aefc5b95165",
      "filename": "backend/vendor/github.com/go-pkgz/expirable-cache/LICENSE",
      "status": "renamed",
      "additions": 8,
      "deletions": 5,
      "changes": 13,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/go-pkgz/expirable-cache/LICENSE",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/go-pkgz/expirable-cache/LICENSE",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/github.com/go-pkgz/expirable-cache/LICENSE?ref=890d7154e777d0673defcd11db3ef792473d68c9",
      "patch": "@@ -1,4 +1,7 @@\n-Copyright (c) 2012-2017 Patrick Mylund Nielsen and the go-cache contributors\n+MIT License\n+\n+Copyright (c) 2020 Umputun\n+Copyright (c) 2020 Dmitry Verhoturov\n \n Permission is hereby granted, free of charge, to any person obtaining a copy\n of this software and associated documentation files (the \"Software\"), to deal\n@@ -7,13 +10,13 @@ to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n copies of the Software, and to permit persons to whom the Software is\n furnished to do so, subject to the following conditions:\n \n-The above copyright notice and this permission notice shall be included in\n-all copies or substantial portions of the Software.\n+The above copyright notice and this permission notice shall be included in all\n+copies or substantial portions of the Software.\n \n THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n-OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n-THE SOFTWARE.\n+OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n+SOFTWARE.",
      "previous_filename": "backend/vendor/github.com/patrickmn/go-cache/LICENSE"
    },
    {
      "sha": "e0ab7b7c1b3576f79ef445abebece81f179e5cc2",
      "filename": "backend/vendor/github.com/go-pkgz/expirable-cache/README.md",
      "status": "added",
      "additions": 70,
      "deletions": 0,
      "changes": 70,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/go-pkgz/expirable-cache/README.md",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/go-pkgz/expirable-cache/README.md",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/github.com/go-pkgz/expirable-cache/README.md?ref=890d7154e777d0673defcd11db3ef792473d68c9",
      "patch": "@@ -0,0 +1,70 @@\n+# expirable-cache\n+\n+[![Build Status](https://github.com/go-pkgz/expirable-cache/workflows/build/badge.svg)](https://github.com/go-pkgz/expirable-cache/actions)\n+[![Coverage Status](https://coveralls.io/repos/github/go-pkgz/expirable-cache/badge.svg?branch=master)](https://coveralls.io/github/go-pkgz/expirable-cache?branch=master)\n+[![godoc](https://godoc.org/github.com/go-pkgz/expirable-cache?status.svg)](https://pkg.go.dev/github.com/go-pkgz/expirable-cache?tab=doc)\n+\n+Package cache implements expirable cache.\n+\n+- Support LRC, LRU and TTL-based eviction.\n+- Package is thread-safe and doesn't spawn any goroutines.\n+- On every Set() call, cache deletes single oldest entry in case it's expired.\n+- In case MaxSize is set, cache deletes the oldest entry disregarding its expiration date to maintain the size,\n+either using LRC or LRU eviction.\n+- In case of default TTL (10 years) and default MaxSize (0, unlimited) the cache will be truly unlimited\n+ and will never delete entries from itself automatically.\n+\n+**Important**: only reliable way of not having expired entries stuck in a cache is to\n+run cache.DeleteExpired periodically using [time.Ticker](https://golang.org/pkg/time/#Ticker),\n+advisable period is 1/2 of TTL.\n+\n+This cache is heavily inspired by [hashicorp/golang-lru](https://github.com/hashicorp/golang-lru) _simplelru_ implementation.\n+\n+### Usage example\n+\n+```go\n+package main\n+\n+import (\n+\t\"fmt\"\n+\t\"time\"\n+\n+\t\"github.com/go-pkgz/expirable-cache\"\n+)\n+\n+func main() {\n+\t// make cache with short TTL and 3 max keys\n+\tc, _ := cache.NewCache(cache.MaxKeys(3), cache.TTL(time.Millisecond*10))\n+\n+\t// set value under key1.\n+\t// with 0 ttl (last parameter) will use cache-wide setting instead (10ms).\n+\tc.Set(\"key1\", \"val1\", 0)\n+\n+\t// get value under key1\n+\tr, ok := c.Get(\"key1\")\n+\n+\t// check for OK value, because otherwise return would be nil and\n+\t// type conversion will panic\n+\tif ok {\n+\t\trstr := r.(string) // convert cached value from interface{} to real type\n+\t\tfmt.Printf(\"value before expiration is found: %v, value: %v\\n\", ok, rstr)\n+\t}\n+\n+\ttime.Sleep(time.Millisecond * 11)\n+\n+\t// get value under key1 after key expiration\n+\tr, ok = c.Get(\"key1\")\n+\t// don't convert to string as with ok == false value would be nil\n+\tfmt.Printf(\"value after expiration is found: %v, value: %v\\n\", ok, r)\n+\n+\t// set value under key2, would evict old entry because it is already expired.\n+\t// ttl (last parameter) overrides cache-wide ttl.\n+\tc.Set(\"key2\", \"val2\", time.Minute*5)\n+\n+\tfmt.Printf(\"%+v\\n\", c)\n+\t// Output:\n+\t// value before expiration is found: true, value: val1\n+\t// value after expiration is found: false, value: <nil>\n+\t// Size: 1, Stats: {Hits:1 Misses:1 Added:2 Evicted:1} (50.0%)\n+}\n+```\n\\ No newline at end of file"
    },
    {
      "sha": "46fb7c5cc0f79403386fb011e3fbbeb5250ddf41",
      "filename": "backend/vendor/github.com/go-pkgz/expirable-cache/cache.go",
      "status": "added",
      "additions": 274,
      "deletions": 0,
      "changes": 274,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/go-pkgz/expirable-cache/cache.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/go-pkgz/expirable-cache/cache.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/github.com/go-pkgz/expirable-cache/cache.go?ref=890d7154e777d0673defcd11db3ef792473d68c9",
      "patch": "@@ -0,0 +1,274 @@\n+// Package cache implements Cache similar to hashicorp/golang-lru\n+//\n+// Support LRC, LRU and TTL-based eviction.\n+// Package is thread-safe and doesn't spawn any goroutines.\n+// On every Set() call, cache deletes single oldest entry in case it's expired.\n+// In case MaxSize is set, cache deletes the oldest entry disregarding its expiration date to maintain the size,\n+// either using LRC or LRU eviction.\n+// In case of default TTL (10 years) and default MaxSize (0, unlimited) the cache will be truly unlimited\n+// and will never delete entries from itself automatically.\n+//\n+// Important: only reliable way of not having expired entries stuck in a cache is to\n+// run cache.DeleteExpired periodically using time.Ticker, advisable period is 1/2 of TTL.\n+package cache\n+\n+import (\n+\t\"container/list\"\n+\t\"fmt\"\n+\t\"sync\"\n+\t\"time\"\n+\n+\t\"github.com/pkg/errors\"\n+)\n+\n+// Cache defines cache interface\n+type Cache interface {\n+\tfmt.Stringer\n+\tSet(key string, value interface{}, ttl time.Duration)\n+\tGet(key string) (interface{}, bool)\n+\tPeek(key string) (interface{}, bool)\n+\tKeys() []string\n+\tLen() int\n+\tInvalidate(key string)\n+\tInvalidateFn(fn func(key string) bool)\n+\tRemoveOldest()\n+\tDeleteExpired()\n+\tPurge()\n+\tStat() Stats\n+}\n+\n+// Stats provides statistics for cache\n+type Stats struct {\n+\tHits, Misses   int // cache effectiveness\n+\tAdded, Evicted int // number of added and evicted records\n+}\n+\n+// cacheImpl provides Cache interface implementation.\n+type cacheImpl struct {\n+\tttl       time.Duration\n+\tmaxKeys   int\n+\tisLRU     bool\n+\tonEvicted func(key string, value interface{})\n+\n+\tsync.Mutex\n+\tstat      Stats\n+\titems     map[string]*list.Element\n+\tevictList *list.List\n+}\n+\n+// noEvictionTTL - very long ttl to prevent eviction\n+const noEvictionTTL = time.Hour * 24 * 365 * 10\n+\n+// NewCache returns a new Cache.\n+// Default MaxKeys is unlimited (0).\n+// Default TTL is 10 years, sane value for expirable cache is 5 minutes.\n+// Default eviction mode is LRC, appropriate option allow to change it to LRU.\n+func NewCache(options ...Option) (Cache, error) {\n+\tres := cacheImpl{\n+\t\titems:     map[string]*list.Element{},\n+\t\tevictList: list.New(),\n+\t\tttl:       noEvictionTTL,\n+\t\tmaxKeys:   0,\n+\t}\n+\n+\tfor _, opt := range options {\n+\t\tif err := opt(&res); err != nil {\n+\t\t\treturn nil, errors.Wrap(err, \"failed to set cache option\")\n+\t\t}\n+\t}\n+\treturn &res, nil\n+}\n+\n+// Set key, ttl of 0 would use cache-wide TTL\n+func (c *cacheImpl) Set(key string, value interface{}, ttl time.Duration) {\n+\tc.Lock()\n+\tdefer c.Unlock()\n+\tnow := time.Now()\n+\tif ttl == 0 {\n+\t\tttl = c.ttl\n+\t}\n+\n+\t// Check for existing item\n+\tif ent, ok := c.items[key]; ok {\n+\t\tc.evictList.MoveToFront(ent)\n+\t\tent.Value.(*cacheItem).value = value\n+\t\tent.Value.(*cacheItem).expiresAt = now.Add(ttl)\n+\t\treturn\n+\t}\n+\n+\t// Add new item\n+\tent := &cacheItem{key: key, value: value, expiresAt: now.Add(ttl)}\n+\tentry := c.evictList.PushFront(ent)\n+\tc.items[key] = entry\n+\tc.stat.Added++\n+\n+\t// Remove oldest entry if it is expired, only in case of non-default TTL.\n+\tif c.ttl != noEvictionTTL || ttl != noEvictionTTL {\n+\t\tc.removeOldestIfExpired()\n+\t}\n+\n+\t// Verify size not exceeded\n+\tif c.maxKeys > 0 && len(c.items) > c.maxKeys {\n+\t\tc.removeOldest()\n+\t}\n+}\n+\n+// Get returns the key value if it's not expired\n+func (c *cacheImpl) Get(key string) (interface{}, bool) {\n+\tc.Lock()\n+\tdefer c.Unlock()\n+\tif ent, ok := c.items[key]; ok {\n+\t\t// Expired item check\n+\t\tif time.Now().After(ent.Value.(*cacheItem).expiresAt) {\n+\t\t\tc.stat.Misses++\n+\t\t\treturn nil, false\n+\t\t}\n+\t\tif c.isLRU {\n+\t\t\tc.evictList.MoveToFront(ent)\n+\t\t}\n+\t\tc.stat.Hits++\n+\t\treturn ent.Value.(*cacheItem).value, true\n+\t}\n+\tc.stat.Misses++\n+\treturn nil, false\n+}\n+\n+// Peek returns the key value (or undefined if not found) without updating the \"recently used\"-ness of the key.\n+// Works exactly the same as Get in case of LRC mode (default one).\n+func (c *cacheImpl) Peek(key string) (interface{}, bool) {\n+\tc.Lock()\n+\tdefer c.Unlock()\n+\tif ent, ok := c.items[key]; ok {\n+\t\t// Expired item check\n+\t\tif time.Now().After(ent.Value.(*cacheItem).expiresAt) {\n+\t\t\tc.stat.Misses++\n+\t\t\treturn nil, false\n+\t\t}\n+\t\tc.stat.Hits++\n+\t\treturn ent.Value.(*cacheItem).value, true\n+\t}\n+\tc.stat.Misses++\n+\treturn nil, false\n+}\n+\n+// Keys returns a slice of the keys in the cache, from oldest to newest.\n+func (c *cacheImpl) Keys() []string {\n+\tc.Lock()\n+\tdefer c.Unlock()\n+\treturn c.keys()\n+}\n+\n+// Len return count of items in cache, including expired\n+func (c *cacheImpl) Len() int {\n+\tc.Lock()\n+\tdefer c.Unlock()\n+\treturn c.evictList.Len()\n+}\n+\n+// Invalidate key (item) from the cache\n+func (c *cacheImpl) Invalidate(key string) {\n+\tc.Lock()\n+\tdefer c.Unlock()\n+\tif ent, ok := c.items[key]; ok {\n+\t\tc.removeElement(ent)\n+\t}\n+}\n+\n+// InvalidateFn deletes multiple keys if predicate is true\n+func (c *cacheImpl) InvalidateFn(fn func(key string) bool) {\n+\tc.Lock()\n+\tdefer c.Unlock()\n+\tfor key, ent := range c.items {\n+\t\tif fn(key) {\n+\t\t\tc.removeElement(ent)\n+\t\t}\n+\t}\n+}\n+\n+// RemoveOldest remove oldest element in the cache\n+func (c *cacheImpl) RemoveOldest() {\n+\tc.Lock()\n+\tdefer c.Unlock()\n+\tc.removeOldest()\n+}\n+\n+// DeleteExpired clears cache of expired items\n+func (c *cacheImpl) DeleteExpired() {\n+\tc.Lock()\n+\tdefer c.Unlock()\n+\tfor _, key := range c.keys() {\n+\t\tif time.Now().After(c.items[key].Value.(*cacheItem).expiresAt) {\n+\t\t\tc.removeElement(c.items[key])\n+\t\t}\n+\t}\n+}\n+\n+// Purge clears the cache completely.\n+func (c *cacheImpl) Purge() {\n+\tc.Lock()\n+\tdefer c.Unlock()\n+\tfor k, v := range c.items {\n+\t\tdelete(c.items, k)\n+\t\tc.stat.Evicted++\n+\t\tif c.onEvicted != nil {\n+\t\t\tc.onEvicted(k, v.Value.(*cacheItem).value)\n+\t\t}\n+\t}\n+\tc.evictList.Init()\n+}\n+\n+// Stat gets the current stats for cache\n+func (c *cacheImpl) Stat() Stats {\n+\tc.Lock()\n+\tdefer c.Unlock()\n+\treturn c.stat\n+}\n+\n+func (c *cacheImpl) String() string {\n+\tstats := c.Stat()\n+\tsize := c.Len()\n+\treturn fmt.Sprintf(\"Size: %d, Stats: %+v (%0.1f%%)\", size, stats, 100*float64(stats.Hits)/float64(stats.Hits+stats.Misses))\n+}\n+\n+// Keys returns a slice of the keys in the cache, from oldest to newest. Has to be called with lock!\n+func (c *cacheImpl) keys() []string {\n+\tkeys := make([]string, 0, len(c.items))\n+\tfor ent := c.evictList.Back(); ent != nil; ent = ent.Prev() {\n+\t\tkeys = append(keys, ent.Value.(*cacheItem).key)\n+\t}\n+\treturn keys\n+}\n+\n+// removeOldest removes the oldest item from the cache. Has to be called with lock!\n+func (c *cacheImpl) removeOldest() {\n+\tent := c.evictList.Back()\n+\tif ent != nil {\n+\t\tc.removeElement(ent)\n+\t}\n+}\n+\n+// removeOldest removes the oldest item from the cache in case it's already expired. Has to be called with lock!\n+func (c *cacheImpl) removeOldestIfExpired() {\n+\tent := c.evictList.Back()\n+\tif ent != nil && time.Now().After(ent.Value.(*cacheItem).expiresAt) {\n+\t\tc.removeElement(ent)\n+\t}\n+}\n+\n+// removeElement is used to remove a given list element from the cache. Has to be called with lock!\n+func (c *cacheImpl) removeElement(e *list.Element) {\n+\tc.evictList.Remove(e)\n+\tkv := e.Value.(*cacheItem)\n+\tdelete(c.items, kv.key)\n+\tc.stat.Evicted++\n+\tif c.onEvicted != nil {\n+\t\tc.onEvicted(kv.key, kv.value)\n+\t}\n+}\n+\n+// cacheItem is used to hold a value in the evictList\n+type cacheItem struct {\n+\texpiresAt time.Time\n+\tkey       string\n+\tvalue     interface{}\n+}"
    },
    {
      "sha": "5609ec4bc0ad14e9663bb8daa33032037bd23bf5",
      "filename": "backend/vendor/github.com/go-pkgz/expirable-cache/go.mod",
      "status": "added",
      "additions": 8,
      "deletions": 0,
      "changes": 8,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/go-pkgz/expirable-cache/go.mod",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/go-pkgz/expirable-cache/go.mod",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/github.com/go-pkgz/expirable-cache/go.mod?ref=890d7154e777d0673defcd11db3ef792473d68c9",
      "patch": "@@ -0,0 +1,8 @@\n+module github.com/go-pkgz/expirable-cache\n+\n+go 1.14\n+\n+require (\n+\tgithub.com/pkg/errors v0.9.1\n+\tgithub.com/stretchr/testify v1.5.1\n+)"
    },
    {
      "sha": "4516e6a8f894693d4578c06d7cf09c4e728ea878",
      "filename": "backend/vendor/github.com/go-pkgz/expirable-cache/go.sum",
      "status": "added",
      "additions": 13,
      "deletions": 0,
      "changes": 13,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/go-pkgz/expirable-cache/go.sum",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/go-pkgz/expirable-cache/go.sum",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/github.com/go-pkgz/expirable-cache/go.sum?ref=890d7154e777d0673defcd11db3ef792473d68c9",
      "patch": "@@ -0,0 +1,13 @@\n+github.com/davecgh/go-spew v1.1.0 h1:ZDRjVQ15GmhC3fiQ8ni8+OwkZQO4DARzQgrnXU1Liz8=\n+github.com/davecgh/go-spew v1.1.0/go.mod h1:J7Y8YcW2NihsgmVo/mv3lAwl/skON4iLHjSsI+c5H38=\n+github.com/pkg/errors v0.9.1 h1:FEBLx1zS214owpjy7qsBeixbURkuhQAwrK5UwLGTwt4=\n+github.com/pkg/errors v0.9.1/go.mod h1:bwawxfHBFNV+L2hUp1rHADufV3IMtnDRdf1r5NINEl0=\n+github.com/pmezard/go-difflib v1.0.0 h1:4DBwDE0NGyQoBHbLQYPwSUPoCMWR5BEzIk/f1lZbAQM=\n+github.com/pmezard/go-difflib v1.0.0/go.mod h1:iKH77koFhYxTK1pcRnkKkqfTogsbg7gZNVY4sRDYZ/4=\n+github.com/stretchr/objx v0.1.0/go.mod h1:HFkY916IF+rwdDfMAkV7OtwuqBVzrE8GR6GFx+wExME=\n+github.com/stretchr/testify v1.5.1 h1:nOGnQDM7FYENwehXlg/kFVnos3rEvtKTjRvOWSzb6H4=\n+github.com/stretchr/testify v1.5.1/go.mod h1:5W2xD1RspED5o8YsWQXVCued0rvSQ+mT+I5cxcmMvtA=\n+gopkg.in/check.v1 v0.0.0-20161208181325-20d25e280405 h1:yhCVgyC4o1eVCa2tZl7eS0r+SDo693bJlVdllGtEeKM=\n+gopkg.in/check.v1 v0.0.0-20161208181325-20d25e280405/go.mod h1:Co6ibVJAznAaIkqp8huTwlJQCZ016jof/cbN4VW5Yz0=\n+gopkg.in/yaml.v2 v2.2.2 h1:ZCJp+EgiOT7lHqUV2J862kp8Qj64Jo6az82+3Td9dZw=\n+gopkg.in/yaml.v2 v2.2.2/go.mod h1:hI93XBmqTisBFMUTm0b8Fm+jr3Dg1NNxqwp+5A1VGuI="
    },
    {
      "sha": "924345a0a017bbc6401180a6fc412fa6fa0821bf",
      "filename": "backend/vendor/github.com/go-pkgz/expirable-cache/options.go",
      "status": "added",
      "additions": 40,
      "deletions": 0,
      "changes": 40,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/go-pkgz/expirable-cache/options.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/go-pkgz/expirable-cache/options.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/github.com/go-pkgz/expirable-cache/options.go?ref=890d7154e777d0673defcd11db3ef792473d68c9",
      "patch": "@@ -0,0 +1,40 @@\n+package cache\n+\n+import \"time\"\n+\n+// Option func type\n+type Option func(lc *cacheImpl) error\n+\n+// OnEvicted called automatically for automatically and manually deleted entries\n+func OnEvicted(fn func(key string, value interface{})) Option {\n+\treturn func(lc *cacheImpl) error {\n+\t\tlc.onEvicted = fn\n+\t\treturn nil\n+\t}\n+}\n+\n+// MaxKeys functional option defines how many keys to keep.\n+// By default it is 0, which means unlimited.\n+func MaxKeys(max int) Option {\n+\treturn func(lc *cacheImpl) error {\n+\t\tlc.maxKeys = max\n+\t\treturn nil\n+\t}\n+}\n+\n+// TTL functional option defines TTL for all cache entries.\n+// By default it is set to 10 years, sane option for expirable cache might be 5 minutes.\n+func TTL(ttl time.Duration) Option {\n+\treturn func(lc *cacheImpl) error {\n+\t\tlc.ttl = ttl\n+\t\treturn nil\n+\t}\n+}\n+\n+// LRU sets cache to LRU (Least Recently Used) eviction mode.\n+func LRU() Option {\n+\treturn func(lc *cacheImpl) error {\n+\t\tlc.isLRU = true\n+\t\treturn nil\n+\t}\n+}"
    },
    {
      "sha": "7fc697c0d2335b5ae63f84c8616377c0c68d1ab8",
      "filename": "backend/vendor/github.com/go-pkgz/jrpc/.golangci.yml",
      "status": "modified",
      "additions": 6,
      "deletions": 10,
      "changes": 16,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/go-pkgz/jrpc/.golangci.yml",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/go-pkgz/jrpc/.golangci.yml",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/github.com/go-pkgz/jrpc/.golangci.yml?ref=890d7154e777d0673defcd11db3ef792473d68c9",
      "patch": "@@ -25,7 +25,6 @@ linters-settings:\n       - wrapperFunc\n \n linters:\n-  disable-all: true\n   enable:\n     - megacheck\n     - golint\n@@ -47,24 +46,21 @@ linters:\n     - gochecknoinits\n     - scopelint\n     - gocritic\n-    - golint\n     - nakedret\n     - gosimple\n     - prealloc\n   fast: false\n-\n+  disable-all: true\n \n run:\n-#  modules-download-mode: vendor\n+  output:\n+    format: tab\n   skip-dirs:\n     - vendor\n-  tests: true\n \n issues:\n   exclude-rules:\n-    - text: \"weak cryptographic primitive\"\n+    - text: \"should have a package comment, unless it's in another file for this package\"\n       linters:\n-        - gosec\n-\n-service:\n-  golangci-lint-version: 1.17.x\n\\ No newline at end of file\n+        - golint\n+  exclude-use-default: false"
    },
    {
      "sha": "0f0c5bdca4574710328bcf3141cda62bc282c5d9",
      "filename": "backend/vendor/github.com/go-pkgz/jrpc/.travis.yml",
      "status": "removed",
      "additions": 0,
      "deletions": 21,
      "changes": 21,
      "blob_url": "https://github.com/umputun/remark42/blob/0d67f7e53d4c07b9d299d2dd2d8d33b75fc83a23/backend/vendor/github.com/go-pkgz/jrpc/.travis.yml",
      "raw_url": "https://github.com/umputun/remark42/raw/0d67f7e53d4c07b9d299d2dd2d8d33b75fc83a23/backend/vendor/github.com/go-pkgz/jrpc/.travis.yml",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/github.com/go-pkgz/jrpc/.travis.yml?ref=0d67f7e53d4c07b9d299d2dd2d8d33b75fc83a23",
      "patch": "@@ -1,21 +0,0 @@\n-language: go\n-\n-go:\n-  - \"1.12.x\"\n-\n-install: true\n-\n-before_install:\n-  - export TZ=America/Chicago\n-  - curl -sfL https://install.goreleaser.com/github.com/golangci/golangci-lint.sh | sh -s -- -b $(go env GOPATH)/bin v1.17.1\n-  - go get github.com/mattn/goveralls\n-  - export PATH=$(pwd)/bin:$PATH\n-\n-script:\n-  - GO111MODULE=on go get ./...\n-  - GO111MODULE=on go mod vendor\n-  - GO111MODULE=on go test  -v -mod=vendor -covermode=count -coverprofile=profile.cov ./... || travis_terminate 1;\n-  - GO111MODULE=on go test -v -covermode=count -coverprofile=profile.cov ./... || travis_terminate 1;\n-  - golangci-lint run -v || travis_terminate 1;\n-  - $GOPATH/bin/goveralls -coverprofile=profile.cov -service=travis-ci\n-"
    },
    {
      "sha": "c1b684bfdd131c354389d84f2b103eba2eaf9536",
      "filename": "backend/vendor/github.com/go-pkgz/jrpc/LICENSE",
      "status": "modified",
      "additions": 1,
      "deletions": 1,
      "changes": 2,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/go-pkgz/jrpc/LICENSE",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/go-pkgz/jrpc/LICENSE",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/github.com/go-pkgz/jrpc/LICENSE?ref=890d7154e777d0673defcd11db3ef792473d68c9",
      "patch": "@@ -1,6 +1,6 @@\n MIT License\n \n-Copyright (c) 2019 Umputun\n+Copyright (c) 2020 Umputun\n \n Permission is hereby granted, free of charge, to any person obtaining a copy\n of this software and associated documentation files (the \"Software\"), to deal"
    },
    {
      "sha": "934b1b8960754527f2d418c37615e3372d08e111",
      "filename": "backend/vendor/github.com/go-pkgz/jrpc/README.md",
      "status": "modified",
      "additions": 3,
      "deletions": 1,
      "changes": 4,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/go-pkgz/jrpc/README.md",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/go-pkgz/jrpc/README.md",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/github.com/go-pkgz/jrpc/README.md?ref=890d7154e777d0673defcd11db3ef792473d68c9",
      "patch": "@@ -73,6 +73,7 @@ if err = json.Unmarshal(*resp.Result, &message); err != nil {\n         }\n      ```\n  </details>\n+ \n * Params can be a struct, primitive type or slice of values, even with different types.\n * Server defines `ServerFn` handler function to react on a POST request. The handler provided by the user.\n * Communication between the server and the caller can be protected with basic auth.\n@@ -89,11 +90,12 @@ if err = json.Unmarshal(*resp.Result, &message); err != nil {\n     }\n    ```\n  </details>\n+ \n * User should encode and decode json payloads on the application level, see provided [examples](https://github.com/go-pkgz/jrpc/tree/master/_example)\n * `jrpc.Server` doesn't support https internally (yet). If used on exposed or non-private networks, should be proxied with something providing https termination (nginx and others). \n \n ## Status\n \n The code was extracted from [remark42](https://github.com/umputun/remark) and still under development. Until v1.x released the\n  API & protocol may change.\n- \n\\ No newline at end of file\n+ "
    },
    {
      "sha": "73e233d91429d34d72c71486edd71e967851cb72",
      "filename": "backend/vendor/github.com/go-pkgz/jrpc/go.mod",
      "status": "modified",
      "additions": 7,
      "deletions": 9,
      "changes": 16,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/go-pkgz/jrpc/go.mod",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/go-pkgz/jrpc/go.mod",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/github.com/go-pkgz/jrpc/go.mod?ref=890d7154e777d0673defcd11db3ef792473d68c9",
      "patch": "@@ -3,14 +3,12 @@ module github.com/go-pkgz/jrpc\n go 1.12\n \n require (\n-\tgithub.com/didip/tollbooth v4.0.2+incompatible\n-\tgithub.com/didip/tollbooth_chi v0.0.0-20170928041846-6ab5f3083f3d\n-\tgithub.com/go-chi/chi v4.0.2+incompatible\n+\tgithub.com/didip/tollbooth/v6 v6.0.1\n+\tgithub.com/didip/tollbooth_chi v0.0.0-20200524181329-8b84cd7183d9\n+\tgithub.com/go-chi/chi v4.1.1+incompatible\n \tgithub.com/go-chi/render v1.0.1\n-\tgithub.com/go-pkgz/rest v1.4.1\n-\tgithub.com/patrickmn/go-cache v2.1.0+incompatible // indirect\n-\tgithub.com/pkg/errors v0.8.1\n-\tgithub.com/stretchr/testify v1.3.0\n-\tgolang.org/x/net v0.0.0-20190724013045-ca1201d0de80 // indirect\n-\tgolang.org/x/time v0.0.0-20190308202827-9d24e82272b4 // indirect\n+\tgithub.com/go-pkgz/rest v1.5.0\n+\tgithub.com/pkg/errors v0.9.1\n+\tgithub.com/stretchr/testify v1.5.1\n+\tgolang.org/x/net v0.0.0-20200520182314-0ba52f642ac2 // indirect\n )"
    },
    {
      "sha": "24258b7d1eeff838cbae8b217f1226020e24bfbe",
      "filename": "backend/vendor/github.com/go-pkgz/jrpc/go.sum",
      "status": "modified",
      "additions": 23,
      "deletions": 19,
      "changes": 42,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/go-pkgz/jrpc/go.sum",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/go-pkgz/jrpc/go.sum",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/github.com/go-pkgz/jrpc/go.sum?ref=890d7154e777d0673defcd11db3ef792473d68c9",
      "patch": "@@ -1,32 +1,36 @@\n github.com/davecgh/go-spew v1.1.0/go.mod h1:J7Y8YcW2NihsgmVo/mv3lAwl/skON4iLHjSsI+c5H38=\n github.com/davecgh/go-spew v1.1.1 h1:vj9j/u1bqnvCEfJOwUhtlOARqs3+rkHYY13jYWTU97c=\n github.com/davecgh/go-spew v1.1.1/go.mod h1:J7Y8YcW2NihsgmVo/mv3lAwl/skON4iLHjSsI+c5H38=\n-github.com/didip/tollbooth v4.0.2+incompatible h1:fVSa33JzSz0hoh2NxpwZtksAzAgd7zjmGO20HCZtF4M=\n-github.com/didip/tollbooth v4.0.2+incompatible/go.mod h1:A9b0665CE6l1KmzpDws2++elm/CsuWBMa5Jv4WY0PEY=\n-github.com/didip/tollbooth_chi v0.0.0-20170928041846-6ab5f3083f3d h1:vs5Nf6IE0N/PwGJ8//zRed4gpCdcr99K2HzX7RuLOQ8=\n-github.com/didip/tollbooth_chi v0.0.0-20170928041846-6ab5f3083f3d/go.mod h1:YWyIfq3y4ArRfWZ9XksmuusP+7Mad+T0iFZ0kv0XG/M=\n-github.com/go-chi/chi v4.0.2+incompatible h1:maB6vn6FqCxrpz4FqWdh4+lwpyZIQS7YEAUcHlgXVRs=\n-github.com/go-chi/chi v4.0.2+incompatible/go.mod h1:eB3wogJHnLi3x/kFX2A+IbTBlXxmMeXJVKy9tTv1XzQ=\n+github.com/didip/tollbooth/v6 v6.0.1 h1:QvLvRpB1G2bzKvkRze0muMUBlGN9H1z7tJ4DH4ypWOU=\n+github.com/didip/tollbooth/v6 v6.0.1/go.mod h1:j2pKs+JQ5PvU/K4jFnrnwntrmfUbYLJE5oSdxR37FD0=\n+github.com/didip/tollbooth_chi v0.0.0-20200524181329-8b84cd7183d9 h1:gTh8fKuI/yLqQtZEPlDX3ZGsiTPZIe0ADHsxXSbwO1I=\n+github.com/didip/tollbooth_chi v0.0.0-20200524181329-8b84cd7183d9/go.mod h1:YWyIfq3y4ArRfWZ9XksmuusP+7Mad+T0iFZ0kv0XG/M=\n+github.com/go-chi/chi v4.1.1+incompatible h1:MmTgB0R8Bt/jccxp+t6S/1VGIKdJw5J74CK/c9tTfA4=\n+github.com/go-chi/chi v4.1.1+incompatible/go.mod h1:eB3wogJHnLi3x/kFX2A+IbTBlXxmMeXJVKy9tTv1XzQ=\n github.com/go-chi/render v1.0.1 h1:4/5tis2cKaNdnv9zFLfXzcquC9HbeZgCnxGnKrltBS8=\n github.com/go-chi/render v1.0.1/go.mod h1:pq4Rr7HbnsdaeHagklXub+p6Wd16Af5l9koip1OvJns=\n-github.com/go-pkgz/rest v1.4.1 h1:DmaVLPH2O7yLehrWOW0uz01d2mVHz9fBR/iuTiPRzaw=\n-github.com/go-pkgz/rest v1.4.1/go.mod h1:COazNj35u3RXAgQNBr6neR599tYP3URiOpsu9p0rOtk=\n-github.com/hashicorp/golang-lru v0.5.0/go.mod h1:/m3WP610KZHVQ1SGc6re/UDhFvYD7pJ4Ao+sR/qLZy8=\n-github.com/patrickmn/go-cache v2.1.0+incompatible h1:HRMgzkcYKYpi3C8ajMPV8OFXaaRUnok+kx1WdO15EQc=\n-github.com/patrickmn/go-cache v2.1.0+incompatible/go.mod h1:3Qf8kWWT7OJRJbdiICTKqZju1ZixQ/KpMGzzAfe6+WQ=\n-github.com/pkg/errors v0.8.0/go.mod h1:bwawxfHBFNV+L2hUp1rHADufV3IMtnDRdf1r5NINEl0=\n-github.com/pkg/errors v0.8.1 h1:iURUrRGxPUNPdy5/HRSm+Yj6okJ6UtLINN0Q9M4+h3I=\n-github.com/pkg/errors v0.8.1/go.mod h1:bwawxfHBFNV+L2hUp1rHADufV3IMtnDRdf1r5NINEl0=\n+github.com/go-pkgz/expirable-cache v0.0.3 h1:rTh6qNPp78z0bQE6HDhXBHUwqnV9i09Vm6dksJLXQDc=\n+github.com/go-pkgz/expirable-cache v0.0.3/go.mod h1:+IauqN00R2FqNRLCLA+X5YljQJrwB179PfiAoMPlTlQ=\n+github.com/go-pkgz/rest v1.5.0 h1:C8SxXcXza4GiUUAn/95iCkvoIrGbS30qpwK19iqlrWQ=\n+github.com/go-pkgz/rest v1.5.0/go.mod h1:nQaM3RhSTUAmbBZWY4hfe4buyeC9VckvhoCktiQXJxI=\n+github.com/pkg/errors v0.9.1 h1:FEBLx1zS214owpjy7qsBeixbURkuhQAwrK5UwLGTwt4=\n+github.com/pkg/errors v0.9.1/go.mod h1:bwawxfHBFNV+L2hUp1rHADufV3IMtnDRdf1r5NINEl0=\n github.com/pmezard/go-difflib v1.0.0 h1:4DBwDE0NGyQoBHbLQYPwSUPoCMWR5BEzIk/f1lZbAQM=\n github.com/pmezard/go-difflib v1.0.0/go.mod h1:iKH77koFhYxTK1pcRnkKkqfTogsbg7gZNVY4sRDYZ/4=\n github.com/stretchr/objx v0.1.0/go.mod h1:HFkY916IF+rwdDfMAkV7OtwuqBVzrE8GR6GFx+wExME=\n-github.com/stretchr/testify v1.3.0 h1:TivCn/peBQ7UY8ooIcPgZFpTNSz0Q2U6UrFlUfqbe0Q=\n github.com/stretchr/testify v1.3.0/go.mod h1:M5WIy9Dh21IEIfnGCwXGc5bZfKNJtfHm1UVUgZn+9EI=\n+github.com/stretchr/testify v1.5.1 h1:nOGnQDM7FYENwehXlg/kFVnos3rEvtKTjRvOWSzb6H4=\n+github.com/stretchr/testify v1.5.1/go.mod h1:5W2xD1RspED5o8YsWQXVCued0rvSQ+mT+I5cxcmMvtA=\n golang.org/x/crypto v0.0.0-20190308221718-c2843e01d9a2/go.mod h1:djNgcEr1/C05ACkg1iLfiJU5Ep61QUkGW8qpdssI0+w=\n-golang.org/x/net v0.0.0-20190724013045-ca1201d0de80 h1:Ao/3l156eZf2AW5wK8a7/smtodRU+gha3+BeqJ69lRk=\n-golang.org/x/net v0.0.0-20190724013045-ca1201d0de80/go.mod h1:z5CRVTTTmAJ677TzLLGU+0bjPO0LkuOLi4/5GtJWs/s=\n+golang.org/x/net v0.0.0-20200520182314-0ba52f642ac2 h1:eDrdRpKgkcCqKZQwyZRyeFZgfqt37SL7Kv3tok06cKE=\n+golang.org/x/net v0.0.0-20200520182314-0ba52f642ac2/go.mod h1:qpuaurCH72eLCgpAm/N6yyVIVM9cpaDIP3A8BGJEC5A=\n golang.org/x/sys v0.0.0-20190215142949-d0b11bdaac8a/go.mod h1:STP8DvDyc/dI5b8T5hshtkjS+E42TnysNCUPdjciGhY=\n+golang.org/x/sys v0.0.0-20200323222414-85ca7c5b95cd/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\n golang.org/x/text v0.3.0 h1:g61tztE5qeGQ89tm6NTjjM9VPIm088od1l6aSorWRWg=\n golang.org/x/text v0.3.0/go.mod h1:NqM8EUOU14njkJ3fqMW+pc6Ldnwhi/IjpwHt7yyuwOQ=\n-golang.org/x/time v0.0.0-20190308202827-9d24e82272b4 h1:SvFZT6jyqRaOeXpc5h/JSfZenJ2O330aBsf7JfSUXmQ=\n-golang.org/x/time v0.0.0-20190308202827-9d24e82272b4/go.mod h1:tRJNPiyCQ0inRvYxbN9jk5I+vvW/OXSQhTDSoE431IQ=\n+golang.org/x/time v0.0.0-20200416051211-89c76fbcd5d1 h1:NusfzzA6yGQ+ua51ck7E3omNUX/JuqbFSaRGqU8CcLI=\n+golang.org/x/time v0.0.0-20200416051211-89c76fbcd5d1/go.mod h1:tRJNPiyCQ0inRvYxbN9jk5I+vvW/OXSQhTDSoE431IQ=\n+gopkg.in/check.v1 v0.0.0-20161208181325-20d25e280405 h1:yhCVgyC4o1eVCa2tZl7eS0r+SDo693bJlVdllGtEeKM=\n+gopkg.in/check.v1 v0.0.0-20161208181325-20d25e280405/go.mod h1:Co6ibVJAznAaIkqp8huTwlJQCZ016jof/cbN4VW5Yz0=\n+gopkg.in/yaml.v2 v2.2.2 h1:ZCJp+EgiOT7lHqUV2J862kp8Qj64Jo6az82+3Td9dZw=\n+gopkg.in/yaml.v2 v2.2.2/go.mod h1:hI93XBmqTisBFMUTm0b8Fm+jr3Dg1NNxqwp+5A1VGuI="
    },
    {
      "sha": "3acdf4b2fc1e38cffe5ace5e925378b06cbfc675",
      "filename": "backend/vendor/github.com/go-pkgz/jrpc/server.go",
      "status": "modified",
      "additions": 46,
      "deletions": 8,
      "changes": 54,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/go-pkgz/jrpc/server.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/go-pkgz/jrpc/server.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/github.com/go-pkgz/jrpc/server.go?ref=890d7154e777d0673defcd11db3ef792473d68c9",
      "patch": "@@ -8,7 +8,7 @@ import (\n \t\"sync\"\n \t\"time\"\n \n-\t\"github.com/didip/tollbooth\"\n+\t\"github.com/didip/tollbooth/v6\"\n \t\"github.com/didip/tollbooth_chi\"\n \t\"github.com/go-chi/chi\"\n \t\"github.com/go-chi/chi/middleware\"\n@@ -25,6 +25,7 @@ type Server struct {\n \tAuthPasswd string // basic auth password, should match Client.AuthPasswd, optional\n \tVersion    string // server version, injected from main and used for informational headers only\n \tAppName    string // plugin name, injected from main and used for informational headers only\n+\tLimits     Limits // all max values and timeouts for the server\n \tLogger     L      // logger, if nil will default to NoOpLogger\n \n \tfuncs struct {\n@@ -38,6 +39,16 @@ type Server struct {\n \t}\n }\n \n+// Limits includes all max values and timeouts for the server\n+type Limits struct {\n+\tServerThrottle    int           // max number of parallel calls for the server\n+\tClientLimit       float64       // max number of call/sec per client\n+\tCallTimeout       time.Duration // max time allowed to finish the call\n+\tReadHeaderTimeout time.Duration // amount of time allowed to read request headers\n+\tWriteTimeout      time.Duration // max duration before timing out writes of the response\n+\tIdleTimeout       time.Duration // max amount of time to wait for the next request when keep-alive enabled\n+}\n+\n // ServerFn handler registered for each method with Add or Group.\n // Implementations provided by consumer and defines response logic.\n type ServerFn func(id uint64, params json.RawMessage) Response\n@@ -53,13 +64,14 @@ func (s *Server) Run(port int) error {\n \tif s.funcs.m == nil && len(s.funcs.m) == 0 {\n \t\treturn errors.Errorf(\"nothing mapped for dispatch, Add has to be called prior to Run\")\n \t}\n+\ts.setDefaultLimits()\n \n \trouter := chi.NewRouter()\n-\trouter.Use(middleware.Throttle(1000), middleware.RealIP, rest.Recoverer(s.Logger))\n+\trouter.Use(middleware.Throttle(s.Limits.ServerThrottle), middleware.RealIP, rest.Recoverer(s.Logger))\n \trouter.Use(rest.AppInfo(s.AppName, \"umputun\", s.Version), rest.Ping)\n-\tlogInfoWithBody := logger.New(logger.Log(s.Logger), logger.WithBody, logger.Prefix(\"[INFO]\")).Handler\n-\trouter.Use(middleware.Timeout(5 * time.Second))\n-\trouter.Use(logInfoWithBody, tollbooth_chi.LimitHandler(tollbooth.NewLimiter(1000, nil)), middleware.NoCache)\n+\tlogInfoWithBody := logger.New(logger.Log(s.Logger), logger.WithBody, logger.Prefix(\"[DEBUG]\")).Handler\n+\trouter.Use(middleware.Timeout(s.Limits.CallTimeout))\n+\trouter.Use(logInfoWithBody, tollbooth_chi.LimitHandler(tollbooth.NewLimiter(s.Limits.ClientLimit, nil)), middleware.NoCache)\n \trouter.Use(s.basicAuth)\n \n \trouter.Post(s.API, s.handler)\n@@ -68,9 +80,9 @@ func (s *Server) Run(port int) error {\n \ts.httpServer.Server = &http.Server{\n \t\tAddr:              fmt.Sprintf(\":%d\", port),\n \t\tHandler:           router,\n-\t\tReadHeaderTimeout: 5 * time.Second,\n-\t\tWriteTimeout:      10 * time.Second,\n-\t\tIdleTimeout:       30 * time.Second,\n+\t\tReadHeaderTimeout: s.Limits.ReadHeaderTimeout,\n+\t\tWriteTimeout:      s.Limits.WriteTimeout,\n+\t\tIdleTimeout:       s.Limits.IdleTimeout,\n \t}\n \ts.httpServer.Unlock()\n \n@@ -162,6 +174,32 @@ func (s *Server) basicAuth(h http.Handler) http.Handler {\n \t})\n }\n \n+func (s *Server) setDefaultLimits() {\n+\tif s.Limits.CallTimeout == 0 {\n+\t\ts.Limits.CallTimeout = 30 * time.Second\n+\t}\n+\n+\tif s.Limits.ClientLimit == 0 {\n+\t\ts.Limits.ClientLimit = 100\n+\t}\n+\n+\tif s.Limits.IdleTimeout == 0 {\n+\t\ts.Limits.IdleTimeout = 5 * time.Second\n+\t}\n+\n+\tif s.Limits.ReadHeaderTimeout == 0 {\n+\t\ts.Limits.ReadHeaderTimeout = 5 * time.Second\n+\t}\n+\n+\tif s.Limits.ServerThrottle == 0 {\n+\t\ts.Limits.ServerThrottle = 1000\n+\t}\n+\n+\tif s.Limits.WriteTimeout == 0 {\n+\t\ts.Limits.WriteTimeout = 10 * time.Second\n+\t}\n+}\n+\n // L defined logger interface used for an optional rest logging\n type L interface {\n \tLogf(format string, args ...interface{})"
    },
    {
      "sha": "0bdeaf6aee16f24d046afbff38926ac6eb7b7d32",
      "filename": "backend/vendor/github.com/go-pkgz/lcw/README.md",
      "status": "modified",
      "additions": 1,
      "deletions": 0,
      "changes": 1,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/go-pkgz/lcw/README.md",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/go-pkgz/lcw/README.md",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/github.com/go-pkgz/lcw/README.md?ref=890d7154e777d0673defcd11db3ef792473d68c9",
      "patch": "@@ -29,6 +29,7 @@ Main features:\n \n ```\n cache := lcw.NewLruCache(lcw.MaxKeys(500), lcw.MaxCacheSize(65536), lcw.MaxValSize(200), lcw.MaxKeySize(32))\n+defer cache.Close()\n \n val, err := cache.Get(\"key123\", func() (lcw.Value, error) {\n     res, err := getDataFromSomeSource(params) // returns string"
    },
    {
      "sha": "89518076b1167cdf1c5ab2bc12dbb8d93f5a7411",
      "filename": "backend/vendor/github.com/go-pkgz/lcw/internal/cache/cache.go",
      "status": "modified",
      "additions": 4,
      "deletions": 4,
      "changes": 8,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/go-pkgz/lcw/internal/cache/cache.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/go-pkgz/lcw/internal/cache/cache.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/github.com/go-pkgz/lcw/internal/cache/cache.go?ref=890d7154e777d0673defcd11db3ef792473d68c9",
      "patch": "@@ -189,17 +189,17 @@ func (c *LoadingCache) Close() {\n \tclose(c.done)\n }\n \n-// keysWithTs includes list of keys with ts. This is for sorting keys\n+// keysWithTS includes list of keys with ts. This is for sorting keys\n // in order to provide least recently added sorting for size-based eviction\n-type keysWithTs []struct {\n+type keysWithTS []struct {\n \tkey string\n \tts  time.Time\n }\n \n // purge records > maxKeys. Has to be called with lock!\n // call with maxKeys 0 will only clear expired entries.\n func (c *LoadingCache) purge(maxKeys int64) {\n-\tkts := keysWithTs{}\n+\tkts := keysWithTS{}\n \n \tfor key, value := range c.data {\n \t\t// ttl eviction\n@@ -210,7 +210,7 @@ func (c *LoadingCache) purge(maxKeys int64) {\n \t\t\t}\n \t\t}\n \n-\t\t// prepare list of keysWithTs for size eviction\n+\t\t// prepare list of keysWithTS for size eviction\n \t\tif maxKeys > 0 && int64(len(c.data)) > maxKeys {\n \t\t\tts := c.data[key].expiresAt\n "
    },
    {
      "sha": "793f92a8e0731aaea21ed22a0c3591f6a66c6158",
      "filename": "backend/vendor/github.com/go-pkgz/lcw/scache.go",
      "status": "modified",
      "additions": 6,
      "deletions": 1,
      "changes": 7,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/go-pkgz/lcw/scache.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/go-pkgz/lcw/scache.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/github.com/go-pkgz/lcw/scache.go?ref=890d7154e777d0673defcd11db3ef792473d68c9",
      "patch": "@@ -7,7 +7,7 @@ import (\n )\n \n // Scache wraps LoadingCache with partitions (sub-system), and scopes.\n-// Simplified interface with just 3 funcs - Get, Flush and Stats\n+// Simplified interface with just 4 funcs - Get, Flush, Stats and Close\n type Scache struct {\n \tlc LoadingCache\n }\n@@ -31,6 +31,11 @@ func (m *Scache) Stat() CacheStat {\n \treturn m.lc.Stat()\n }\n \n+// Close calls Close function of the underlying cache\n+func (m *Scache) Close() error {\n+\treturn m.lc.Close()\n+}\n+\n // Flush clears cache and calls postFlushFn async\n func (m *Scache) Flush(req FlusherRequest) {\n \tif len(req.scopes) == 0 {"
    },
    {
      "sha": "1eb75ef68e448f6a726e601ceef9772aef33cd40",
      "filename": "backend/vendor/github.com/klauspost/compress/LICENSE",
      "status": "added",
      "additions": 28,
      "deletions": 0,
      "changes": 28,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/klauspost/compress/LICENSE",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/klauspost/compress/LICENSE",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/github.com/klauspost/compress/LICENSE?ref=890d7154e777d0673defcd11db3ef792473d68c9",
      "patch": "@@ -0,0 +1,28 @@\n+Copyright (c) 2012 The Go Authors. All rights reserved.\n+Copyright (c) 2019 Klaus Post. All rights reserved.\n+\n+Redistribution and use in source and binary forms, with or without\n+modification, are permitted provided that the following conditions are\n+met:\n+\n+   * Redistributions of source code must retain the above copyright\n+notice, this list of conditions and the following disclaimer.\n+   * Redistributions in binary form must reproduce the above\n+copyright notice, this list of conditions and the following disclaimer\n+in the documentation and/or other materials provided with the\n+distribution.\n+   * Neither the name of Google Inc. nor the names of its\n+contributors may be used to endorse or promote products derived from\n+this software without specific prior written permission.\n+\n+THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n+\"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\n+LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\n+A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\n+OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\n+SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\n+LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\n+DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\n+THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n+(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n+OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE."
    },
    {
      "sha": "27d8ed56fcc428210592e80f85fd2203b2fe5373",
      "filename": "backend/vendor/github.com/klauspost/compress/fse/README.md",
      "status": "added",
      "additions": 79,
      "deletions": 0,
      "changes": 79,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/klauspost/compress/fse/README.md",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/klauspost/compress/fse/README.md",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/github.com/klauspost/compress/fse/README.md?ref=890d7154e777d0673defcd11db3ef792473d68c9",
      "patch": "@@ -0,0 +1,79 @@\n+# Finite State Entropy\n+\n+This package provides Finite State Entropy encoding and decoding.\n+            \n+Finite State Entropy (also referenced as [tANS](https://en.wikipedia.org/wiki/Asymmetric_numeral_systems#tANS)) \n+encoding provides a fast near-optimal symbol encoding/decoding\n+for byte blocks as implemented in [zstandard](https://github.com/facebook/zstd).\n+\n+This can be used for compressing input with a lot of similar input values to the smallest number of bytes.\n+This does not perform any multi-byte [dictionary coding](https://en.wikipedia.org/wiki/Dictionary_coder) as LZ coders,\n+but it can be used as a secondary step to compressors (like Snappy) that does not do entropy encoding. \n+\n+* [Godoc documentation](https://godoc.org/github.com/klauspost/compress/fse)\n+\n+## News\n+\n+ * Feb 2018: First implementation released. Consider this beta software for now.\n+\n+# Usage\n+\n+This package provides a low level interface that allows to compress single independent blocks. \n+\n+Each block is separate, and there is no built in integrity checks. \n+This means that the caller should keep track of block sizes and also do checksums if needed.  \n+\n+Compressing a block is done via the [`Compress`](https://godoc.org/github.com/klauspost/compress/fse#Compress) function.\n+You must provide input and will receive the output and maybe an error.\n+\n+These error values can be returned:\n+\n+| Error               | Description                                                                 |\n+|---------------------|-----------------------------------------------------------------------------|\n+| `<nil>`             | Everything ok, output is returned                                           |\n+| `ErrIncompressible` | Returned when input is judged to be too hard to compress                    |\n+| `ErrUseRLE`         | Returned from the compressor when the input is a single byte value repeated |\n+| `(error)`           | An internal error occurred.                                                 |\n+\n+As can be seen above there are errors that will be returned even under normal operation so it is important to handle these.\n+\n+To reduce allocations you can provide a [`Scratch`](https://godoc.org/github.com/klauspost/compress/fse#Scratch) object \n+that can be re-used for successive calls. Both compression and decompression accepts a `Scratch` object, and the same \n+object can be used for both.   \n+\n+Be aware, that when re-using a `Scratch` object that the *output* buffer is also re-used, so if you are still using this\n+you must set the `Out` field in the scratch to nil. The same buffer is used for compression and decompression output.\n+\n+Decompressing is done by calling the [`Decompress`](https://godoc.org/github.com/klauspost/compress/fse#Decompress) function.\n+You must provide the output from the compression stage, at exactly the size you got back. If you receive an error back\n+your input was likely corrupted. \n+\n+It is important to note that a successful decoding does *not* mean your output matches your original input. \n+There are no integrity checks, so relying on errors from the decompressor does not assure your data is valid.\n+\n+For more detailed usage, see examples in the [godoc documentation](https://godoc.org/github.com/klauspost/compress/fse#pkg-examples).\n+\n+# Performance\n+\n+A lot of factors are affecting speed. Block sizes and compressibility of the material are primary factors.  \n+All compression functions are currently only running on the calling goroutine so only one core will be used per block.  \n+\n+The compressor is significantly faster if symbols are kept as small as possible. The highest byte value of the input\n+is used to reduce some of the processing, so if all your input is above byte value 64 for instance, it may be \n+beneficial to transpose all your input values down by 64.   \n+\n+With moderate block sizes around 64k speed are typically 200MB/s per core for compression and \n+around 300MB/s decompression speed. \n+\n+The same hardware typically does Huffman (deflate) encoding at 125MB/s and decompression at 100MB/s. \n+\n+# Plans\n+\n+At one point, more internals will be exposed to facilitate more \"expert\" usage of the components. \n+\n+A streaming interface is also likely to be implemented. Likely compatible with [FSE stream format](https://github.com/Cyan4973/FiniteStateEntropy/blob/dev/programs/fileio.c#L261).  \n+\n+# Contributing\n+\n+Contributions are always welcome. Be aware that adding public functions will require good justification and breaking \n+changes will likely not be accepted. If in doubt open an issue before writing the PR.  \n\\ No newline at end of file"
    },
    {
      "sha": "b9db204f59d6bde459a287baf0f421cd42f63ce0",
      "filename": "backend/vendor/github.com/klauspost/compress/fse/bitreader.go",
      "status": "added",
      "additions": 107,
      "deletions": 0,
      "changes": 107,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/klauspost/compress/fse/bitreader.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/klauspost/compress/fse/bitreader.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/github.com/klauspost/compress/fse/bitreader.go?ref=890d7154e777d0673defcd11db3ef792473d68c9",
      "patch": "@@ -0,0 +1,107 @@\n+// Copyright 2018 Klaus Post. All rights reserved.\n+// Use of this source code is governed by a BSD-style\n+// license that can be found in the LICENSE file.\n+// Based on work Copyright (c) 2013, Yann Collet, released under BSD License.\n+\n+package fse\n+\n+import (\n+\t\"errors\"\n+\t\"io\"\n+)\n+\n+// bitReader reads a bitstream in reverse.\n+// The last set bit indicates the start of the stream and is used\n+// for aligning the input.\n+type bitReader struct {\n+\tin       []byte\n+\toff      uint // next byte to read is at in[off - 1]\n+\tvalue    uint64\n+\tbitsRead uint8\n+}\n+\n+// init initializes and resets the bit reader.\n+func (b *bitReader) init(in []byte) error {\n+\tif len(in) < 1 {\n+\t\treturn errors.New(\"corrupt stream: too short\")\n+\t}\n+\tb.in = in\n+\tb.off = uint(len(in))\n+\t// The highest bit of the last byte indicates where to start\n+\tv := in[len(in)-1]\n+\tif v == 0 {\n+\t\treturn errors.New(\"corrupt stream, did not find end of stream\")\n+\t}\n+\tb.bitsRead = 64\n+\tb.value = 0\n+\tb.fill()\n+\tb.fill()\n+\tb.bitsRead += 8 - uint8(highBits(uint32(v)))\n+\treturn nil\n+}\n+\n+// getBits will return n bits. n can be 0.\n+func (b *bitReader) getBits(n uint8) uint16 {\n+\tif n == 0 || b.bitsRead >= 64 {\n+\t\treturn 0\n+\t}\n+\treturn b.getBitsFast(n)\n+}\n+\n+// getBitsFast requires that at least one bit is requested every time.\n+// There are no checks if the buffer is filled.\n+func (b *bitReader) getBitsFast(n uint8) uint16 {\n+\tconst regMask = 64 - 1\n+\tv := uint16((b.value << (b.bitsRead & regMask)) >> ((regMask + 1 - n) & regMask))\n+\tb.bitsRead += n\n+\treturn v\n+}\n+\n+// fillFast() will make sure at least 32 bits are available.\n+// There must be at least 4 bytes available.\n+func (b *bitReader) fillFast() {\n+\tif b.bitsRead < 32 {\n+\t\treturn\n+\t}\n+\t// Do single re-slice to avoid bounds checks.\n+\tv := b.in[b.off-4 : b.off]\n+\tlow := (uint32(v[0])) | (uint32(v[1]) << 8) | (uint32(v[2]) << 16) | (uint32(v[3]) << 24)\n+\tb.value = (b.value << 32) | uint64(low)\n+\tb.bitsRead -= 32\n+\tb.off -= 4\n+}\n+\n+// fill() will make sure at least 32 bits are available.\n+func (b *bitReader) fill() {\n+\tif b.bitsRead < 32 {\n+\t\treturn\n+\t}\n+\tif b.off > 4 {\n+\t\tv := b.in[b.off-4 : b.off]\n+\t\tlow := (uint32(v[0])) | (uint32(v[1]) << 8) | (uint32(v[2]) << 16) | (uint32(v[3]) << 24)\n+\t\tb.value = (b.value << 32) | uint64(low)\n+\t\tb.bitsRead -= 32\n+\t\tb.off -= 4\n+\t\treturn\n+\t}\n+\tfor b.off > 0 {\n+\t\tb.value = (b.value << 8) | uint64(b.in[b.off-1])\n+\t\tb.bitsRead -= 8\n+\t\tb.off--\n+\t}\n+}\n+\n+// finished returns true if all bits have been read from the bit stream.\n+func (b *bitReader) finished() bool {\n+\treturn b.off == 0 && b.bitsRead >= 64\n+}\n+\n+// close the bitstream and returns an error if out-of-buffer reads occurred.\n+func (b *bitReader) close() error {\n+\t// Release reference.\n+\tb.in = nil\n+\tif b.bitsRead > 64 {\n+\t\treturn io.ErrUnexpectedEOF\n+\t}\n+\treturn nil\n+}"
    },
    {
      "sha": "43e463611b15ccd0f92710da6c6ac8502dbf009a",
      "filename": "backend/vendor/github.com/klauspost/compress/fse/bitwriter.go",
      "status": "added",
      "additions": 168,
      "deletions": 0,
      "changes": 168,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/klauspost/compress/fse/bitwriter.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/klauspost/compress/fse/bitwriter.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/github.com/klauspost/compress/fse/bitwriter.go?ref=890d7154e777d0673defcd11db3ef792473d68c9",
      "patch": "@@ -0,0 +1,168 @@\n+// Copyright 2018 Klaus Post. All rights reserved.\n+// Use of this source code is governed by a BSD-style\n+// license that can be found in the LICENSE file.\n+// Based on work Copyright (c) 2013, Yann Collet, released under BSD License.\n+\n+package fse\n+\n+import \"fmt\"\n+\n+// bitWriter will write bits.\n+// First bit will be LSB of the first byte of output.\n+type bitWriter struct {\n+\tbitContainer uint64\n+\tnBits        uint8\n+\tout          []byte\n+}\n+\n+// bitMask16 is bitmasks. Has extra to avoid bounds check.\n+var bitMask16 = [32]uint16{\n+\t0, 1, 3, 7, 0xF, 0x1F,\n+\t0x3F, 0x7F, 0xFF, 0x1FF, 0x3FF, 0x7FF,\n+\t0xFFF, 0x1FFF, 0x3FFF, 0x7FFF, 0xFFFF, 0xFFFF,\n+\t0xFFFF, 0xFFFF, 0xFFFF, 0xFFFF, 0xFFFF, 0xFFFF,\n+\t0xFFFF, 0xFFFF} /* up to 16 bits */\n+\n+// addBits16NC will add up to 16 bits.\n+// It will not check if there is space for them,\n+// so the caller must ensure that it has flushed recently.\n+func (b *bitWriter) addBits16NC(value uint16, bits uint8) {\n+\tb.bitContainer |= uint64(value&bitMask16[bits&31]) << (b.nBits & 63)\n+\tb.nBits += bits\n+}\n+\n+// addBits16Clean will add up to 16 bits. value may not contain more set bits than indicated.\n+// It will not check if there is space for them, so the caller must ensure that it has flushed recently.\n+func (b *bitWriter) addBits16Clean(value uint16, bits uint8) {\n+\tb.bitContainer |= uint64(value) << (b.nBits & 63)\n+\tb.nBits += bits\n+}\n+\n+// addBits16ZeroNC will add up to 16 bits.\n+// It will not check if there is space for them,\n+// so the caller must ensure that it has flushed recently.\n+// This is fastest if bits can be zero.\n+func (b *bitWriter) addBits16ZeroNC(value uint16, bits uint8) {\n+\tif bits == 0 {\n+\t\treturn\n+\t}\n+\tvalue <<= (16 - bits) & 15\n+\tvalue >>= (16 - bits) & 15\n+\tb.bitContainer |= uint64(value) << (b.nBits & 63)\n+\tb.nBits += bits\n+}\n+\n+// flush will flush all pending full bytes.\n+// There will be at least 56 bits available for writing when this has been called.\n+// Using flush32 is faster, but leaves less space for writing.\n+func (b *bitWriter) flush() {\n+\tv := b.nBits >> 3\n+\tswitch v {\n+\tcase 0:\n+\tcase 1:\n+\t\tb.out = append(b.out,\n+\t\t\tbyte(b.bitContainer),\n+\t\t)\n+\tcase 2:\n+\t\tb.out = append(b.out,\n+\t\t\tbyte(b.bitContainer),\n+\t\t\tbyte(b.bitContainer>>8),\n+\t\t)\n+\tcase 3:\n+\t\tb.out = append(b.out,\n+\t\t\tbyte(b.bitContainer),\n+\t\t\tbyte(b.bitContainer>>8),\n+\t\t\tbyte(b.bitContainer>>16),\n+\t\t)\n+\tcase 4:\n+\t\tb.out = append(b.out,\n+\t\t\tbyte(b.bitContainer),\n+\t\t\tbyte(b.bitContainer>>8),\n+\t\t\tbyte(b.bitContainer>>16),\n+\t\t\tbyte(b.bitContainer>>24),\n+\t\t)\n+\tcase 5:\n+\t\tb.out = append(b.out,\n+\t\t\tbyte(b.bitContainer),\n+\t\t\tbyte(b.bitContainer>>8),\n+\t\t\tbyte(b.bitContainer>>16),\n+\t\t\tbyte(b.bitContainer>>24),\n+\t\t\tbyte(b.bitContainer>>32),\n+\t\t)\n+\tcase 6:\n+\t\tb.out = append(b.out,\n+\t\t\tbyte(b.bitContainer),\n+\t\t\tbyte(b.bitContainer>>8),\n+\t\t\tbyte(b.bitContainer>>16),\n+\t\t\tbyte(b.bitContainer>>24),\n+\t\t\tbyte(b.bitContainer>>32),\n+\t\t\tbyte(b.bitContainer>>40),\n+\t\t)\n+\tcase 7:\n+\t\tb.out = append(b.out,\n+\t\t\tbyte(b.bitContainer),\n+\t\t\tbyte(b.bitContainer>>8),\n+\t\t\tbyte(b.bitContainer>>16),\n+\t\t\tbyte(b.bitContainer>>24),\n+\t\t\tbyte(b.bitContainer>>32),\n+\t\t\tbyte(b.bitContainer>>40),\n+\t\t\tbyte(b.bitContainer>>48),\n+\t\t)\n+\tcase 8:\n+\t\tb.out = append(b.out,\n+\t\t\tbyte(b.bitContainer),\n+\t\t\tbyte(b.bitContainer>>8),\n+\t\t\tbyte(b.bitContainer>>16),\n+\t\t\tbyte(b.bitContainer>>24),\n+\t\t\tbyte(b.bitContainer>>32),\n+\t\t\tbyte(b.bitContainer>>40),\n+\t\t\tbyte(b.bitContainer>>48),\n+\t\t\tbyte(b.bitContainer>>56),\n+\t\t)\n+\tdefault:\n+\t\tpanic(fmt.Errorf(\"bits (%d) > 64\", b.nBits))\n+\t}\n+\tb.bitContainer >>= v << 3\n+\tb.nBits &= 7\n+}\n+\n+// flush32 will flush out, so there are at least 32 bits available for writing.\n+func (b *bitWriter) flush32() {\n+\tif b.nBits < 32 {\n+\t\treturn\n+\t}\n+\tb.out = append(b.out,\n+\t\tbyte(b.bitContainer),\n+\t\tbyte(b.bitContainer>>8),\n+\t\tbyte(b.bitContainer>>16),\n+\t\tbyte(b.bitContainer>>24))\n+\tb.nBits -= 32\n+\tb.bitContainer >>= 32\n+}\n+\n+// flushAlign will flush remaining full bytes and align to next byte boundary.\n+func (b *bitWriter) flushAlign() {\n+\tnbBytes := (b.nBits + 7) >> 3\n+\tfor i := uint8(0); i < nbBytes; i++ {\n+\t\tb.out = append(b.out, byte(b.bitContainer>>(i*8)))\n+\t}\n+\tb.nBits = 0\n+\tb.bitContainer = 0\n+}\n+\n+// close will write the alignment bit and write the final byte(s)\n+// to the output.\n+func (b *bitWriter) close() error {\n+\t// End mark\n+\tb.addBits16Clean(1, 1)\n+\t// flush until next byte.\n+\tb.flushAlign()\n+\treturn nil\n+}\n+\n+// reset and continue writing by appending to out.\n+func (b *bitWriter) reset(out []byte) {\n+\tb.bitContainer = 0\n+\tb.nBits = 0\n+\tb.out = out\n+}"
    },
    {
      "sha": "f228a46cdf6ea34f83eb8bae6945a97b73354951",
      "filename": "backend/vendor/github.com/klauspost/compress/fse/bytereader.go",
      "status": "added",
      "additions": 56,
      "deletions": 0,
      "changes": 56,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/klauspost/compress/fse/bytereader.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/klauspost/compress/fse/bytereader.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/github.com/klauspost/compress/fse/bytereader.go?ref=890d7154e777d0673defcd11db3ef792473d68c9",
      "patch": "@@ -0,0 +1,56 @@\n+// Copyright 2018 Klaus Post. All rights reserved.\n+// Use of this source code is governed by a BSD-style\n+// license that can be found in the LICENSE file.\n+// Based on work Copyright (c) 2013, Yann Collet, released under BSD License.\n+\n+package fse\n+\n+// byteReader provides a byte reader that reads\n+// little endian values from a byte stream.\n+// The input stream is manually advanced.\n+// The reader performs no bounds checks.\n+type byteReader struct {\n+\tb   []byte\n+\toff int\n+}\n+\n+// init will initialize the reader and set the input.\n+func (b *byteReader) init(in []byte) {\n+\tb.b = in\n+\tb.off = 0\n+}\n+\n+// advance the stream b n bytes.\n+func (b *byteReader) advance(n uint) {\n+\tb.off += int(n)\n+}\n+\n+// Int32 returns a little endian int32 starting at current offset.\n+func (b byteReader) Int32() int32 {\n+\tb2 := b.b[b.off : b.off+4 : b.off+4]\n+\tv3 := int32(b2[3])\n+\tv2 := int32(b2[2])\n+\tv1 := int32(b2[1])\n+\tv0 := int32(b2[0])\n+\treturn v0 | (v1 << 8) | (v2 << 16) | (v3 << 24)\n+}\n+\n+// Uint32 returns a little endian uint32 starting at current offset.\n+func (b byteReader) Uint32() uint32 {\n+\tb2 := b.b[b.off : b.off+4 : b.off+4]\n+\tv3 := uint32(b2[3])\n+\tv2 := uint32(b2[2])\n+\tv1 := uint32(b2[1])\n+\tv0 := uint32(b2[0])\n+\treturn v0 | (v1 << 8) | (v2 << 16) | (v3 << 24)\n+}\n+\n+// unread returns the unread portion of the input.\n+func (b byteReader) unread() []byte {\n+\treturn b.b[b.off:]\n+}\n+\n+// remain will return the number of bytes remaining.\n+func (b byteReader) remain() int {\n+\treturn len(b.b) - b.off\n+}"
    },
    {
      "sha": "b69237c9b8f56e86f5c2e49111d86f907300ecb1",
      "filename": "backend/vendor/github.com/klauspost/compress/fse/compress.go",
      "status": "added",
      "additions": 684,
      "deletions": 0,
      "changes": 684,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/klauspost/compress/fse/compress.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/klauspost/compress/fse/compress.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/github.com/klauspost/compress/fse/compress.go?ref=890d7154e777d0673defcd11db3ef792473d68c9",
      "patch": "@@ -0,0 +1,684 @@\n+// Copyright 2018 Klaus Post. All rights reserved.\n+// Use of this source code is governed by a BSD-style\n+// license that can be found in the LICENSE file.\n+// Based on work Copyright (c) 2013, Yann Collet, released under BSD License.\n+\n+package fse\n+\n+import (\n+\t\"errors\"\n+\t\"fmt\"\n+)\n+\n+// Compress the input bytes. Input must be < 2GB.\n+// Provide a Scratch buffer to avoid memory allocations.\n+// Note that the output is also kept in the scratch buffer.\n+// If input is too hard to compress, ErrIncompressible is returned.\n+// If input is a single byte value repeated ErrUseRLE is returned.\n+func Compress(in []byte, s *Scratch) ([]byte, error) {\n+\tif len(in) <= 1 {\n+\t\treturn nil, ErrIncompressible\n+\t}\n+\tif len(in) > (2<<30)-1 {\n+\t\treturn nil, errors.New(\"input too big, must be < 2GB\")\n+\t}\n+\ts, err := s.prepare(in)\n+\tif err != nil {\n+\t\treturn nil, err\n+\t}\n+\n+\t// Create histogram, if none was provided.\n+\tmaxCount := s.maxCount\n+\tif maxCount == 0 {\n+\t\tmaxCount = s.countSimple(in)\n+\t}\n+\t// Reset for next run.\n+\ts.clearCount = true\n+\ts.maxCount = 0\n+\tif maxCount == len(in) {\n+\t\t// One symbol, use RLE\n+\t\treturn nil, ErrUseRLE\n+\t}\n+\tif maxCount == 1 || maxCount < (len(in)>>7) {\n+\t\t// Each symbol present maximum once or too well distributed.\n+\t\treturn nil, ErrIncompressible\n+\t}\n+\ts.optimalTableLog()\n+\terr = s.normalizeCount()\n+\tif err != nil {\n+\t\treturn nil, err\n+\t}\n+\terr = s.writeCount()\n+\tif err != nil {\n+\t\treturn nil, err\n+\t}\n+\n+\tif false {\n+\t\terr = s.validateNorm()\n+\t\tif err != nil {\n+\t\t\treturn nil, err\n+\t\t}\n+\t}\n+\n+\terr = s.buildCTable()\n+\tif err != nil {\n+\t\treturn nil, err\n+\t}\n+\terr = s.compress(in)\n+\tif err != nil {\n+\t\treturn nil, err\n+\t}\n+\ts.Out = s.bw.out\n+\t// Check if we compressed.\n+\tif len(s.Out) >= len(in) {\n+\t\treturn nil, ErrIncompressible\n+\t}\n+\treturn s.Out, nil\n+}\n+\n+// cState contains the compression state of a stream.\n+type cState struct {\n+\tbw         *bitWriter\n+\tstateTable []uint16\n+\tstate      uint16\n+}\n+\n+// init will initialize the compression state to the first symbol of the stream.\n+func (c *cState) init(bw *bitWriter, ct *cTable, tableLog uint8, first symbolTransform) {\n+\tc.bw = bw\n+\tc.stateTable = ct.stateTable\n+\n+\tnbBitsOut := (first.deltaNbBits + (1 << 15)) >> 16\n+\tim := int32((nbBitsOut << 16) - first.deltaNbBits)\n+\tlu := (im >> nbBitsOut) + first.deltaFindState\n+\tc.state = c.stateTable[lu]\n+\treturn\n+}\n+\n+// encode the output symbol provided and write it to the bitstream.\n+func (c *cState) encode(symbolTT symbolTransform) {\n+\tnbBitsOut := (uint32(c.state) + symbolTT.deltaNbBits) >> 16\n+\tdstState := int32(c.state>>(nbBitsOut&15)) + symbolTT.deltaFindState\n+\tc.bw.addBits16NC(c.state, uint8(nbBitsOut))\n+\tc.state = c.stateTable[dstState]\n+}\n+\n+// encode the output symbol provided and write it to the bitstream.\n+func (c *cState) encodeZero(symbolTT symbolTransform) {\n+\tnbBitsOut := (uint32(c.state) + symbolTT.deltaNbBits) >> 16\n+\tdstState := int32(c.state>>(nbBitsOut&15)) + symbolTT.deltaFindState\n+\tc.bw.addBits16ZeroNC(c.state, uint8(nbBitsOut))\n+\tc.state = c.stateTable[dstState]\n+}\n+\n+// flush will write the tablelog to the output and flush the remaining full bytes.\n+func (c *cState) flush(tableLog uint8) {\n+\tc.bw.flush32()\n+\tc.bw.addBits16NC(c.state, tableLog)\n+\tc.bw.flush()\n+}\n+\n+// compress is the main compression loop that will encode the input from the last byte to the first.\n+func (s *Scratch) compress(src []byte) error {\n+\tif len(src) <= 2 {\n+\t\treturn errors.New(\"compress: src too small\")\n+\t}\n+\ttt := s.ct.symbolTT[:256]\n+\ts.bw.reset(s.Out)\n+\n+\t// Our two states each encodes every second byte.\n+\t// Last byte encoded (first byte decoded) will always be encoded by c1.\n+\tvar c1, c2 cState\n+\n+\t// Encode so remaining size is divisible by 4.\n+\tip := len(src)\n+\tif ip&1 == 1 {\n+\t\tc1.init(&s.bw, &s.ct, s.actualTableLog, tt[src[ip-1]])\n+\t\tc2.init(&s.bw, &s.ct, s.actualTableLog, tt[src[ip-2]])\n+\t\tc1.encodeZero(tt[src[ip-3]])\n+\t\tip -= 3\n+\t} else {\n+\t\tc2.init(&s.bw, &s.ct, s.actualTableLog, tt[src[ip-1]])\n+\t\tc1.init(&s.bw, &s.ct, s.actualTableLog, tt[src[ip-2]])\n+\t\tip -= 2\n+\t}\n+\tif ip&2 != 0 {\n+\t\tc2.encodeZero(tt[src[ip-1]])\n+\t\tc1.encodeZero(tt[src[ip-2]])\n+\t\tip -= 2\n+\t}\n+\n+\t// Main compression loop.\n+\tswitch {\n+\tcase !s.zeroBits && s.actualTableLog <= 8:\n+\t\t// We can encode 4 symbols without requiring a flush.\n+\t\t// We do not need to check if any output is 0 bits.\n+\t\tfor ip >= 4 {\n+\t\t\ts.bw.flush32()\n+\t\t\tv3, v2, v1, v0 := src[ip-4], src[ip-3], src[ip-2], src[ip-1]\n+\t\t\tc2.encode(tt[v0])\n+\t\t\tc1.encode(tt[v1])\n+\t\t\tc2.encode(tt[v2])\n+\t\t\tc1.encode(tt[v3])\n+\t\t\tip -= 4\n+\t\t}\n+\tcase !s.zeroBits:\n+\t\t// We do not need to check if any output is 0 bits.\n+\t\tfor ip >= 4 {\n+\t\t\ts.bw.flush32()\n+\t\t\tv3, v2, v1, v0 := src[ip-4], src[ip-3], src[ip-2], src[ip-1]\n+\t\t\tc2.encode(tt[v0])\n+\t\t\tc1.encode(tt[v1])\n+\t\t\ts.bw.flush32()\n+\t\t\tc2.encode(tt[v2])\n+\t\t\tc1.encode(tt[v3])\n+\t\t\tip -= 4\n+\t\t}\n+\tcase s.actualTableLog <= 8:\n+\t\t// We can encode 4 symbols without requiring a flush\n+\t\tfor ip >= 4 {\n+\t\t\ts.bw.flush32()\n+\t\t\tv3, v2, v1, v0 := src[ip-4], src[ip-3], src[ip-2], src[ip-1]\n+\t\t\tc2.encodeZero(tt[v0])\n+\t\t\tc1.encodeZero(tt[v1])\n+\t\t\tc2.encodeZero(tt[v2])\n+\t\t\tc1.encodeZero(tt[v3])\n+\t\t\tip -= 4\n+\t\t}\n+\tdefault:\n+\t\tfor ip >= 4 {\n+\t\t\ts.bw.flush32()\n+\t\t\tv3, v2, v1, v0 := src[ip-4], src[ip-3], src[ip-2], src[ip-1]\n+\t\t\tc2.encodeZero(tt[v0])\n+\t\t\tc1.encodeZero(tt[v1])\n+\t\t\ts.bw.flush32()\n+\t\t\tc2.encodeZero(tt[v2])\n+\t\t\tc1.encodeZero(tt[v3])\n+\t\t\tip -= 4\n+\t\t}\n+\t}\n+\n+\t// Flush final state.\n+\t// Used to initialize state when decoding.\n+\tc2.flush(s.actualTableLog)\n+\tc1.flush(s.actualTableLog)\n+\n+\treturn s.bw.close()\n+}\n+\n+// writeCount will write the normalized histogram count to header.\n+// This is read back by readNCount.\n+func (s *Scratch) writeCount() error {\n+\tvar (\n+\t\ttableLog  = s.actualTableLog\n+\t\ttableSize = 1 << tableLog\n+\t\tprevious0 bool\n+\t\tcharnum   uint16\n+\n+\t\tmaxHeaderSize = ((int(s.symbolLen) * int(tableLog)) >> 3) + 3\n+\n+\t\t// Write Table Size\n+\t\tbitStream = uint32(tableLog - minTablelog)\n+\t\tbitCount  = uint(4)\n+\t\tremaining = int16(tableSize + 1) /* +1 for extra accuracy */\n+\t\tthreshold = int16(tableSize)\n+\t\tnbBits    = uint(tableLog + 1)\n+\t)\n+\tif cap(s.Out) < maxHeaderSize {\n+\t\ts.Out = make([]byte, 0, s.br.remain()+maxHeaderSize)\n+\t}\n+\toutP := uint(0)\n+\tout := s.Out[:maxHeaderSize]\n+\n+\t// stops at 1\n+\tfor remaining > 1 {\n+\t\tif previous0 {\n+\t\t\tstart := charnum\n+\t\t\tfor s.norm[charnum] == 0 {\n+\t\t\t\tcharnum++\n+\t\t\t}\n+\t\t\tfor charnum >= start+24 {\n+\t\t\t\tstart += 24\n+\t\t\t\tbitStream += uint32(0xFFFF) << bitCount\n+\t\t\t\tout[outP] = byte(bitStream)\n+\t\t\t\tout[outP+1] = byte(bitStream >> 8)\n+\t\t\t\toutP += 2\n+\t\t\t\tbitStream >>= 16\n+\t\t\t}\n+\t\t\tfor charnum >= start+3 {\n+\t\t\t\tstart += 3\n+\t\t\t\tbitStream += 3 << bitCount\n+\t\t\t\tbitCount += 2\n+\t\t\t}\n+\t\t\tbitStream += uint32(charnum-start) << bitCount\n+\t\t\tbitCount += 2\n+\t\t\tif bitCount > 16 {\n+\t\t\t\tout[outP] = byte(bitStream)\n+\t\t\t\tout[outP+1] = byte(bitStream >> 8)\n+\t\t\t\toutP += 2\n+\t\t\t\tbitStream >>= 16\n+\t\t\t\tbitCount -= 16\n+\t\t\t}\n+\t\t}\n+\n+\t\tcount := s.norm[charnum]\n+\t\tcharnum++\n+\t\tmax := (2*threshold - 1) - remaining\n+\t\tif count < 0 {\n+\t\t\tremaining += count\n+\t\t} else {\n+\t\t\tremaining -= count\n+\t\t}\n+\t\tcount++ // +1 for extra accuracy\n+\t\tif count >= threshold {\n+\t\t\tcount += max // [0..max[ [max..threshold[ (...) [threshold+max 2*threshold[\n+\t\t}\n+\t\tbitStream += uint32(count) << bitCount\n+\t\tbitCount += nbBits\n+\t\tif count < max {\n+\t\t\tbitCount--\n+\t\t}\n+\n+\t\tprevious0 = count == 1\n+\t\tif remaining < 1 {\n+\t\t\treturn errors.New(\"internal error: remaining<1\")\n+\t\t}\n+\t\tfor remaining < threshold {\n+\t\t\tnbBits--\n+\t\t\tthreshold >>= 1\n+\t\t}\n+\n+\t\tif bitCount > 16 {\n+\t\t\tout[outP] = byte(bitStream)\n+\t\t\tout[outP+1] = byte(bitStream >> 8)\n+\t\t\toutP += 2\n+\t\t\tbitStream >>= 16\n+\t\t\tbitCount -= 16\n+\t\t}\n+\t}\n+\n+\tout[outP] = byte(bitStream)\n+\tout[outP+1] = byte(bitStream >> 8)\n+\toutP += (bitCount + 7) / 8\n+\n+\tif uint16(charnum) > s.symbolLen {\n+\t\treturn errors.New(\"internal error: charnum > s.symbolLen\")\n+\t}\n+\ts.Out = out[:outP]\n+\treturn nil\n+}\n+\n+// symbolTransform contains the state transform for a symbol.\n+type symbolTransform struct {\n+\tdeltaFindState int32\n+\tdeltaNbBits    uint32\n+}\n+\n+// String prints values as a human readable string.\n+func (s symbolTransform) String() string {\n+\treturn fmt.Sprintf(\"dnbits: %08x, fs:%d\", s.deltaNbBits, s.deltaFindState)\n+}\n+\n+// cTable contains tables used for compression.\n+type cTable struct {\n+\ttableSymbol []byte\n+\tstateTable  []uint16\n+\tsymbolTT    []symbolTransform\n+}\n+\n+// allocCtable will allocate tables needed for compression.\n+// If existing tables a re big enough, they are simply re-used.\n+func (s *Scratch) allocCtable() {\n+\ttableSize := 1 << s.actualTableLog\n+\t// get tableSymbol that is big enough.\n+\tif cap(s.ct.tableSymbol) < int(tableSize) {\n+\t\ts.ct.tableSymbol = make([]byte, tableSize)\n+\t}\n+\ts.ct.tableSymbol = s.ct.tableSymbol[:tableSize]\n+\n+\tctSize := tableSize\n+\tif cap(s.ct.stateTable) < ctSize {\n+\t\ts.ct.stateTable = make([]uint16, ctSize)\n+\t}\n+\ts.ct.stateTable = s.ct.stateTable[:ctSize]\n+\n+\tif cap(s.ct.symbolTT) < 256 {\n+\t\ts.ct.symbolTT = make([]symbolTransform, 256)\n+\t}\n+\ts.ct.symbolTT = s.ct.symbolTT[:256]\n+}\n+\n+// buildCTable will populate the compression table so it is ready to be used.\n+func (s *Scratch) buildCTable() error {\n+\ttableSize := uint32(1 << s.actualTableLog)\n+\thighThreshold := tableSize - 1\n+\tvar cumul [maxSymbolValue + 2]int16\n+\n+\ts.allocCtable()\n+\ttableSymbol := s.ct.tableSymbol[:tableSize]\n+\t// symbol start positions\n+\t{\n+\t\tcumul[0] = 0\n+\t\tfor ui, v := range s.norm[:s.symbolLen-1] {\n+\t\t\tu := byte(ui) // one less than reference\n+\t\t\tif v == -1 {\n+\t\t\t\t// Low proba symbol\n+\t\t\t\tcumul[u+1] = cumul[u] + 1\n+\t\t\t\ttableSymbol[highThreshold] = u\n+\t\t\t\thighThreshold--\n+\t\t\t} else {\n+\t\t\t\tcumul[u+1] = cumul[u] + v\n+\t\t\t}\n+\t\t}\n+\t\t// Encode last symbol separately to avoid overflowing u\n+\t\tu := int(s.symbolLen - 1)\n+\t\tv := s.norm[s.symbolLen-1]\n+\t\tif v == -1 {\n+\t\t\t// Low proba symbol\n+\t\t\tcumul[u+1] = cumul[u] + 1\n+\t\t\ttableSymbol[highThreshold] = byte(u)\n+\t\t\thighThreshold--\n+\t\t} else {\n+\t\t\tcumul[u+1] = cumul[u] + v\n+\t\t}\n+\t\tif uint32(cumul[s.symbolLen]) != tableSize {\n+\t\t\treturn fmt.Errorf(\"internal error: expected cumul[s.symbolLen] (%d) == tableSize (%d)\", cumul[s.symbolLen], tableSize)\n+\t\t}\n+\t\tcumul[s.symbolLen] = int16(tableSize) + 1\n+\t}\n+\t// Spread symbols\n+\ts.zeroBits = false\n+\t{\n+\t\tstep := tableStep(tableSize)\n+\t\ttableMask := tableSize - 1\n+\t\tvar position uint32\n+\t\t// if any symbol > largeLimit, we may have 0 bits output.\n+\t\tlargeLimit := int16(1 << (s.actualTableLog - 1))\n+\t\tfor ui, v := range s.norm[:s.symbolLen] {\n+\t\t\tsymbol := byte(ui)\n+\t\t\tif v > largeLimit {\n+\t\t\t\ts.zeroBits = true\n+\t\t\t}\n+\t\t\tfor nbOccurrences := int16(0); nbOccurrences < v; nbOccurrences++ {\n+\t\t\t\ttableSymbol[position] = symbol\n+\t\t\t\tposition = (position + step) & tableMask\n+\t\t\t\tfor position > highThreshold {\n+\t\t\t\t\tposition = (position + step) & tableMask\n+\t\t\t\t} /* Low proba area */\n+\t\t\t}\n+\t\t}\n+\n+\t\t// Check if we have gone through all positions\n+\t\tif position != 0 {\n+\t\t\treturn errors.New(\"position!=0\")\n+\t\t}\n+\t}\n+\n+\t// Build table\n+\ttable := s.ct.stateTable\n+\t{\n+\t\ttsi := int(tableSize)\n+\t\tfor u, v := range tableSymbol {\n+\t\t\t// TableU16 : sorted by symbol order; gives next state value\n+\t\t\ttable[cumul[v]] = uint16(tsi + u)\n+\t\t\tcumul[v]++\n+\t\t}\n+\t}\n+\n+\t// Build Symbol Transformation Table\n+\t{\n+\t\ttotal := int16(0)\n+\t\tsymbolTT := s.ct.symbolTT[:s.symbolLen]\n+\t\ttableLog := s.actualTableLog\n+\t\ttl := (uint32(tableLog) << 16) - (1 << tableLog)\n+\t\tfor i, v := range s.norm[:s.symbolLen] {\n+\t\t\tswitch v {\n+\t\t\tcase 0:\n+\t\t\tcase -1, 1:\n+\t\t\t\tsymbolTT[i].deltaNbBits = tl\n+\t\t\t\tsymbolTT[i].deltaFindState = int32(total - 1)\n+\t\t\t\ttotal++\n+\t\t\tdefault:\n+\t\t\t\tmaxBitsOut := uint32(tableLog) - highBits(uint32(v-1))\n+\t\t\t\tminStatePlus := uint32(v) << maxBitsOut\n+\t\t\t\tsymbolTT[i].deltaNbBits = (maxBitsOut << 16) - minStatePlus\n+\t\t\t\tsymbolTT[i].deltaFindState = int32(total - v)\n+\t\t\t\ttotal += v\n+\t\t\t}\n+\t\t}\n+\t\tif total != int16(tableSize) {\n+\t\t\treturn fmt.Errorf(\"total mismatch %d (got) != %d (want)\", total, tableSize)\n+\t\t}\n+\t}\n+\treturn nil\n+}\n+\n+// countSimple will create a simple histogram in s.count.\n+// Returns the biggest count.\n+// Does not update s.clearCount.\n+func (s *Scratch) countSimple(in []byte) (max int) {\n+\tfor _, v := range in {\n+\t\ts.count[v]++\n+\t}\n+\tm := uint32(0)\n+\tfor i, v := range s.count[:] {\n+\t\tif v > m {\n+\t\t\tm = v\n+\t\t}\n+\t\tif v > 0 {\n+\t\t\ts.symbolLen = uint16(i) + 1\n+\t\t}\n+\t}\n+\treturn int(m)\n+}\n+\n+// minTableLog provides the minimum logSize to safely represent a distribution.\n+func (s *Scratch) minTableLog() uint8 {\n+\tminBitsSrc := highBits(uint32(s.br.remain()-1)) + 1\n+\tminBitsSymbols := highBits(uint32(s.symbolLen-1)) + 2\n+\tif minBitsSrc < minBitsSymbols {\n+\t\treturn uint8(minBitsSrc)\n+\t}\n+\treturn uint8(minBitsSymbols)\n+}\n+\n+// optimalTableLog calculates and sets the optimal tableLog in s.actualTableLog\n+func (s *Scratch) optimalTableLog() {\n+\ttableLog := s.TableLog\n+\tminBits := s.minTableLog()\n+\tmaxBitsSrc := uint8(highBits(uint32(s.br.remain()-1))) - 2\n+\tif maxBitsSrc < tableLog {\n+\t\t// Accuracy can be reduced\n+\t\ttableLog = maxBitsSrc\n+\t}\n+\tif minBits > tableLog {\n+\t\ttableLog = minBits\n+\t}\n+\t// Need a minimum to safely represent all symbol values\n+\tif tableLog < minTablelog {\n+\t\ttableLog = minTablelog\n+\t}\n+\tif tableLog > maxTableLog {\n+\t\ttableLog = maxTableLog\n+\t}\n+\ts.actualTableLog = tableLog\n+}\n+\n+var rtbTable = [...]uint32{0, 473195, 504333, 520860, 550000, 700000, 750000, 830000}\n+\n+// normalizeCount will normalize the count of the symbols so\n+// the total is equal to the table size.\n+func (s *Scratch) normalizeCount() error {\n+\tvar (\n+\t\ttableLog          = s.actualTableLog\n+\t\tscale             = 62 - uint64(tableLog)\n+\t\tstep              = (1 << 62) / uint64(s.br.remain())\n+\t\tvStep             = uint64(1) << (scale - 20)\n+\t\tstillToDistribute = int16(1 << tableLog)\n+\t\tlargest           int\n+\t\tlargestP          int16\n+\t\tlowThreshold      = (uint32)(s.br.remain() >> tableLog)\n+\t)\n+\n+\tfor i, cnt := range s.count[:s.symbolLen] {\n+\t\t// already handled\n+\t\t// if (count[s] == s.length) return 0;   /* rle special case */\n+\n+\t\tif cnt == 0 {\n+\t\t\ts.norm[i] = 0\n+\t\t\tcontinue\n+\t\t}\n+\t\tif cnt <= lowThreshold {\n+\t\t\ts.norm[i] = -1\n+\t\t\tstillToDistribute--\n+\t\t} else {\n+\t\t\tproba := (int16)((uint64(cnt) * step) >> scale)\n+\t\t\tif proba < 8 {\n+\t\t\t\trestToBeat := vStep * uint64(rtbTable[proba])\n+\t\t\t\tv := uint64(cnt)*step - (uint64(proba) << scale)\n+\t\t\t\tif v > restToBeat {\n+\t\t\t\t\tproba++\n+\t\t\t\t}\n+\t\t\t}\n+\t\t\tif proba > largestP {\n+\t\t\t\tlargestP = proba\n+\t\t\t\tlargest = i\n+\t\t\t}\n+\t\t\ts.norm[i] = proba\n+\t\t\tstillToDistribute -= proba\n+\t\t}\n+\t}\n+\n+\tif -stillToDistribute >= (s.norm[largest] >> 1) {\n+\t\t// corner case, need another normalization method\n+\t\treturn s.normalizeCount2()\n+\t}\n+\ts.norm[largest] += stillToDistribute\n+\treturn nil\n+}\n+\n+// Secondary normalization method.\n+// To be used when primary method fails.\n+func (s *Scratch) normalizeCount2() error {\n+\tconst notYetAssigned = -2\n+\tvar (\n+\t\tdistributed  uint32\n+\t\ttotal        = uint32(s.br.remain())\n+\t\ttableLog     = s.actualTableLog\n+\t\tlowThreshold = uint32(total >> tableLog)\n+\t\tlowOne       = uint32((total * 3) >> (tableLog + 1))\n+\t)\n+\tfor i, cnt := range s.count[:s.symbolLen] {\n+\t\tif cnt == 0 {\n+\t\t\ts.norm[i] = 0\n+\t\t\tcontinue\n+\t\t}\n+\t\tif cnt <= lowThreshold {\n+\t\t\ts.norm[i] = -1\n+\t\t\tdistributed++\n+\t\t\ttotal -= cnt\n+\t\t\tcontinue\n+\t\t}\n+\t\tif cnt <= lowOne {\n+\t\t\ts.norm[i] = 1\n+\t\t\tdistributed++\n+\t\t\ttotal -= cnt\n+\t\t\tcontinue\n+\t\t}\n+\t\ts.norm[i] = notYetAssigned\n+\t}\n+\ttoDistribute := (1 << tableLog) - distributed\n+\n+\tif (total / toDistribute) > lowOne {\n+\t\t// risk of rounding to zero\n+\t\tlowOne = uint32((total * 3) / (toDistribute * 2))\n+\t\tfor i, cnt := range s.count[:s.symbolLen] {\n+\t\t\tif (s.norm[i] == notYetAssigned) && (cnt <= lowOne) {\n+\t\t\t\ts.norm[i] = 1\n+\t\t\t\tdistributed++\n+\t\t\t\ttotal -= cnt\n+\t\t\t\tcontinue\n+\t\t\t}\n+\t\t}\n+\t\ttoDistribute = (1 << tableLog) - distributed\n+\t}\n+\tif distributed == uint32(s.symbolLen)+1 {\n+\t\t// all values are pretty poor;\n+\t\t//   probably incompressible data (should have already been detected);\n+\t\t//   find max, then give all remaining points to max\n+\t\tvar maxV int\n+\t\tvar maxC uint32\n+\t\tfor i, cnt := range s.count[:s.symbolLen] {\n+\t\t\tif cnt > maxC {\n+\t\t\t\tmaxV = i\n+\t\t\t\tmaxC = cnt\n+\t\t\t}\n+\t\t}\n+\t\ts.norm[maxV] += int16(toDistribute)\n+\t\treturn nil\n+\t}\n+\n+\tif total == 0 {\n+\t\t// all of the symbols were low enough for the lowOne or lowThreshold\n+\t\tfor i := uint32(0); toDistribute > 0; i = (i + 1) % (uint32(s.symbolLen)) {\n+\t\t\tif s.norm[i] > 0 {\n+\t\t\t\ttoDistribute--\n+\t\t\t\ts.norm[i]++\n+\t\t\t}\n+\t\t}\n+\t\treturn nil\n+\t}\n+\n+\tvar (\n+\t\tvStepLog = 62 - uint64(tableLog)\n+\t\tmid      = uint64((1 << (vStepLog - 1)) - 1)\n+\t\trStep    = (((1 << vStepLog) * uint64(toDistribute)) + mid) / uint64(total) // scale on remaining\n+\t\ttmpTotal = mid\n+\t)\n+\tfor i, cnt := range s.count[:s.symbolLen] {\n+\t\tif s.norm[i] == notYetAssigned {\n+\t\t\tvar (\n+\t\t\t\tend    = tmpTotal + uint64(cnt)*rStep\n+\t\t\t\tsStart = uint32(tmpTotal >> vStepLog)\n+\t\t\t\tsEnd   = uint32(end >> vStepLog)\n+\t\t\t\tweight = sEnd - sStart\n+\t\t\t)\n+\t\t\tif weight < 1 {\n+\t\t\t\treturn errors.New(\"weight < 1\")\n+\t\t\t}\n+\t\t\ts.norm[i] = int16(weight)\n+\t\t\ttmpTotal = end\n+\t\t}\n+\t}\n+\treturn nil\n+}\n+\n+// validateNorm validates the normalized histogram table.\n+func (s *Scratch) validateNorm() (err error) {\n+\tvar total int\n+\tfor _, v := range s.norm[:s.symbolLen] {\n+\t\tif v >= 0 {\n+\t\t\ttotal += int(v)\n+\t\t} else {\n+\t\t\ttotal -= int(v)\n+\t\t}\n+\t}\n+\tdefer func() {\n+\t\tif err == nil {\n+\t\t\treturn\n+\t\t}\n+\t\tfmt.Printf(\"selected TableLog: %d, Symbol length: %d\\n\", s.actualTableLog, s.symbolLen)\n+\t\tfor i, v := range s.norm[:s.symbolLen] {\n+\t\t\tfmt.Printf(\"%3d: %5d -> %4d \\n\", i, s.count[i], v)\n+\t\t}\n+\t}()\n+\tif total != (1 << s.actualTableLog) {\n+\t\treturn fmt.Errorf(\"warning: Total == %d != %d\", total, 1<<s.actualTableLog)\n+\t}\n+\tfor i, v := range s.count[s.symbolLen:] {\n+\t\tif v != 0 {\n+\t\t\treturn fmt.Errorf(\"warning: Found symbol out of range, %d after cut\", i)\n+\t\t}\n+\t}\n+\treturn nil\n+}"
    },
    {
      "sha": "413ec3b3cd8cc3f2a0423a29021d563c67f7ca05",
      "filename": "backend/vendor/github.com/klauspost/compress/fse/decompress.go",
      "status": "added",
      "additions": 374,
      "deletions": 0,
      "changes": 374,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/klauspost/compress/fse/decompress.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/klauspost/compress/fse/decompress.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/github.com/klauspost/compress/fse/decompress.go?ref=890d7154e777d0673defcd11db3ef792473d68c9",
      "patch": "@@ -0,0 +1,374 @@\n+package fse\n+\n+import (\n+\t\"errors\"\n+\t\"fmt\"\n+)\n+\n+const (\n+\ttablelogAbsoluteMax = 15\n+)\n+\n+// Decompress a block of data.\n+// You can provide a scratch buffer to avoid allocations.\n+// If nil is provided a temporary one will be allocated.\n+// It is possible, but by no way guaranteed that corrupt data will\n+// return an error.\n+// It is up to the caller to verify integrity of the returned data.\n+// Use a predefined Scrach to set maximum acceptable output size.\n+func Decompress(b []byte, s *Scratch) ([]byte, error) {\n+\ts, err := s.prepare(b)\n+\tif err != nil {\n+\t\treturn nil, err\n+\t}\n+\ts.Out = s.Out[:0]\n+\terr = s.readNCount()\n+\tif err != nil {\n+\t\treturn nil, err\n+\t}\n+\terr = s.buildDtable()\n+\tif err != nil {\n+\t\treturn nil, err\n+\t}\n+\terr = s.decompress()\n+\tif err != nil {\n+\t\treturn nil, err\n+\t}\n+\n+\treturn s.Out, nil\n+}\n+\n+// readNCount will read the symbol distribution so decoding tables can be constructed.\n+func (s *Scratch) readNCount() error {\n+\tvar (\n+\t\tcharnum   uint16\n+\t\tprevious0 bool\n+\t\tb         = &s.br\n+\t)\n+\tiend := b.remain()\n+\tif iend < 4 {\n+\t\treturn errors.New(\"input too small\")\n+\t}\n+\tbitStream := b.Uint32()\n+\tnbBits := uint((bitStream & 0xF) + minTablelog) // extract tableLog\n+\tif nbBits > tablelogAbsoluteMax {\n+\t\treturn errors.New(\"tableLog too large\")\n+\t}\n+\tbitStream >>= 4\n+\tbitCount := uint(4)\n+\n+\ts.actualTableLog = uint8(nbBits)\n+\tremaining := int32((1 << nbBits) + 1)\n+\tthreshold := int32(1 << nbBits)\n+\tgotTotal := int32(0)\n+\tnbBits++\n+\n+\tfor remaining > 1 {\n+\t\tif previous0 {\n+\t\t\tn0 := charnum\n+\t\t\tfor (bitStream & 0xFFFF) == 0xFFFF {\n+\t\t\t\tn0 += 24\n+\t\t\t\tif b.off < iend-5 {\n+\t\t\t\t\tb.advance(2)\n+\t\t\t\t\tbitStream = b.Uint32() >> bitCount\n+\t\t\t\t} else {\n+\t\t\t\t\tbitStream >>= 16\n+\t\t\t\t\tbitCount += 16\n+\t\t\t\t}\n+\t\t\t}\n+\t\t\tfor (bitStream & 3) == 3 {\n+\t\t\t\tn0 += 3\n+\t\t\t\tbitStream >>= 2\n+\t\t\t\tbitCount += 2\n+\t\t\t}\n+\t\t\tn0 += uint16(bitStream & 3)\n+\t\t\tbitCount += 2\n+\t\t\tif n0 > maxSymbolValue {\n+\t\t\t\treturn errors.New(\"maxSymbolValue too small\")\n+\t\t\t}\n+\t\t\tfor charnum < n0 {\n+\t\t\t\ts.norm[charnum&0xff] = 0\n+\t\t\t\tcharnum++\n+\t\t\t}\n+\n+\t\t\tif b.off <= iend-7 || b.off+int(bitCount>>3) <= iend-4 {\n+\t\t\t\tb.advance(bitCount >> 3)\n+\t\t\t\tbitCount &= 7\n+\t\t\t\tbitStream = b.Uint32() >> bitCount\n+\t\t\t} else {\n+\t\t\t\tbitStream >>= 2\n+\t\t\t}\n+\t\t}\n+\n+\t\tmax := (2*(threshold) - 1) - (remaining)\n+\t\tvar count int32\n+\n+\t\tif (int32(bitStream) & (threshold - 1)) < max {\n+\t\t\tcount = int32(bitStream) & (threshold - 1)\n+\t\t\tbitCount += nbBits - 1\n+\t\t} else {\n+\t\t\tcount = int32(bitStream) & (2*threshold - 1)\n+\t\t\tif count >= threshold {\n+\t\t\t\tcount -= max\n+\t\t\t}\n+\t\t\tbitCount += nbBits\n+\t\t}\n+\n+\t\tcount-- // extra accuracy\n+\t\tif count < 0 {\n+\t\t\t// -1 means +1\n+\t\t\tremaining += count\n+\t\t\tgotTotal -= count\n+\t\t} else {\n+\t\t\tremaining -= count\n+\t\t\tgotTotal += count\n+\t\t}\n+\t\ts.norm[charnum&0xff] = int16(count)\n+\t\tcharnum++\n+\t\tprevious0 = count == 0\n+\t\tfor remaining < threshold {\n+\t\t\tnbBits--\n+\t\t\tthreshold >>= 1\n+\t\t}\n+\t\tif b.off <= iend-7 || b.off+int(bitCount>>3) <= iend-4 {\n+\t\t\tb.advance(bitCount >> 3)\n+\t\t\tbitCount &= 7\n+\t\t} else {\n+\t\t\tbitCount -= (uint)(8 * (len(b.b) - 4 - b.off))\n+\t\t\tb.off = len(b.b) - 4\n+\t\t}\n+\t\tbitStream = b.Uint32() >> (bitCount & 31)\n+\t}\n+\ts.symbolLen = charnum\n+\n+\tif s.symbolLen <= 1 {\n+\t\treturn fmt.Errorf(\"symbolLen (%d) too small\", s.symbolLen)\n+\t}\n+\tif s.symbolLen > maxSymbolValue+1 {\n+\t\treturn fmt.Errorf(\"symbolLen (%d) too big\", s.symbolLen)\n+\t}\n+\tif remaining != 1 {\n+\t\treturn fmt.Errorf(\"corruption detected (remaining %d != 1)\", remaining)\n+\t}\n+\tif bitCount > 32 {\n+\t\treturn fmt.Errorf(\"corruption detected (bitCount %d > 32)\", bitCount)\n+\t}\n+\tif gotTotal != 1<<s.actualTableLog {\n+\t\treturn fmt.Errorf(\"corruption detected (total %d != %d)\", gotTotal, 1<<s.actualTableLog)\n+\t}\n+\tb.advance((bitCount + 7) >> 3)\n+\treturn nil\n+}\n+\n+// decSymbol contains information about a state entry,\n+// Including the state offset base, the output symbol and\n+// the number of bits to read for the low part of the destination state.\n+type decSymbol struct {\n+\tnewState uint16\n+\tsymbol   uint8\n+\tnbBits   uint8\n+}\n+\n+// allocDtable will allocate decoding tables if they are not big enough.\n+func (s *Scratch) allocDtable() {\n+\ttableSize := 1 << s.actualTableLog\n+\tif cap(s.decTable) < int(tableSize) {\n+\t\ts.decTable = make([]decSymbol, tableSize)\n+\t}\n+\ts.decTable = s.decTable[:tableSize]\n+\n+\tif cap(s.ct.tableSymbol) < 256 {\n+\t\ts.ct.tableSymbol = make([]byte, 256)\n+\t}\n+\ts.ct.tableSymbol = s.ct.tableSymbol[:256]\n+\n+\tif cap(s.ct.stateTable) < 256 {\n+\t\ts.ct.stateTable = make([]uint16, 256)\n+\t}\n+\ts.ct.stateTable = s.ct.stateTable[:256]\n+}\n+\n+// buildDtable will build the decoding table.\n+func (s *Scratch) buildDtable() error {\n+\ttableSize := uint32(1 << s.actualTableLog)\n+\thighThreshold := tableSize - 1\n+\ts.allocDtable()\n+\tsymbolNext := s.ct.stateTable[:256]\n+\n+\t// Init, lay down lowprob symbols\n+\ts.zeroBits = false\n+\t{\n+\t\tlargeLimit := int16(1 << (s.actualTableLog - 1))\n+\t\tfor i, v := range s.norm[:s.symbolLen] {\n+\t\t\tif v == -1 {\n+\t\t\t\ts.decTable[highThreshold].symbol = uint8(i)\n+\t\t\t\thighThreshold--\n+\t\t\t\tsymbolNext[i] = 1\n+\t\t\t} else {\n+\t\t\t\tif v >= largeLimit {\n+\t\t\t\t\ts.zeroBits = true\n+\t\t\t\t}\n+\t\t\t\tsymbolNext[i] = uint16(v)\n+\t\t\t}\n+\t\t}\n+\t}\n+\t// Spread symbols\n+\t{\n+\t\ttableMask := tableSize - 1\n+\t\tstep := tableStep(tableSize)\n+\t\tposition := uint32(0)\n+\t\tfor ss, v := range s.norm[:s.symbolLen] {\n+\t\t\tfor i := 0; i < int(v); i++ {\n+\t\t\t\ts.decTable[position].symbol = uint8(ss)\n+\t\t\t\tposition = (position + step) & tableMask\n+\t\t\t\tfor position > highThreshold {\n+\t\t\t\t\t// lowprob area\n+\t\t\t\t\tposition = (position + step) & tableMask\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t\tif position != 0 {\n+\t\t\t// position must reach all cells once, otherwise normalizedCounter is incorrect\n+\t\t\treturn errors.New(\"corrupted input (position != 0)\")\n+\t\t}\n+\t}\n+\n+\t// Build Decoding table\n+\t{\n+\t\ttableSize := uint16(1 << s.actualTableLog)\n+\t\tfor u, v := range s.decTable {\n+\t\t\tsymbol := v.symbol\n+\t\t\tnextState := symbolNext[symbol]\n+\t\t\tsymbolNext[symbol] = nextState + 1\n+\t\t\tnBits := s.actualTableLog - byte(highBits(uint32(nextState)))\n+\t\t\ts.decTable[u].nbBits = nBits\n+\t\t\tnewState := (nextState << nBits) - tableSize\n+\t\t\tif newState >= tableSize {\n+\t\t\t\treturn fmt.Errorf(\"newState (%d) outside table size (%d)\", newState, tableSize)\n+\t\t\t}\n+\t\t\tif newState == uint16(u) && nBits == 0 {\n+\t\t\t\t// Seems weird that this is possible with nbits > 0.\n+\t\t\t\treturn fmt.Errorf(\"newState (%d) == oldState (%d) and no bits\", newState, u)\n+\t\t\t}\n+\t\t\ts.decTable[u].newState = newState\n+\t\t}\n+\t}\n+\treturn nil\n+}\n+\n+// decompress will decompress the bitstream.\n+// If the buffer is over-read an error is returned.\n+func (s *Scratch) decompress() error {\n+\tbr := &s.bits\n+\tbr.init(s.br.unread())\n+\n+\tvar s1, s2 decoder\n+\t// Initialize and decode first state and symbol.\n+\ts1.init(br, s.decTable, s.actualTableLog)\n+\ts2.init(br, s.decTable, s.actualTableLog)\n+\n+\t// Use temp table to avoid bound checks/append penalty.\n+\tvar tmp = s.ct.tableSymbol[:256]\n+\tvar off uint8\n+\n+\t// Main part\n+\tif !s.zeroBits {\n+\t\tfor br.off >= 8 {\n+\t\t\tbr.fillFast()\n+\t\t\ttmp[off+0] = s1.nextFast()\n+\t\t\ttmp[off+1] = s2.nextFast()\n+\t\t\tbr.fillFast()\n+\t\t\ttmp[off+2] = s1.nextFast()\n+\t\t\ttmp[off+3] = s2.nextFast()\n+\t\t\toff += 4\n+\t\t\t// When off is 0, we have overflowed and should write.\n+\t\t\tif off == 0 {\n+\t\t\t\ts.Out = append(s.Out, tmp...)\n+\t\t\t\tif len(s.Out) >= s.DecompressLimit {\n+\t\t\t\t\treturn fmt.Errorf(\"output size (%d) > DecompressLimit (%d)\", len(s.Out), s.DecompressLimit)\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t} else {\n+\t\tfor br.off >= 8 {\n+\t\t\tbr.fillFast()\n+\t\t\ttmp[off+0] = s1.next()\n+\t\t\ttmp[off+1] = s2.next()\n+\t\t\tbr.fillFast()\n+\t\t\ttmp[off+2] = s1.next()\n+\t\t\ttmp[off+3] = s2.next()\n+\t\t\toff += 4\n+\t\t\tif off == 0 {\n+\t\t\t\ts.Out = append(s.Out, tmp...)\n+\t\t\t\t// When off is 0, we have overflowed and should write.\n+\t\t\t\tif len(s.Out) >= s.DecompressLimit {\n+\t\t\t\t\treturn fmt.Errorf(\"output size (%d) > DecompressLimit (%d)\", len(s.Out), s.DecompressLimit)\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t}\n+\ts.Out = append(s.Out, tmp[:off]...)\n+\n+\t// Final bits, a bit more expensive check\n+\tfor {\n+\t\tif s1.finished() {\n+\t\t\ts.Out = append(s.Out, s1.final(), s2.final())\n+\t\t\tbreak\n+\t\t}\n+\t\tbr.fill()\n+\t\ts.Out = append(s.Out, s1.next())\n+\t\tif s2.finished() {\n+\t\t\ts.Out = append(s.Out, s2.final(), s1.final())\n+\t\t\tbreak\n+\t\t}\n+\t\ts.Out = append(s.Out, s2.next())\n+\t\tif len(s.Out) >= s.DecompressLimit {\n+\t\t\treturn fmt.Errorf(\"output size (%d) > DecompressLimit (%d)\", len(s.Out), s.DecompressLimit)\n+\t\t}\n+\t}\n+\treturn br.close()\n+}\n+\n+// decoder keeps track of the current state and updates it from the bitstream.\n+type decoder struct {\n+\tstate uint16\n+\tbr    *bitReader\n+\tdt    []decSymbol\n+}\n+\n+// init will initialize the decoder and read the first state from the stream.\n+func (d *decoder) init(in *bitReader, dt []decSymbol, tableLog uint8) {\n+\td.dt = dt\n+\td.br = in\n+\td.state = uint16(in.getBits(tableLog))\n+}\n+\n+// next returns the next symbol and sets the next state.\n+// At least tablelog bits must be available in the bit reader.\n+func (d *decoder) next() uint8 {\n+\tn := &d.dt[d.state]\n+\tlowBits := d.br.getBits(n.nbBits)\n+\td.state = n.newState + lowBits\n+\treturn n.symbol\n+}\n+\n+// finished returns true if all bits have been read from the bitstream\n+// and the next state would require reading bits from the input.\n+func (d *decoder) finished() bool {\n+\treturn d.br.finished() && d.dt[d.state].nbBits > 0\n+}\n+\n+// final returns the current state symbol without decoding the next.\n+func (d *decoder) final() uint8 {\n+\treturn d.dt[d.state].symbol\n+}\n+\n+// nextFast returns the next symbol and sets the next state.\n+// This can only be used if no symbols are 0 bits.\n+// At least tablelog bits must be available in the bit reader.\n+func (d *decoder) nextFast() uint8 {\n+\tn := d.dt[d.state]\n+\tlowBits := d.br.getBitsFast(n.nbBits)\n+\td.state = n.newState + lowBits\n+\treturn n.symbol\n+}"
    },
    {
      "sha": "075357b5b1604c9e5a1bb41489efdc18c2e62005",
      "filename": "backend/vendor/github.com/klauspost/compress/fse/fse.go",
      "status": "added",
      "additions": 143,
      "deletions": 0,
      "changes": 143,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/klauspost/compress/fse/fse.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/klauspost/compress/fse/fse.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/github.com/klauspost/compress/fse/fse.go?ref=890d7154e777d0673defcd11db3ef792473d68c9",
      "patch": "@@ -0,0 +1,143 @@\n+// Copyright 2018 Klaus Post. All rights reserved.\n+// Use of this source code is governed by a BSD-style\n+// license that can be found in the LICENSE file.\n+// Based on work Copyright (c) 2013, Yann Collet, released under BSD License.\n+\n+// Package fse provides Finite State Entropy encoding and decoding.\n+//\n+// Finite State Entropy encoding provides a fast near-optimal symbol encoding/decoding\n+// for byte blocks as implemented in zstd.\n+//\n+// See https://github.com/klauspost/compress/tree/master/fse for more information.\n+package fse\n+\n+import (\n+\t\"errors\"\n+\t\"fmt\"\n+\t\"math/bits\"\n+)\n+\n+const (\n+\t/*!MEMORY_USAGE :\n+\t *  Memory usage formula : N->2^N Bytes (examples : 10 -> 1KB; 12 -> 4KB ; 16 -> 64KB; 20 -> 1MB; etc.)\n+\t *  Increasing memory usage improves compression ratio\n+\t *  Reduced memory usage can improve speed, due to cache effect\n+\t *  Recommended max value is 14, for 16KB, which nicely fits into Intel x86 L1 cache */\n+\tmaxMemoryUsage     = 14\n+\tdefaultMemoryUsage = 13\n+\n+\tmaxTableLog     = maxMemoryUsage - 2\n+\tmaxTablesize    = 1 << maxTableLog\n+\tdefaultTablelog = defaultMemoryUsage - 2\n+\tminTablelog     = 5\n+\tmaxSymbolValue  = 255\n+)\n+\n+var (\n+\t// ErrIncompressible is returned when input is judged to be too hard to compress.\n+\tErrIncompressible = errors.New(\"input is not compressible\")\n+\n+\t// ErrUseRLE is returned from the compressor when the input is a single byte value repeated.\n+\tErrUseRLE = errors.New(\"input is single value repeated\")\n+)\n+\n+// Scratch provides temporary storage for compression and decompression.\n+type Scratch struct {\n+\t// Private\n+\tcount          [maxSymbolValue + 1]uint32\n+\tnorm           [maxSymbolValue + 1]int16\n+\tsymbolLen      uint16 // Length of active part of the symbol table.\n+\tactualTableLog uint8  // Selected tablelog.\n+\tbr             byteReader\n+\tbits           bitReader\n+\tbw             bitWriter\n+\tct             cTable      // Compression tables.\n+\tdecTable       []decSymbol // Decompression table.\n+\tzeroBits       bool        // no bits has prob > 50%.\n+\tclearCount     bool        // clear count\n+\tmaxCount       int         // count of the most probable symbol\n+\n+\t// Per block parameters.\n+\t// These can be used to override compression parameters of the block.\n+\t// Do not touch, unless you know what you are doing.\n+\n+\t// Out is output buffer.\n+\t// If the scratch is re-used before the caller is done processing the output,\n+\t// set this field to nil.\n+\t// Otherwise the output buffer will be re-used for next Compression/Decompression step\n+\t// and allocation will be avoided.\n+\tOut []byte\n+\n+\t// MaxSymbolValue will override the maximum symbol value of the next block.\n+\tMaxSymbolValue uint8\n+\n+\t// TableLog will attempt to override the tablelog for the next block.\n+\tTableLog uint8\n+\n+\t// DecompressLimit limits the maximum decoded size acceptable.\n+\t// If > 0 decompression will stop when approximately this many bytes\n+\t// has been decoded.\n+\t// If 0, maximum size will be 2GB.\n+\tDecompressLimit int\n+}\n+\n+// Histogram allows to populate the histogram and skip that step in the compression,\n+// It otherwise allows to inspect the histogram when compression is done.\n+// To indicate that you have populated the histogram call HistogramFinished\n+// with the value of the highest populated symbol, as well as the number of entries\n+// in the most populated entry. These are accepted at face value.\n+// The returned slice will always be length 256.\n+func (s *Scratch) Histogram() []uint32 {\n+\treturn s.count[:]\n+}\n+\n+// HistogramFinished can be called to indicate that the histogram has been populated.\n+// maxSymbol is the index of the highest set symbol of the next data segment.\n+// maxCount is the number of entries in the most populated entry.\n+// These are accepted at face value.\n+func (s *Scratch) HistogramFinished(maxSymbol uint8, maxCount int) {\n+\ts.maxCount = maxCount\n+\ts.symbolLen = uint16(maxSymbol) + 1\n+\ts.clearCount = maxCount != 0\n+}\n+\n+// prepare will prepare and allocate scratch tables used for both compression and decompression.\n+func (s *Scratch) prepare(in []byte) (*Scratch, error) {\n+\tif s == nil {\n+\t\ts = &Scratch{}\n+\t}\n+\tif s.MaxSymbolValue == 0 {\n+\t\ts.MaxSymbolValue = 255\n+\t}\n+\tif s.TableLog == 0 {\n+\t\ts.TableLog = defaultTablelog\n+\t}\n+\tif s.TableLog > maxTableLog {\n+\t\treturn nil, fmt.Errorf(\"tableLog (%d) > maxTableLog (%d)\", s.TableLog, maxTableLog)\n+\t}\n+\tif cap(s.Out) == 0 {\n+\t\ts.Out = make([]byte, 0, len(in))\n+\t}\n+\tif s.clearCount && s.maxCount == 0 {\n+\t\tfor i := range s.count {\n+\t\t\ts.count[i] = 0\n+\t\t}\n+\t\ts.clearCount = false\n+\t}\n+\ts.br.init(in)\n+\tif s.DecompressLimit == 0 {\n+\t\t// Max size 2GB.\n+\t\ts.DecompressLimit = (2 << 30) - 1\n+\t}\n+\n+\treturn s, nil\n+}\n+\n+// tableStep returns the next table index.\n+func tableStep(tableSize uint32) uint32 {\n+\treturn (tableSize >> 1) + (tableSize >> 3) + 3\n+}\n+\n+func highBits(val uint32) (n uint32) {\n+\treturn uint32(bits.Len32(val) - 1)\n+}"
    },
    {
      "sha": "b3d262958f85658c64b6faa6739c341e71d8826c",
      "filename": "backend/vendor/github.com/klauspost/compress/huff0/.gitignore",
      "status": "added",
      "additions": 1,
      "deletions": 0,
      "changes": 1,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/klauspost/compress/huff0/.gitignore",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/klauspost/compress/huff0/.gitignore",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/github.com/klauspost/compress/huff0/.gitignore?ref=890d7154e777d0673defcd11db3ef792473d68c9",
      "patch": "@@ -0,0 +1 @@\n+/huff0-fuzz.zip"
    },
    {
      "sha": "f2f4ff8439e7556362560db0d3991bcec4be0e5a",
      "filename": "backend/vendor/github.com/klauspost/compress/huff0/README.md",
      "status": "added",
      "additions": 87,
      "deletions": 0,
      "changes": 87,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/klauspost/compress/huff0/README.md",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/klauspost/compress/huff0/README.md",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/github.com/klauspost/compress/huff0/README.md?ref=890d7154e777d0673defcd11db3ef792473d68c9",
      "patch": "@@ -0,0 +1,87 @@\n+# Huff0 entropy compression\n+\n+This package provides Huff0 encoding and decoding as used in zstd.\n+            \n+[Huff0](https://github.com/Cyan4973/FiniteStateEntropy#new-generation-entropy-coders), \n+a Huffman codec designed for modern CPU, featuring OoO (Out of Order) operations on multiple ALU \n+(Arithmetic Logic Unit), achieving extremely fast compression and decompression speeds.\n+\n+This can be used for compressing input with a lot of similar input values to the smallest number of bytes.\n+This does not perform any multi-byte [dictionary coding](https://en.wikipedia.org/wiki/Dictionary_coder) as LZ coders,\n+but it can be used as a secondary step to compressors (like Snappy) that does not do entropy encoding. \n+\n+* [Godoc documentation](https://godoc.org/github.com/klauspost/compress/huff0)\n+\n+THIS PACKAGE IS NOT CONSIDERED STABLE AND API OR ENCODING MAY CHANGE IN THE FUTURE.\n+\n+## News\n+\n+ * Mar 2018: First implementation released. Consider this beta software for now.\n+\n+# Usage\n+\n+This package provides a low level interface that allows to compress single independent blocks. \n+\n+Each block is separate, and there is no built in integrity checks. \n+This means that the caller should keep track of block sizes and also do checksums if needed.  \n+\n+Compressing a block is done via the [`Compress1X`](https://godoc.org/github.com/klauspost/compress/huff0#Compress1X) and \n+[`Compress4X`](https://godoc.org/github.com/klauspost/compress/huff0#Compress4X) functions.\n+You must provide input and will receive the output and maybe an error.\n+\n+These error values can be returned:\n+\n+| Error               | Description                                                                 |\n+|---------------------|-----------------------------------------------------------------------------|\n+| `<nil>`             | Everything ok, output is returned                                           |\n+| `ErrIncompressible` | Returned when input is judged to be too hard to compress                    |\n+| `ErrUseRLE`         | Returned from the compressor when the input is a single byte value repeated |\n+| `ErrTooBig`         | Returned if the input block exceeds the maximum allowed size (128 Kib)      |\n+| `(error)`           | An internal error occurred.                                                 |\n+\n+\n+As can be seen above some of there are errors that will be returned even under normal operation so it is important to handle these.\n+\n+To reduce allocations you can provide a [`Scratch`](https://godoc.org/github.com/klauspost/compress/huff0#Scratch) object \n+that can be re-used for successive calls. Both compression and decompression accepts a `Scratch` object, and the same \n+object can be used for both.   \n+\n+Be aware, that when re-using a `Scratch` object that the *output* buffer is also re-used, so if you are still using this\n+you must set the `Out` field in the scratch to nil. The same buffer is used for compression and decompression output.\n+\n+The `Scratch` object will retain state that allows to re-use previous tables for encoding and decoding.  \n+\n+## Tables and re-use\n+\n+Huff0 allows for reusing tables from the previous block to save space if that is expected to give better/faster results. \n+\n+The Scratch object allows you to set a [`ReusePolicy`](https://godoc.org/github.com/klauspost/compress/huff0#ReusePolicy) \n+that controls this behaviour. See the documentation for details. This can be altered between each block.\n+\n+Do however note that this information is *not* stored in the output block and it is up to the users of the package to\n+record whether [`ReadTable`](https://godoc.org/github.com/klauspost/compress/huff0#ReadTable) should be called,\n+based on the boolean reported back from the CompressXX call. \n+\n+If you want to store the table separate from the data, you can access them as `OutData` and `OutTable` on the \n+[`Scratch`](https://godoc.org/github.com/klauspost/compress/huff0#Scratch) object.\n+\n+## Decompressing\n+\n+The first part of decoding is to initialize the decoding table through [`ReadTable`](https://godoc.org/github.com/klauspost/compress/huff0#ReadTable).\n+This will initialize the decoding tables. \n+You can supply the complete block to `ReadTable` and it will return the data part of the block \n+which can be given to the decompressor. \n+\n+Decompressing is done by calling the [`Decompress1X`](https://godoc.org/github.com/klauspost/compress/huff0#Scratch.Decompress1X) \n+or [`Decompress4X`](https://godoc.org/github.com/klauspost/compress/huff0#Scratch.Decompress4X) function.\n+\n+You must provide the output from the compression stage, at exactly the size you got back. If you receive an error back\n+your input was likely corrupted. \n+\n+It is important to note that a successful decoding does *not* mean your output matches your original input. \n+There are no integrity checks, so relying on errors from the decompressor does not assure your data is valid.\n+\n+# Contributing\n+\n+Contributions are always welcome. Be aware that adding public functions will require good justification and breaking \n+changes will likely not be accepted. If in doubt open an issue before writing the PR.\n\\ No newline at end of file"
    },
    {
      "sha": "7d0903c701022c6b24706bf52b11c82a8929cef4",
      "filename": "backend/vendor/github.com/klauspost/compress/huff0/bitreader.go",
      "status": "added",
      "additions": 115,
      "deletions": 0,
      "changes": 115,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/klauspost/compress/huff0/bitreader.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/klauspost/compress/huff0/bitreader.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/github.com/klauspost/compress/huff0/bitreader.go?ref=890d7154e777d0673defcd11db3ef792473d68c9",
      "patch": "@@ -0,0 +1,115 @@\n+// Copyright 2018 Klaus Post. All rights reserved.\n+// Use of this source code is governed by a BSD-style\n+// license that can be found in the LICENSE file.\n+// Based on work Copyright (c) 2013, Yann Collet, released under BSD License.\n+\n+package huff0\n+\n+import (\n+\t\"errors\"\n+\t\"io\"\n+)\n+\n+// bitReader reads a bitstream in reverse.\n+// The last set bit indicates the start of the stream and is used\n+// for aligning the input.\n+type bitReader struct {\n+\tin       []byte\n+\toff      uint // next byte to read is at in[off - 1]\n+\tvalue    uint64\n+\tbitsRead uint8\n+}\n+\n+// init initializes and resets the bit reader.\n+func (b *bitReader) init(in []byte) error {\n+\tif len(in) < 1 {\n+\t\treturn errors.New(\"corrupt stream: too short\")\n+\t}\n+\tb.in = in\n+\tb.off = uint(len(in))\n+\t// The highest bit of the last byte indicates where to start\n+\tv := in[len(in)-1]\n+\tif v == 0 {\n+\t\treturn errors.New(\"corrupt stream, did not find end of stream\")\n+\t}\n+\tb.bitsRead = 64\n+\tb.value = 0\n+\tb.fill()\n+\tb.fill()\n+\tb.bitsRead += 8 - uint8(highBit32(uint32(v)))\n+\treturn nil\n+}\n+\n+// getBits will return n bits. n can be 0.\n+func (b *bitReader) getBits(n uint8) uint16 {\n+\tif n == 0 || b.bitsRead >= 64 {\n+\t\treturn 0\n+\t}\n+\treturn b.getBitsFast(n)\n+}\n+\n+// getBitsFast requires that at least one bit is requested every time.\n+// There are no checks if the buffer is filled.\n+func (b *bitReader) getBitsFast(n uint8) uint16 {\n+\tconst regMask = 64 - 1\n+\tv := uint16((b.value << (b.bitsRead & regMask)) >> ((regMask + 1 - n) & regMask))\n+\tb.bitsRead += n\n+\treturn v\n+}\n+\n+// peekBitsFast requires that at least one bit is requested every time.\n+// There are no checks if the buffer is filled.\n+func (b *bitReader) peekBitsFast(n uint8) uint16 {\n+\tconst regMask = 64 - 1\n+\tv := uint16((b.value << (b.bitsRead & regMask)) >> ((regMask + 1 - n) & regMask))\n+\treturn v\n+}\n+\n+// fillFast() will make sure at least 32 bits are available.\n+// There must be at least 4 bytes available.\n+func (b *bitReader) fillFast() {\n+\tif b.bitsRead < 32 {\n+\t\treturn\n+\t}\n+\t// Do single re-slice to avoid bounds checks.\n+\tv := b.in[b.off-4 : b.off]\n+\tlow := (uint32(v[0])) | (uint32(v[1]) << 8) | (uint32(v[2]) << 16) | (uint32(v[3]) << 24)\n+\tb.value = (b.value << 32) | uint64(low)\n+\tb.bitsRead -= 32\n+\tb.off -= 4\n+}\n+\n+// fill() will make sure at least 32 bits are available.\n+func (b *bitReader) fill() {\n+\tif b.bitsRead < 32 {\n+\t\treturn\n+\t}\n+\tif b.off > 4 {\n+\t\tv := b.in[b.off-4 : b.off]\n+\t\tlow := (uint32(v[0])) | (uint32(v[1]) << 8) | (uint32(v[2]) << 16) | (uint32(v[3]) << 24)\n+\t\tb.value = (b.value << 32) | uint64(low)\n+\t\tb.bitsRead -= 32\n+\t\tb.off -= 4\n+\t\treturn\n+\t}\n+\tfor b.off > 0 {\n+\t\tb.value = (b.value << 8) | uint64(b.in[b.off-1])\n+\t\tb.bitsRead -= 8\n+\t\tb.off--\n+\t}\n+}\n+\n+// finished returns true if all bits have been read from the bit stream.\n+func (b *bitReader) finished() bool {\n+\treturn b.off == 0 && b.bitsRead >= 64\n+}\n+\n+// close the bitstream and returns an error if out-of-buffer reads occurred.\n+func (b *bitReader) close() error {\n+\t// Release reference.\n+\tb.in = nil\n+\tif b.bitsRead > 64 {\n+\t\treturn io.ErrUnexpectedEOF\n+\t}\n+\treturn nil\n+}"
    },
    {
      "sha": "bda4021efd3a597894677d468b8b2ba35579768f",
      "filename": "backend/vendor/github.com/klauspost/compress/huff0/bitwriter.go",
      "status": "added",
      "additions": 197,
      "deletions": 0,
      "changes": 197,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/klauspost/compress/huff0/bitwriter.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/klauspost/compress/huff0/bitwriter.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/github.com/klauspost/compress/huff0/bitwriter.go?ref=890d7154e777d0673defcd11db3ef792473d68c9",
      "patch": "@@ -0,0 +1,197 @@\n+// Copyright 2018 Klaus Post. All rights reserved.\n+// Use of this source code is governed by a BSD-style\n+// license that can be found in the LICENSE file.\n+// Based on work Copyright (c) 2013, Yann Collet, released under BSD License.\n+\n+package huff0\n+\n+import \"fmt\"\n+\n+// bitWriter will write bits.\n+// First bit will be LSB of the first byte of output.\n+type bitWriter struct {\n+\tbitContainer uint64\n+\tnBits        uint8\n+\tout          []byte\n+}\n+\n+// bitMask16 is bitmasks. Has extra to avoid bounds check.\n+var bitMask16 = [32]uint16{\n+\t0, 1, 3, 7, 0xF, 0x1F,\n+\t0x3F, 0x7F, 0xFF, 0x1FF, 0x3FF, 0x7FF,\n+\t0xFFF, 0x1FFF, 0x3FFF, 0x7FFF, 0xFFFF, 0xFFFF,\n+\t0xFFFF, 0xFFFF, 0xFFFF, 0xFFFF, 0xFFFF, 0xFFFF,\n+\t0xFFFF, 0xFFFF} /* up to 16 bits */\n+\n+// addBits16NC will add up to 16 bits.\n+// It will not check if there is space for them,\n+// so the caller must ensure that it has flushed recently.\n+func (b *bitWriter) addBits16NC(value uint16, bits uint8) {\n+\tb.bitContainer |= uint64(value&bitMask16[bits&31]) << (b.nBits & 63)\n+\tb.nBits += bits\n+}\n+\n+// addBits16Clean will add up to 16 bits. value may not contain more set bits than indicated.\n+// It will not check if there is space for them, so the caller must ensure that it has flushed recently.\n+func (b *bitWriter) addBits16Clean(value uint16, bits uint8) {\n+\tb.bitContainer |= uint64(value) << (b.nBits & 63)\n+\tb.nBits += bits\n+}\n+\n+// encSymbol will add up to 16 bits. value may not contain more set bits than indicated.\n+// It will not check if there is space for them, so the caller must ensure that it has flushed recently.\n+func (b *bitWriter) encSymbol(ct cTable, symbol byte) {\n+\tenc := ct[symbol]\n+\tb.bitContainer |= uint64(enc.val) << (b.nBits & 63)\n+\tb.nBits += enc.nBits\n+}\n+\n+// encTwoSymbols will add up to 32 bits. value may not contain more set bits than indicated.\n+// It will not check if there is space for them, so the caller must ensure that it has flushed recently.\n+func (b *bitWriter) encTwoSymbols(ct cTable, av, bv byte) {\n+\tencA := ct[av]\n+\tencB := ct[bv]\n+\tsh := b.nBits & 63\n+\tcombined := uint64(encA.val) | (uint64(encB.val) << (encA.nBits & 63))\n+\tb.bitContainer |= combined << sh\n+\tb.nBits += encA.nBits + encB.nBits\n+}\n+\n+// addBits16ZeroNC will add up to 16 bits.\n+// It will not check if there is space for them,\n+// so the caller must ensure that it has flushed recently.\n+// This is fastest if bits can be zero.\n+func (b *bitWriter) addBits16ZeroNC(value uint16, bits uint8) {\n+\tif bits == 0 {\n+\t\treturn\n+\t}\n+\tvalue <<= (16 - bits) & 15\n+\tvalue >>= (16 - bits) & 15\n+\tb.bitContainer |= uint64(value) << (b.nBits & 63)\n+\tb.nBits += bits\n+}\n+\n+// flush will flush all pending full bytes.\n+// There will be at least 56 bits available for writing when this has been called.\n+// Using flush32 is faster, but leaves less space for writing.\n+func (b *bitWriter) flush() {\n+\tv := b.nBits >> 3\n+\tswitch v {\n+\tcase 0:\n+\t\treturn\n+\tcase 1:\n+\t\tb.out = append(b.out,\n+\t\t\tbyte(b.bitContainer),\n+\t\t)\n+\t\tb.bitContainer >>= 1 << 3\n+\tcase 2:\n+\t\tb.out = append(b.out,\n+\t\t\tbyte(b.bitContainer),\n+\t\t\tbyte(b.bitContainer>>8),\n+\t\t)\n+\t\tb.bitContainer >>= 2 << 3\n+\tcase 3:\n+\t\tb.out = append(b.out,\n+\t\t\tbyte(b.bitContainer),\n+\t\t\tbyte(b.bitContainer>>8),\n+\t\t\tbyte(b.bitContainer>>16),\n+\t\t)\n+\t\tb.bitContainer >>= 3 << 3\n+\tcase 4:\n+\t\tb.out = append(b.out,\n+\t\t\tbyte(b.bitContainer),\n+\t\t\tbyte(b.bitContainer>>8),\n+\t\t\tbyte(b.bitContainer>>16),\n+\t\t\tbyte(b.bitContainer>>24),\n+\t\t)\n+\t\tb.bitContainer >>= 4 << 3\n+\tcase 5:\n+\t\tb.out = append(b.out,\n+\t\t\tbyte(b.bitContainer),\n+\t\t\tbyte(b.bitContainer>>8),\n+\t\t\tbyte(b.bitContainer>>16),\n+\t\t\tbyte(b.bitContainer>>24),\n+\t\t\tbyte(b.bitContainer>>32),\n+\t\t)\n+\t\tb.bitContainer >>= 5 << 3\n+\tcase 6:\n+\t\tb.out = append(b.out,\n+\t\t\tbyte(b.bitContainer),\n+\t\t\tbyte(b.bitContainer>>8),\n+\t\t\tbyte(b.bitContainer>>16),\n+\t\t\tbyte(b.bitContainer>>24),\n+\t\t\tbyte(b.bitContainer>>32),\n+\t\t\tbyte(b.bitContainer>>40),\n+\t\t)\n+\t\tb.bitContainer >>= 6 << 3\n+\tcase 7:\n+\t\tb.out = append(b.out,\n+\t\t\tbyte(b.bitContainer),\n+\t\t\tbyte(b.bitContainer>>8),\n+\t\t\tbyte(b.bitContainer>>16),\n+\t\t\tbyte(b.bitContainer>>24),\n+\t\t\tbyte(b.bitContainer>>32),\n+\t\t\tbyte(b.bitContainer>>40),\n+\t\t\tbyte(b.bitContainer>>48),\n+\t\t)\n+\t\tb.bitContainer >>= 7 << 3\n+\tcase 8:\n+\t\tb.out = append(b.out,\n+\t\t\tbyte(b.bitContainer),\n+\t\t\tbyte(b.bitContainer>>8),\n+\t\t\tbyte(b.bitContainer>>16),\n+\t\t\tbyte(b.bitContainer>>24),\n+\t\t\tbyte(b.bitContainer>>32),\n+\t\t\tbyte(b.bitContainer>>40),\n+\t\t\tbyte(b.bitContainer>>48),\n+\t\t\tbyte(b.bitContainer>>56),\n+\t\t)\n+\t\tb.bitContainer = 0\n+\t\tb.nBits = 0\n+\t\treturn\n+\tdefault:\n+\t\tpanic(fmt.Errorf(\"bits (%d) > 64\", b.nBits))\n+\t}\n+\tb.nBits &= 7\n+}\n+\n+// flush32 will flush out, so there are at least 32 bits available for writing.\n+func (b *bitWriter) flush32() {\n+\tif b.nBits < 32 {\n+\t\treturn\n+\t}\n+\tb.out = append(b.out,\n+\t\tbyte(b.bitContainer),\n+\t\tbyte(b.bitContainer>>8),\n+\t\tbyte(b.bitContainer>>16),\n+\t\tbyte(b.bitContainer>>24))\n+\tb.nBits -= 32\n+\tb.bitContainer >>= 32\n+}\n+\n+// flushAlign will flush remaining full bytes and align to next byte boundary.\n+func (b *bitWriter) flushAlign() {\n+\tnbBytes := (b.nBits + 7) >> 3\n+\tfor i := uint8(0); i < nbBytes; i++ {\n+\t\tb.out = append(b.out, byte(b.bitContainer>>(i*8)))\n+\t}\n+\tb.nBits = 0\n+\tb.bitContainer = 0\n+}\n+\n+// close will write the alignment bit and write the final byte(s)\n+// to the output.\n+func (b *bitWriter) close() error {\n+\t// End mark\n+\tb.addBits16Clean(1, 1)\n+\t// flush until next byte.\n+\tb.flushAlign()\n+\treturn nil\n+}\n+\n+// reset and continue writing by appending to out.\n+func (b *bitWriter) reset(out []byte) {\n+\tb.bitContainer = 0\n+\tb.nBits = 0\n+\tb.out = out\n+}"
    },
    {
      "sha": "50bcdf6ea99ce456802f645b08d4489b364349c8",
      "filename": "backend/vendor/github.com/klauspost/compress/huff0/bytereader.go",
      "status": "added",
      "additions": 54,
      "deletions": 0,
      "changes": 54,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/klauspost/compress/huff0/bytereader.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/klauspost/compress/huff0/bytereader.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/github.com/klauspost/compress/huff0/bytereader.go?ref=890d7154e777d0673defcd11db3ef792473d68c9",
      "patch": "@@ -0,0 +1,54 @@\n+// Copyright 2018 Klaus Post. All rights reserved.\n+// Use of this source code is governed by a BSD-style\n+// license that can be found in the LICENSE file.\n+// Based on work Copyright (c) 2013, Yann Collet, released under BSD License.\n+\n+package huff0\n+\n+// byteReader provides a byte reader that reads\n+// little endian values from a byte stream.\n+// The input stream is manually advanced.\n+// The reader performs no bounds checks.\n+type byteReader struct {\n+\tb   []byte\n+\toff int\n+}\n+\n+// init will initialize the reader and set the input.\n+func (b *byteReader) init(in []byte) {\n+\tb.b = in\n+\tb.off = 0\n+}\n+\n+// advance the stream b n bytes.\n+func (b *byteReader) advance(n uint) {\n+\tb.off += int(n)\n+}\n+\n+// Int32 returns a little endian int32 starting at current offset.\n+func (b byteReader) Int32() int32 {\n+\tv3 := int32(b.b[b.off+3])\n+\tv2 := int32(b.b[b.off+2])\n+\tv1 := int32(b.b[b.off+1])\n+\tv0 := int32(b.b[b.off])\n+\treturn (v3 << 24) | (v2 << 16) | (v1 << 8) | v0\n+}\n+\n+// Uint32 returns a little endian uint32 starting at current offset.\n+func (b byteReader) Uint32() uint32 {\n+\tv3 := uint32(b.b[b.off+3])\n+\tv2 := uint32(b.b[b.off+2])\n+\tv1 := uint32(b.b[b.off+1])\n+\tv0 := uint32(b.b[b.off])\n+\treturn (v3 << 24) | (v2 << 16) | (v1 << 8) | v0\n+}\n+\n+// unread returns the unread portion of the input.\n+func (b byteReader) unread() []byte {\n+\treturn b.b[b.off:]\n+}\n+\n+// remain will return the number of bytes remaining.\n+func (b byteReader) remain() int {\n+\treturn len(b.b) - b.off\n+}"
    },
    {
      "sha": "125429d74d23183d8a071745cfe1432da964ff85",
      "filename": "backend/vendor/github.com/klauspost/compress/huff0/compress.go",
      "status": "added",
      "additions": 625,
      "deletions": 0,
      "changes": 625,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/klauspost/compress/huff0/compress.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/klauspost/compress/huff0/compress.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/github.com/klauspost/compress/huff0/compress.go?ref=890d7154e777d0673defcd11db3ef792473d68c9",
      "patch": "@@ -0,0 +1,625 @@\n+package huff0\n+\n+import (\n+\t\"fmt\"\n+\t\"runtime\"\n+\t\"sync\"\n+)\n+\n+// Compress1X will compress the input.\n+// The output can be decoded using Decompress1X.\n+// Supply a Scratch object. The scratch object contains state about re-use,\n+// So when sharing across independent encodes, be sure to set the re-use policy.\n+func Compress1X(in []byte, s *Scratch) (out []byte, reUsed bool, err error) {\n+\ts, err = s.prepare(in)\n+\tif err != nil {\n+\t\treturn nil, false, err\n+\t}\n+\treturn compress(in, s, s.compress1X)\n+}\n+\n+// Compress4X will compress the input. The input is split into 4 independent blocks\n+// and compressed similar to Compress1X.\n+// The output can be decoded using Decompress4X.\n+// Supply a Scratch object. The scratch object contains state about re-use,\n+// So when sharing across independent encodes, be sure to set the re-use policy.\n+func Compress4X(in []byte, s *Scratch) (out []byte, reUsed bool, err error) {\n+\ts, err = s.prepare(in)\n+\tif err != nil {\n+\t\treturn nil, false, err\n+\t}\n+\tif false {\n+\t\t// TODO: compress4Xp only slightly faster.\n+\t\tconst parallelThreshold = 8 << 10\n+\t\tif len(in) < parallelThreshold || runtime.GOMAXPROCS(0) == 1 {\n+\t\t\treturn compress(in, s, s.compress4X)\n+\t\t}\n+\t\treturn compress(in, s, s.compress4Xp)\n+\t}\n+\treturn compress(in, s, s.compress4X)\n+}\n+\n+func compress(in []byte, s *Scratch, compressor func(src []byte) ([]byte, error)) (out []byte, reUsed bool, err error) {\n+\t// Nuke previous table if we cannot reuse anyway.\n+\tif s.Reuse == ReusePolicyNone {\n+\t\ts.prevTable = s.prevTable[:0]\n+\t}\n+\n+\t// Create histogram, if none was provided.\n+\tmaxCount := s.maxCount\n+\tvar canReuse = false\n+\tif maxCount == 0 {\n+\t\tmaxCount, canReuse = s.countSimple(in)\n+\t} else {\n+\t\tcanReuse = s.canUseTable(s.prevTable)\n+\t}\n+\n+\t// We want the output size to be less than this:\n+\twantSize := len(in)\n+\tif s.WantLogLess > 0 {\n+\t\twantSize -= wantSize >> s.WantLogLess\n+\t}\n+\n+\t// Reset for next run.\n+\ts.clearCount = true\n+\ts.maxCount = 0\n+\tif maxCount >= len(in) {\n+\t\tif maxCount > len(in) {\n+\t\t\treturn nil, false, fmt.Errorf(\"maxCount (%d) > length (%d)\", maxCount, len(in))\n+\t\t}\n+\t\tif len(in) == 1 {\n+\t\t\treturn nil, false, ErrIncompressible\n+\t\t}\n+\t\t// One symbol, use RLE\n+\t\treturn nil, false, ErrUseRLE\n+\t}\n+\tif maxCount == 1 || maxCount < (len(in)>>7) {\n+\t\t// Each symbol present maximum once or too well distributed.\n+\t\treturn nil, false, ErrIncompressible\n+\t}\n+\n+\tif s.Reuse == ReusePolicyPrefer && canReuse {\n+\t\tkeepTable := s.cTable\n+\t\ts.cTable = s.prevTable\n+\t\ts.Out, err = compressor(in)\n+\t\ts.cTable = keepTable\n+\t\tif err == nil && len(s.Out) < wantSize {\n+\t\t\ts.OutData = s.Out\n+\t\t\treturn s.Out, true, nil\n+\t\t}\n+\t\t// Do not attempt to re-use later.\n+\t\ts.prevTable = s.prevTable[:0]\n+\t}\n+\n+\t// Calculate new table.\n+\ts.optimalTableLog()\n+\terr = s.buildCTable()\n+\tif err != nil {\n+\t\treturn nil, false, err\n+\t}\n+\n+\tif false && !s.canUseTable(s.cTable) {\n+\t\tpanic(\"invalid table generated\")\n+\t}\n+\n+\tif s.Reuse == ReusePolicyAllow && canReuse {\n+\t\thSize := len(s.Out)\n+\t\toldSize := s.prevTable.estimateSize(s.count[:s.symbolLen])\n+\t\tnewSize := s.cTable.estimateSize(s.count[:s.symbolLen])\n+\t\tif oldSize <= hSize+newSize || hSize+12 >= wantSize {\n+\t\t\t// Retain cTable even if we re-use.\n+\t\t\tkeepTable := s.cTable\n+\t\t\ts.cTable = s.prevTable\n+\t\t\ts.Out, err = compressor(in)\n+\t\t\ts.cTable = keepTable\n+\t\t\tif err != nil {\n+\t\t\t\treturn nil, false, err\n+\t\t\t}\n+\t\t\tif len(s.Out) >= wantSize {\n+\t\t\t\treturn nil, false, ErrIncompressible\n+\t\t\t}\n+\t\t\ts.OutData = s.Out\n+\t\t\treturn s.Out, true, nil\n+\t\t}\n+\t}\n+\n+\t// Use new table\n+\terr = s.cTable.write(s)\n+\tif err != nil {\n+\t\ts.OutTable = nil\n+\t\treturn nil, false, err\n+\t}\n+\ts.OutTable = s.Out\n+\n+\t// Compress using new table\n+\ts.Out, err = compressor(in)\n+\tif err != nil {\n+\t\ts.OutTable = nil\n+\t\treturn nil, false, err\n+\t}\n+\tif len(s.Out) >= wantSize {\n+\t\ts.OutTable = nil\n+\t\treturn nil, false, ErrIncompressible\n+\t}\n+\t// Move current table into previous.\n+\ts.prevTable, s.cTable = s.cTable, s.prevTable[:0]\n+\ts.OutData = s.Out[len(s.OutTable):]\n+\treturn s.Out, false, nil\n+}\n+\n+func (s *Scratch) compress1X(src []byte) ([]byte, error) {\n+\treturn s.compress1xDo(s.Out, src)\n+}\n+\n+func (s *Scratch) compress1xDo(dst, src []byte) ([]byte, error) {\n+\tvar bw = bitWriter{out: dst}\n+\n+\t// N is length divisible by 4.\n+\tn := len(src)\n+\tn -= n & 3\n+\tcTable := s.cTable[:256]\n+\n+\t// Encode last bytes.\n+\tfor i := len(src) & 3; i > 0; i-- {\n+\t\tbw.encSymbol(cTable, src[n+i-1])\n+\t}\n+\tn -= 4\n+\tif s.actualTableLog <= 8 {\n+\t\tfor ; n >= 0; n -= 4 {\n+\t\t\ttmp := src[n : n+4]\n+\t\t\t// tmp should be len 4\n+\t\t\tbw.flush32()\n+\t\t\tbw.encTwoSymbols(cTable, tmp[3], tmp[2])\n+\t\t\tbw.encTwoSymbols(cTable, tmp[1], tmp[0])\n+\t\t}\n+\t} else {\n+\t\tfor ; n >= 0; n -= 4 {\n+\t\t\ttmp := src[n : n+4]\n+\t\t\t// tmp should be len 4\n+\t\t\tbw.flush32()\n+\t\t\tbw.encTwoSymbols(cTable, tmp[3], tmp[2])\n+\t\t\tbw.flush32()\n+\t\t\tbw.encTwoSymbols(cTable, tmp[1], tmp[0])\n+\t\t}\n+\t}\n+\terr := bw.close()\n+\treturn bw.out, err\n+}\n+\n+var sixZeros [6]byte\n+\n+func (s *Scratch) compress4X(src []byte) ([]byte, error) {\n+\tif len(src) < 12 {\n+\t\treturn nil, ErrIncompressible\n+\t}\n+\tsegmentSize := (len(src) + 3) / 4\n+\n+\t// Add placeholder for output length\n+\toffsetIdx := len(s.Out)\n+\ts.Out = append(s.Out, sixZeros[:]...)\n+\n+\tfor i := 0; i < 4; i++ {\n+\t\ttoDo := src\n+\t\tif len(toDo) > segmentSize {\n+\t\t\ttoDo = toDo[:segmentSize]\n+\t\t}\n+\t\tsrc = src[len(toDo):]\n+\n+\t\tvar err error\n+\t\tidx := len(s.Out)\n+\t\ts.Out, err = s.compress1xDo(s.Out, toDo)\n+\t\tif err != nil {\n+\t\t\treturn nil, err\n+\t\t}\n+\t\t// Write compressed length as little endian before block.\n+\t\tif i < 3 {\n+\t\t\t// Last length is not written.\n+\t\t\tlength := len(s.Out) - idx\n+\t\t\ts.Out[i*2+offsetIdx] = byte(length)\n+\t\t\ts.Out[i*2+offsetIdx+1] = byte(length >> 8)\n+\t\t}\n+\t}\n+\n+\treturn s.Out, nil\n+}\n+\n+// compress4Xp will compress 4 streams using separate goroutines.\n+func (s *Scratch) compress4Xp(src []byte) ([]byte, error) {\n+\tif len(src) < 12 {\n+\t\treturn nil, ErrIncompressible\n+\t}\n+\t// Add placeholder for output length\n+\ts.Out = s.Out[:6]\n+\n+\tsegmentSize := (len(src) + 3) / 4\n+\tvar wg sync.WaitGroup\n+\tvar errs [4]error\n+\twg.Add(4)\n+\tfor i := 0; i < 4; i++ {\n+\t\ttoDo := src\n+\t\tif len(toDo) > segmentSize {\n+\t\t\ttoDo = toDo[:segmentSize]\n+\t\t}\n+\t\tsrc = src[len(toDo):]\n+\n+\t\t// Separate goroutine for each block.\n+\t\tgo func(i int) {\n+\t\t\ts.tmpOut[i], errs[i] = s.compress1xDo(s.tmpOut[i][:0], toDo)\n+\t\t\twg.Done()\n+\t\t}(i)\n+\t}\n+\twg.Wait()\n+\tfor i := 0; i < 4; i++ {\n+\t\tif errs[i] != nil {\n+\t\t\treturn nil, errs[i]\n+\t\t}\n+\t\to := s.tmpOut[i]\n+\t\t// Write compressed length as little endian before block.\n+\t\tif i < 3 {\n+\t\t\t// Last length is not written.\n+\t\t\ts.Out[i*2] = byte(len(o))\n+\t\t\ts.Out[i*2+1] = byte(len(o) >> 8)\n+\t\t}\n+\n+\t\t// Write output.\n+\t\ts.Out = append(s.Out, o...)\n+\t}\n+\treturn s.Out, nil\n+}\n+\n+// countSimple will create a simple histogram in s.count.\n+// Returns the biggest count.\n+// Does not update s.clearCount.\n+func (s *Scratch) countSimple(in []byte) (max int, reuse bool) {\n+\treuse = true\n+\tfor _, v := range in {\n+\t\ts.count[v]++\n+\t}\n+\tm := uint32(0)\n+\tif len(s.prevTable) > 0 {\n+\t\tfor i, v := range s.count[:] {\n+\t\t\tif v > m {\n+\t\t\t\tm = v\n+\t\t\t}\n+\t\t\tif v > 0 {\n+\t\t\t\ts.symbolLen = uint16(i) + 1\n+\t\t\t\tif i >= len(s.prevTable) {\n+\t\t\t\t\treuse = false\n+\t\t\t\t} else {\n+\t\t\t\t\tif s.prevTable[i].nBits == 0 {\n+\t\t\t\t\t\treuse = false\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t\treturn int(m), reuse\n+\t}\n+\tfor i, v := range s.count[:] {\n+\t\tif v > m {\n+\t\t\tm = v\n+\t\t}\n+\t\tif v > 0 {\n+\t\t\ts.symbolLen = uint16(i) + 1\n+\t\t}\n+\t}\n+\treturn int(m), false\n+}\n+\n+func (s *Scratch) canUseTable(c cTable) bool {\n+\tif len(c) < int(s.symbolLen) {\n+\t\treturn false\n+\t}\n+\tfor i, v := range s.count[:s.symbolLen] {\n+\t\tif v != 0 && c[i].nBits == 0 {\n+\t\t\treturn false\n+\t\t}\n+\t}\n+\treturn true\n+}\n+\n+// minTableLog provides the minimum logSize to safely represent a distribution.\n+func (s *Scratch) minTableLog() uint8 {\n+\tminBitsSrc := highBit32(uint32(s.br.remain()-1)) + 1\n+\tminBitsSymbols := highBit32(uint32(s.symbolLen-1)) + 2\n+\tif minBitsSrc < minBitsSymbols {\n+\t\treturn uint8(minBitsSrc)\n+\t}\n+\treturn uint8(minBitsSymbols)\n+}\n+\n+// optimalTableLog calculates and sets the optimal tableLog in s.actualTableLog\n+func (s *Scratch) optimalTableLog() {\n+\ttableLog := s.TableLog\n+\tminBits := s.minTableLog()\n+\tmaxBitsSrc := uint8(highBit32(uint32(s.br.remain()-1))) - 2\n+\tif maxBitsSrc < tableLog {\n+\t\t// Accuracy can be reduced\n+\t\ttableLog = maxBitsSrc\n+\t}\n+\tif minBits > tableLog {\n+\t\ttableLog = minBits\n+\t}\n+\t// Need a minimum to safely represent all symbol values\n+\tif tableLog < minTablelog {\n+\t\ttableLog = minTablelog\n+\t}\n+\tif tableLog > tableLogMax {\n+\t\ttableLog = tableLogMax\n+\t}\n+\ts.actualTableLog = tableLog\n+}\n+\n+type cTableEntry struct {\n+\tval   uint16\n+\tnBits uint8\n+\t// We have 8 bits extra\n+}\n+\n+const huffNodesMask = huffNodesLen - 1\n+\n+func (s *Scratch) buildCTable() error {\n+\ts.huffSort()\n+\tif cap(s.cTable) < maxSymbolValue+1 {\n+\t\ts.cTable = make([]cTableEntry, s.symbolLen, maxSymbolValue+1)\n+\t} else {\n+\t\ts.cTable = s.cTable[:s.symbolLen]\n+\t\tfor i := range s.cTable {\n+\t\t\ts.cTable[i] = cTableEntry{}\n+\t\t}\n+\t}\n+\n+\tvar startNode = int16(s.symbolLen)\n+\tnonNullRank := s.symbolLen - 1\n+\n+\tnodeNb := int16(startNode)\n+\thuffNode := s.nodes[1 : huffNodesLen+1]\n+\n+\t// This overlays the slice above, but allows \"-1\" index lookups.\n+\t// Different from reference implementation.\n+\thuffNode0 := s.nodes[0 : huffNodesLen+1]\n+\n+\tfor huffNode[nonNullRank].count == 0 {\n+\t\tnonNullRank--\n+\t}\n+\n+\tlowS := int16(nonNullRank)\n+\tnodeRoot := nodeNb + lowS - 1\n+\tlowN := nodeNb\n+\thuffNode[nodeNb].count = huffNode[lowS].count + huffNode[lowS-1].count\n+\thuffNode[lowS].parent, huffNode[lowS-1].parent = uint16(nodeNb), uint16(nodeNb)\n+\tnodeNb++\n+\tlowS -= 2\n+\tfor n := nodeNb; n <= nodeRoot; n++ {\n+\t\thuffNode[n].count = 1 << 30\n+\t}\n+\t// fake entry, strong barrier\n+\thuffNode0[0].count = 1 << 31\n+\n+\t// create parents\n+\tfor nodeNb <= nodeRoot {\n+\t\tvar n1, n2 int16\n+\t\tif huffNode0[lowS+1].count < huffNode0[lowN+1].count {\n+\t\t\tn1 = lowS\n+\t\t\tlowS--\n+\t\t} else {\n+\t\t\tn1 = lowN\n+\t\t\tlowN++\n+\t\t}\n+\t\tif huffNode0[lowS+1].count < huffNode0[lowN+1].count {\n+\t\t\tn2 = lowS\n+\t\t\tlowS--\n+\t\t} else {\n+\t\t\tn2 = lowN\n+\t\t\tlowN++\n+\t\t}\n+\n+\t\thuffNode[nodeNb].count = huffNode0[n1+1].count + huffNode0[n2+1].count\n+\t\thuffNode0[n1+1].parent, huffNode0[n2+1].parent = uint16(nodeNb), uint16(nodeNb)\n+\t\tnodeNb++\n+\t}\n+\n+\t// distribute weights (unlimited tree height)\n+\thuffNode[nodeRoot].nbBits = 0\n+\tfor n := nodeRoot - 1; n >= startNode; n-- {\n+\t\thuffNode[n].nbBits = huffNode[huffNode[n].parent].nbBits + 1\n+\t}\n+\tfor n := uint16(0); n <= nonNullRank; n++ {\n+\t\thuffNode[n].nbBits = huffNode[huffNode[n].parent].nbBits + 1\n+\t}\n+\ts.actualTableLog = s.setMaxHeight(int(nonNullRank))\n+\tmaxNbBits := s.actualTableLog\n+\n+\t// fill result into tree (val, nbBits)\n+\tif maxNbBits > tableLogMax {\n+\t\treturn fmt.Errorf(\"internal error: maxNbBits (%d) > tableLogMax (%d)\", maxNbBits, tableLogMax)\n+\t}\n+\tvar nbPerRank [tableLogMax + 1]uint16\n+\tvar valPerRank [16]uint16\n+\tfor _, v := range huffNode[:nonNullRank+1] {\n+\t\tnbPerRank[v.nbBits]++\n+\t}\n+\t// determine stating value per rank\n+\t{\n+\t\tmin := uint16(0)\n+\t\tfor n := maxNbBits; n > 0; n-- {\n+\t\t\t// get starting value within each rank\n+\t\t\tvalPerRank[n] = min\n+\t\t\tmin += nbPerRank[n]\n+\t\t\tmin >>= 1\n+\t\t}\n+\t}\n+\n+\t// push nbBits per symbol, symbol order\n+\tfor _, v := range huffNode[:nonNullRank+1] {\n+\t\ts.cTable[v.symbol].nBits = v.nbBits\n+\t}\n+\n+\t// assign value within rank, symbol order\n+\tt := s.cTable[:s.symbolLen]\n+\tfor n, val := range t {\n+\t\tnbits := val.nBits & 15\n+\t\tv := valPerRank[nbits]\n+\t\tt[n].val = v\n+\t\tvalPerRank[nbits] = v + 1\n+\t}\n+\n+\treturn nil\n+}\n+\n+// huffSort will sort symbols, decreasing order.\n+func (s *Scratch) huffSort() {\n+\ttype rankPos struct {\n+\t\tbase    uint32\n+\t\tcurrent uint32\n+\t}\n+\n+\t// Clear nodes\n+\tnodes := s.nodes[:huffNodesLen+1]\n+\ts.nodes = nodes\n+\tnodes = nodes[1 : huffNodesLen+1]\n+\n+\t// Sort into buckets based on length of symbol count.\n+\tvar rank [32]rankPos\n+\tfor _, v := range s.count[:s.symbolLen] {\n+\t\tr := highBit32(v+1) & 31\n+\t\trank[r].base++\n+\t}\n+\t// maxBitLength is log2(BlockSizeMax) + 1\n+\tconst maxBitLength = 18 + 1\n+\tfor n := maxBitLength; n > 0; n-- {\n+\t\trank[n-1].base += rank[n].base\n+\t}\n+\tfor n := range rank[:maxBitLength] {\n+\t\trank[n].current = rank[n].base\n+\t}\n+\tfor n, c := range s.count[:s.symbolLen] {\n+\t\tr := (highBit32(c+1) + 1) & 31\n+\t\tpos := rank[r].current\n+\t\trank[r].current++\n+\t\tprev := nodes[(pos-1)&huffNodesMask]\n+\t\tfor pos > rank[r].base && c > prev.count {\n+\t\t\tnodes[pos&huffNodesMask] = prev\n+\t\t\tpos--\n+\t\t\tprev = nodes[(pos-1)&huffNodesMask]\n+\t\t}\n+\t\tnodes[pos&huffNodesMask] = nodeElt{count: c, symbol: byte(n)}\n+\t}\n+\treturn\n+}\n+\n+func (s *Scratch) setMaxHeight(lastNonNull int) uint8 {\n+\tmaxNbBits := s.actualTableLog\n+\thuffNode := s.nodes[1 : huffNodesLen+1]\n+\t//huffNode = huffNode[: huffNodesLen]\n+\n+\tlargestBits := huffNode[lastNonNull].nbBits\n+\n+\t// early exit : no elt > maxNbBits\n+\tif largestBits <= maxNbBits {\n+\t\treturn largestBits\n+\t}\n+\ttotalCost := int(0)\n+\tbaseCost := int(1) << (largestBits - maxNbBits)\n+\tn := uint32(lastNonNull)\n+\n+\tfor huffNode[n].nbBits > maxNbBits {\n+\t\ttotalCost += baseCost - (1 << (largestBits - huffNode[n].nbBits))\n+\t\thuffNode[n].nbBits = maxNbBits\n+\t\tn--\n+\t}\n+\t// n stops at huffNode[n].nbBits <= maxNbBits\n+\n+\tfor huffNode[n].nbBits == maxNbBits {\n+\t\tn--\n+\t}\n+\t// n end at index of smallest symbol using < maxNbBits\n+\n+\t// renorm totalCost\n+\ttotalCost >>= largestBits - maxNbBits /* note : totalCost is necessarily a multiple of baseCost */\n+\n+\t// repay normalized cost\n+\t{\n+\t\tconst noSymbol = 0xF0F0F0F0\n+\t\tvar rankLast [tableLogMax + 2]uint32\n+\n+\t\tfor i := range rankLast[:] {\n+\t\t\trankLast[i] = noSymbol\n+\t\t}\n+\n+\t\t// Get pos of last (smallest) symbol per rank\n+\t\t{\n+\t\t\tcurrentNbBits := uint8(maxNbBits)\n+\t\t\tfor pos := int(n); pos >= 0; pos-- {\n+\t\t\t\tif huffNode[pos].nbBits >= currentNbBits {\n+\t\t\t\t\tcontinue\n+\t\t\t\t}\n+\t\t\t\tcurrentNbBits = huffNode[pos].nbBits // < maxNbBits\n+\t\t\t\trankLast[maxNbBits-currentNbBits] = uint32(pos)\n+\t\t\t}\n+\t\t}\n+\n+\t\tfor totalCost > 0 {\n+\t\t\tnBitsToDecrease := uint8(highBit32(uint32(totalCost))) + 1\n+\n+\t\t\tfor ; nBitsToDecrease > 1; nBitsToDecrease-- {\n+\t\t\t\thighPos := rankLast[nBitsToDecrease]\n+\t\t\t\tlowPos := rankLast[nBitsToDecrease-1]\n+\t\t\t\tif highPos == noSymbol {\n+\t\t\t\t\tcontinue\n+\t\t\t\t}\n+\t\t\t\tif lowPos == noSymbol {\n+\t\t\t\t\tbreak\n+\t\t\t\t}\n+\t\t\t\thighTotal := huffNode[highPos].count\n+\t\t\t\tlowTotal := 2 * huffNode[lowPos].count\n+\t\t\t\tif highTotal <= lowTotal {\n+\t\t\t\t\tbreak\n+\t\t\t\t}\n+\t\t\t}\n+\t\t\t// only triggered when no more rank 1 symbol left => find closest one (note : there is necessarily at least one !)\n+\t\t\t// HUF_MAX_TABLELOG test just to please gcc 5+; but it should not be necessary\n+\t\t\t// FIXME: try to remove\n+\t\t\tfor (nBitsToDecrease <= tableLogMax) && (rankLast[nBitsToDecrease] == noSymbol) {\n+\t\t\t\tnBitsToDecrease++\n+\t\t\t}\n+\t\t\ttotalCost -= 1 << (nBitsToDecrease - 1)\n+\t\t\tif rankLast[nBitsToDecrease-1] == noSymbol {\n+\t\t\t\t// this rank is no longer empty\n+\t\t\t\trankLast[nBitsToDecrease-1] = rankLast[nBitsToDecrease]\n+\t\t\t}\n+\t\t\thuffNode[rankLast[nBitsToDecrease]].nbBits++\n+\t\t\tif rankLast[nBitsToDecrease] == 0 {\n+\t\t\t\t/* special case, reached largest symbol */\n+\t\t\t\trankLast[nBitsToDecrease] = noSymbol\n+\t\t\t} else {\n+\t\t\t\trankLast[nBitsToDecrease]--\n+\t\t\t\tif huffNode[rankLast[nBitsToDecrease]].nbBits != maxNbBits-nBitsToDecrease {\n+\t\t\t\t\trankLast[nBitsToDecrease] = noSymbol /* this rank is now empty */\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\n+\t\tfor totalCost < 0 { /* Sometimes, cost correction overshoot */\n+\t\t\tif rankLast[1] == noSymbol { /* special case : no rank 1 symbol (using maxNbBits-1); let's create one from largest rank 0 (using maxNbBits) */\n+\t\t\t\tfor huffNode[n].nbBits == maxNbBits {\n+\t\t\t\t\tn--\n+\t\t\t\t}\n+\t\t\t\thuffNode[n+1].nbBits--\n+\t\t\t\trankLast[1] = n + 1\n+\t\t\t\ttotalCost++\n+\t\t\t\tcontinue\n+\t\t\t}\n+\t\t\thuffNode[rankLast[1]+1].nbBits--\n+\t\t\trankLast[1]++\n+\t\t\ttotalCost++\n+\t\t}\n+\t}\n+\treturn maxNbBits\n+}\n+\n+type nodeElt struct {\n+\tcount  uint32\n+\tparent uint16\n+\tsymbol byte\n+\tnbBits uint8\n+}"
    },
    {
      "sha": "97ae66a4ac7cc34098a4af80b26bdb421d1f6767",
      "filename": "backend/vendor/github.com/klauspost/compress/huff0/decompress.go",
      "status": "added",
      "additions": 472,
      "deletions": 0,
      "changes": 472,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/klauspost/compress/huff0/decompress.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/klauspost/compress/huff0/decompress.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/github.com/klauspost/compress/huff0/decompress.go?ref=890d7154e777d0673defcd11db3ef792473d68c9",
      "patch": "@@ -0,0 +1,472 @@\n+package huff0\n+\n+import (\n+\t\"errors\"\n+\t\"fmt\"\n+\t\"io\"\n+\n+\t\"github.com/klauspost/compress/fse\"\n+)\n+\n+type dTable struct {\n+\tsingle []dEntrySingle\n+\tdouble []dEntryDouble\n+}\n+\n+// single-symbols decoding\n+type dEntrySingle struct {\n+\tentry uint16\n+}\n+\n+// double-symbols decoding\n+type dEntryDouble struct {\n+\tseq   uint16\n+\tnBits uint8\n+\tlen   uint8\n+}\n+\n+// ReadTable will read a table from the input.\n+// The size of the input may be larger than the table definition.\n+// Any content remaining after the table definition will be returned.\n+// If no Scratch is provided a new one is allocated.\n+// The returned Scratch can be used for decoding input using this table.\n+func ReadTable(in []byte, s *Scratch) (s2 *Scratch, remain []byte, err error) {\n+\ts, err = s.prepare(in)\n+\tif err != nil {\n+\t\treturn s, nil, err\n+\t}\n+\tif len(in) <= 1 {\n+\t\treturn s, nil, errors.New(\"input too small for table\")\n+\t}\n+\tiSize := in[0]\n+\tin = in[1:]\n+\tif iSize >= 128 {\n+\t\t// Uncompressed\n+\t\toSize := iSize - 127\n+\t\tiSize = (oSize + 1) / 2\n+\t\tif int(iSize) > len(in) {\n+\t\t\treturn s, nil, errors.New(\"input too small for table\")\n+\t\t}\n+\t\tfor n := uint8(0); n < oSize; n += 2 {\n+\t\t\tv := in[n/2]\n+\t\t\ts.huffWeight[n] = v >> 4\n+\t\t\ts.huffWeight[n+1] = v & 15\n+\t\t}\n+\t\ts.symbolLen = uint16(oSize)\n+\t\tin = in[iSize:]\n+\t} else {\n+\t\tif len(in) <= int(iSize) {\n+\t\t\treturn s, nil, errors.New(\"input too small for table\")\n+\t\t}\n+\t\t// FSE compressed weights\n+\t\ts.fse.DecompressLimit = 255\n+\t\thw := s.huffWeight[:]\n+\t\ts.fse.Out = hw\n+\t\tb, err := fse.Decompress(in[:iSize], s.fse)\n+\t\ts.fse.Out = nil\n+\t\tif err != nil {\n+\t\t\treturn s, nil, err\n+\t\t}\n+\t\tif len(b) > 255 {\n+\t\t\treturn s, nil, errors.New(\"corrupt input: output table too large\")\n+\t\t}\n+\t\ts.symbolLen = uint16(len(b))\n+\t\tin = in[iSize:]\n+\t}\n+\n+\t// collect weight stats\n+\tvar rankStats [16]uint32\n+\tweightTotal := uint32(0)\n+\tfor _, v := range s.huffWeight[:s.symbolLen] {\n+\t\tif v > tableLogMax {\n+\t\t\treturn s, nil, errors.New(\"corrupt input: weight too large\")\n+\t\t}\n+\t\tv2 := v & 15\n+\t\trankStats[v2]++\n+\t\tweightTotal += (1 << v2) >> 1\n+\t}\n+\tif weightTotal == 0 {\n+\t\treturn s, nil, errors.New(\"corrupt input: weights zero\")\n+\t}\n+\n+\t// get last non-null symbol weight (implied, total must be 2^n)\n+\t{\n+\t\ttableLog := highBit32(weightTotal) + 1\n+\t\tif tableLog > tableLogMax {\n+\t\t\treturn s, nil, errors.New(\"corrupt input: tableLog too big\")\n+\t\t}\n+\t\ts.actualTableLog = uint8(tableLog)\n+\t\t// determine last weight\n+\t\t{\n+\t\t\ttotal := uint32(1) << tableLog\n+\t\t\trest := total - weightTotal\n+\t\t\tverif := uint32(1) << highBit32(rest)\n+\t\t\tlastWeight := highBit32(rest) + 1\n+\t\t\tif verif != rest {\n+\t\t\t\t// last value must be a clean power of 2\n+\t\t\t\treturn s, nil, errors.New(\"corrupt input: last value not power of two\")\n+\t\t\t}\n+\t\t\ts.huffWeight[s.symbolLen] = uint8(lastWeight)\n+\t\t\ts.symbolLen++\n+\t\t\trankStats[lastWeight]++\n+\t\t}\n+\t}\n+\n+\tif (rankStats[1] < 2) || (rankStats[1]&1 != 0) {\n+\t\t// by construction : at least 2 elts of rank 1, must be even\n+\t\treturn s, nil, errors.New(\"corrupt input: min elt size, even check failed \")\n+\t}\n+\n+\t// TODO: Choose between single/double symbol decoding\n+\n+\t// Calculate starting value for each rank\n+\t{\n+\t\tvar nextRankStart uint32\n+\t\tfor n := uint8(1); n < s.actualTableLog+1; n++ {\n+\t\t\tcurrent := nextRankStart\n+\t\t\tnextRankStart += rankStats[n] << (n - 1)\n+\t\t\trankStats[n] = current\n+\t\t}\n+\t}\n+\n+\t// fill DTable (always full size)\n+\ttSize := 1 << tableLogMax\n+\tif len(s.dt.single) != tSize {\n+\t\ts.dt.single = make([]dEntrySingle, tSize)\n+\t}\n+\tfor n, w := range s.huffWeight[:s.symbolLen] {\n+\t\tif w == 0 {\n+\t\t\tcontinue\n+\t\t}\n+\t\tlength := (uint32(1) << w) >> 1\n+\t\td := dEntrySingle{\n+\t\t\tentry: uint16(s.actualTableLog+1-w) | (uint16(n) << 8),\n+\t\t}\n+\t\tsingle := s.dt.single[rankStats[w] : rankStats[w]+length]\n+\t\tfor i := range single {\n+\t\t\tsingle[i] = d\n+\t\t}\n+\t\trankStats[w] += length\n+\t}\n+\treturn s, in, nil\n+}\n+\n+// Decompress1X will decompress a 1X encoded stream.\n+// The length of the supplied input must match the end of a block exactly.\n+// Before this is called, the table must be initialized with ReadTable unless\n+// the encoder re-used the table.\n+func (s *Scratch) Decompress1X(in []byte) (out []byte, err error) {\n+\tif len(s.dt.single) == 0 {\n+\t\treturn nil, errors.New(\"no table loaded\")\n+\t}\n+\tvar br bitReader\n+\terr = br.init(in)\n+\tif err != nil {\n+\t\treturn nil, err\n+\t}\n+\ts.Out = s.Out[:0]\n+\n+\tdecode := func() byte {\n+\t\tval := br.peekBitsFast(s.actualTableLog) /* note : actualTableLog >= 1 */\n+\t\tv := s.dt.single[val]\n+\t\tbr.bitsRead += uint8(v.entry)\n+\t\treturn uint8(v.entry >> 8)\n+\t}\n+\thasDec := func(v dEntrySingle) byte {\n+\t\tbr.bitsRead += uint8(v.entry)\n+\t\treturn uint8(v.entry >> 8)\n+\t}\n+\n+\t// Avoid bounds check by always having full sized table.\n+\tconst tlSize = 1 << tableLogMax\n+\tconst tlMask = tlSize - 1\n+\tdt := s.dt.single[:tlSize]\n+\n+\t// Use temp table to avoid bound checks/append penalty.\n+\tvar tmp = s.huffWeight[:256]\n+\tvar off uint8\n+\n+\tfor br.off >= 8 {\n+\t\tbr.fillFast()\n+\t\ttmp[off+0] = hasDec(dt[br.peekBitsFast(s.actualTableLog)&tlMask])\n+\t\ttmp[off+1] = hasDec(dt[br.peekBitsFast(s.actualTableLog)&tlMask])\n+\t\tbr.fillFast()\n+\t\ttmp[off+2] = hasDec(dt[br.peekBitsFast(s.actualTableLog)&tlMask])\n+\t\ttmp[off+3] = hasDec(dt[br.peekBitsFast(s.actualTableLog)&tlMask])\n+\t\toff += 4\n+\t\tif off == 0 {\n+\t\t\tif len(s.Out)+256 > s.MaxDecodedSize {\n+\t\t\t\tbr.close()\n+\t\t\t\treturn nil, ErrMaxDecodedSizeExceeded\n+\t\t\t}\n+\t\t\ts.Out = append(s.Out, tmp...)\n+\t\t}\n+\t}\n+\n+\tif len(s.Out)+int(off) > s.MaxDecodedSize {\n+\t\tbr.close()\n+\t\treturn nil, ErrMaxDecodedSizeExceeded\n+\t}\n+\ts.Out = append(s.Out, tmp[:off]...)\n+\n+\tfor !br.finished() {\n+\t\tbr.fill()\n+\t\tif len(s.Out) >= s.MaxDecodedSize {\n+\t\t\tbr.close()\n+\t\t\treturn nil, ErrMaxDecodedSizeExceeded\n+\t\t}\n+\t\ts.Out = append(s.Out, decode())\n+\t}\n+\treturn s.Out, br.close()\n+}\n+\n+// Decompress4X will decompress a 4X encoded stream.\n+// Before this is called, the table must be initialized with ReadTable unless\n+// the encoder re-used the table.\n+// The length of the supplied input must match the end of a block exactly.\n+// The destination size of the uncompressed data must be known and provided.\n+func (s *Scratch) Decompress4X(in []byte, dstSize int) (out []byte, err error) {\n+\tif len(s.dt.single) == 0 {\n+\t\treturn nil, errors.New(\"no table loaded\")\n+\t}\n+\tif len(in) < 6+(4*1) {\n+\t\treturn nil, errors.New(\"input too small\")\n+\t}\n+\tif dstSize > s.MaxDecodedSize {\n+\t\treturn nil, ErrMaxDecodedSizeExceeded\n+\t}\n+\t// TODO: We do not detect when we overrun a buffer, except if the last one does.\n+\n+\tvar br [4]bitReader\n+\tstart := 6\n+\tfor i := 0; i < 3; i++ {\n+\t\tlength := int(in[i*2]) | (int(in[i*2+1]) << 8)\n+\t\tif start+length >= len(in) {\n+\t\t\treturn nil, errors.New(\"truncated input (or invalid offset)\")\n+\t\t}\n+\t\terr = br[i].init(in[start : start+length])\n+\t\tif err != nil {\n+\t\t\treturn nil, err\n+\t\t}\n+\t\tstart += length\n+\t}\n+\terr = br[3].init(in[start:])\n+\tif err != nil {\n+\t\treturn nil, err\n+\t}\n+\n+\t// Prepare output\n+\tif cap(s.Out) < dstSize {\n+\t\ts.Out = make([]byte, 0, dstSize)\n+\t}\n+\ts.Out = s.Out[:dstSize]\n+\t// destination, offset to match first output\n+\tdstOut := s.Out\n+\tdstEvery := (dstSize + 3) / 4\n+\n+\tconst tlSize = 1 << tableLogMax\n+\tconst tlMask = tlSize - 1\n+\tsingle := s.dt.single[:tlSize]\n+\n+\tdecode := func(br *bitReader) byte {\n+\t\tval := br.peekBitsFast(s.actualTableLog) /* note : actualTableLog >= 1 */\n+\t\tv := single[val&tlMask]\n+\t\tbr.bitsRead += uint8(v.entry)\n+\t\treturn uint8(v.entry >> 8)\n+\t}\n+\n+\t// Use temp table to avoid bound checks/append penalty.\n+\tvar tmp = s.huffWeight[:256]\n+\tvar off uint8\n+\tvar decoded int\n+\n+\t// Decode 2 values from each decoder/loop.\n+\tconst bufoff = 256 / 4\n+bigloop:\n+\tfor {\n+\t\tfor i := range br {\n+\t\t\tbr := &br[i]\n+\t\t\tif br.off < 4 {\n+\t\t\t\tbreak bigloop\n+\t\t\t}\n+\t\t\tbr.fillFast()\n+\t\t}\n+\n+\t\t{\n+\t\t\tconst stream = 0\n+\t\t\tval := br[stream].peekBitsFast(s.actualTableLog)\n+\t\t\tv := single[val&tlMask]\n+\t\t\tbr[stream].bitsRead += uint8(v.entry)\n+\n+\t\t\tval2 := br[stream].peekBitsFast(s.actualTableLog)\n+\t\t\tv2 := single[val2&tlMask]\n+\t\t\ttmp[off+bufoff*stream+1] = uint8(v2.entry >> 8)\n+\t\t\ttmp[off+bufoff*stream] = uint8(v.entry >> 8)\n+\t\t\tbr[stream].bitsRead += uint8(v2.entry)\n+\t\t}\n+\n+\t\t{\n+\t\t\tconst stream = 1\n+\t\t\tval := br[stream].peekBitsFast(s.actualTableLog)\n+\t\t\tv := single[val&tlMask]\n+\t\t\tbr[stream].bitsRead += uint8(v.entry)\n+\n+\t\t\tval2 := br[stream].peekBitsFast(s.actualTableLog)\n+\t\t\tv2 := single[val2&tlMask]\n+\t\t\ttmp[off+bufoff*stream+1] = uint8(v2.entry >> 8)\n+\t\t\ttmp[off+bufoff*stream] = uint8(v.entry >> 8)\n+\t\t\tbr[stream].bitsRead += uint8(v2.entry)\n+\t\t}\n+\n+\t\t{\n+\t\t\tconst stream = 2\n+\t\t\tval := br[stream].peekBitsFast(s.actualTableLog)\n+\t\t\tv := single[val&tlMask]\n+\t\t\tbr[stream].bitsRead += uint8(v.entry)\n+\n+\t\t\tval2 := br[stream].peekBitsFast(s.actualTableLog)\n+\t\t\tv2 := single[val2&tlMask]\n+\t\t\ttmp[off+bufoff*stream+1] = uint8(v2.entry >> 8)\n+\t\t\ttmp[off+bufoff*stream] = uint8(v.entry >> 8)\n+\t\t\tbr[stream].bitsRead += uint8(v2.entry)\n+\t\t}\n+\n+\t\t{\n+\t\t\tconst stream = 3\n+\t\t\tval := br[stream].peekBitsFast(s.actualTableLog)\n+\t\t\tv := single[val&tlMask]\n+\t\t\tbr[stream].bitsRead += uint8(v.entry)\n+\n+\t\t\tval2 := br[stream].peekBitsFast(s.actualTableLog)\n+\t\t\tv2 := single[val2&tlMask]\n+\t\t\ttmp[off+bufoff*stream+1] = uint8(v2.entry >> 8)\n+\t\t\ttmp[off+bufoff*stream] = uint8(v.entry >> 8)\n+\t\t\tbr[stream].bitsRead += uint8(v2.entry)\n+\t\t}\n+\n+\t\toff += 2\n+\n+\t\tif off == bufoff {\n+\t\t\tif bufoff > dstEvery {\n+\t\t\t\treturn nil, errors.New(\"corruption detected: stream overrun 1\")\n+\t\t\t}\n+\t\t\tcopy(dstOut, tmp[:bufoff])\n+\t\t\tcopy(dstOut[dstEvery:], tmp[bufoff:bufoff*2])\n+\t\t\tcopy(dstOut[dstEvery*2:], tmp[bufoff*2:bufoff*3])\n+\t\t\tcopy(dstOut[dstEvery*3:], tmp[bufoff*3:bufoff*4])\n+\t\t\toff = 0\n+\t\t\tdstOut = dstOut[bufoff:]\n+\t\t\tdecoded += 256\n+\t\t\t// There must at least be 3 buffers left.\n+\t\t\tif len(dstOut) < dstEvery*3 {\n+\t\t\t\treturn nil, errors.New(\"corruption detected: stream overrun 2\")\n+\t\t\t}\n+\t\t}\n+\t}\n+\tif off > 0 {\n+\t\tioff := int(off)\n+\t\tif len(dstOut) < dstEvery*3+ioff {\n+\t\t\treturn nil, errors.New(\"corruption detected: stream overrun 3\")\n+\t\t}\n+\t\tcopy(dstOut, tmp[:off])\n+\t\tcopy(dstOut[dstEvery:dstEvery+ioff], tmp[bufoff:bufoff*2])\n+\t\tcopy(dstOut[dstEvery*2:dstEvery*2+ioff], tmp[bufoff*2:bufoff*3])\n+\t\tcopy(dstOut[dstEvery*3:dstEvery*3+ioff], tmp[bufoff*3:bufoff*4])\n+\t\tdecoded += int(off) * 4\n+\t\tdstOut = dstOut[off:]\n+\t}\n+\n+\t// Decode remaining.\n+\tfor i := range br {\n+\t\toffset := dstEvery * i\n+\t\tbr := &br[i]\n+\t\tfor !br.finished() {\n+\t\t\tbr.fill()\n+\t\t\tif offset >= len(dstOut) {\n+\t\t\t\treturn nil, errors.New(\"corruption detected: stream overrun 4\")\n+\t\t\t}\n+\t\t\tdstOut[offset] = decode(br)\n+\t\t\toffset++\n+\t\t}\n+\t\tdecoded += offset - dstEvery*i\n+\t\terr = br.close()\n+\t\tif err != nil {\n+\t\t\treturn nil, err\n+\t\t}\n+\t}\n+\tif dstSize != decoded {\n+\t\treturn nil, errors.New(\"corruption detected: short output block\")\n+\t}\n+\treturn s.Out, nil\n+}\n+\n+// matches will compare a decoding table to a coding table.\n+// Errors are written to the writer.\n+// Nothing will be written if table is ok.\n+func (s *Scratch) matches(ct cTable, w io.Writer) {\n+\tif s == nil || len(s.dt.single) == 0 {\n+\t\treturn\n+\t}\n+\tdt := s.dt.single[:1<<s.actualTableLog]\n+\ttablelog := s.actualTableLog\n+\tok := 0\n+\tbroken := 0\n+\tfor sym, enc := range ct {\n+\t\terrs := 0\n+\t\tbroken++\n+\t\tif enc.nBits == 0 {\n+\t\t\tfor _, dec := range dt {\n+\t\t\t\tif uint8(dec.entry>>8) == byte(sym) {\n+\t\t\t\t\tfmt.Fprintf(w, \"symbol %x has decoder, but no encoder\\n\", sym)\n+\t\t\t\t\terrs++\n+\t\t\t\t\tbreak\n+\t\t\t\t}\n+\t\t\t}\n+\t\t\tif errs == 0 {\n+\t\t\t\tbroken--\n+\t\t\t}\n+\t\t\tcontinue\n+\t\t}\n+\t\t// Unused bits in input\n+\t\tub := tablelog - enc.nBits\n+\t\ttop := enc.val << ub\n+\t\t// decoder looks at top bits.\n+\t\tdec := dt[top]\n+\t\tif uint8(dec.entry) != enc.nBits {\n+\t\t\tfmt.Fprintf(w, \"symbol 0x%x bit size mismatch (enc: %d, dec:%d).\\n\", sym, enc.nBits, uint8(dec.entry))\n+\t\t\terrs++\n+\t\t}\n+\t\tif uint8(dec.entry>>8) != uint8(sym) {\n+\t\t\tfmt.Fprintf(w, \"symbol 0x%x decoder output mismatch (enc: %d, dec:%d).\\n\", sym, sym, uint8(dec.entry>>8))\n+\t\t\terrs++\n+\t\t}\n+\t\tif errs > 0 {\n+\t\t\tfmt.Fprintf(w, \"%d errros in base, stopping\\n\", errs)\n+\t\t\tcontinue\n+\t\t}\n+\t\t// Ensure that all combinations are covered.\n+\t\tfor i := uint16(0); i < (1 << ub); i++ {\n+\t\t\tvval := top | i\n+\t\t\tdec := dt[vval]\n+\t\t\tif uint8(dec.entry) != enc.nBits {\n+\t\t\t\tfmt.Fprintf(w, \"symbol 0x%x bit size mismatch (enc: %d, dec:%d).\\n\", vval, enc.nBits, uint8(dec.entry))\n+\t\t\t\terrs++\n+\t\t\t}\n+\t\t\tif uint8(dec.entry>>8) != uint8(sym) {\n+\t\t\t\tfmt.Fprintf(w, \"symbol 0x%x decoder output mismatch (enc: %d, dec:%d).\\n\", vval, sym, uint8(dec.entry>>8))\n+\t\t\t\terrs++\n+\t\t\t}\n+\t\t\tif errs > 20 {\n+\t\t\t\tfmt.Fprintf(w, \"%d errros, stopping\\n\", errs)\n+\t\t\t\tbreak\n+\t\t\t}\n+\t\t}\n+\t\tif errs == 0 {\n+\t\t\tok++\n+\t\t\tbroken--\n+\t\t}\n+\t}\n+\tif broken > 0 {\n+\t\tfmt.Fprintf(w, \"%d broken, %d ok\\n\", broken, ok)\n+\t}\n+}"
    },
    {
      "sha": "6bc23bbf00ff271e36f01f392122b7da9eed8f67",
      "filename": "backend/vendor/github.com/klauspost/compress/huff0/huff0.go",
      "status": "added",
      "additions": 258,
      "deletions": 0,
      "changes": 258,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/klauspost/compress/huff0/huff0.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/klauspost/compress/huff0/huff0.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/github.com/klauspost/compress/huff0/huff0.go?ref=890d7154e777d0673defcd11db3ef792473d68c9",
      "patch": "@@ -0,0 +1,258 @@\n+// Package huff0 provides fast huffman encoding as used in zstd.\n+//\n+// See README.md at https://github.com/klauspost/compress/tree/master/huff0 for details.\n+package huff0\n+\n+import (\n+\t\"errors\"\n+\t\"fmt\"\n+\t\"math\"\n+\t\"math/bits\"\n+\n+\t\"github.com/klauspost/compress/fse\"\n+)\n+\n+const (\n+\tmaxSymbolValue = 255\n+\n+\t// zstandard limits tablelog to 11, see:\n+\t// https://github.com/facebook/zstd/blob/dev/doc/zstd_compression_format.md#huffman-tree-description\n+\ttableLogMax     = 11\n+\ttableLogDefault = 11\n+\tminTablelog     = 5\n+\thuffNodesLen    = 512\n+\n+\t// BlockSizeMax is maximum input size for a single block uncompressed.\n+\tBlockSizeMax = 1<<18 - 1\n+)\n+\n+var (\n+\t// ErrIncompressible is returned when input is judged to be too hard to compress.\n+\tErrIncompressible = errors.New(\"input is not compressible\")\n+\n+\t// ErrUseRLE is returned from the compressor when the input is a single byte value repeated.\n+\tErrUseRLE = errors.New(\"input is single value repeated\")\n+\n+\t// ErrTooBig is return if input is too large for a single block.\n+\tErrTooBig = errors.New(\"input too big\")\n+\n+\t// ErrMaxDecodedSizeExceeded is return if input is too large for a single block.\n+\tErrMaxDecodedSizeExceeded = errors.New(\"maximum output size exceeded\")\n+)\n+\n+type ReusePolicy uint8\n+\n+const (\n+\t// ReusePolicyAllow will allow reuse if it produces smaller output.\n+\tReusePolicyAllow ReusePolicy = iota\n+\n+\t// ReusePolicyPrefer will re-use aggressively if possible.\n+\t// This will not check if a new table will produce smaller output,\n+\t// except if the current table is impossible to use or\n+\t// compressed output is bigger than input.\n+\tReusePolicyPrefer\n+\n+\t// ReusePolicyNone will disable re-use of tables.\n+\t// This is slightly faster than ReusePolicyAllow but may produce larger output.\n+\tReusePolicyNone\n+)\n+\n+type Scratch struct {\n+\tcount [maxSymbolValue + 1]uint32\n+\n+\t// Per block parameters.\n+\t// These can be used to override compression parameters of the block.\n+\t// Do not touch, unless you know what you are doing.\n+\n+\t// Out is output buffer.\n+\t// If the scratch is re-used before the caller is done processing the output,\n+\t// set this field to nil.\n+\t// Otherwise the output buffer will be re-used for next Compression/Decompression step\n+\t// and allocation will be avoided.\n+\tOut []byte\n+\n+\t// OutTable will contain the table data only, if a new table has been generated.\n+\t// Slice of the returned data.\n+\tOutTable []byte\n+\n+\t// OutData will contain the compressed data.\n+\t// Slice of the returned data.\n+\tOutData []byte\n+\n+\t// MaxSymbolValue will override the maximum symbol value of the next block.\n+\tMaxSymbolValue uint8\n+\n+\t// TableLog will attempt to override the tablelog for the next block.\n+\t// Must be <= 11.\n+\tTableLog uint8\n+\n+\t// Reuse will specify the reuse policy\n+\tReuse ReusePolicy\n+\n+\t// WantLogLess allows to specify a log 2 reduction that should at least be achieved,\n+\t// otherwise the block will be returned as incompressible.\n+\t// The reduction should then at least be (input size >> WantLogLess)\n+\t// If WantLogLess == 0 any improvement will do.\n+\tWantLogLess uint8\n+\n+\t// MaxDecodedSize will set the maximum allowed output size.\n+\t// This value will automatically be set to BlockSizeMax if not set.\n+\t// Decoders will return ErrMaxDecodedSizeExceeded is this limit is exceeded.\n+\tMaxDecodedSize int\n+\n+\tbr             byteReader\n+\tsymbolLen      uint16 // Length of active part of the symbol table.\n+\tmaxCount       int    // count of the most probable symbol\n+\tclearCount     bool   // clear count\n+\tactualTableLog uint8  // Selected tablelog.\n+\tprevTable      cTable // Table used for previous compression.\n+\tcTable         cTable // compression table\n+\tdt             dTable // decompression table\n+\tnodes          []nodeElt\n+\ttmpOut         [4][]byte\n+\tfse            *fse.Scratch\n+\thuffWeight     [maxSymbolValue + 1]byte\n+}\n+\n+func (s *Scratch) prepare(in []byte) (*Scratch, error) {\n+\tif len(in) > BlockSizeMax {\n+\t\treturn nil, ErrTooBig\n+\t}\n+\tif s == nil {\n+\t\ts = &Scratch{}\n+\t}\n+\tif s.MaxSymbolValue == 0 {\n+\t\ts.MaxSymbolValue = maxSymbolValue\n+\t}\n+\tif s.TableLog == 0 {\n+\t\ts.TableLog = tableLogDefault\n+\t}\n+\tif s.TableLog > tableLogMax {\n+\t\treturn nil, fmt.Errorf(\"tableLog (%d) > maxTableLog (%d)\", s.TableLog, tableLogMax)\n+\t}\n+\tif s.MaxDecodedSize <= 0 || s.MaxDecodedSize > BlockSizeMax {\n+\t\ts.MaxDecodedSize = BlockSizeMax\n+\t}\n+\tif s.clearCount && s.maxCount == 0 {\n+\t\tfor i := range s.count {\n+\t\t\ts.count[i] = 0\n+\t\t}\n+\t\ts.clearCount = false\n+\t}\n+\tif cap(s.Out) == 0 {\n+\t\ts.Out = make([]byte, 0, len(in))\n+\t}\n+\ts.Out = s.Out[:0]\n+\n+\ts.OutTable = nil\n+\ts.OutData = nil\n+\tif cap(s.nodes) < huffNodesLen+1 {\n+\t\ts.nodes = make([]nodeElt, 0, huffNodesLen+1)\n+\t}\n+\ts.nodes = s.nodes[:0]\n+\tif s.fse == nil {\n+\t\ts.fse = &fse.Scratch{}\n+\t}\n+\ts.br.init(in)\n+\n+\treturn s, nil\n+}\n+\n+type cTable []cTableEntry\n+\n+func (c cTable) write(s *Scratch) error {\n+\tvar (\n+\t\t// precomputed conversion table\n+\t\tbitsToWeight [tableLogMax + 1]byte\n+\t\thuffLog      = s.actualTableLog\n+\t\t// last weight is not saved.\n+\t\tmaxSymbolValue = uint8(s.symbolLen - 1)\n+\t\thuffWeight     = s.huffWeight[:256]\n+\t)\n+\tconst (\n+\t\tmaxFSETableLog = 6\n+\t)\n+\t// convert to weight\n+\tbitsToWeight[0] = 0\n+\tfor n := uint8(1); n < huffLog+1; n++ {\n+\t\tbitsToWeight[n] = huffLog + 1 - n\n+\t}\n+\n+\t// Acquire histogram for FSE.\n+\thist := s.fse.Histogram()\n+\thist = hist[:256]\n+\tfor i := range hist[:16] {\n+\t\thist[i] = 0\n+\t}\n+\tfor n := uint8(0); n < maxSymbolValue; n++ {\n+\t\tv := bitsToWeight[c[n].nBits] & 15\n+\t\thuffWeight[n] = v\n+\t\thist[v]++\n+\t}\n+\n+\t// FSE compress if feasible.\n+\tif maxSymbolValue >= 2 {\n+\t\thuffMaxCnt := uint32(0)\n+\t\thuffMax := uint8(0)\n+\t\tfor i, v := range hist[:16] {\n+\t\t\tif v == 0 {\n+\t\t\t\tcontinue\n+\t\t\t}\n+\t\t\thuffMax = byte(i)\n+\t\t\tif v > huffMaxCnt {\n+\t\t\t\thuffMaxCnt = v\n+\t\t\t}\n+\t\t}\n+\t\ts.fse.HistogramFinished(huffMax, int(huffMaxCnt))\n+\t\ts.fse.TableLog = maxFSETableLog\n+\t\tb, err := fse.Compress(huffWeight[:maxSymbolValue], s.fse)\n+\t\tif err == nil && len(b) < int(s.symbolLen>>1) {\n+\t\t\ts.Out = append(s.Out, uint8(len(b)))\n+\t\t\ts.Out = append(s.Out, b...)\n+\t\t\treturn nil\n+\t\t}\n+\t\t// Unable to compress (RLE/uncompressible)\n+\t}\n+\t// write raw values as 4-bits (max : 15)\n+\tif maxSymbolValue > (256 - 128) {\n+\t\t// should not happen : likely means source cannot be compressed\n+\t\treturn ErrIncompressible\n+\t}\n+\top := s.Out\n+\t// special case, pack weights 4 bits/weight.\n+\top = append(op, 128|(maxSymbolValue-1))\n+\t// be sure it doesn't cause msan issue in final combination\n+\thuffWeight[maxSymbolValue] = 0\n+\tfor n := uint16(0); n < uint16(maxSymbolValue); n += 2 {\n+\t\top = append(op, (huffWeight[n]<<4)|huffWeight[n+1])\n+\t}\n+\ts.Out = op\n+\treturn nil\n+}\n+\n+// estimateSize returns the estimated size in bytes of the input represented in the\n+// histogram supplied.\n+func (c cTable) estimateSize(hist []uint32) int {\n+\tnbBits := uint32(7)\n+\tfor i, v := range c[:len(hist)] {\n+\t\tnbBits += uint32(v.nBits) * hist[i]\n+\t}\n+\treturn int(nbBits >> 3)\n+}\n+\n+// minSize returns the minimum possible size considering the shannon limit.\n+func (s *Scratch) minSize(total int) int {\n+\tnbBits := float64(7)\n+\tfTotal := float64(total)\n+\tfor _, v := range s.count[:s.symbolLen] {\n+\t\tn := float64(v)\n+\t\tif n > 0 {\n+\t\t\tnbBits += math.Log2(fTotal/n) * n\n+\t\t}\n+\t}\n+\treturn int(nbBits) >> 3\n+}\n+\n+func highBit32(val uint32) (n uint32) {\n+\treturn uint32(bits.Len32(val) - 1)\n+}"
    },
    {
      "sha": "042091d9b3b0d93b7070e05e11a35b4131c826f7",
      "filename": "backend/vendor/github.com/klauspost/compress/snappy/.gitignore",
      "status": "added",
      "additions": 16,
      "deletions": 0,
      "changes": 16,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/klauspost/compress/snappy/.gitignore",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/klauspost/compress/snappy/.gitignore",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/github.com/klauspost/compress/snappy/.gitignore?ref=890d7154e777d0673defcd11db3ef792473d68c9",
      "patch": "@@ -0,0 +1,16 @@\n+cmd/snappytool/snappytool\n+testdata/bench\n+\n+# These explicitly listed benchmark data files are for an obsolete version of\n+# snappy_test.go.\n+testdata/alice29.txt\n+testdata/asyoulik.txt\n+testdata/fireworks.jpeg\n+testdata/geo.protodata\n+testdata/html\n+testdata/html_x_4\n+testdata/kppkn.gtb\n+testdata/lcet10.txt\n+testdata/paper-100k.pdf\n+testdata/plrabn12.txt\n+testdata/urls.10K"
    },
    {
      "sha": "bcfa19520af9be47bf00b12b35e3e65d2435428c",
      "filename": "backend/vendor/github.com/klauspost/compress/snappy/AUTHORS",
      "status": "added",
      "additions": 15,
      "deletions": 0,
      "changes": 15,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/klauspost/compress/snappy/AUTHORS",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/klauspost/compress/snappy/AUTHORS",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/github.com/klauspost/compress/snappy/AUTHORS?ref=890d7154e777d0673defcd11db3ef792473d68c9",
      "patch": "@@ -0,0 +1,15 @@\n+# This is the official list of Snappy-Go authors for copyright purposes.\n+# This file is distinct from the CONTRIBUTORS files.\n+# See the latter for an explanation.\n+\n+# Names should be added to this file as\n+#\tName or Organization <email address>\n+# The email address is not required for organizations.\n+\n+# Please keep the list sorted.\n+\n+Damian Gryski <dgryski@gmail.com>\n+Google Inc.\n+Jan Mercl <0xjnml@gmail.com>\n+Rodolfo Carvalho <rhcarvalho@gmail.com>\n+Sebastien Binet <seb.binet@gmail.com>"
    },
    {
      "sha": "931ae31606f8c09ea5487f6ac4b419d7844ce25e",
      "filename": "backend/vendor/github.com/klauspost/compress/snappy/CONTRIBUTORS",
      "status": "added",
      "additions": 37,
      "deletions": 0,
      "changes": 37,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/klauspost/compress/snappy/CONTRIBUTORS",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/klauspost/compress/snappy/CONTRIBUTORS",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/github.com/klauspost/compress/snappy/CONTRIBUTORS?ref=890d7154e777d0673defcd11db3ef792473d68c9",
      "patch": "@@ -0,0 +1,37 @@\n+# This is the official list of people who can contribute\n+# (and typically have contributed) code to the Snappy-Go repository.\n+# The AUTHORS file lists the copyright holders; this file\n+# lists people.  For example, Google employees are listed here\n+# but not in AUTHORS, because Google holds the copyright.\n+#\n+# The submission process automatically checks to make sure\n+# that people submitting code are listed in this file (by email address).\n+#\n+# Names should be added to this file only after verifying that\n+# the individual or the individual's organization has agreed to\n+# the appropriate Contributor License Agreement, found here:\n+#\n+#     http://code.google.com/legal/individual-cla-v1.0.html\n+#     http://code.google.com/legal/corporate-cla-v1.0.html\n+#\n+# The agreement for individuals can be filled out on the web.\n+#\n+# When adding J Random Contributor's name to this file,\n+# either J's name or J's organization's name should be\n+# added to the AUTHORS file, depending on whether the\n+# individual or corporate CLA was used.\n+\n+# Names should be added to this file like so:\n+#     Name <email address>\n+\n+# Please keep the list sorted.\n+\n+Damian Gryski <dgryski@gmail.com>\n+Jan Mercl <0xjnml@gmail.com>\n+Kai Backman <kaib@golang.org>\n+Marc-Antoine Ruel <maruel@chromium.org>\n+Nigel Tao <nigeltao@golang.org>\n+Rob Pike <r@golang.org>\n+Rodolfo Carvalho <rhcarvalho@gmail.com>\n+Russ Cox <rsc@golang.org>\n+Sebastien Binet <seb.binet@gmail.com>"
    },
    {
      "sha": "6050c10f4c8b4c22f50c83715f44f12419f763be",
      "filename": "backend/vendor/github.com/klauspost/compress/snappy/LICENSE",
      "status": "added",
      "additions": 27,
      "deletions": 0,
      "changes": 27,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/klauspost/compress/snappy/LICENSE",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/klauspost/compress/snappy/LICENSE",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/github.com/klauspost/compress/snappy/LICENSE?ref=890d7154e777d0673defcd11db3ef792473d68c9",
      "patch": "@@ -0,0 +1,27 @@\n+Copyright (c) 2011 The Snappy-Go Authors. All rights reserved.\n+\n+Redistribution and use in source and binary forms, with or without\n+modification, are permitted provided that the following conditions are\n+met:\n+\n+   * Redistributions of source code must retain the above copyright\n+notice, this list of conditions and the following disclaimer.\n+   * Redistributions in binary form must reproduce the above\n+copyright notice, this list of conditions and the following disclaimer\n+in the documentation and/or other materials provided with the\n+distribution.\n+   * Neither the name of Google Inc. nor the names of its\n+contributors may be used to endorse or promote products derived from\n+this software without specific prior written permission.\n+\n+THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n+\"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\n+LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\n+A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\n+OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\n+SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\n+LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\n+DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\n+THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n+(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n+OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE."
    },
    {
      "sha": "cea12879a0eae937f6ecdb6243f64591c5217fef",
      "filename": "backend/vendor/github.com/klauspost/compress/snappy/README",
      "status": "added",
      "additions": 107,
      "deletions": 0,
      "changes": 107,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/klauspost/compress/snappy/README",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/klauspost/compress/snappy/README",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/github.com/klauspost/compress/snappy/README?ref=890d7154e777d0673defcd11db3ef792473d68c9",
      "patch": "@@ -0,0 +1,107 @@\n+The Snappy compression format in the Go programming language.\n+\n+To download and install from source:\n+$ go get github.com/golang/snappy\n+\n+Unless otherwise noted, the Snappy-Go source files are distributed\n+under the BSD-style license found in the LICENSE file.\n+\n+\n+\n+Benchmarks.\n+\n+The golang/snappy benchmarks include compressing (Z) and decompressing (U) ten\n+or so files, the same set used by the C++ Snappy code (github.com/google/snappy\n+and note the \"google\", not \"golang\"). On an \"Intel(R) Core(TM) i7-3770 CPU @\n+3.40GHz\", Go's GOARCH=amd64 numbers as of 2016-05-29:\n+\n+\"go test -test.bench=.\"\n+\n+_UFlat0-8         2.19GB/s ± 0%  html\n+_UFlat1-8         1.41GB/s ± 0%  urls\n+_UFlat2-8         23.5GB/s ± 2%  jpg\n+_UFlat3-8         1.91GB/s ± 0%  jpg_200\n+_UFlat4-8         14.0GB/s ± 1%  pdf\n+_UFlat5-8         1.97GB/s ± 0%  html4\n+_UFlat6-8          814MB/s ± 0%  txt1\n+_UFlat7-8          785MB/s ± 0%  txt2\n+_UFlat8-8          857MB/s ± 0%  txt3\n+_UFlat9-8          719MB/s ± 1%  txt4\n+_UFlat10-8        2.84GB/s ± 0%  pb\n+_UFlat11-8        1.05GB/s ± 0%  gaviota\n+\n+_ZFlat0-8         1.04GB/s ± 0%  html\n+_ZFlat1-8          534MB/s ± 0%  urls\n+_ZFlat2-8         15.7GB/s ± 1%  jpg\n+_ZFlat3-8          740MB/s ± 3%  jpg_200\n+_ZFlat4-8         9.20GB/s ± 1%  pdf\n+_ZFlat5-8          991MB/s ± 0%  html4\n+_ZFlat6-8          379MB/s ± 0%  txt1\n+_ZFlat7-8          352MB/s ± 0%  txt2\n+_ZFlat8-8          396MB/s ± 1%  txt3\n+_ZFlat9-8          327MB/s ± 1%  txt4\n+_ZFlat10-8        1.33GB/s ± 1%  pb\n+_ZFlat11-8         605MB/s ± 1%  gaviota\n+\n+\n+\n+\"go test -test.bench=. -tags=noasm\"\n+\n+_UFlat0-8          621MB/s ± 2%  html\n+_UFlat1-8          494MB/s ± 1%  urls\n+_UFlat2-8         23.2GB/s ± 1%  jpg\n+_UFlat3-8         1.12GB/s ± 1%  jpg_200\n+_UFlat4-8         4.35GB/s ± 1%  pdf\n+_UFlat5-8          609MB/s ± 0%  html4\n+_UFlat6-8          296MB/s ± 0%  txt1\n+_UFlat7-8          288MB/s ± 0%  txt2\n+_UFlat8-8          309MB/s ± 1%  txt3\n+_UFlat9-8          280MB/s ± 1%  txt4\n+_UFlat10-8         753MB/s ± 0%  pb\n+_UFlat11-8         400MB/s ± 0%  gaviota\n+\n+_ZFlat0-8          409MB/s ± 1%  html\n+_ZFlat1-8          250MB/s ± 1%  urls\n+_ZFlat2-8         12.3GB/s ± 1%  jpg\n+_ZFlat3-8          132MB/s ± 0%  jpg_200\n+_ZFlat4-8         2.92GB/s ± 0%  pdf\n+_ZFlat5-8          405MB/s ± 1%  html4\n+_ZFlat6-8          179MB/s ± 1%  txt1\n+_ZFlat7-8          170MB/s ± 1%  txt2\n+_ZFlat8-8          189MB/s ± 1%  txt3\n+_ZFlat9-8          164MB/s ± 1%  txt4\n+_ZFlat10-8         479MB/s ± 1%  pb\n+_ZFlat11-8         270MB/s ± 1%  gaviota\n+\n+\n+\n+For comparison (Go's encoded output is byte-for-byte identical to C++'s), here\n+are the numbers from C++ Snappy's\n+\n+make CXXFLAGS=\"-O2 -DNDEBUG -g\" clean snappy_unittest.log && cat snappy_unittest.log\n+\n+BM_UFlat/0     2.4GB/s  html\n+BM_UFlat/1     1.4GB/s  urls\n+BM_UFlat/2    21.8GB/s  jpg\n+BM_UFlat/3     1.5GB/s  jpg_200\n+BM_UFlat/4    13.3GB/s  pdf\n+BM_UFlat/5     2.1GB/s  html4\n+BM_UFlat/6     1.0GB/s  txt1\n+BM_UFlat/7   959.4MB/s  txt2\n+BM_UFlat/8     1.0GB/s  txt3\n+BM_UFlat/9   864.5MB/s  txt4\n+BM_UFlat/10    2.9GB/s  pb\n+BM_UFlat/11    1.2GB/s  gaviota\n+\n+BM_ZFlat/0   944.3MB/s  html (22.31 %)\n+BM_ZFlat/1   501.6MB/s  urls (47.78 %)\n+BM_ZFlat/2    14.3GB/s  jpg (99.95 %)\n+BM_ZFlat/3   538.3MB/s  jpg_200 (73.00 %)\n+BM_ZFlat/4     8.3GB/s  pdf (83.30 %)\n+BM_ZFlat/5   903.5MB/s  html4 (22.52 %)\n+BM_ZFlat/6   336.0MB/s  txt1 (57.88 %)\n+BM_ZFlat/7   312.3MB/s  txt2 (61.91 %)\n+BM_ZFlat/8   353.1MB/s  txt3 (54.99 %)\n+BM_ZFlat/9   289.9MB/s  txt4 (66.26 %)\n+BM_ZFlat/10    1.2GB/s  pb (19.68 %)\n+BM_ZFlat/11  527.4MB/s  gaviota (37.72 %)"
    },
    {
      "sha": "72efb0353ddfc02dc509b67b1332c1d3595ccb6a",
      "filename": "backend/vendor/github.com/klauspost/compress/snappy/decode.go",
      "status": "added",
      "additions": 237,
      "deletions": 0,
      "changes": 237,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/klauspost/compress/snappy/decode.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/klauspost/compress/snappy/decode.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/github.com/klauspost/compress/snappy/decode.go?ref=890d7154e777d0673defcd11db3ef792473d68c9",
      "patch": "@@ -0,0 +1,237 @@\n+// Copyright 2011 The Snappy-Go Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style\n+// license that can be found in the LICENSE file.\n+\n+package snappy\n+\n+import (\n+\t\"encoding/binary\"\n+\t\"errors\"\n+\t\"io\"\n+)\n+\n+var (\n+\t// ErrCorrupt reports that the input is invalid.\n+\tErrCorrupt = errors.New(\"snappy: corrupt input\")\n+\t// ErrTooLarge reports that the uncompressed length is too large.\n+\tErrTooLarge = errors.New(\"snappy: decoded block is too large\")\n+\t// ErrUnsupported reports that the input isn't supported.\n+\tErrUnsupported = errors.New(\"snappy: unsupported input\")\n+\n+\terrUnsupportedLiteralLength = errors.New(\"snappy: unsupported literal length\")\n+)\n+\n+// DecodedLen returns the length of the decoded block.\n+func DecodedLen(src []byte) (int, error) {\n+\tv, _, err := decodedLen(src)\n+\treturn v, err\n+}\n+\n+// decodedLen returns the length of the decoded block and the number of bytes\n+// that the length header occupied.\n+func decodedLen(src []byte) (blockLen, headerLen int, err error) {\n+\tv, n := binary.Uvarint(src)\n+\tif n <= 0 || v > 0xffffffff {\n+\t\treturn 0, 0, ErrCorrupt\n+\t}\n+\n+\tconst wordSize = 32 << (^uint(0) >> 32 & 1)\n+\tif wordSize == 32 && v > 0x7fffffff {\n+\t\treturn 0, 0, ErrTooLarge\n+\t}\n+\treturn int(v), n, nil\n+}\n+\n+const (\n+\tdecodeErrCodeCorrupt                  = 1\n+\tdecodeErrCodeUnsupportedLiteralLength = 2\n+)\n+\n+// Decode returns the decoded form of src. The returned slice may be a sub-\n+// slice of dst if dst was large enough to hold the entire decoded block.\n+// Otherwise, a newly allocated slice will be returned.\n+//\n+// The dst and src must not overlap. It is valid to pass a nil dst.\n+func Decode(dst, src []byte) ([]byte, error) {\n+\tdLen, s, err := decodedLen(src)\n+\tif err != nil {\n+\t\treturn nil, err\n+\t}\n+\tif dLen <= len(dst) {\n+\t\tdst = dst[:dLen]\n+\t} else {\n+\t\tdst = make([]byte, dLen)\n+\t}\n+\tswitch decode(dst, src[s:]) {\n+\tcase 0:\n+\t\treturn dst, nil\n+\tcase decodeErrCodeUnsupportedLiteralLength:\n+\t\treturn nil, errUnsupportedLiteralLength\n+\t}\n+\treturn nil, ErrCorrupt\n+}\n+\n+// NewReader returns a new Reader that decompresses from r, using the framing\n+// format described at\n+// https://github.com/google/snappy/blob/master/framing_format.txt\n+func NewReader(r io.Reader) *Reader {\n+\treturn &Reader{\n+\t\tr:       r,\n+\t\tdecoded: make([]byte, maxBlockSize),\n+\t\tbuf:     make([]byte, maxEncodedLenOfMaxBlockSize+checksumSize),\n+\t}\n+}\n+\n+// Reader is an io.Reader that can read Snappy-compressed bytes.\n+type Reader struct {\n+\tr       io.Reader\n+\terr     error\n+\tdecoded []byte\n+\tbuf     []byte\n+\t// decoded[i:j] contains decoded bytes that have not yet been passed on.\n+\ti, j       int\n+\treadHeader bool\n+}\n+\n+// Reset discards any buffered data, resets all state, and switches the Snappy\n+// reader to read from r. This permits reusing a Reader rather than allocating\n+// a new one.\n+func (r *Reader) Reset(reader io.Reader) {\n+\tr.r = reader\n+\tr.err = nil\n+\tr.i = 0\n+\tr.j = 0\n+\tr.readHeader = false\n+}\n+\n+func (r *Reader) readFull(p []byte, allowEOF bool) (ok bool) {\n+\tif _, r.err = io.ReadFull(r.r, p); r.err != nil {\n+\t\tif r.err == io.ErrUnexpectedEOF || (r.err == io.EOF && !allowEOF) {\n+\t\t\tr.err = ErrCorrupt\n+\t\t}\n+\t\treturn false\n+\t}\n+\treturn true\n+}\n+\n+// Read satisfies the io.Reader interface.\n+func (r *Reader) Read(p []byte) (int, error) {\n+\tif r.err != nil {\n+\t\treturn 0, r.err\n+\t}\n+\tfor {\n+\t\tif r.i < r.j {\n+\t\t\tn := copy(p, r.decoded[r.i:r.j])\n+\t\t\tr.i += n\n+\t\t\treturn n, nil\n+\t\t}\n+\t\tif !r.readFull(r.buf[:4], true) {\n+\t\t\treturn 0, r.err\n+\t\t}\n+\t\tchunkType := r.buf[0]\n+\t\tif !r.readHeader {\n+\t\t\tif chunkType != chunkTypeStreamIdentifier {\n+\t\t\t\tr.err = ErrCorrupt\n+\t\t\t\treturn 0, r.err\n+\t\t\t}\n+\t\t\tr.readHeader = true\n+\t\t}\n+\t\tchunkLen := int(r.buf[1]) | int(r.buf[2])<<8 | int(r.buf[3])<<16\n+\t\tif chunkLen > len(r.buf) {\n+\t\t\tr.err = ErrUnsupported\n+\t\t\treturn 0, r.err\n+\t\t}\n+\n+\t\t// The chunk types are specified at\n+\t\t// https://github.com/google/snappy/blob/master/framing_format.txt\n+\t\tswitch chunkType {\n+\t\tcase chunkTypeCompressedData:\n+\t\t\t// Section 4.2. Compressed data (chunk type 0x00).\n+\t\t\tif chunkLen < checksumSize {\n+\t\t\t\tr.err = ErrCorrupt\n+\t\t\t\treturn 0, r.err\n+\t\t\t}\n+\t\t\tbuf := r.buf[:chunkLen]\n+\t\t\tif !r.readFull(buf, false) {\n+\t\t\t\treturn 0, r.err\n+\t\t\t}\n+\t\t\tchecksum := uint32(buf[0]) | uint32(buf[1])<<8 | uint32(buf[2])<<16 | uint32(buf[3])<<24\n+\t\t\tbuf = buf[checksumSize:]\n+\n+\t\t\tn, err := DecodedLen(buf)\n+\t\t\tif err != nil {\n+\t\t\t\tr.err = err\n+\t\t\t\treturn 0, r.err\n+\t\t\t}\n+\t\t\tif n > len(r.decoded) {\n+\t\t\t\tr.err = ErrCorrupt\n+\t\t\t\treturn 0, r.err\n+\t\t\t}\n+\t\t\tif _, err := Decode(r.decoded, buf); err != nil {\n+\t\t\t\tr.err = err\n+\t\t\t\treturn 0, r.err\n+\t\t\t}\n+\t\t\tif crc(r.decoded[:n]) != checksum {\n+\t\t\t\tr.err = ErrCorrupt\n+\t\t\t\treturn 0, r.err\n+\t\t\t}\n+\t\t\tr.i, r.j = 0, n\n+\t\t\tcontinue\n+\n+\t\tcase chunkTypeUncompressedData:\n+\t\t\t// Section 4.3. Uncompressed data (chunk type 0x01).\n+\t\t\tif chunkLen < checksumSize {\n+\t\t\t\tr.err = ErrCorrupt\n+\t\t\t\treturn 0, r.err\n+\t\t\t}\n+\t\t\tbuf := r.buf[:checksumSize]\n+\t\t\tif !r.readFull(buf, false) {\n+\t\t\t\treturn 0, r.err\n+\t\t\t}\n+\t\t\tchecksum := uint32(buf[0]) | uint32(buf[1])<<8 | uint32(buf[2])<<16 | uint32(buf[3])<<24\n+\t\t\t// Read directly into r.decoded instead of via r.buf.\n+\t\t\tn := chunkLen - checksumSize\n+\t\t\tif n > len(r.decoded) {\n+\t\t\t\tr.err = ErrCorrupt\n+\t\t\t\treturn 0, r.err\n+\t\t\t}\n+\t\t\tif !r.readFull(r.decoded[:n], false) {\n+\t\t\t\treturn 0, r.err\n+\t\t\t}\n+\t\t\tif crc(r.decoded[:n]) != checksum {\n+\t\t\t\tr.err = ErrCorrupt\n+\t\t\t\treturn 0, r.err\n+\t\t\t}\n+\t\t\tr.i, r.j = 0, n\n+\t\t\tcontinue\n+\n+\t\tcase chunkTypeStreamIdentifier:\n+\t\t\t// Section 4.1. Stream identifier (chunk type 0xff).\n+\t\t\tif chunkLen != len(magicBody) {\n+\t\t\t\tr.err = ErrCorrupt\n+\t\t\t\treturn 0, r.err\n+\t\t\t}\n+\t\t\tif !r.readFull(r.buf[:len(magicBody)], false) {\n+\t\t\t\treturn 0, r.err\n+\t\t\t}\n+\t\t\tfor i := 0; i < len(magicBody); i++ {\n+\t\t\t\tif r.buf[i] != magicBody[i] {\n+\t\t\t\t\tr.err = ErrCorrupt\n+\t\t\t\t\treturn 0, r.err\n+\t\t\t\t}\n+\t\t\t}\n+\t\t\tcontinue\n+\t\t}\n+\n+\t\tif chunkType <= 0x7f {\n+\t\t\t// Section 4.5. Reserved unskippable chunks (chunk types 0x02-0x7f).\n+\t\t\tr.err = ErrUnsupported\n+\t\t\treturn 0, r.err\n+\t\t}\n+\t\t// Section 4.4 Padding (chunk type 0xfe).\n+\t\t// Section 4.6. Reserved skippable chunks (chunk types 0x80-0xfd).\n+\t\tif !r.readFull(r.buf[:chunkLen], false) {\n+\t\t\treturn 0, r.err\n+\t\t}\n+\t}\n+}"
    },
    {
      "sha": "fcd192b849eda1c391d5460b24ce4e263f3a6b90",
      "filename": "backend/vendor/github.com/klauspost/compress/snappy/decode_amd64.go",
      "status": "added",
      "additions": 14,
      "deletions": 0,
      "changes": 14,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/klauspost/compress/snappy/decode_amd64.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/klauspost/compress/snappy/decode_amd64.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/github.com/klauspost/compress/snappy/decode_amd64.go?ref=890d7154e777d0673defcd11db3ef792473d68c9",
      "patch": "@@ -0,0 +1,14 @@\n+// Copyright 2016 The Snappy-Go Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style\n+// license that can be found in the LICENSE file.\n+\n+// +build !appengine\n+// +build gc\n+// +build !noasm\n+\n+package snappy\n+\n+// decode has the same semantics as in decode_other.go.\n+//\n+//go:noescape\n+func decode(dst, src []byte) int"
    },
    {
      "sha": "1c66e37234dc73a4d1fb07ea9974acd8fc734326",
      "filename": "backend/vendor/github.com/klauspost/compress/snappy/decode_amd64.s",
      "status": "added",
      "additions": 482,
      "deletions": 0,
      "changes": 482,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/klauspost/compress/snappy/decode_amd64.s",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/klauspost/compress/snappy/decode_amd64.s",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/github.com/klauspost/compress/snappy/decode_amd64.s?ref=890d7154e777d0673defcd11db3ef792473d68c9",
      "patch": "@@ -0,0 +1,482 @@\n+// Copyright 2016 The Go Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style\n+// license that can be found in the LICENSE file.\n+\n+// +build !appengine\n+// +build gc\n+// +build !noasm\n+\n+#include \"textflag.h\"\n+\n+// The asm code generally follows the pure Go code in decode_other.go, except\n+// where marked with a \"!!!\".\n+\n+// func decode(dst, src []byte) int\n+//\n+// All local variables fit into registers. The non-zero stack size is only to\n+// spill registers and push args when issuing a CALL. The register allocation:\n+//\t- AX\tscratch\n+//\t- BX\tscratch\n+//\t- CX\tlength or x\n+//\t- DX\toffset\n+//\t- SI\t&src[s]\n+//\t- DI\t&dst[d]\n+//\t+ R8\tdst_base\n+//\t+ R9\tdst_len\n+//\t+ R10\tdst_base + dst_len\n+//\t+ R11\tsrc_base\n+//\t+ R12\tsrc_len\n+//\t+ R13\tsrc_base + src_len\n+//\t- R14\tused by doCopy\n+//\t- R15\tused by doCopy\n+//\n+// The registers R8-R13 (marked with a \"+\") are set at the start of the\n+// function, and after a CALL returns, and are not otherwise modified.\n+//\n+// The d variable is implicitly DI - R8,  and len(dst)-d is R10 - DI.\n+// The s variable is implicitly SI - R11, and len(src)-s is R13 - SI.\n+TEXT ·decode(SB), NOSPLIT, $48-56\n+\t// Initialize SI, DI and R8-R13.\n+\tMOVQ dst_base+0(FP), R8\n+\tMOVQ dst_len+8(FP), R9\n+\tMOVQ R8, DI\n+\tMOVQ R8, R10\n+\tADDQ R9, R10\n+\tMOVQ src_base+24(FP), R11\n+\tMOVQ src_len+32(FP), R12\n+\tMOVQ R11, SI\n+\tMOVQ R11, R13\n+\tADDQ R12, R13\n+\n+loop:\n+\t// for s < len(src)\n+\tCMPQ SI, R13\n+\tJEQ  end\n+\n+\t// CX = uint32(src[s])\n+\t//\n+\t// switch src[s] & 0x03\n+\tMOVBLZX (SI), CX\n+\tMOVL    CX, BX\n+\tANDL    $3, BX\n+\tCMPL    BX, $1\n+\tJAE     tagCopy\n+\n+\t// ----------------------------------------\n+\t// The code below handles literal tags.\n+\n+\t// case tagLiteral:\n+\t// x := uint32(src[s] >> 2)\n+\t// switch\n+\tSHRL $2, CX\n+\tCMPL CX, $60\n+\tJAE  tagLit60Plus\n+\n+\t// case x < 60:\n+\t// s++\n+\tINCQ SI\n+\n+doLit:\n+\t// This is the end of the inner \"switch\", when we have a literal tag.\n+\t//\n+\t// We assume that CX == x and x fits in a uint32, where x is the variable\n+\t// used in the pure Go decode_other.go code.\n+\n+\t// length = int(x) + 1\n+\t//\n+\t// Unlike the pure Go code, we don't need to check if length <= 0 because\n+\t// CX can hold 64 bits, so the increment cannot overflow.\n+\tINCQ CX\n+\n+\t// Prepare to check if copying length bytes will run past the end of dst or\n+\t// src.\n+\t//\n+\t// AX = len(dst) - d\n+\t// BX = len(src) - s\n+\tMOVQ R10, AX\n+\tSUBQ DI, AX\n+\tMOVQ R13, BX\n+\tSUBQ SI, BX\n+\n+\t// !!! Try a faster technique for short (16 or fewer bytes) copies.\n+\t//\n+\t// if length > 16 || len(dst)-d < 16 || len(src)-s < 16 {\n+\t//   goto callMemmove // Fall back on calling runtime·memmove.\n+\t// }\n+\t//\n+\t// The C++ snappy code calls this TryFastAppend. It also checks len(src)-s\n+\t// against 21 instead of 16, because it cannot assume that all of its input\n+\t// is contiguous in memory and so it needs to leave enough source bytes to\n+\t// read the next tag without refilling buffers, but Go's Decode assumes\n+\t// contiguousness (the src argument is a []byte).\n+\tCMPQ CX, $16\n+\tJGT  callMemmove\n+\tCMPQ AX, $16\n+\tJLT  callMemmove\n+\tCMPQ BX, $16\n+\tJLT  callMemmove\n+\n+\t// !!! Implement the copy from src to dst as a 16-byte load and store.\n+\t// (Decode's documentation says that dst and src must not overlap.)\n+\t//\n+\t// This always copies 16 bytes, instead of only length bytes, but that's\n+\t// OK. If the input is a valid Snappy encoding then subsequent iterations\n+\t// will fix up the overrun. Otherwise, Decode returns a nil []byte (and a\n+\t// non-nil error), so the overrun will be ignored.\n+\t//\n+\t// Note that on amd64, it is legal and cheap to issue unaligned 8-byte or\n+\t// 16-byte loads and stores. This technique probably wouldn't be as\n+\t// effective on architectures that are fussier about alignment.\n+\tMOVOU 0(SI), X0\n+\tMOVOU X0, 0(DI)\n+\n+\t// d += length\n+\t// s += length\n+\tADDQ CX, DI\n+\tADDQ CX, SI\n+\tJMP  loop\n+\n+callMemmove:\n+\t// if length > len(dst)-d || length > len(src)-s { etc }\n+\tCMPQ CX, AX\n+\tJGT  errCorrupt\n+\tCMPQ CX, BX\n+\tJGT  errCorrupt\n+\n+\t// copy(dst[d:], src[s:s+length])\n+\t//\n+\t// This means calling runtime·memmove(&dst[d], &src[s], length), so we push\n+\t// DI, SI and CX as arguments. Coincidentally, we also need to spill those\n+\t// three registers to the stack, to save local variables across the CALL.\n+\tMOVQ DI, 0(SP)\n+\tMOVQ SI, 8(SP)\n+\tMOVQ CX, 16(SP)\n+\tMOVQ DI, 24(SP)\n+\tMOVQ SI, 32(SP)\n+\tMOVQ CX, 40(SP)\n+\tCALL runtime·memmove(SB)\n+\n+\t// Restore local variables: unspill registers from the stack and\n+\t// re-calculate R8-R13.\n+\tMOVQ 24(SP), DI\n+\tMOVQ 32(SP), SI\n+\tMOVQ 40(SP), CX\n+\tMOVQ dst_base+0(FP), R8\n+\tMOVQ dst_len+8(FP), R9\n+\tMOVQ R8, R10\n+\tADDQ R9, R10\n+\tMOVQ src_base+24(FP), R11\n+\tMOVQ src_len+32(FP), R12\n+\tMOVQ R11, R13\n+\tADDQ R12, R13\n+\n+\t// d += length\n+\t// s += length\n+\tADDQ CX, DI\n+\tADDQ CX, SI\n+\tJMP  loop\n+\n+tagLit60Plus:\n+\t// !!! This fragment does the\n+\t//\n+\t// s += x - 58; if uint(s) > uint(len(src)) { etc }\n+\t//\n+\t// checks. In the asm version, we code it once instead of once per switch case.\n+\tADDQ CX, SI\n+\tSUBQ $58, SI\n+\tCMPQ SI, R13\n+\tJA   errCorrupt\n+\n+\t// case x == 60:\n+\tCMPL CX, $61\n+\tJEQ  tagLit61\n+\tJA   tagLit62Plus\n+\n+\t// x = uint32(src[s-1])\n+\tMOVBLZX -1(SI), CX\n+\tJMP     doLit\n+\n+tagLit61:\n+\t// case x == 61:\n+\t// x = uint32(src[s-2]) | uint32(src[s-1])<<8\n+\tMOVWLZX -2(SI), CX\n+\tJMP     doLit\n+\n+tagLit62Plus:\n+\tCMPL CX, $62\n+\tJA   tagLit63\n+\n+\t// case x == 62:\n+\t// x = uint32(src[s-3]) | uint32(src[s-2])<<8 | uint32(src[s-1])<<16\n+\tMOVWLZX -3(SI), CX\n+\tMOVBLZX -1(SI), BX\n+\tSHLL    $16, BX\n+\tORL     BX, CX\n+\tJMP     doLit\n+\n+tagLit63:\n+\t// case x == 63:\n+\t// x = uint32(src[s-4]) | uint32(src[s-3])<<8 | uint32(src[s-2])<<16 | uint32(src[s-1])<<24\n+\tMOVL -4(SI), CX\n+\tJMP  doLit\n+\n+// The code above handles literal tags.\n+// ----------------------------------------\n+// The code below handles copy tags.\n+\n+tagCopy4:\n+\t// case tagCopy4:\n+\t// s += 5\n+\tADDQ $5, SI\n+\n+\t// if uint(s) > uint(len(src)) { etc }\n+\tCMPQ SI, R13\n+\tJA   errCorrupt\n+\n+\t// length = 1 + int(src[s-5])>>2\n+\tSHRQ $2, CX\n+\tINCQ CX\n+\n+\t// offset = int(uint32(src[s-4]) | uint32(src[s-3])<<8 | uint32(src[s-2])<<16 | uint32(src[s-1])<<24)\n+\tMOVLQZX -4(SI), DX\n+\tJMP     doCopy\n+\n+tagCopy2:\n+\t// case tagCopy2:\n+\t// s += 3\n+\tADDQ $3, SI\n+\n+\t// if uint(s) > uint(len(src)) { etc }\n+\tCMPQ SI, R13\n+\tJA   errCorrupt\n+\n+\t// length = 1 + int(src[s-3])>>2\n+\tSHRQ $2, CX\n+\tINCQ CX\n+\n+\t// offset = int(uint32(src[s-2]) | uint32(src[s-1])<<8)\n+\tMOVWQZX -2(SI), DX\n+\tJMP     doCopy\n+\n+tagCopy:\n+\t// We have a copy tag. We assume that:\n+\t//\t- BX == src[s] & 0x03\n+\t//\t- CX == src[s]\n+\tCMPQ BX, $2\n+\tJEQ  tagCopy2\n+\tJA   tagCopy4\n+\n+\t// case tagCopy1:\n+\t// s += 2\n+\tADDQ $2, SI\n+\n+\t// if uint(s) > uint(len(src)) { etc }\n+\tCMPQ SI, R13\n+\tJA   errCorrupt\n+\n+\t// offset = int(uint32(src[s-2])&0xe0<<3 | uint32(src[s-1]))\n+\tMOVQ    CX, DX\n+\tANDQ    $0xe0, DX\n+\tSHLQ    $3, DX\n+\tMOVBQZX -1(SI), BX\n+\tORQ     BX, DX\n+\n+\t// length = 4 + int(src[s-2])>>2&0x7\n+\tSHRQ $2, CX\n+\tANDQ $7, CX\n+\tADDQ $4, CX\n+\n+doCopy:\n+\t// This is the end of the outer \"switch\", when we have a copy tag.\n+\t//\n+\t// We assume that:\n+\t//\t- CX == length && CX > 0\n+\t//\t- DX == offset\n+\n+\t// if offset <= 0 { etc }\n+\tCMPQ DX, $0\n+\tJLE  errCorrupt\n+\n+\t// if d < offset { etc }\n+\tMOVQ DI, BX\n+\tSUBQ R8, BX\n+\tCMPQ BX, DX\n+\tJLT  errCorrupt\n+\n+\t// if length > len(dst)-d { etc }\n+\tMOVQ R10, BX\n+\tSUBQ DI, BX\n+\tCMPQ CX, BX\n+\tJGT  errCorrupt\n+\n+\t// forwardCopy(dst[d:d+length], dst[d-offset:]); d += length\n+\t//\n+\t// Set:\n+\t//\t- R14 = len(dst)-d\n+\t//\t- R15 = &dst[d-offset]\n+\tMOVQ R10, R14\n+\tSUBQ DI, R14\n+\tMOVQ DI, R15\n+\tSUBQ DX, R15\n+\n+\t// !!! Try a faster technique for short (16 or fewer bytes) forward copies.\n+\t//\n+\t// First, try using two 8-byte load/stores, similar to the doLit technique\n+\t// above. Even if dst[d:d+length] and dst[d-offset:] can overlap, this is\n+\t// still OK if offset >= 8. Note that this has to be two 8-byte load/stores\n+\t// and not one 16-byte load/store, and the first store has to be before the\n+\t// second load, due to the overlap if offset is in the range [8, 16).\n+\t//\n+\t// if length > 16 || offset < 8 || len(dst)-d < 16 {\n+\t//   goto slowForwardCopy\n+\t// }\n+\t// copy 16 bytes\n+\t// d += length\n+\tCMPQ CX, $16\n+\tJGT  slowForwardCopy\n+\tCMPQ DX, $8\n+\tJLT  slowForwardCopy\n+\tCMPQ R14, $16\n+\tJLT  slowForwardCopy\n+\tMOVQ 0(R15), AX\n+\tMOVQ AX, 0(DI)\n+\tMOVQ 8(R15), BX\n+\tMOVQ BX, 8(DI)\n+\tADDQ CX, DI\n+\tJMP  loop\n+\n+slowForwardCopy:\n+\t// !!! If the forward copy is longer than 16 bytes, or if offset < 8, we\n+\t// can still try 8-byte load stores, provided we can overrun up to 10 extra\n+\t// bytes. As above, the overrun will be fixed up by subsequent iterations\n+\t// of the outermost loop.\n+\t//\n+\t// The C++ snappy code calls this technique IncrementalCopyFastPath. Its\n+\t// commentary says:\n+\t//\n+\t// ----\n+\t//\n+\t// The main part of this loop is a simple copy of eight bytes at a time\n+\t// until we've copied (at least) the requested amount of bytes.  However,\n+\t// if d and d-offset are less than eight bytes apart (indicating a\n+\t// repeating pattern of length < 8), we first need to expand the pattern in\n+\t// order to get the correct results. For instance, if the buffer looks like\n+\t// this, with the eight-byte <d-offset> and <d> patterns marked as\n+\t// intervals:\n+\t//\n+\t//    abxxxxxxxxxxxx\n+\t//    [------]           d-offset\n+\t//      [------]         d\n+\t//\n+\t// a single eight-byte copy from <d-offset> to <d> will repeat the pattern\n+\t// once, after which we can move <d> two bytes without moving <d-offset>:\n+\t//\n+\t//    ababxxxxxxxxxx\n+\t//    [------]           d-offset\n+\t//        [------]       d\n+\t//\n+\t// and repeat the exercise until the two no longer overlap.\n+\t//\n+\t// This allows us to do very well in the special case of one single byte\n+\t// repeated many times, without taking a big hit for more general cases.\n+\t//\n+\t// The worst case of extra writing past the end of the match occurs when\n+\t// offset == 1 and length == 1; the last copy will read from byte positions\n+\t// [0..7] and write to [4..11], whereas it was only supposed to write to\n+\t// position 1. Thus, ten excess bytes.\n+\t//\n+\t// ----\n+\t//\n+\t// That \"10 byte overrun\" worst case is confirmed by Go's\n+\t// TestSlowForwardCopyOverrun, which also tests the fixUpSlowForwardCopy\n+\t// and finishSlowForwardCopy algorithm.\n+\t//\n+\t// if length > len(dst)-d-10 {\n+\t//   goto verySlowForwardCopy\n+\t// }\n+\tSUBQ $10, R14\n+\tCMPQ CX, R14\n+\tJGT  verySlowForwardCopy\n+\n+makeOffsetAtLeast8:\n+\t// !!! As above, expand the pattern so that offset >= 8 and we can use\n+\t// 8-byte load/stores.\n+\t//\n+\t// for offset < 8 {\n+\t//   copy 8 bytes from dst[d-offset:] to dst[d:]\n+\t//   length -= offset\n+\t//   d      += offset\n+\t//   offset += offset\n+\t//   // The two previous lines together means that d-offset, and therefore\n+\t//   // R15, is unchanged.\n+\t// }\n+\tCMPQ DX, $8\n+\tJGE  fixUpSlowForwardCopy\n+\tMOVQ (R15), BX\n+\tMOVQ BX, (DI)\n+\tSUBQ DX, CX\n+\tADDQ DX, DI\n+\tADDQ DX, DX\n+\tJMP  makeOffsetAtLeast8\n+\n+fixUpSlowForwardCopy:\n+\t// !!! Add length (which might be negative now) to d (implied by DI being\n+\t// &dst[d]) so that d ends up at the right place when we jump back to the\n+\t// top of the loop. Before we do that, though, we save DI to AX so that, if\n+\t// length is positive, copying the remaining length bytes will write to the\n+\t// right place.\n+\tMOVQ DI, AX\n+\tADDQ CX, DI\n+\n+finishSlowForwardCopy:\n+\t// !!! Repeat 8-byte load/stores until length <= 0. Ending with a negative\n+\t// length means that we overrun, but as above, that will be fixed up by\n+\t// subsequent iterations of the outermost loop.\n+\tCMPQ CX, $0\n+\tJLE  loop\n+\tMOVQ (R15), BX\n+\tMOVQ BX, (AX)\n+\tADDQ $8, R15\n+\tADDQ $8, AX\n+\tSUBQ $8, CX\n+\tJMP  finishSlowForwardCopy\n+\n+verySlowForwardCopy:\n+\t// verySlowForwardCopy is a simple implementation of forward copy. In C\n+\t// parlance, this is a do/while loop instead of a while loop, since we know\n+\t// that length > 0. In Go syntax:\n+\t//\n+\t// for {\n+\t//   dst[d] = dst[d - offset]\n+\t//   d++\n+\t//   length--\n+\t//   if length == 0 {\n+\t//     break\n+\t//   }\n+\t// }\n+\tMOVB (R15), BX\n+\tMOVB BX, (DI)\n+\tINCQ R15\n+\tINCQ DI\n+\tDECQ CX\n+\tJNZ  verySlowForwardCopy\n+\tJMP  loop\n+\n+// The code above handles copy tags.\n+// ----------------------------------------\n+\n+end:\n+\t// This is the end of the \"for s < len(src)\".\n+\t//\n+\t// if d != len(dst) { etc }\n+\tCMPQ DI, R10\n+\tJNE  errCorrupt\n+\n+\t// return 0\n+\tMOVQ $0, ret+48(FP)\n+\tRET\n+\n+errCorrupt:\n+\t// return decodeErrCodeCorrupt\n+\tMOVQ $1, ret+48(FP)\n+\tRET"
    },
    {
      "sha": "94a96c5d7b858acd337aee678134fe5d6407ca8b",
      "filename": "backend/vendor/github.com/klauspost/compress/snappy/decode_other.go",
      "status": "added",
      "additions": 115,
      "deletions": 0,
      "changes": 115,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/klauspost/compress/snappy/decode_other.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/klauspost/compress/snappy/decode_other.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/github.com/klauspost/compress/snappy/decode_other.go?ref=890d7154e777d0673defcd11db3ef792473d68c9",
      "patch": "@@ -0,0 +1,115 @@\n+// Copyright 2016 The Snappy-Go Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style\n+// license that can be found in the LICENSE file.\n+\n+// +build !amd64 appengine !gc noasm\n+\n+package snappy\n+\n+// decode writes the decoding of src to dst. It assumes that the varint-encoded\n+// length of the decompressed bytes has already been read, and that len(dst)\n+// equals that length.\n+//\n+// It returns 0 on success or a decodeErrCodeXxx error code on failure.\n+func decode(dst, src []byte) int {\n+\tvar d, s, offset, length int\n+\tfor s < len(src) {\n+\t\tswitch src[s] & 0x03 {\n+\t\tcase tagLiteral:\n+\t\t\tx := uint32(src[s] >> 2)\n+\t\t\tswitch {\n+\t\t\tcase x < 60:\n+\t\t\t\ts++\n+\t\t\tcase x == 60:\n+\t\t\t\ts += 2\n+\t\t\t\tif uint(s) > uint(len(src)) { // The uint conversions catch overflow from the previous line.\n+\t\t\t\t\treturn decodeErrCodeCorrupt\n+\t\t\t\t}\n+\t\t\t\tx = uint32(src[s-1])\n+\t\t\tcase x == 61:\n+\t\t\t\ts += 3\n+\t\t\t\tif uint(s) > uint(len(src)) { // The uint conversions catch overflow from the previous line.\n+\t\t\t\t\treturn decodeErrCodeCorrupt\n+\t\t\t\t}\n+\t\t\t\tx = uint32(src[s-2]) | uint32(src[s-1])<<8\n+\t\t\tcase x == 62:\n+\t\t\t\ts += 4\n+\t\t\t\tif uint(s) > uint(len(src)) { // The uint conversions catch overflow from the previous line.\n+\t\t\t\t\treturn decodeErrCodeCorrupt\n+\t\t\t\t}\n+\t\t\t\tx = uint32(src[s-3]) | uint32(src[s-2])<<8 | uint32(src[s-1])<<16\n+\t\t\tcase x == 63:\n+\t\t\t\ts += 5\n+\t\t\t\tif uint(s) > uint(len(src)) { // The uint conversions catch overflow from the previous line.\n+\t\t\t\t\treturn decodeErrCodeCorrupt\n+\t\t\t\t}\n+\t\t\t\tx = uint32(src[s-4]) | uint32(src[s-3])<<8 | uint32(src[s-2])<<16 | uint32(src[s-1])<<24\n+\t\t\t}\n+\t\t\tlength = int(x) + 1\n+\t\t\tif length <= 0 {\n+\t\t\t\treturn decodeErrCodeUnsupportedLiteralLength\n+\t\t\t}\n+\t\t\tif length > len(dst)-d || length > len(src)-s {\n+\t\t\t\treturn decodeErrCodeCorrupt\n+\t\t\t}\n+\t\t\tcopy(dst[d:], src[s:s+length])\n+\t\t\td += length\n+\t\t\ts += length\n+\t\t\tcontinue\n+\n+\t\tcase tagCopy1:\n+\t\t\ts += 2\n+\t\t\tif uint(s) > uint(len(src)) { // The uint conversions catch overflow from the previous line.\n+\t\t\t\treturn decodeErrCodeCorrupt\n+\t\t\t}\n+\t\t\tlength = 4 + int(src[s-2])>>2&0x7\n+\t\t\toffset = int(uint32(src[s-2])&0xe0<<3 | uint32(src[s-1]))\n+\n+\t\tcase tagCopy2:\n+\t\t\ts += 3\n+\t\t\tif uint(s) > uint(len(src)) { // The uint conversions catch overflow from the previous line.\n+\t\t\t\treturn decodeErrCodeCorrupt\n+\t\t\t}\n+\t\t\tlength = 1 + int(src[s-3])>>2\n+\t\t\toffset = int(uint32(src[s-2]) | uint32(src[s-1])<<8)\n+\n+\t\tcase tagCopy4:\n+\t\t\ts += 5\n+\t\t\tif uint(s) > uint(len(src)) { // The uint conversions catch overflow from the previous line.\n+\t\t\t\treturn decodeErrCodeCorrupt\n+\t\t\t}\n+\t\t\tlength = 1 + int(src[s-5])>>2\n+\t\t\toffset = int(uint32(src[s-4]) | uint32(src[s-3])<<8 | uint32(src[s-2])<<16 | uint32(src[s-1])<<24)\n+\t\t}\n+\n+\t\tif offset <= 0 || d < offset || length > len(dst)-d {\n+\t\t\treturn decodeErrCodeCorrupt\n+\t\t}\n+\t\t// Copy from an earlier sub-slice of dst to a later sub-slice.\n+\t\t// If no overlap, use the built-in copy:\n+\t\tif offset > length {\n+\t\t\tcopy(dst[d:d+length], dst[d-offset:])\n+\t\t\td += length\n+\t\t\tcontinue\n+\t\t}\n+\n+\t\t// Unlike the built-in copy function, this byte-by-byte copy always runs\n+\t\t// forwards, even if the slices overlap. Conceptually, this is:\n+\t\t//\n+\t\t// d += forwardCopy(dst[d:d+length], dst[d-offset:])\n+\t\t//\n+\t\t// We align the slices into a and b and show the compiler they are the same size.\n+\t\t// This allows the loop to run without bounds checks.\n+\t\ta := dst[d : d+length]\n+\t\tb := dst[d-offset:]\n+\t\tb = b[:len(a)]\n+\t\tfor i := range a {\n+\t\t\ta[i] = b[i]\n+\t\t}\n+\t\td += length\n+\t}\n+\tif d != len(dst) {\n+\t\treturn decodeErrCodeCorrupt\n+\t}\n+\treturn 0\n+}"
    },
    {
      "sha": "8d393e904bb3126decbc1bec4fb2b9ffee02f1d3",
      "filename": "backend/vendor/github.com/klauspost/compress/snappy/encode.go",
      "status": "added",
      "additions": 285,
      "deletions": 0,
      "changes": 285,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/klauspost/compress/snappy/encode.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/klauspost/compress/snappy/encode.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/github.com/klauspost/compress/snappy/encode.go?ref=890d7154e777d0673defcd11db3ef792473d68c9",
      "patch": "@@ -0,0 +1,285 @@\n+// Copyright 2011 The Snappy-Go Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style\n+// license that can be found in the LICENSE file.\n+\n+package snappy\n+\n+import (\n+\t\"encoding/binary\"\n+\t\"errors\"\n+\t\"io\"\n+)\n+\n+// Encode returns the encoded form of src. The returned slice may be a sub-\n+// slice of dst if dst was large enough to hold the entire encoded block.\n+// Otherwise, a newly allocated slice will be returned.\n+//\n+// The dst and src must not overlap. It is valid to pass a nil dst.\n+func Encode(dst, src []byte) []byte {\n+\tif n := MaxEncodedLen(len(src)); n < 0 {\n+\t\tpanic(ErrTooLarge)\n+\t} else if len(dst) < n {\n+\t\tdst = make([]byte, n)\n+\t}\n+\n+\t// The block starts with the varint-encoded length of the decompressed bytes.\n+\td := binary.PutUvarint(dst, uint64(len(src)))\n+\n+\tfor len(src) > 0 {\n+\t\tp := src\n+\t\tsrc = nil\n+\t\tif len(p) > maxBlockSize {\n+\t\t\tp, src = p[:maxBlockSize], p[maxBlockSize:]\n+\t\t}\n+\t\tif len(p) < minNonLiteralBlockSize {\n+\t\t\td += emitLiteral(dst[d:], p)\n+\t\t} else {\n+\t\t\td += encodeBlock(dst[d:], p)\n+\t\t}\n+\t}\n+\treturn dst[:d]\n+}\n+\n+// inputMargin is the minimum number of extra input bytes to keep, inside\n+// encodeBlock's inner loop. On some architectures, this margin lets us\n+// implement a fast path for emitLiteral, where the copy of short (<= 16 byte)\n+// literals can be implemented as a single load to and store from a 16-byte\n+// register. That literal's actual length can be as short as 1 byte, so this\n+// can copy up to 15 bytes too much, but that's OK as subsequent iterations of\n+// the encoding loop will fix up the copy overrun, and this inputMargin ensures\n+// that we don't overrun the dst and src buffers.\n+const inputMargin = 16 - 1\n+\n+// minNonLiteralBlockSize is the minimum size of the input to encodeBlock that\n+// could be encoded with a copy tag. This is the minimum with respect to the\n+// algorithm used by encodeBlock, not a minimum enforced by the file format.\n+//\n+// The encoded output must start with at least a 1 byte literal, as there are\n+// no previous bytes to copy. A minimal (1 byte) copy after that, generated\n+// from an emitCopy call in encodeBlock's main loop, would require at least\n+// another inputMargin bytes, for the reason above: we want any emitLiteral\n+// calls inside encodeBlock's main loop to use the fast path if possible, which\n+// requires being able to overrun by inputMargin bytes. Thus,\n+// minNonLiteralBlockSize equals 1 + 1 + inputMargin.\n+//\n+// The C++ code doesn't use this exact threshold, but it could, as discussed at\n+// https://groups.google.com/d/topic/snappy-compression/oGbhsdIJSJ8/discussion\n+// The difference between Go (2+inputMargin) and C++ (inputMargin) is purely an\n+// optimization. It should not affect the encoded form. This is tested by\n+// TestSameEncodingAsCppShortCopies.\n+const minNonLiteralBlockSize = 1 + 1 + inputMargin\n+\n+// MaxEncodedLen returns the maximum length of a snappy block, given its\n+// uncompressed length.\n+//\n+// It will return a negative value if srcLen is too large to encode.\n+func MaxEncodedLen(srcLen int) int {\n+\tn := uint64(srcLen)\n+\tif n > 0xffffffff {\n+\t\treturn -1\n+\t}\n+\t// Compressed data can be defined as:\n+\t//    compressed := item* literal*\n+\t//    item       := literal* copy\n+\t//\n+\t// The trailing literal sequence has a space blowup of at most 62/60\n+\t// since a literal of length 60 needs one tag byte + one extra byte\n+\t// for length information.\n+\t//\n+\t// Item blowup is trickier to measure. Suppose the \"copy\" op copies\n+\t// 4 bytes of data. Because of a special check in the encoding code,\n+\t// we produce a 4-byte copy only if the offset is < 65536. Therefore\n+\t// the copy op takes 3 bytes to encode, and this type of item leads\n+\t// to at most the 62/60 blowup for representing literals.\n+\t//\n+\t// Suppose the \"copy\" op copies 5 bytes of data. If the offset is big\n+\t// enough, it will take 5 bytes to encode the copy op. Therefore the\n+\t// worst case here is a one-byte literal followed by a five-byte copy.\n+\t// That is, 6 bytes of input turn into 7 bytes of \"compressed\" data.\n+\t//\n+\t// This last factor dominates the blowup, so the final estimate is:\n+\tn = 32 + n + n/6\n+\tif n > 0xffffffff {\n+\t\treturn -1\n+\t}\n+\treturn int(n)\n+}\n+\n+var errClosed = errors.New(\"snappy: Writer is closed\")\n+\n+// NewWriter returns a new Writer that compresses to w.\n+//\n+// The Writer returned does not buffer writes. There is no need to Flush or\n+// Close such a Writer.\n+//\n+// Deprecated: the Writer returned is not suitable for many small writes, only\n+// for few large writes. Use NewBufferedWriter instead, which is efficient\n+// regardless of the frequency and shape of the writes, and remember to Close\n+// that Writer when done.\n+func NewWriter(w io.Writer) *Writer {\n+\treturn &Writer{\n+\t\tw:    w,\n+\t\tobuf: make([]byte, obufLen),\n+\t}\n+}\n+\n+// NewBufferedWriter returns a new Writer that compresses to w, using the\n+// framing format described at\n+// https://github.com/google/snappy/blob/master/framing_format.txt\n+//\n+// The Writer returned buffers writes. Users must call Close to guarantee all\n+// data has been forwarded to the underlying io.Writer. They may also call\n+// Flush zero or more times before calling Close.\n+func NewBufferedWriter(w io.Writer) *Writer {\n+\treturn &Writer{\n+\t\tw:    w,\n+\t\tibuf: make([]byte, 0, maxBlockSize),\n+\t\tobuf: make([]byte, obufLen),\n+\t}\n+}\n+\n+// Writer is an io.Writer that can write Snappy-compressed bytes.\n+type Writer struct {\n+\tw   io.Writer\n+\terr error\n+\n+\t// ibuf is a buffer for the incoming (uncompressed) bytes.\n+\t//\n+\t// Its use is optional. For backwards compatibility, Writers created by the\n+\t// NewWriter function have ibuf == nil, do not buffer incoming bytes, and\n+\t// therefore do not need to be Flush'ed or Close'd.\n+\tibuf []byte\n+\n+\t// obuf is a buffer for the outgoing (compressed) bytes.\n+\tobuf []byte\n+\n+\t// wroteStreamHeader is whether we have written the stream header.\n+\twroteStreamHeader bool\n+}\n+\n+// Reset discards the writer's state and switches the Snappy writer to write to\n+// w. This permits reusing a Writer rather than allocating a new one.\n+func (w *Writer) Reset(writer io.Writer) {\n+\tw.w = writer\n+\tw.err = nil\n+\tif w.ibuf != nil {\n+\t\tw.ibuf = w.ibuf[:0]\n+\t}\n+\tw.wroteStreamHeader = false\n+}\n+\n+// Write satisfies the io.Writer interface.\n+func (w *Writer) Write(p []byte) (nRet int, errRet error) {\n+\tif w.ibuf == nil {\n+\t\t// Do not buffer incoming bytes. This does not perform or compress well\n+\t\t// if the caller of Writer.Write writes many small slices. This\n+\t\t// behavior is therefore deprecated, but still supported for backwards\n+\t\t// compatibility with code that doesn't explicitly Flush or Close.\n+\t\treturn w.write(p)\n+\t}\n+\n+\t// The remainder of this method is based on bufio.Writer.Write from the\n+\t// standard library.\n+\n+\tfor len(p) > (cap(w.ibuf)-len(w.ibuf)) && w.err == nil {\n+\t\tvar n int\n+\t\tif len(w.ibuf) == 0 {\n+\t\t\t// Large write, empty buffer.\n+\t\t\t// Write directly from p to avoid copy.\n+\t\t\tn, _ = w.write(p)\n+\t\t} else {\n+\t\t\tn = copy(w.ibuf[len(w.ibuf):cap(w.ibuf)], p)\n+\t\t\tw.ibuf = w.ibuf[:len(w.ibuf)+n]\n+\t\t\tw.Flush()\n+\t\t}\n+\t\tnRet += n\n+\t\tp = p[n:]\n+\t}\n+\tif w.err != nil {\n+\t\treturn nRet, w.err\n+\t}\n+\tn := copy(w.ibuf[len(w.ibuf):cap(w.ibuf)], p)\n+\tw.ibuf = w.ibuf[:len(w.ibuf)+n]\n+\tnRet += n\n+\treturn nRet, nil\n+}\n+\n+func (w *Writer) write(p []byte) (nRet int, errRet error) {\n+\tif w.err != nil {\n+\t\treturn 0, w.err\n+\t}\n+\tfor len(p) > 0 {\n+\t\tobufStart := len(magicChunk)\n+\t\tif !w.wroteStreamHeader {\n+\t\t\tw.wroteStreamHeader = true\n+\t\t\tcopy(w.obuf, magicChunk)\n+\t\t\tobufStart = 0\n+\t\t}\n+\n+\t\tvar uncompressed []byte\n+\t\tif len(p) > maxBlockSize {\n+\t\t\tuncompressed, p = p[:maxBlockSize], p[maxBlockSize:]\n+\t\t} else {\n+\t\t\tuncompressed, p = p, nil\n+\t\t}\n+\t\tchecksum := crc(uncompressed)\n+\n+\t\t// Compress the buffer, discarding the result if the improvement\n+\t\t// isn't at least 12.5%.\n+\t\tcompressed := Encode(w.obuf[obufHeaderLen:], uncompressed)\n+\t\tchunkType := uint8(chunkTypeCompressedData)\n+\t\tchunkLen := 4 + len(compressed)\n+\t\tobufEnd := obufHeaderLen + len(compressed)\n+\t\tif len(compressed) >= len(uncompressed)-len(uncompressed)/8 {\n+\t\t\tchunkType = chunkTypeUncompressedData\n+\t\t\tchunkLen = 4 + len(uncompressed)\n+\t\t\tobufEnd = obufHeaderLen\n+\t\t}\n+\n+\t\t// Fill in the per-chunk header that comes before the body.\n+\t\tw.obuf[len(magicChunk)+0] = chunkType\n+\t\tw.obuf[len(magicChunk)+1] = uint8(chunkLen >> 0)\n+\t\tw.obuf[len(magicChunk)+2] = uint8(chunkLen >> 8)\n+\t\tw.obuf[len(magicChunk)+3] = uint8(chunkLen >> 16)\n+\t\tw.obuf[len(magicChunk)+4] = uint8(checksum >> 0)\n+\t\tw.obuf[len(magicChunk)+5] = uint8(checksum >> 8)\n+\t\tw.obuf[len(magicChunk)+6] = uint8(checksum >> 16)\n+\t\tw.obuf[len(magicChunk)+7] = uint8(checksum >> 24)\n+\n+\t\tif _, err := w.w.Write(w.obuf[obufStart:obufEnd]); err != nil {\n+\t\t\tw.err = err\n+\t\t\treturn nRet, err\n+\t\t}\n+\t\tif chunkType == chunkTypeUncompressedData {\n+\t\t\tif _, err := w.w.Write(uncompressed); err != nil {\n+\t\t\t\tw.err = err\n+\t\t\t\treturn nRet, err\n+\t\t\t}\n+\t\t}\n+\t\tnRet += len(uncompressed)\n+\t}\n+\treturn nRet, nil\n+}\n+\n+// Flush flushes the Writer to its underlying io.Writer.\n+func (w *Writer) Flush() error {\n+\tif w.err != nil {\n+\t\treturn w.err\n+\t}\n+\tif len(w.ibuf) == 0 {\n+\t\treturn nil\n+\t}\n+\tw.write(w.ibuf)\n+\tw.ibuf = w.ibuf[:0]\n+\treturn w.err\n+}\n+\n+// Close calls Flush and then closes the Writer.\n+func (w *Writer) Close() error {\n+\tw.Flush()\n+\tret := w.err\n+\tif w.err == nil {\n+\t\tw.err = errClosed\n+\t}\n+\treturn ret\n+}"
    },
    {
      "sha": "150d91bc8be57d3cc659ceefbd11f08932106459",
      "filename": "backend/vendor/github.com/klauspost/compress/snappy/encode_amd64.go",
      "status": "added",
      "additions": 29,
      "deletions": 0,
      "changes": 29,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/klauspost/compress/snappy/encode_amd64.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/klauspost/compress/snappy/encode_amd64.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/github.com/klauspost/compress/snappy/encode_amd64.go?ref=890d7154e777d0673defcd11db3ef792473d68c9",
      "patch": "@@ -0,0 +1,29 @@\n+// Copyright 2016 The Snappy-Go Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style\n+// license that can be found in the LICENSE file.\n+\n+// +build !appengine\n+// +build gc\n+// +build !noasm\n+\n+package snappy\n+\n+// emitLiteral has the same semantics as in encode_other.go.\n+//\n+//go:noescape\n+func emitLiteral(dst, lit []byte) int\n+\n+// emitCopy has the same semantics as in encode_other.go.\n+//\n+//go:noescape\n+func emitCopy(dst []byte, offset, length int) int\n+\n+// extendMatch has the same semantics as in encode_other.go.\n+//\n+//go:noescape\n+func extendMatch(src []byte, i, j int) int\n+\n+// encodeBlock has the same semantics as in encode_other.go.\n+//\n+//go:noescape\n+func encodeBlock(dst, src []byte) (d int)"
    },
    {
      "sha": "adfd979fe277aa548dc545ab9940a9ad0118fe2d",
      "filename": "backend/vendor/github.com/klauspost/compress/snappy/encode_amd64.s",
      "status": "added",
      "additions": 730,
      "deletions": 0,
      "changes": 730,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/klauspost/compress/snappy/encode_amd64.s",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/klauspost/compress/snappy/encode_amd64.s",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/github.com/klauspost/compress/snappy/encode_amd64.s?ref=890d7154e777d0673defcd11db3ef792473d68c9",
      "patch": "@@ -0,0 +1,730 @@\n+// Copyright 2016 The Go Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style\n+// license that can be found in the LICENSE file.\n+\n+// +build !appengine\n+// +build gc\n+// +build !noasm\n+\n+#include \"textflag.h\"\n+\n+// The XXX lines assemble on Go 1.4, 1.5 and 1.7, but not 1.6, due to a\n+// Go toolchain regression. See https://github.com/golang/go/issues/15426 and\n+// https://github.com/golang/snappy/issues/29\n+//\n+// As a workaround, the package was built with a known good assembler, and\n+// those instructions were disassembled by \"objdump -d\" to yield the\n+//\t4e 0f b7 7c 5c 78       movzwq 0x78(%rsp,%r11,2),%r15\n+// style comments, in AT&T asm syntax. Note that rsp here is a physical\n+// register, not Go/asm's SP pseudo-register (see https://golang.org/doc/asm).\n+// The instructions were then encoded as \"BYTE $0x..\" sequences, which assemble\n+// fine on Go 1.6.\n+\n+// The asm code generally follows the pure Go code in encode_other.go, except\n+// where marked with a \"!!!\".\n+\n+// ----------------------------------------------------------------------------\n+\n+// func emitLiteral(dst, lit []byte) int\n+//\n+// All local variables fit into registers. The register allocation:\n+//\t- AX\tlen(lit)\n+//\t- BX\tn\n+//\t- DX\treturn value\n+//\t- DI\t&dst[i]\n+//\t- R10\t&lit[0]\n+//\n+// The 24 bytes of stack space is to call runtime·memmove.\n+//\n+// The unusual register allocation of local variables, such as R10 for the\n+// source pointer, matches the allocation used at the call site in encodeBlock,\n+// which makes it easier to manually inline this function.\n+TEXT ·emitLiteral(SB), NOSPLIT, $24-56\n+\tMOVQ dst_base+0(FP), DI\n+\tMOVQ lit_base+24(FP), R10\n+\tMOVQ lit_len+32(FP), AX\n+\tMOVQ AX, DX\n+\tMOVL AX, BX\n+\tSUBL $1, BX\n+\n+\tCMPL BX, $60\n+\tJLT  oneByte\n+\tCMPL BX, $256\n+\tJLT  twoBytes\n+\n+threeBytes:\n+\tMOVB $0xf4, 0(DI)\n+\tMOVW BX, 1(DI)\n+\tADDQ $3, DI\n+\tADDQ $3, DX\n+\tJMP  memmove\n+\n+twoBytes:\n+\tMOVB $0xf0, 0(DI)\n+\tMOVB BX, 1(DI)\n+\tADDQ $2, DI\n+\tADDQ $2, DX\n+\tJMP  memmove\n+\n+oneByte:\n+\tSHLB $2, BX\n+\tMOVB BX, 0(DI)\n+\tADDQ $1, DI\n+\tADDQ $1, DX\n+\n+memmove:\n+\tMOVQ DX, ret+48(FP)\n+\n+\t// copy(dst[i:], lit)\n+\t//\n+\t// This means calling runtime·memmove(&dst[i], &lit[0], len(lit)), so we push\n+\t// DI, R10 and AX as arguments.\n+\tMOVQ DI, 0(SP)\n+\tMOVQ R10, 8(SP)\n+\tMOVQ AX, 16(SP)\n+\tCALL runtime·memmove(SB)\n+\tRET\n+\n+// ----------------------------------------------------------------------------\n+\n+// func emitCopy(dst []byte, offset, length int) int\n+//\n+// All local variables fit into registers. The register allocation:\n+//\t- AX\tlength\n+//\t- SI\t&dst[0]\n+//\t- DI\t&dst[i]\n+//\t- R11\toffset\n+//\n+// The unusual register allocation of local variables, such as R11 for the\n+// offset, matches the allocation used at the call site in encodeBlock, which\n+// makes it easier to manually inline this function.\n+TEXT ·emitCopy(SB), NOSPLIT, $0-48\n+\tMOVQ dst_base+0(FP), DI\n+\tMOVQ DI, SI\n+\tMOVQ offset+24(FP), R11\n+\tMOVQ length+32(FP), AX\n+\n+loop0:\n+\t// for length >= 68 { etc }\n+\tCMPL AX, $68\n+\tJLT  step1\n+\n+\t// Emit a length 64 copy, encoded as 3 bytes.\n+\tMOVB $0xfe, 0(DI)\n+\tMOVW R11, 1(DI)\n+\tADDQ $3, DI\n+\tSUBL $64, AX\n+\tJMP  loop0\n+\n+step1:\n+\t// if length > 64 { etc }\n+\tCMPL AX, $64\n+\tJLE  step2\n+\n+\t// Emit a length 60 copy, encoded as 3 bytes.\n+\tMOVB $0xee, 0(DI)\n+\tMOVW R11, 1(DI)\n+\tADDQ $3, DI\n+\tSUBL $60, AX\n+\n+step2:\n+\t// if length >= 12 || offset >= 2048 { goto step3 }\n+\tCMPL AX, $12\n+\tJGE  step3\n+\tCMPL R11, $2048\n+\tJGE  step3\n+\n+\t// Emit the remaining copy, encoded as 2 bytes.\n+\tMOVB R11, 1(DI)\n+\tSHRL $8, R11\n+\tSHLB $5, R11\n+\tSUBB $4, AX\n+\tSHLB $2, AX\n+\tORB  AX, R11\n+\tORB  $1, R11\n+\tMOVB R11, 0(DI)\n+\tADDQ $2, DI\n+\n+\t// Return the number of bytes written.\n+\tSUBQ SI, DI\n+\tMOVQ DI, ret+40(FP)\n+\tRET\n+\n+step3:\n+\t// Emit the remaining copy, encoded as 3 bytes.\n+\tSUBL $1, AX\n+\tSHLB $2, AX\n+\tORB  $2, AX\n+\tMOVB AX, 0(DI)\n+\tMOVW R11, 1(DI)\n+\tADDQ $3, DI\n+\n+\t// Return the number of bytes written.\n+\tSUBQ SI, DI\n+\tMOVQ DI, ret+40(FP)\n+\tRET\n+\n+// ----------------------------------------------------------------------------\n+\n+// func extendMatch(src []byte, i, j int) int\n+//\n+// All local variables fit into registers. The register allocation:\n+//\t- DX\t&src[0]\n+//\t- SI\t&src[j]\n+//\t- R13\t&src[len(src) - 8]\n+//\t- R14\t&src[len(src)]\n+//\t- R15\t&src[i]\n+//\n+// The unusual register allocation of local variables, such as R15 for a source\n+// pointer, matches the allocation used at the call site in encodeBlock, which\n+// makes it easier to manually inline this function.\n+TEXT ·extendMatch(SB), NOSPLIT, $0-48\n+\tMOVQ src_base+0(FP), DX\n+\tMOVQ src_len+8(FP), R14\n+\tMOVQ i+24(FP), R15\n+\tMOVQ j+32(FP), SI\n+\tADDQ DX, R14\n+\tADDQ DX, R15\n+\tADDQ DX, SI\n+\tMOVQ R14, R13\n+\tSUBQ $8, R13\n+\n+cmp8:\n+\t// As long as we are 8 or more bytes before the end of src, we can load and\n+\t// compare 8 bytes at a time. If those 8 bytes are equal, repeat.\n+\tCMPQ SI, R13\n+\tJA   cmp1\n+\tMOVQ (R15), AX\n+\tMOVQ (SI), BX\n+\tCMPQ AX, BX\n+\tJNE  bsf\n+\tADDQ $8, R15\n+\tADDQ $8, SI\n+\tJMP  cmp8\n+\n+bsf:\n+\t// If those 8 bytes were not equal, XOR the two 8 byte values, and return\n+\t// the index of the first byte that differs. The BSF instruction finds the\n+\t// least significant 1 bit, the amd64 architecture is little-endian, and\n+\t// the shift by 3 converts a bit index to a byte index.\n+\tXORQ AX, BX\n+\tBSFQ BX, BX\n+\tSHRQ $3, BX\n+\tADDQ BX, SI\n+\n+\t// Convert from &src[ret] to ret.\n+\tSUBQ DX, SI\n+\tMOVQ SI, ret+40(FP)\n+\tRET\n+\n+cmp1:\n+\t// In src's tail, compare 1 byte at a time.\n+\tCMPQ SI, R14\n+\tJAE  extendMatchEnd\n+\tMOVB (R15), AX\n+\tMOVB (SI), BX\n+\tCMPB AX, BX\n+\tJNE  extendMatchEnd\n+\tADDQ $1, R15\n+\tADDQ $1, SI\n+\tJMP  cmp1\n+\n+extendMatchEnd:\n+\t// Convert from &src[ret] to ret.\n+\tSUBQ DX, SI\n+\tMOVQ SI, ret+40(FP)\n+\tRET\n+\n+// ----------------------------------------------------------------------------\n+\n+// func encodeBlock(dst, src []byte) (d int)\n+//\n+// All local variables fit into registers, other than \"var table\". The register\n+// allocation:\n+//\t- AX\t.\t.\n+//\t- BX\t.\t.\n+//\t- CX\t56\tshift (note that amd64 shifts by non-immediates must use CX).\n+//\t- DX\t64\t&src[0], tableSize\n+//\t- SI\t72\t&src[s]\n+//\t- DI\t80\t&dst[d]\n+//\t- R9\t88\tsLimit\n+//\t- R10\t.\t&src[nextEmit]\n+//\t- R11\t96\tprevHash, currHash, nextHash, offset\n+//\t- R12\t104\t&src[base], skip\n+//\t- R13\t.\t&src[nextS], &src[len(src) - 8]\n+//\t- R14\t.\tlen(src), bytesBetweenHashLookups, &src[len(src)], x\n+//\t- R15\t112\tcandidate\n+//\n+// The second column (56, 64, etc) is the stack offset to spill the registers\n+// when calling other functions. We could pack this slightly tighter, but it's\n+// simpler to have a dedicated spill map independent of the function called.\n+//\n+// \"var table [maxTableSize]uint16\" takes up 32768 bytes of stack space. An\n+// extra 56 bytes, to call other functions, and an extra 64 bytes, to spill\n+// local variables (registers) during calls gives 32768 + 56 + 64 = 32888.\n+TEXT ·encodeBlock(SB), 0, $32888-56\n+\tMOVQ dst_base+0(FP), DI\n+\tMOVQ src_base+24(FP), SI\n+\tMOVQ src_len+32(FP), R14\n+\n+\t// shift, tableSize := uint32(32-8), 1<<8\n+\tMOVQ $24, CX\n+\tMOVQ $256, DX\n+\n+calcShift:\n+\t// for ; tableSize < maxTableSize && tableSize < len(src); tableSize *= 2 {\n+\t//\tshift--\n+\t// }\n+\tCMPQ DX, $16384\n+\tJGE  varTable\n+\tCMPQ DX, R14\n+\tJGE  varTable\n+\tSUBQ $1, CX\n+\tSHLQ $1, DX\n+\tJMP  calcShift\n+\n+varTable:\n+\t// var table [maxTableSize]uint16\n+\t//\n+\t// In the asm code, unlike the Go code, we can zero-initialize only the\n+\t// first tableSize elements. Each uint16 element is 2 bytes and each MOVOU\n+\t// writes 16 bytes, so we can do only tableSize/8 writes instead of the\n+\t// 2048 writes that would zero-initialize all of table's 32768 bytes.\n+\tSHRQ $3, DX\n+\tLEAQ table-32768(SP), BX\n+\tPXOR X0, X0\n+\n+memclr:\n+\tMOVOU X0, 0(BX)\n+\tADDQ  $16, BX\n+\tSUBQ  $1, DX\n+\tJNZ   memclr\n+\n+\t// !!! DX = &src[0]\n+\tMOVQ SI, DX\n+\n+\t// sLimit := len(src) - inputMargin\n+\tMOVQ R14, R9\n+\tSUBQ $15, R9\n+\n+\t// !!! Pre-emptively spill CX, DX and R9 to the stack. Their values don't\n+\t// change for the rest of the function.\n+\tMOVQ CX, 56(SP)\n+\tMOVQ DX, 64(SP)\n+\tMOVQ R9, 88(SP)\n+\n+\t// nextEmit := 0\n+\tMOVQ DX, R10\n+\n+\t// s := 1\n+\tADDQ $1, SI\n+\n+\t// nextHash := hash(load32(src, s), shift)\n+\tMOVL  0(SI), R11\n+\tIMULL $0x1e35a7bd, R11\n+\tSHRL  CX, R11\n+\n+outer:\n+\t// for { etc }\n+\n+\t// skip := 32\n+\tMOVQ $32, R12\n+\n+\t// nextS := s\n+\tMOVQ SI, R13\n+\n+\t// candidate := 0\n+\tMOVQ $0, R15\n+\n+inner0:\n+\t// for { etc }\n+\n+\t// s := nextS\n+\tMOVQ R13, SI\n+\n+\t// bytesBetweenHashLookups := skip >> 5\n+\tMOVQ R12, R14\n+\tSHRQ $5, R14\n+\n+\t// nextS = s + bytesBetweenHashLookups\n+\tADDQ R14, R13\n+\n+\t// skip += bytesBetweenHashLookups\n+\tADDQ R14, R12\n+\n+\t// if nextS > sLimit { goto emitRemainder }\n+\tMOVQ R13, AX\n+\tSUBQ DX, AX\n+\tCMPQ AX, R9\n+\tJA   emitRemainder\n+\n+\t// candidate = int(table[nextHash])\n+\t// XXX: MOVWQZX table-32768(SP)(R11*2), R15\n+\t// XXX: 4e 0f b7 7c 5c 78       movzwq 0x78(%rsp,%r11,2),%r15\n+\tBYTE $0x4e\n+\tBYTE $0x0f\n+\tBYTE $0xb7\n+\tBYTE $0x7c\n+\tBYTE $0x5c\n+\tBYTE $0x78\n+\n+\t// table[nextHash] = uint16(s)\n+\tMOVQ SI, AX\n+\tSUBQ DX, AX\n+\n+\t// XXX: MOVW AX, table-32768(SP)(R11*2)\n+\t// XXX: 66 42 89 44 5c 78       mov    %ax,0x78(%rsp,%r11,2)\n+\tBYTE $0x66\n+\tBYTE $0x42\n+\tBYTE $0x89\n+\tBYTE $0x44\n+\tBYTE $0x5c\n+\tBYTE $0x78\n+\n+\t// nextHash = hash(load32(src, nextS), shift)\n+\tMOVL  0(R13), R11\n+\tIMULL $0x1e35a7bd, R11\n+\tSHRL  CX, R11\n+\n+\t// if load32(src, s) != load32(src, candidate) { continue } break\n+\tMOVL 0(SI), AX\n+\tMOVL (DX)(R15*1), BX\n+\tCMPL AX, BX\n+\tJNE  inner0\n+\n+fourByteMatch:\n+\t// As per the encode_other.go code:\n+\t//\n+\t// A 4-byte match has been found. We'll later see etc.\n+\n+\t// !!! Jump to a fast path for short (<= 16 byte) literals. See the comment\n+\t// on inputMargin in encode.go.\n+\tMOVQ SI, AX\n+\tSUBQ R10, AX\n+\tCMPQ AX, $16\n+\tJLE  emitLiteralFastPath\n+\n+\t// ----------------------------------------\n+\t// Begin inline of the emitLiteral call.\n+\t//\n+\t// d += emitLiteral(dst[d:], src[nextEmit:s])\n+\n+\tMOVL AX, BX\n+\tSUBL $1, BX\n+\n+\tCMPL BX, $60\n+\tJLT  inlineEmitLiteralOneByte\n+\tCMPL BX, $256\n+\tJLT  inlineEmitLiteralTwoBytes\n+\n+inlineEmitLiteralThreeBytes:\n+\tMOVB $0xf4, 0(DI)\n+\tMOVW BX, 1(DI)\n+\tADDQ $3, DI\n+\tJMP  inlineEmitLiteralMemmove\n+\n+inlineEmitLiteralTwoBytes:\n+\tMOVB $0xf0, 0(DI)\n+\tMOVB BX, 1(DI)\n+\tADDQ $2, DI\n+\tJMP  inlineEmitLiteralMemmove\n+\n+inlineEmitLiteralOneByte:\n+\tSHLB $2, BX\n+\tMOVB BX, 0(DI)\n+\tADDQ $1, DI\n+\n+inlineEmitLiteralMemmove:\n+\t// Spill local variables (registers) onto the stack; call; unspill.\n+\t//\n+\t// copy(dst[i:], lit)\n+\t//\n+\t// This means calling runtime·memmove(&dst[i], &lit[0], len(lit)), so we push\n+\t// DI, R10 and AX as arguments.\n+\tMOVQ DI, 0(SP)\n+\tMOVQ R10, 8(SP)\n+\tMOVQ AX, 16(SP)\n+\tADDQ AX, DI              // Finish the \"d +=\" part of \"d += emitLiteral(etc)\".\n+\tMOVQ SI, 72(SP)\n+\tMOVQ DI, 80(SP)\n+\tMOVQ R15, 112(SP)\n+\tCALL runtime·memmove(SB)\n+\tMOVQ 56(SP), CX\n+\tMOVQ 64(SP), DX\n+\tMOVQ 72(SP), SI\n+\tMOVQ 80(SP), DI\n+\tMOVQ 88(SP), R9\n+\tMOVQ 112(SP), R15\n+\tJMP  inner1\n+\n+inlineEmitLiteralEnd:\n+\t// End inline of the emitLiteral call.\n+\t// ----------------------------------------\n+\n+emitLiteralFastPath:\n+\t// !!! Emit the 1-byte encoding \"uint8(len(lit)-1)<<2\".\n+\tMOVB AX, BX\n+\tSUBB $1, BX\n+\tSHLB $2, BX\n+\tMOVB BX, (DI)\n+\tADDQ $1, DI\n+\n+\t// !!! Implement the copy from lit to dst as a 16-byte load and store.\n+\t// (Encode's documentation says that dst and src must not overlap.)\n+\t//\n+\t// This always copies 16 bytes, instead of only len(lit) bytes, but that's\n+\t// OK. Subsequent iterations will fix up the overrun.\n+\t//\n+\t// Note that on amd64, it is legal and cheap to issue unaligned 8-byte or\n+\t// 16-byte loads and stores. This technique probably wouldn't be as\n+\t// effective on architectures that are fussier about alignment.\n+\tMOVOU 0(R10), X0\n+\tMOVOU X0, 0(DI)\n+\tADDQ  AX, DI\n+\n+inner1:\n+\t// for { etc }\n+\n+\t// base := s\n+\tMOVQ SI, R12\n+\n+\t// !!! offset := base - candidate\n+\tMOVQ R12, R11\n+\tSUBQ R15, R11\n+\tSUBQ DX, R11\n+\n+\t// ----------------------------------------\n+\t// Begin inline of the extendMatch call.\n+\t//\n+\t// s = extendMatch(src, candidate+4, s+4)\n+\n+\t// !!! R14 = &src[len(src)]\n+\tMOVQ src_len+32(FP), R14\n+\tADDQ DX, R14\n+\n+\t// !!! R13 = &src[len(src) - 8]\n+\tMOVQ R14, R13\n+\tSUBQ $8, R13\n+\n+\t// !!! R15 = &src[candidate + 4]\n+\tADDQ $4, R15\n+\tADDQ DX, R15\n+\n+\t// !!! s += 4\n+\tADDQ $4, SI\n+\n+inlineExtendMatchCmp8:\n+\t// As long as we are 8 or more bytes before the end of src, we can load and\n+\t// compare 8 bytes at a time. If those 8 bytes are equal, repeat.\n+\tCMPQ SI, R13\n+\tJA   inlineExtendMatchCmp1\n+\tMOVQ (R15), AX\n+\tMOVQ (SI), BX\n+\tCMPQ AX, BX\n+\tJNE  inlineExtendMatchBSF\n+\tADDQ $8, R15\n+\tADDQ $8, SI\n+\tJMP  inlineExtendMatchCmp8\n+\n+inlineExtendMatchBSF:\n+\t// If those 8 bytes were not equal, XOR the two 8 byte values, and return\n+\t// the index of the first byte that differs. The BSF instruction finds the\n+\t// least significant 1 bit, the amd64 architecture is little-endian, and\n+\t// the shift by 3 converts a bit index to a byte index.\n+\tXORQ AX, BX\n+\tBSFQ BX, BX\n+\tSHRQ $3, BX\n+\tADDQ BX, SI\n+\tJMP  inlineExtendMatchEnd\n+\n+inlineExtendMatchCmp1:\n+\t// In src's tail, compare 1 byte at a time.\n+\tCMPQ SI, R14\n+\tJAE  inlineExtendMatchEnd\n+\tMOVB (R15), AX\n+\tMOVB (SI), BX\n+\tCMPB AX, BX\n+\tJNE  inlineExtendMatchEnd\n+\tADDQ $1, R15\n+\tADDQ $1, SI\n+\tJMP  inlineExtendMatchCmp1\n+\n+inlineExtendMatchEnd:\n+\t// End inline of the extendMatch call.\n+\t// ----------------------------------------\n+\n+\t// ----------------------------------------\n+\t// Begin inline of the emitCopy call.\n+\t//\n+\t// d += emitCopy(dst[d:], base-candidate, s-base)\n+\n+\t// !!! length := s - base\n+\tMOVQ SI, AX\n+\tSUBQ R12, AX\n+\n+inlineEmitCopyLoop0:\n+\t// for length >= 68 { etc }\n+\tCMPL AX, $68\n+\tJLT  inlineEmitCopyStep1\n+\n+\t// Emit a length 64 copy, encoded as 3 bytes.\n+\tMOVB $0xfe, 0(DI)\n+\tMOVW R11, 1(DI)\n+\tADDQ $3, DI\n+\tSUBL $64, AX\n+\tJMP  inlineEmitCopyLoop0\n+\n+inlineEmitCopyStep1:\n+\t// if length > 64 { etc }\n+\tCMPL AX, $64\n+\tJLE  inlineEmitCopyStep2\n+\n+\t// Emit a length 60 copy, encoded as 3 bytes.\n+\tMOVB $0xee, 0(DI)\n+\tMOVW R11, 1(DI)\n+\tADDQ $3, DI\n+\tSUBL $60, AX\n+\n+inlineEmitCopyStep2:\n+\t// if length >= 12 || offset >= 2048 { goto inlineEmitCopyStep3 }\n+\tCMPL AX, $12\n+\tJGE  inlineEmitCopyStep3\n+\tCMPL R11, $2048\n+\tJGE  inlineEmitCopyStep3\n+\n+\t// Emit the remaining copy, encoded as 2 bytes.\n+\tMOVB R11, 1(DI)\n+\tSHRL $8, R11\n+\tSHLB $5, R11\n+\tSUBB $4, AX\n+\tSHLB $2, AX\n+\tORB  AX, R11\n+\tORB  $1, R11\n+\tMOVB R11, 0(DI)\n+\tADDQ $2, DI\n+\tJMP  inlineEmitCopyEnd\n+\n+inlineEmitCopyStep3:\n+\t// Emit the remaining copy, encoded as 3 bytes.\n+\tSUBL $1, AX\n+\tSHLB $2, AX\n+\tORB  $2, AX\n+\tMOVB AX, 0(DI)\n+\tMOVW R11, 1(DI)\n+\tADDQ $3, DI\n+\n+inlineEmitCopyEnd:\n+\t// End inline of the emitCopy call.\n+\t// ----------------------------------------\n+\n+\t// nextEmit = s\n+\tMOVQ SI, R10\n+\n+\t// if s >= sLimit { goto emitRemainder }\n+\tMOVQ SI, AX\n+\tSUBQ DX, AX\n+\tCMPQ AX, R9\n+\tJAE  emitRemainder\n+\n+\t// As per the encode_other.go code:\n+\t//\n+\t// We could immediately etc.\n+\n+\t// x := load64(src, s-1)\n+\tMOVQ -1(SI), R14\n+\n+\t// prevHash := hash(uint32(x>>0), shift)\n+\tMOVL  R14, R11\n+\tIMULL $0x1e35a7bd, R11\n+\tSHRL  CX, R11\n+\n+\t// table[prevHash] = uint16(s-1)\n+\tMOVQ SI, AX\n+\tSUBQ DX, AX\n+\tSUBQ $1, AX\n+\n+\t// XXX: MOVW AX, table-32768(SP)(R11*2)\n+\t// XXX: 66 42 89 44 5c 78       mov    %ax,0x78(%rsp,%r11,2)\n+\tBYTE $0x66\n+\tBYTE $0x42\n+\tBYTE $0x89\n+\tBYTE $0x44\n+\tBYTE $0x5c\n+\tBYTE $0x78\n+\n+\t// currHash := hash(uint32(x>>8), shift)\n+\tSHRQ  $8, R14\n+\tMOVL  R14, R11\n+\tIMULL $0x1e35a7bd, R11\n+\tSHRL  CX, R11\n+\n+\t// candidate = int(table[currHash])\n+\t// XXX: MOVWQZX table-32768(SP)(R11*2), R15\n+\t// XXX: 4e 0f b7 7c 5c 78       movzwq 0x78(%rsp,%r11,2),%r15\n+\tBYTE $0x4e\n+\tBYTE $0x0f\n+\tBYTE $0xb7\n+\tBYTE $0x7c\n+\tBYTE $0x5c\n+\tBYTE $0x78\n+\n+\t// table[currHash] = uint16(s)\n+\tADDQ $1, AX\n+\n+\t// XXX: MOVW AX, table-32768(SP)(R11*2)\n+\t// XXX: 66 42 89 44 5c 78       mov    %ax,0x78(%rsp,%r11,2)\n+\tBYTE $0x66\n+\tBYTE $0x42\n+\tBYTE $0x89\n+\tBYTE $0x44\n+\tBYTE $0x5c\n+\tBYTE $0x78\n+\n+\t// if uint32(x>>8) == load32(src, candidate) { continue }\n+\tMOVL (DX)(R15*1), BX\n+\tCMPL R14, BX\n+\tJEQ  inner1\n+\n+\t// nextHash = hash(uint32(x>>16), shift)\n+\tSHRQ  $8, R14\n+\tMOVL  R14, R11\n+\tIMULL $0x1e35a7bd, R11\n+\tSHRL  CX, R11\n+\n+\t// s++\n+\tADDQ $1, SI\n+\n+\t// break out of the inner1 for loop, i.e. continue the outer loop.\n+\tJMP outer\n+\n+emitRemainder:\n+\t// if nextEmit < len(src) { etc }\n+\tMOVQ src_len+32(FP), AX\n+\tADDQ DX, AX\n+\tCMPQ R10, AX\n+\tJEQ  encodeBlockEnd\n+\n+\t// d += emitLiteral(dst[d:], src[nextEmit:])\n+\t//\n+\t// Push args.\n+\tMOVQ DI, 0(SP)\n+\tMOVQ $0, 8(SP)   // Unnecessary, as the callee ignores it, but conservative.\n+\tMOVQ $0, 16(SP)  // Unnecessary, as the callee ignores it, but conservative.\n+\tMOVQ R10, 24(SP)\n+\tSUBQ R10, AX\n+\tMOVQ AX, 32(SP)\n+\tMOVQ AX, 40(SP)  // Unnecessary, as the callee ignores it, but conservative.\n+\n+\t// Spill local variables (registers) onto the stack; call; unspill.\n+\tMOVQ DI, 80(SP)\n+\tCALL ·emitLiteral(SB)\n+\tMOVQ 80(SP), DI\n+\n+\t// Finish the \"d +=\" part of \"d += emitLiteral(etc)\".\n+\tADDQ 48(SP), DI\n+\n+encodeBlockEnd:\n+\tMOVQ dst_base+0(FP), AX\n+\tSUBQ AX, DI\n+\tMOVQ DI, d+48(FP)\n+\tRET"
    },
    {
      "sha": "dbcae905e6e047ba3c00f68057f5bf8541e981fa",
      "filename": "backend/vendor/github.com/klauspost/compress/snappy/encode_other.go",
      "status": "added",
      "additions": 238,
      "deletions": 0,
      "changes": 238,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/klauspost/compress/snappy/encode_other.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/klauspost/compress/snappy/encode_other.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/github.com/klauspost/compress/snappy/encode_other.go?ref=890d7154e777d0673defcd11db3ef792473d68c9",
      "patch": "@@ -0,0 +1,238 @@\n+// Copyright 2016 The Snappy-Go Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style\n+// license that can be found in the LICENSE file.\n+\n+// +build !amd64 appengine !gc noasm\n+\n+package snappy\n+\n+func load32(b []byte, i int) uint32 {\n+\tb = b[i : i+4 : len(b)] // Help the compiler eliminate bounds checks on the next line.\n+\treturn uint32(b[0]) | uint32(b[1])<<8 | uint32(b[2])<<16 | uint32(b[3])<<24\n+}\n+\n+func load64(b []byte, i int) uint64 {\n+\tb = b[i : i+8 : len(b)] // Help the compiler eliminate bounds checks on the next line.\n+\treturn uint64(b[0]) | uint64(b[1])<<8 | uint64(b[2])<<16 | uint64(b[3])<<24 |\n+\t\tuint64(b[4])<<32 | uint64(b[5])<<40 | uint64(b[6])<<48 | uint64(b[7])<<56\n+}\n+\n+// emitLiteral writes a literal chunk and returns the number of bytes written.\n+//\n+// It assumes that:\n+//\tdst is long enough to hold the encoded bytes\n+//\t1 <= len(lit) && len(lit) <= 65536\n+func emitLiteral(dst, lit []byte) int {\n+\ti, n := 0, uint(len(lit)-1)\n+\tswitch {\n+\tcase n < 60:\n+\t\tdst[0] = uint8(n)<<2 | tagLiteral\n+\t\ti = 1\n+\tcase n < 1<<8:\n+\t\tdst[0] = 60<<2 | tagLiteral\n+\t\tdst[1] = uint8(n)\n+\t\ti = 2\n+\tdefault:\n+\t\tdst[0] = 61<<2 | tagLiteral\n+\t\tdst[1] = uint8(n)\n+\t\tdst[2] = uint8(n >> 8)\n+\t\ti = 3\n+\t}\n+\treturn i + copy(dst[i:], lit)\n+}\n+\n+// emitCopy writes a copy chunk and returns the number of bytes written.\n+//\n+// It assumes that:\n+//\tdst is long enough to hold the encoded bytes\n+//\t1 <= offset && offset <= 65535\n+//\t4 <= length && length <= 65535\n+func emitCopy(dst []byte, offset, length int) int {\n+\ti := 0\n+\t// The maximum length for a single tagCopy1 or tagCopy2 op is 64 bytes. The\n+\t// threshold for this loop is a little higher (at 68 = 64 + 4), and the\n+\t// length emitted down below is is a little lower (at 60 = 64 - 4), because\n+\t// it's shorter to encode a length 67 copy as a length 60 tagCopy2 followed\n+\t// by a length 7 tagCopy1 (which encodes as 3+2 bytes) than to encode it as\n+\t// a length 64 tagCopy2 followed by a length 3 tagCopy2 (which encodes as\n+\t// 3+3 bytes). The magic 4 in the 64±4 is because the minimum length for a\n+\t// tagCopy1 op is 4 bytes, which is why a length 3 copy has to be an\n+\t// encodes-as-3-bytes tagCopy2 instead of an encodes-as-2-bytes tagCopy1.\n+\tfor length >= 68 {\n+\t\t// Emit a length 64 copy, encoded as 3 bytes.\n+\t\tdst[i+0] = 63<<2 | tagCopy2\n+\t\tdst[i+1] = uint8(offset)\n+\t\tdst[i+2] = uint8(offset >> 8)\n+\t\ti += 3\n+\t\tlength -= 64\n+\t}\n+\tif length > 64 {\n+\t\t// Emit a length 60 copy, encoded as 3 bytes.\n+\t\tdst[i+0] = 59<<2 | tagCopy2\n+\t\tdst[i+1] = uint8(offset)\n+\t\tdst[i+2] = uint8(offset >> 8)\n+\t\ti += 3\n+\t\tlength -= 60\n+\t}\n+\tif length >= 12 || offset >= 2048 {\n+\t\t// Emit the remaining copy, encoded as 3 bytes.\n+\t\tdst[i+0] = uint8(length-1)<<2 | tagCopy2\n+\t\tdst[i+1] = uint8(offset)\n+\t\tdst[i+2] = uint8(offset >> 8)\n+\t\treturn i + 3\n+\t}\n+\t// Emit the remaining copy, encoded as 2 bytes.\n+\tdst[i+0] = uint8(offset>>8)<<5 | uint8(length-4)<<2 | tagCopy1\n+\tdst[i+1] = uint8(offset)\n+\treturn i + 2\n+}\n+\n+// extendMatch returns the largest k such that k <= len(src) and that\n+// src[i:i+k-j] and src[j:k] have the same contents.\n+//\n+// It assumes that:\n+//\t0 <= i && i < j && j <= len(src)\n+func extendMatch(src []byte, i, j int) int {\n+\tfor ; j < len(src) && src[i] == src[j]; i, j = i+1, j+1 {\n+\t}\n+\treturn j\n+}\n+\n+func hash(u, shift uint32) uint32 {\n+\treturn (u * 0x1e35a7bd) >> shift\n+}\n+\n+// encodeBlock encodes a non-empty src to a guaranteed-large-enough dst. It\n+// assumes that the varint-encoded length of the decompressed bytes has already\n+// been written.\n+//\n+// It also assumes that:\n+//\tlen(dst) >= MaxEncodedLen(len(src)) &&\n+// \tminNonLiteralBlockSize <= len(src) && len(src) <= maxBlockSize\n+func encodeBlock(dst, src []byte) (d int) {\n+\t// Initialize the hash table. Its size ranges from 1<<8 to 1<<14 inclusive.\n+\t// The table element type is uint16, as s < sLimit and sLimit < len(src)\n+\t// and len(src) <= maxBlockSize and maxBlockSize == 65536.\n+\tconst (\n+\t\tmaxTableSize = 1 << 14\n+\t\t// tableMask is redundant, but helps the compiler eliminate bounds\n+\t\t// checks.\n+\t\ttableMask = maxTableSize - 1\n+\t)\n+\tshift := uint32(32 - 8)\n+\tfor tableSize := 1 << 8; tableSize < maxTableSize && tableSize < len(src); tableSize *= 2 {\n+\t\tshift--\n+\t}\n+\t// In Go, all array elements are zero-initialized, so there is no advantage\n+\t// to a smaller tableSize per se. However, it matches the C++ algorithm,\n+\t// and in the asm versions of this code, we can get away with zeroing only\n+\t// the first tableSize elements.\n+\tvar table [maxTableSize]uint16\n+\n+\t// sLimit is when to stop looking for offset/length copies. The inputMargin\n+\t// lets us use a fast path for emitLiteral in the main loop, while we are\n+\t// looking for copies.\n+\tsLimit := len(src) - inputMargin\n+\n+\t// nextEmit is where in src the next emitLiteral should start from.\n+\tnextEmit := 0\n+\n+\t// The encoded form must start with a literal, as there are no previous\n+\t// bytes to copy, so we start looking for hash matches at s == 1.\n+\ts := 1\n+\tnextHash := hash(load32(src, s), shift)\n+\n+\tfor {\n+\t\t// Copied from the C++ snappy implementation:\n+\t\t//\n+\t\t// Heuristic match skipping: If 32 bytes are scanned with no matches\n+\t\t// found, start looking only at every other byte. If 32 more bytes are\n+\t\t// scanned (or skipped), look at every third byte, etc.. When a match\n+\t\t// is found, immediately go back to looking at every byte. This is a\n+\t\t// small loss (~5% performance, ~0.1% density) for compressible data\n+\t\t// due to more bookkeeping, but for non-compressible data (such as\n+\t\t// JPEG) it's a huge win since the compressor quickly \"realizes\" the\n+\t\t// data is incompressible and doesn't bother looking for matches\n+\t\t// everywhere.\n+\t\t//\n+\t\t// The \"skip\" variable keeps track of how many bytes there are since\n+\t\t// the last match; dividing it by 32 (ie. right-shifting by five) gives\n+\t\t// the number of bytes to move ahead for each iteration.\n+\t\tskip := 32\n+\n+\t\tnextS := s\n+\t\tcandidate := 0\n+\t\tfor {\n+\t\t\ts = nextS\n+\t\t\tbytesBetweenHashLookups := skip >> 5\n+\t\t\tnextS = s + bytesBetweenHashLookups\n+\t\t\tskip += bytesBetweenHashLookups\n+\t\t\tif nextS > sLimit {\n+\t\t\t\tgoto emitRemainder\n+\t\t\t}\n+\t\t\tcandidate = int(table[nextHash&tableMask])\n+\t\t\ttable[nextHash&tableMask] = uint16(s)\n+\t\t\tnextHash = hash(load32(src, nextS), shift)\n+\t\t\tif load32(src, s) == load32(src, candidate) {\n+\t\t\t\tbreak\n+\t\t\t}\n+\t\t}\n+\n+\t\t// A 4-byte match has been found. We'll later see if more than 4 bytes\n+\t\t// match. But, prior to the match, src[nextEmit:s] are unmatched. Emit\n+\t\t// them as literal bytes.\n+\t\td += emitLiteral(dst[d:], src[nextEmit:s])\n+\n+\t\t// Call emitCopy, and then see if another emitCopy could be our next\n+\t\t// move. Repeat until we find no match for the input immediately after\n+\t\t// what was consumed by the last emitCopy call.\n+\t\t//\n+\t\t// If we exit this loop normally then we need to call emitLiteral next,\n+\t\t// though we don't yet know how big the literal will be. We handle that\n+\t\t// by proceeding to the next iteration of the main loop. We also can\n+\t\t// exit this loop via goto if we get close to exhausting the input.\n+\t\tfor {\n+\t\t\t// Invariant: we have a 4-byte match at s, and no need to emit any\n+\t\t\t// literal bytes prior to s.\n+\t\t\tbase := s\n+\n+\t\t\t// Extend the 4-byte match as long as possible.\n+\t\t\t//\n+\t\t\t// This is an inlined version of:\n+\t\t\t//\ts = extendMatch(src, candidate+4, s+4)\n+\t\t\ts += 4\n+\t\t\tfor i := candidate + 4; s < len(src) && src[i] == src[s]; i, s = i+1, s+1 {\n+\t\t\t}\n+\n+\t\t\td += emitCopy(dst[d:], base-candidate, s-base)\n+\t\t\tnextEmit = s\n+\t\t\tif s >= sLimit {\n+\t\t\t\tgoto emitRemainder\n+\t\t\t}\n+\n+\t\t\t// We could immediately start working at s now, but to improve\n+\t\t\t// compression we first update the hash table at s-1 and at s. If\n+\t\t\t// another emitCopy is not our next move, also calculate nextHash\n+\t\t\t// at s+1. At least on GOARCH=amd64, these three hash calculations\n+\t\t\t// are faster as one load64 call (with some shifts) instead of\n+\t\t\t// three load32 calls.\n+\t\t\tx := load64(src, s-1)\n+\t\t\tprevHash := hash(uint32(x>>0), shift)\n+\t\t\ttable[prevHash&tableMask] = uint16(s - 1)\n+\t\t\tcurrHash := hash(uint32(x>>8), shift)\n+\t\t\tcandidate = int(table[currHash&tableMask])\n+\t\t\ttable[currHash&tableMask] = uint16(s)\n+\t\t\tif uint32(x>>8) != load32(src, candidate) {\n+\t\t\t\tnextHash = hash(uint32(x>>16), shift)\n+\t\t\t\ts++\n+\t\t\t\tbreak\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+emitRemainder:\n+\tif nextEmit < len(src) {\n+\t\td += emitLiteral(dst[d:], src[nextEmit:])\n+\t}\n+\treturn d\n+}"
    },
    {
      "sha": "d24eb4b47c377d1e572b854a2c15295d968abc00",
      "filename": "backend/vendor/github.com/klauspost/compress/snappy/runbench.cmd",
      "status": "added",
      "additions": 2,
      "deletions": 0,
      "changes": 2,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/klauspost/compress/snappy/runbench.cmd",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/klauspost/compress/snappy/runbench.cmd",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/github.com/klauspost/compress/snappy/runbench.cmd?ref=890d7154e777d0673defcd11db3ef792473d68c9",
      "patch": "@@ -0,0 +1,2 @@\n+del old.txt\n+go test -bench=. >>old.txt && go test -bench=. >>old.txt && go test -bench=. >>old.txt && benchstat -delta-test=ttest old.txt new.txt"
    },
    {
      "sha": "74a36689e878a34e9237a9369f402f4e0efa9832",
      "filename": "backend/vendor/github.com/klauspost/compress/snappy/snappy.go",
      "status": "added",
      "additions": 98,
      "deletions": 0,
      "changes": 98,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/klauspost/compress/snappy/snappy.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/klauspost/compress/snappy/snappy.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/github.com/klauspost/compress/snappy/snappy.go?ref=890d7154e777d0673defcd11db3ef792473d68c9",
      "patch": "@@ -0,0 +1,98 @@\n+// Copyright 2011 The Snappy-Go Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style\n+// license that can be found in the LICENSE file.\n+\n+// Package snappy implements the Snappy compression format. It aims for very\n+// high speeds and reasonable compression.\n+//\n+// There are actually two Snappy formats: block and stream. They are related,\n+// but different: trying to decompress block-compressed data as a Snappy stream\n+// will fail, and vice versa. The block format is the Decode and Encode\n+// functions and the stream format is the Reader and Writer types.\n+//\n+// The block format, the more common case, is used when the complete size (the\n+// number of bytes) of the original data is known upfront, at the time\n+// compression starts. The stream format, also known as the framing format, is\n+// for when that isn't always true.\n+//\n+// The canonical, C++ implementation is at https://github.com/google/snappy and\n+// it only implements the block format.\n+package snappy\n+\n+import (\n+\t\"hash/crc32\"\n+)\n+\n+/*\n+Each encoded block begins with the varint-encoded length of the decoded data,\n+followed by a sequence of chunks. Chunks begin and end on byte boundaries. The\n+first byte of each chunk is broken into its 2 least and 6 most significant bits\n+called l and m: l ranges in [0, 4) and m ranges in [0, 64). l is the chunk tag.\n+Zero means a literal tag. All other values mean a copy tag.\n+\n+For literal tags:\n+  - If m < 60, the next 1 + m bytes are literal bytes.\n+  - Otherwise, let n be the little-endian unsigned integer denoted by the next\n+    m - 59 bytes. The next 1 + n bytes after that are literal bytes.\n+\n+For copy tags, length bytes are copied from offset bytes ago, in the style of\n+Lempel-Ziv compression algorithms. In particular:\n+  - For l == 1, the offset ranges in [0, 1<<11) and the length in [4, 12).\n+    The length is 4 + the low 3 bits of m. The high 3 bits of m form bits 8-10\n+    of the offset. The next byte is bits 0-7 of the offset.\n+  - For l == 2, the offset ranges in [0, 1<<16) and the length in [1, 65).\n+    The length is 1 + m. The offset is the little-endian unsigned integer\n+    denoted by the next 2 bytes.\n+  - For l == 3, this tag is a legacy format that is no longer issued by most\n+    encoders. Nonetheless, the offset ranges in [0, 1<<32) and the length in\n+    [1, 65). The length is 1 + m. The offset is the little-endian unsigned\n+    integer denoted by the next 4 bytes.\n+*/\n+const (\n+\ttagLiteral = 0x00\n+\ttagCopy1   = 0x01\n+\ttagCopy2   = 0x02\n+\ttagCopy4   = 0x03\n+)\n+\n+const (\n+\tchecksumSize    = 4\n+\tchunkHeaderSize = 4\n+\tmagicChunk      = \"\\xff\\x06\\x00\\x00\" + magicBody\n+\tmagicBody       = \"sNaPpY\"\n+\n+\t// maxBlockSize is the maximum size of the input to encodeBlock. It is not\n+\t// part of the wire format per se, but some parts of the encoder assume\n+\t// that an offset fits into a uint16.\n+\t//\n+\t// Also, for the framing format (Writer type instead of Encode function),\n+\t// https://github.com/google/snappy/blob/master/framing_format.txt says\n+\t// that \"the uncompressed data in a chunk must be no longer than 65536\n+\t// bytes\".\n+\tmaxBlockSize = 65536\n+\n+\t// maxEncodedLenOfMaxBlockSize equals MaxEncodedLen(maxBlockSize), but is\n+\t// hard coded to be a const instead of a variable, so that obufLen can also\n+\t// be a const. Their equivalence is confirmed by\n+\t// TestMaxEncodedLenOfMaxBlockSize.\n+\tmaxEncodedLenOfMaxBlockSize = 76490\n+\n+\tobufHeaderLen = len(magicChunk) + checksumSize + chunkHeaderSize\n+\tobufLen       = obufHeaderLen + maxEncodedLenOfMaxBlockSize\n+)\n+\n+const (\n+\tchunkTypeCompressedData   = 0x00\n+\tchunkTypeUncompressedData = 0x01\n+\tchunkTypePadding          = 0xfe\n+\tchunkTypeStreamIdentifier = 0xff\n+)\n+\n+var crcTable = crc32.MakeTable(crc32.Castagnoli)\n+\n+// crc implements the checksum specified in section 3 of\n+// https://github.com/google/snappy/blob/master/framing_format.txt\n+func crc(b []byte) uint32 {\n+\tc := crc32.Update(0, crcTable, b)\n+\treturn uint32(c>>15|c<<17) + 0xa282ead8\n+}"
    },
    {
      "sha": "bc977a302346201570280f3602d5eb3381fd74c3",
      "filename": "backend/vendor/github.com/klauspost/compress/zstd/README.md",
      "status": "added",
      "additions": 393,
      "deletions": 0,
      "changes": 393,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/klauspost/compress/zstd/README.md",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/klauspost/compress/zstd/README.md",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/github.com/klauspost/compress/zstd/README.md?ref=890d7154e777d0673defcd11db3ef792473d68c9",
      "patch": "@@ -0,0 +1,393 @@\n+# zstd \n+\n+[Zstandard](https://facebook.github.io/zstd/) is a real-time compression algorithm, providing high compression ratios. \n+It offers a very wide range of compression / speed trade-off, while being backed by a very fast decoder.\n+A high performance compression algorithm is implemented. For now focused on speed. \n+\n+This package provides [compression](#Compressor) to and [decompression](#Decompressor) of Zstandard content. \n+Note that custom dictionaries are not supported yet, so if your code relies on that, \n+you cannot use the package as-is.\n+\n+This package is pure Go and without use of \"unsafe\". \n+If a significant speedup can be achieved using \"unsafe\", it may be added as an option later.\n+\n+The `zstd` package is provided as open source software using a Go standard license.\n+\n+Currently the package is heavily optimized for 64 bit processors and will be significantly slower on 32 bit processors.\n+\n+## Installation\n+\n+Install using `go get -u github.com/klauspost/compress`. The package is located in `github.com/klauspost/compress/zstd`.\n+\n+Godoc Documentation: https://godoc.org/github.com/klauspost/compress/zstd\n+\n+\n+## Compressor\n+\n+### Status: \n+\n+STABLE - there may always be subtle bugs, a wide variety of content has been tested and the library is actively \n+used by several projects. This library is being continuously [fuzz-tested](https://github.com/klauspost/compress-fuzz),\n+kindly supplied by [fuzzit.dev](https://fuzzit.dev/).\n+\n+There may still be specific combinations of data types/size/settings that could lead to edge cases, \n+so as always, testing is recommended.  \n+\n+For now, a high speed (fastest) and medium-fast (default) compressor has been implemented. \n+\n+The \"Fastest\" compression ratio is roughly equivalent to zstd level 1. \n+The \"Default\" compression ratio is roughly equivalent to zstd level 3 (default).\n+\n+In terms of speed, it is typically 2x as fast as the stdlib deflate/gzip in its fastest mode. \n+The compression ratio compared to stdlib is around level 3, but usually 3x as fast.\n+\n+Compared to cgo zstd, the speed is around level 3 (default), but compression slightly worse, between level 1&2.\n+\n+ \n+### Usage\n+\n+An Encoder can be used for either compressing a stream via the\n+`io.WriteCloser` interface supported by the Encoder or as multiple independent\n+tasks via the `EncodeAll` function.\n+Smaller encodes are encouraged to use the EncodeAll function.\n+Use `NewWriter` to create a new instance that can be used for both.\n+\n+To create a writer with default options, do like this:\n+\n+```Go\n+// Compress input to output.\n+func Compress(in io.Reader, out io.Writer) error {\n+    w, err := NewWriter(output)\n+    if err != nil {\n+        return err\n+    }\n+    _, err := io.Copy(w, input)\n+    if err != nil {\n+        enc.Close()\n+        return err\n+    }\n+    return enc.Close()\n+}\n+```\n+\n+Now you can encode by writing data to `enc`. The output will be finished writing when `Close()` is called.\n+Even if your encode fails, you should still call `Close()` to release any resources that may be held up.  \n+\n+The above is fine for big encodes. However, whenever possible try to *reuse* the writer.\n+\n+To reuse the encoder, you can use the `Reset(io.Writer)` function to change to another output. \n+This will allow the encoder to reuse all resources and avoid wasteful allocations. \n+\n+Currently stream encoding has 'light' concurrency, meaning up to 2 goroutines can be working on part \n+of a stream. This is independent of the `WithEncoderConcurrency(n)`, but that is likely to change \n+in the future. So if you want to limit concurrency for future updates, specify the concurrency\n+you would like.\n+\n+You can specify your desired compression level using `WithEncoderLevel()` option. Currently only pre-defined \n+compression settings can be specified.\n+\n+#### Future Compatibility Guarantees\n+\n+This will be an evolving project. When using this package it is important to note that both the compression efficiency and speed may change.\n+\n+The goal will be to keep the default efficiency at the default zstd (level 3). \n+However the encoding should never be assumed to remain the same, \n+and you should not use hashes of compressed output for similarity checks.\n+\n+The Encoder can be assumed to produce the same output from the exact same code version.\n+However, the may be modes in the future that break this, \n+although they will not be enabled without an explicit option.   \n+\n+This encoder is not designed to (and will probably never) output the exact same bitstream as the reference encoder.\n+\n+Also note, that the cgo decompressor currently does not [report all errors on invalid input](https://github.com/DataDog/zstd/issues/59),\n+[omits error checks](https://github.com/DataDog/zstd/issues/61), [ignores checksums](https://github.com/DataDog/zstd/issues/43) \n+and seems to ignore concatenated streams, even though [it is part of the spec](https://github.com/facebook/zstd/blob/dev/doc/zstd_compression_format.md#frames).\n+\n+#### Blocks\n+\n+For compressing small blocks, the returned encoder has a function called `EncodeAll(src, dst []byte) []byte`.\n+\n+`EncodeAll` will encode all input in src and append it to dst.\n+This function can be called concurrently, but each call will only run on a single goroutine.\n+\n+Encoded blocks can be concatenated and the result will be the combined input stream.\n+Data compressed with EncodeAll can be decoded with the Decoder, using either a stream or `DecodeAll`.\n+\n+Especially when encoding blocks you should take special care to reuse the encoder. \n+This will effectively make it run without allocations after a warmup period. \n+To make it run completely without allocations, supply a destination buffer with space for all content.   \n+\n+```Go\n+import \"github.com/klauspost/compress/zstd\"\n+\n+// Create a writer that caches compressors.\n+// For this operation type we supply a nil Reader.\n+var encoder, _ = zstd.NewWriter(nil)\n+\n+// Compress a buffer. \n+// If you have a destination buffer, the allocation in the call can also be eliminated.\n+func Compress(src []byte) []byte {\n+    return encoder.EncodeAll(src, make([]byte, 0, len(src)))\n+} \n+```\n+\n+You can control the maximum number of concurrent encodes using the `WithEncoderConcurrency(n)` \n+option when creating the writer.\n+\n+Using the Encoder for both a stream and individual blocks concurrently is safe. \n+\n+### Performance\n+\n+I have collected some speed examples to compare speed and compression against other compressors.\n+\n+* `file` is the input file.\n+* `out` is the compressor used. `zskp` is this package. `gzstd` is gzip standard library. `zstd` is the Datadog cgo library.\n+* `level` is the compression level used. For `zskp` level 1 is \"fastest\", level 2 is \"default\".\n+* `insize`/`outsize` is the input/output size.\n+* `millis` is the number of milliseconds used for compression.\n+* `mb/s` is megabytes (2^20 bytes) per second.\n+\n+```\n+The test data for the Large Text Compression Benchmark is the first\n+10^9 bytes of the English Wikipedia dump on Mar. 3, 2006.\n+http://mattmahoney.net/dc/textdata.html\n+\n+file    out     level   insize  outsize     millis  mb/s\n+enwik9  zskp    1   1000000000  343833033   5840    163.30\n+enwik9  zskp    2   1000000000  317822183   8449    112.87\n+enwik9  gzstd   1   1000000000  382578136   13627   69.98\n+enwik9  gzstd   3   1000000000  349139651   22344   42.68\n+enwik9  zstd    1   1000000000  357416379   4838    197.12\n+enwik9  zstd    3   1000000000  313734522   7556    126.21\n+\n+GOB stream of binary data. Highly compressible.\n+https://files.klauspost.com/compress/gob-stream.7z\n+\n+file        out level   insize      outsize     millis  mb/s\n+gob-stream  zskp    1   1911399616  234981983   5100    357.42\n+gob-stream  zskp    2   1911399616  208674003   6698    272.15\n+gob-stream  gzstd   1   1911399616  357382641   14727   123.78\n+gob-stream  gzstd   3   1911399616  327835097   17005   107.19\n+gob-stream  zstd    1   1911399616  250787165   4075    447.22\n+gob-stream  zstd    3   1911399616  208191888   5511    330.77\n+\n+Highly compressible JSON file. Similar to logs in a lot of ways.\n+https://files.klauspost.com/compress/adresser.001.gz\n+\n+file            out level   insize      outsize     millis  mb/s\n+adresser.001    zskp    1   1073741824  18510122    1477    692.83\n+adresser.001    zskp    2   1073741824  19831697    1705    600.59\n+adresser.001    gzstd   1   1073741824  47755503    3079    332.47\n+adresser.001    gzstd   3   1073741824  40052381    3051    335.63\n+adresser.001    zstd    1   1073741824  16135896    994     1030.18\n+adresser.001    zstd    3   1073741824  17794465    905     1131.49\n+\n+VM Image, Linux mint with a few installed applications:\n+https://files.klauspost.com/compress/rawstudio-mint14.7z\n+\n+file    out level   insize  outsize millis  mb/s\n+rawstudio-mint14.tar    zskp    1   8558382592  3648168838  33398   244.38\n+rawstudio-mint14.tar    zskp    2   8558382592  3376721436  50962   160.16\n+rawstudio-mint14.tar    gzstd   1   8558382592  3926257486  84712   96.35\n+rawstudio-mint14.tar    gzstd   3   8558382592  3740711978  176344  46.28\n+rawstudio-mint14.tar    zstd    1   8558382592  3607859742  27903   292.51\n+rawstudio-mint14.tar    zstd    3   8558382592  3341710879  46700   174.77\n+\n+\n+The test data is designed to test archivers in realistic backup scenarios.\n+http://mattmahoney.net/dc/10gb.html\n+\n+file    out level   insize  outsize millis  mb/s\n+10gb.tar    zskp    1   10065157632 4883149814  45715   209.97\n+10gb.tar    zskp    2   10065157632 4638110010  60970   157.44\n+10gb.tar    gzstd   1   10065157632 5198296126  97769   98.18\n+10gb.tar    gzstd   3   10065157632 4932665487  313427  30.63\n+10gb.tar    zstd    1   10065157632 4940796535  40391   237.65\n+10gb.tar    zstd    3   10065157632 4638618579  52911   181.42\n+\n+Silesia Corpus:\n+http://sun.aei.polsl.pl/~sdeor/corpus/silesia.zip\n+\n+file    out level   insize  outsize millis  mb/s\n+silesia.tar zskp    1   211947520   73025800    1108    182.26\n+silesia.tar zskp    2   211947520   67674684    1599    126.41\n+silesia.tar gzstd   1   211947520   80007735    2515    80.37\n+silesia.tar gzstd   3   211947520   73133380    4259    47.45\n+silesia.tar zstd    1   211947520   73513991    933     216.64\n+silesia.tar zstd    3   211947520   66793301    1377    146.79\n+```\n+\n+### Converters\n+\n+As part of the development process a *Snappy* -> *Zstandard* converter was also built.\n+\n+This can convert a *framed* [Snappy Stream](https://godoc.org/github.com/golang/snappy#Writer) to a zstd stream. \n+Note that a single block is not framed.\n+\n+Conversion is done by converting the stream directly from Snappy without intermediate full decoding.\n+Therefore the compression ratio is much less than what can be done by a full decompression\n+and compression, and a faulty Snappy stream may lead to a faulty Zstandard stream without\n+any errors being generated.\n+No CRC value is being generated and not all CRC values of the Snappy stream are checked.\n+However, it provides really fast re-compression of Snappy streams.\n+\n+\n+```\n+BenchmarkSnappy_ConvertSilesia-8           1  1156001600 ns/op   183.35 MB/s\n+Snappy len 103008711 -> zstd len 82687318\n+\n+BenchmarkSnappy_Enwik9-8           1  6472998400 ns/op   154.49 MB/s\n+Snappy len 508028601 -> zstd len 390921079\n+```\n+\n+\n+```Go\n+    s := zstd.SnappyConverter{}\n+    n, err = s.Convert(input, output)\n+    if err != nil {\n+        fmt.Println(\"Re-compressed stream to\", n, \"bytes\")\n+    }\n+```\n+\n+The converter `s` can be reused to avoid allocations, even after errors.\n+\n+\n+## Decompressor\n+\n+Staus: STABLE - there may still be subtle bugs, but a wide variety of content has been tested.\n+\n+This library is being continuously [fuzz-tested](https://github.com/klauspost/compress-fuzz),\n+kindly supplied by [fuzzit.dev](https://fuzzit.dev/). \n+The main purpose of the fuzz testing is to ensure that it is not possible to crash the decoder, \n+or run it past its limits with ANY input provided.  \n+ \n+### Usage\n+\n+The package has been designed for two main usages, big streams of data and smaller in-memory buffers. \n+There are two main usages of the package for these. Both of them are accessed by creating a `Decoder`.\n+\n+For streaming use a simple setup could look like this:\n+\n+```Go\n+import \"github.com/klauspost/compress/zstd\"\n+\n+func Decompress(in io.Reader, out io.Writer) error {\n+    d, err := zstd.NewReader(input)\n+    if err != nil {\n+        return err\n+    }\n+    defer d.Close()\n+    \n+    // Copy content...\n+    _, err := io.Copy(out, d)\n+    return err\n+}\n+```\n+\n+It is important to use the \"Close\" function when you no longer need the Reader to stop running goroutines. \n+See \"Allocation-less operation\" below.\n+\n+For decoding buffers, it could look something like this:\n+\n+```Go\n+import \"github.com/klauspost/compress/zstd\"\n+\n+// Create a reader that caches decompressors.\n+// For this operation type we supply a nil Reader.\n+var decoder, _ = zstd.NewReader(nil)\n+\n+// Decompress a buffer. We don't supply a destination buffer,\n+// so it will be allocated by the decoder.\n+func Decompress(src []byte) ([]byte, error) {\n+    return decoder.DecodeAll(src, nil)\n+} \n+```\n+\n+Both of these cases should provide the functionality needed. \n+The decoder can be used for *concurrent* decompression of multiple buffers. \n+It will only allow a certain number of concurrent operations to run. \n+To tweak that yourself use the `WithDecoderConcurrency(n)` option when creating the decoder.   \n+\n+### Allocation-less operation\n+\n+The decoder has been designed to operate without allocations after a warmup. \n+\n+This means that you should *store* the decoder for best performance. \n+To re-use a stream decoder, use the `Reset(r io.Reader) error` to switch to another stream.\n+A decoder can safely be re-used even if the previous stream failed.\n+\n+To release the resources, you must call the `Close()` function on a decoder.\n+After this it can *no longer be reused*, but all running goroutines will be stopped.\n+So you *must* use this if you will no longer need the Reader.\n+\n+For decompressing smaller buffers a single decoder can be used.\n+When decoding buffers, you can supply a destination slice with length 0 and your expected capacity.\n+In this case no unneeded allocations should be made. \n+\n+### Concurrency\n+\n+The buffer decoder does everything on the same goroutine and does nothing concurrently.\n+It can however decode several buffers concurrently. Use `WithDecoderConcurrency(n)` to limit that.\n+\n+The stream decoder operates on\n+\n+* One goroutine reads input and splits the input to several block decoders.\n+* A number of decoders will decode blocks.\n+* A goroutine coordinates these blocks and sends history from one to the next.\n+\n+So effectively this also means the decoder will \"read ahead\" and prepare data to always be available for output.\n+\n+Since \"blocks\" are quite dependent on the output of the previous block stream decoding will only have limited concurrency.\n+\n+In practice this means that concurrency is often limited to utilizing about 2 cores effectively.\n+ \n+ \n+### Benchmarks\n+\n+These are some examples of performance compared to [datadog cgo library](https://github.com/DataDog/zstd).\n+\n+The first two are streaming decodes and the last are smaller inputs. \n+ \n+```\n+BenchmarkDecoderSilesia-8             20       642550210 ns/op   329.85 MB/s      3101 B/op        8 allocs/op\n+BenchmarkDecoderSilesiaCgo-8         100       384930000 ns/op   550.61 MB/s    451878 B/op     9713 allocs/op\n+\n+BenchmarkDecoderEnwik9-2              10        3146000080 ns/op         317.86 MB/s        2649 B/op          9 allocs/op\n+BenchmarkDecoderEnwik9Cgo-2           20        1905900000 ns/op         524.69 MB/s     1125120 B/op      45785 allocs/op\n+\n+BenchmarkDecoder_DecodeAll/z000000.zst-8               200     7049994 ns/op   138.26 MB/s        40 B/op        2 allocs/op\n+BenchmarkDecoder_DecodeAll/z000001.zst-8            100000       19560 ns/op    97.49 MB/s        40 B/op        2 allocs/op\n+BenchmarkDecoder_DecodeAll/z000002.zst-8              5000      297599 ns/op   236.99 MB/s        40 B/op        2 allocs/op\n+BenchmarkDecoder_DecodeAll/z000003.zst-8              2000      725502 ns/op   141.17 MB/s        40 B/op        2 allocs/op\n+BenchmarkDecoder_DecodeAll/z000004.zst-8            200000        9314 ns/op    54.54 MB/s        40 B/op        2 allocs/op\n+BenchmarkDecoder_DecodeAll/z000005.zst-8             10000      137500 ns/op   104.72 MB/s        40 B/op        2 allocs/op\n+BenchmarkDecoder_DecodeAll/z000006.zst-8               500     2316009 ns/op   206.06 MB/s        40 B/op        2 allocs/op\n+BenchmarkDecoder_DecodeAll/z000007.zst-8             20000       64499 ns/op   344.90 MB/s        40 B/op        2 allocs/op\n+BenchmarkDecoder_DecodeAll/z000008.zst-8             50000       24900 ns/op   219.56 MB/s        40 B/op        2 allocs/op\n+BenchmarkDecoder_DecodeAll/z000009.zst-8              1000     2348999 ns/op   154.01 MB/s        40 B/op        2 allocs/op\n+\n+BenchmarkDecoder_DecodeAllCgo/z000000.zst-8            500     4268005 ns/op   228.38 MB/s   1228849 B/op        3 allocs/op\n+BenchmarkDecoder_DecodeAllCgo/z000001.zst-8         100000       15250 ns/op   125.05 MB/s      2096 B/op        3 allocs/op\n+BenchmarkDecoder_DecodeAllCgo/z000002.zst-8          10000      147399 ns/op   478.49 MB/s     73776 B/op        3 allocs/op\n+BenchmarkDecoder_DecodeAllCgo/z000003.zst-8           5000      320798 ns/op   319.27 MB/s    139312 B/op        3 allocs/op\n+BenchmarkDecoder_DecodeAllCgo/z000004.zst-8         200000       10004 ns/op    50.77 MB/s       560 B/op        3 allocs/op\n+BenchmarkDecoder_DecodeAllCgo/z000005.zst-8          20000       73599 ns/op   195.64 MB/s     19120 B/op        3 allocs/op\n+BenchmarkDecoder_DecodeAllCgo/z000006.zst-8           1000     1119003 ns/op   426.48 MB/s    557104 B/op        3 allocs/op\n+BenchmarkDecoder_DecodeAllCgo/z000007.zst-8          20000      103450 ns/op   215.04 MB/s     71296 B/op        9 allocs/op\n+BenchmarkDecoder_DecodeAllCgo/z000008.zst-8         100000       20130 ns/op   271.58 MB/s      6192 B/op        3 allocs/op\n+BenchmarkDecoder_DecodeAllCgo/z000009.zst-8           2000     1123500 ns/op   322.00 MB/s    368688 B/op        3 allocs/op\n+```\n+\n+This reflects the performance around May 2019, but this may be out of date.\n+\n+# Contributions\n+\n+Contributions are always welcome. \n+For new features/fixes, remember to add tests and for performance enhancements include benchmarks.\n+\n+For sending files for reproducing errors use a service like [goobox](https://goobox.io/#/upload) or similar to share your files.\n+\n+For general feedback and experience reports, feel free to open an issue or write me on [Twitter](https://twitter.com/sh0dan).\n+\n+This package includes the excellent [`github.com/cespare/xxhash`](https://github.com/cespare/xxhash) package Copyright (c) 2016 Caleb Spare."
    },
    {
      "sha": "15d79d439fa92ed8abedf06225b1c2cd6d84fdef",
      "filename": "backend/vendor/github.com/klauspost/compress/zstd/bitreader.go",
      "status": "added",
      "additions": 121,
      "deletions": 0,
      "changes": 121,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/klauspost/compress/zstd/bitreader.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/klauspost/compress/zstd/bitreader.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/github.com/klauspost/compress/zstd/bitreader.go?ref=890d7154e777d0673defcd11db3ef792473d68c9",
      "patch": "@@ -0,0 +1,121 @@\n+// Copyright 2019+ Klaus Post. All rights reserved.\n+// License information can be found in the LICENSE file.\n+// Based on work by Yann Collet, released under BSD License.\n+\n+package zstd\n+\n+import (\n+\t\"errors\"\n+\t\"io\"\n+\t\"math/bits\"\n+)\n+\n+// bitReader reads a bitstream in reverse.\n+// The last set bit indicates the start of the stream and is used\n+// for aligning the input.\n+type bitReader struct {\n+\tin       []byte\n+\toff      uint   // next byte to read is at in[off - 1]\n+\tvalue    uint64 // Maybe use [16]byte, but shifting is awkward.\n+\tbitsRead uint8\n+}\n+\n+// init initializes and resets the bit reader.\n+func (b *bitReader) init(in []byte) error {\n+\tif len(in) < 1 {\n+\t\treturn errors.New(\"corrupt stream: too short\")\n+\t}\n+\tb.in = in\n+\tb.off = uint(len(in))\n+\t// The highest bit of the last byte indicates where to start\n+\tv := in[len(in)-1]\n+\tif v == 0 {\n+\t\treturn errors.New(\"corrupt stream, did not find end of stream\")\n+\t}\n+\tb.bitsRead = 64\n+\tb.value = 0\n+\tb.fill()\n+\tb.fill()\n+\tb.bitsRead += 8 - uint8(highBits(uint32(v)))\n+\treturn nil\n+}\n+\n+// getBits will return n bits. n can be 0.\n+func (b *bitReader) getBits(n uint8) int {\n+\tif n == 0 /*|| b.bitsRead >= 64 */ {\n+\t\treturn 0\n+\t}\n+\treturn b.getBitsFast(n)\n+}\n+\n+// getBitsFast requires that at least one bit is requested every time.\n+// There are no checks if the buffer is filled.\n+func (b *bitReader) getBitsFast(n uint8) int {\n+\tconst regMask = 64 - 1\n+\tv := uint32((b.value << (b.bitsRead & regMask)) >> ((regMask + 1 - n) & regMask))\n+\tb.bitsRead += n\n+\treturn int(v)\n+}\n+\n+// fillFast() will make sure at least 32 bits are available.\n+// There must be at least 4 bytes available.\n+func (b *bitReader) fillFast() {\n+\tif b.bitsRead < 32 {\n+\t\treturn\n+\t}\n+\t// Do single re-slice to avoid bounds checks.\n+\tv := b.in[b.off-4 : b.off]\n+\tlow := (uint32(v[0])) | (uint32(v[1]) << 8) | (uint32(v[2]) << 16) | (uint32(v[3]) << 24)\n+\tb.value = (b.value << 32) | uint64(low)\n+\tb.bitsRead -= 32\n+\tb.off -= 4\n+}\n+\n+// fill() will make sure at least 32 bits are available.\n+func (b *bitReader) fill() {\n+\tif b.bitsRead < 32 {\n+\t\treturn\n+\t}\n+\tif b.off >= 4 {\n+\t\tv := b.in[b.off-4 : b.off]\n+\t\tlow := (uint32(v[0])) | (uint32(v[1]) << 8) | (uint32(v[2]) << 16) | (uint32(v[3]) << 24)\n+\t\tb.value = (b.value << 32) | uint64(low)\n+\t\tb.bitsRead -= 32\n+\t\tb.off -= 4\n+\t\treturn\n+\t}\n+\tfor b.off > 0 {\n+\t\tb.value = (b.value << 8) | uint64(b.in[b.off-1])\n+\t\tb.bitsRead -= 8\n+\t\tb.off--\n+\t}\n+}\n+\n+// finished returns true if all bits have been read from the bit stream.\n+func (b *bitReader) finished() bool {\n+\treturn b.off == 0 && b.bitsRead >= 64\n+}\n+\n+// overread returns true if more bits have been requested than is on the stream.\n+func (b *bitReader) overread() bool {\n+\treturn b.bitsRead > 64\n+}\n+\n+// remain returns the number of bits remaining.\n+func (b *bitReader) remain() uint {\n+\treturn b.off*8 + 64 - uint(b.bitsRead)\n+}\n+\n+// close the bitstream and returns an error if out-of-buffer reads occurred.\n+func (b *bitReader) close() error {\n+\t// Release reference.\n+\tb.in = nil\n+\tif b.bitsRead > 64 {\n+\t\treturn io.ErrUnexpectedEOF\n+\t}\n+\treturn nil\n+}\n+\n+func highBits(val uint32) (n uint32) {\n+\treturn uint32(bits.Len32(val) - 1)\n+}"
    },
    {
      "sha": "303ae90f944736ec8f9dec70c8cd55e6c351a789",
      "filename": "backend/vendor/github.com/klauspost/compress/zstd/bitwriter.go",
      "status": "added",
      "additions": 169,
      "deletions": 0,
      "changes": 169,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/klauspost/compress/zstd/bitwriter.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/klauspost/compress/zstd/bitwriter.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/github.com/klauspost/compress/zstd/bitwriter.go?ref=890d7154e777d0673defcd11db3ef792473d68c9",
      "patch": "@@ -0,0 +1,169 @@\n+// Copyright 2018 Klaus Post. All rights reserved.\n+// Use of this source code is governed by a BSD-style\n+// license that can be found in the LICENSE file.\n+// Based on work Copyright (c) 2013, Yann Collet, released under BSD License.\n+\n+package zstd\n+\n+import \"fmt\"\n+\n+// bitWriter will write bits.\n+// First bit will be LSB of the first byte of output.\n+type bitWriter struct {\n+\tbitContainer uint64\n+\tnBits        uint8\n+\tout          []byte\n+}\n+\n+// bitMask16 is bitmasks. Has extra to avoid bounds check.\n+var bitMask16 = [32]uint16{\n+\t0, 1, 3, 7, 0xF, 0x1F,\n+\t0x3F, 0x7F, 0xFF, 0x1FF, 0x3FF, 0x7FF,\n+\t0xFFF, 0x1FFF, 0x3FFF, 0x7FFF, 0xFFFF, 0xFFFF,\n+\t0xFFFF, 0xFFFF, 0xFFFF, 0xFFFF, 0xFFFF, 0xFFFF,\n+\t0xFFFF, 0xFFFF} /* up to 16 bits */\n+\n+var bitMask32 = [32]uint32{\n+\t0, 1, 3, 7, 0xF, 0x1F, 0x3F, 0x7F, 0xFF,\n+\t0x1FF, 0x3FF, 0x7FF, 0xFFF, 0x1FFF, 0x3FFF, 0x7FFF, 0xFFFF,\n+\t0x1ffff, 0x3ffff, 0x7FFFF, 0xfFFFF, 0x1fFFFF, 0x3fFFFF, 0x7fFFFF, 0xffFFFF,\n+\t0x1ffFFFF, 0x3ffFFFF, 0x7ffFFFF, 0xfffFFFF, 0x1fffFFFF, 0x3fffFFFF, 0x7fffFFFF,\n+} // up to 32 bits\n+\n+// addBits16NC will add up to 16 bits.\n+// It will not check if there is space for them,\n+// so the caller must ensure that it has flushed recently.\n+func (b *bitWriter) addBits16NC(value uint16, bits uint8) {\n+\tb.bitContainer |= uint64(value&bitMask16[bits&31]) << (b.nBits & 63)\n+\tb.nBits += bits\n+}\n+\n+// addBits32NC will add up to 32 bits.\n+// It will not check if there is space for them,\n+// so the caller must ensure that it has flushed recently.\n+func (b *bitWriter) addBits32NC(value uint32, bits uint8) {\n+\tb.bitContainer |= uint64(value&bitMask32[bits&31]) << (b.nBits & 63)\n+\tb.nBits += bits\n+}\n+\n+// addBits16Clean will add up to 16 bits. value may not contain more set bits than indicated.\n+// It will not check if there is space for them, so the caller must ensure that it has flushed recently.\n+func (b *bitWriter) addBits16Clean(value uint16, bits uint8) {\n+\tb.bitContainer |= uint64(value) << (b.nBits & 63)\n+\tb.nBits += bits\n+}\n+\n+// flush will flush all pending full bytes.\n+// There will be at least 56 bits available for writing when this has been called.\n+// Using flush32 is faster, but leaves less space for writing.\n+func (b *bitWriter) flush() {\n+\tv := b.nBits >> 3\n+\tswitch v {\n+\tcase 0:\n+\tcase 1:\n+\t\tb.out = append(b.out,\n+\t\t\tbyte(b.bitContainer),\n+\t\t)\n+\tcase 2:\n+\t\tb.out = append(b.out,\n+\t\t\tbyte(b.bitContainer),\n+\t\t\tbyte(b.bitContainer>>8),\n+\t\t)\n+\tcase 3:\n+\t\tb.out = append(b.out,\n+\t\t\tbyte(b.bitContainer),\n+\t\t\tbyte(b.bitContainer>>8),\n+\t\t\tbyte(b.bitContainer>>16),\n+\t\t)\n+\tcase 4:\n+\t\tb.out = append(b.out,\n+\t\t\tbyte(b.bitContainer),\n+\t\t\tbyte(b.bitContainer>>8),\n+\t\t\tbyte(b.bitContainer>>16),\n+\t\t\tbyte(b.bitContainer>>24),\n+\t\t)\n+\tcase 5:\n+\t\tb.out = append(b.out,\n+\t\t\tbyte(b.bitContainer),\n+\t\t\tbyte(b.bitContainer>>8),\n+\t\t\tbyte(b.bitContainer>>16),\n+\t\t\tbyte(b.bitContainer>>24),\n+\t\t\tbyte(b.bitContainer>>32),\n+\t\t)\n+\tcase 6:\n+\t\tb.out = append(b.out,\n+\t\t\tbyte(b.bitContainer),\n+\t\t\tbyte(b.bitContainer>>8),\n+\t\t\tbyte(b.bitContainer>>16),\n+\t\t\tbyte(b.bitContainer>>24),\n+\t\t\tbyte(b.bitContainer>>32),\n+\t\t\tbyte(b.bitContainer>>40),\n+\t\t)\n+\tcase 7:\n+\t\tb.out = append(b.out,\n+\t\t\tbyte(b.bitContainer),\n+\t\t\tbyte(b.bitContainer>>8),\n+\t\t\tbyte(b.bitContainer>>16),\n+\t\t\tbyte(b.bitContainer>>24),\n+\t\t\tbyte(b.bitContainer>>32),\n+\t\t\tbyte(b.bitContainer>>40),\n+\t\t\tbyte(b.bitContainer>>48),\n+\t\t)\n+\tcase 8:\n+\t\tb.out = append(b.out,\n+\t\t\tbyte(b.bitContainer),\n+\t\t\tbyte(b.bitContainer>>8),\n+\t\t\tbyte(b.bitContainer>>16),\n+\t\t\tbyte(b.bitContainer>>24),\n+\t\t\tbyte(b.bitContainer>>32),\n+\t\t\tbyte(b.bitContainer>>40),\n+\t\t\tbyte(b.bitContainer>>48),\n+\t\t\tbyte(b.bitContainer>>56),\n+\t\t)\n+\tdefault:\n+\t\tpanic(fmt.Errorf(\"bits (%d) > 64\", b.nBits))\n+\t}\n+\tb.bitContainer >>= v << 3\n+\tb.nBits &= 7\n+}\n+\n+// flush32 will flush out, so there are at least 32 bits available for writing.\n+func (b *bitWriter) flush32() {\n+\tif b.nBits < 32 {\n+\t\treturn\n+\t}\n+\tb.out = append(b.out,\n+\t\tbyte(b.bitContainer),\n+\t\tbyte(b.bitContainer>>8),\n+\t\tbyte(b.bitContainer>>16),\n+\t\tbyte(b.bitContainer>>24))\n+\tb.nBits -= 32\n+\tb.bitContainer >>= 32\n+}\n+\n+// flushAlign will flush remaining full bytes and align to next byte boundary.\n+func (b *bitWriter) flushAlign() {\n+\tnbBytes := (b.nBits + 7) >> 3\n+\tfor i := uint8(0); i < nbBytes; i++ {\n+\t\tb.out = append(b.out, byte(b.bitContainer>>(i*8)))\n+\t}\n+\tb.nBits = 0\n+\tb.bitContainer = 0\n+}\n+\n+// close will write the alignment bit and write the final byte(s)\n+// to the output.\n+func (b *bitWriter) close() error {\n+\t// End mark\n+\tb.addBits16Clean(1, 1)\n+\t// flush until next byte.\n+\tb.flushAlign()\n+\treturn nil\n+}\n+\n+// reset and continue writing by appending to out.\n+func (b *bitWriter) reset(out []byte) {\n+\tb.bitContainer = 0\n+\tb.nBits = 0\n+\tb.out = out\n+}"
    },
    {
      "sha": "ed670bcc7ad475e681992809a2516d41054fd41b",
      "filename": "backend/vendor/github.com/klauspost/compress/zstd/blockdec.go",
      "status": "added",
      "additions": 716,
      "deletions": 0,
      "changes": 716,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/klauspost/compress/zstd/blockdec.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/klauspost/compress/zstd/blockdec.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/github.com/klauspost/compress/zstd/blockdec.go?ref=890d7154e777d0673defcd11db3ef792473d68c9",
      "patch": "@@ -0,0 +1,716 @@\n+// Copyright 2019+ Klaus Post. All rights reserved.\n+// License information can be found in the LICENSE file.\n+// Based on work by Yann Collet, released under BSD License.\n+\n+package zstd\n+\n+import (\n+\t\"errors\"\n+\t\"fmt\"\n+\t\"io\"\n+\t\"sync\"\n+\n+\t\"github.com/klauspost/compress/huff0\"\n+\t\"github.com/klauspost/compress/zstd/internal/xxhash\"\n+)\n+\n+type blockType uint8\n+\n+//go:generate stringer -type=blockType,literalsBlockType,seqCompMode,tableIndex\n+\n+const (\n+\tblockTypeRaw blockType = iota\n+\tblockTypeRLE\n+\tblockTypeCompressed\n+\tblockTypeReserved\n+)\n+\n+type literalsBlockType uint8\n+\n+const (\n+\tliteralsBlockRaw literalsBlockType = iota\n+\tliteralsBlockRLE\n+\tliteralsBlockCompressed\n+\tliteralsBlockTreeless\n+)\n+\n+const (\n+\t// maxCompressedBlockSize is the biggest allowed compressed block size (128KB)\n+\tmaxCompressedBlockSize = 128 << 10\n+\n+\t// Maximum possible block size (all Raw+Uncompressed).\n+\tmaxBlockSize = (1 << 21) - 1\n+\n+\t// https://github.com/facebook/zstd/blob/dev/doc/zstd_compression_format.md#literals_section_header\n+\tmaxCompressedLiteralSize = 1 << 18\n+\tmaxRLELiteralSize        = 1 << 20\n+\tmaxMatchLen              = 131074\n+\tmaxSequences             = 0x7f00 + 0xffff\n+\n+\t// We support slightly less than the reference decoder to be able to\n+\t// use ints on 32 bit archs.\n+\tmaxOffsetBits = 30\n+)\n+\n+var (\n+\thuffDecoderPool = sync.Pool{New: func() interface{} {\n+\t\treturn &huff0.Scratch{}\n+\t}}\n+\n+\tfseDecoderPool = sync.Pool{New: func() interface{} {\n+\t\treturn &fseDecoder{}\n+\t}}\n+)\n+\n+type blockDec struct {\n+\t// Raw source data of the block.\n+\tdata        []byte\n+\tdataStorage []byte\n+\n+\t// Destination of the decoded data.\n+\tdst []byte\n+\n+\t// Buffer for literals data.\n+\tliteralBuf []byte\n+\n+\t// Window size of the block.\n+\tWindowSize uint64\n+\tType       blockType\n+\tRLESize    uint32\n+\n+\t// Is this the last block of a frame?\n+\tLast bool\n+\n+\t// Use less memory\n+\tlowMem      bool\n+\thistory     chan *history\n+\tinput       chan struct{}\n+\tresult      chan decodeOutput\n+\tsequenceBuf []seq\n+\ttmp         [4]byte\n+\terr         error\n+\tdecWG       sync.WaitGroup\n+}\n+\n+func (b *blockDec) String() string {\n+\tif b == nil {\n+\t\treturn \"<nil>\"\n+\t}\n+\treturn fmt.Sprintf(\"Steam Size: %d, Type: %v, Last: %t, Window: %d\", len(b.data), b.Type, b.Last, b.WindowSize)\n+}\n+\n+func newBlockDec(lowMem bool) *blockDec {\n+\tb := blockDec{\n+\t\tlowMem:  lowMem,\n+\t\tresult:  make(chan decodeOutput, 1),\n+\t\tinput:   make(chan struct{}, 1),\n+\t\thistory: make(chan *history, 1),\n+\t}\n+\tb.decWG.Add(1)\n+\tgo b.startDecoder()\n+\treturn &b\n+}\n+\n+// reset will reset the block.\n+// Input must be a start of a block and will be at the end of the block when returned.\n+func (b *blockDec) reset(br byteBuffer, windowSize uint64) error {\n+\tb.WindowSize = windowSize\n+\ttmp := br.readSmall(3)\n+\tif tmp == nil {\n+\t\tif debug {\n+\t\t\tprintln(\"Reading block header:\", io.ErrUnexpectedEOF)\n+\t\t}\n+\t\treturn io.ErrUnexpectedEOF\n+\t}\n+\tbh := uint32(tmp[0]) | (uint32(tmp[1]) << 8) | (uint32(tmp[2]) << 16)\n+\tb.Last = bh&1 != 0\n+\tb.Type = blockType((bh >> 1) & 3)\n+\t// find size.\n+\tcSize := int(bh >> 3)\n+\tswitch b.Type {\n+\tcase blockTypeReserved:\n+\t\treturn ErrReservedBlockType\n+\tcase blockTypeRLE:\n+\t\tb.RLESize = uint32(cSize)\n+\t\tcSize = 1\n+\tcase blockTypeCompressed:\n+\t\tif debug {\n+\t\t\tprintln(\"Data size on stream:\", cSize)\n+\t\t}\n+\t\tb.RLESize = 0\n+\t\tif cSize > maxCompressedBlockSize || uint64(cSize) > b.WindowSize {\n+\t\t\tif debug {\n+\t\t\t\tprintf(\"compressed block too big: csize:%d block: %+v\\n\", uint64(cSize), b)\n+\t\t\t}\n+\t\t\treturn ErrCompressedSizeTooBig\n+\t\t}\n+\tdefault:\n+\t\tb.RLESize = 0\n+\t}\n+\n+\t// Read block data.\n+\tif cap(b.dataStorage) < cSize {\n+\t\tif b.lowMem {\n+\t\t\tb.dataStorage = make([]byte, 0, cSize)\n+\t\t} else {\n+\t\t\tb.dataStorage = make([]byte, 0, maxBlockSize)\n+\t\t}\n+\t}\n+\tif cap(b.dst) <= maxBlockSize {\n+\t\tb.dst = make([]byte, 0, maxBlockSize+1)\n+\t}\n+\tvar err error\n+\tb.data, err = br.readBig(cSize, b.dataStorage)\n+\tif err != nil {\n+\t\tif debug {\n+\t\t\tprintln(\"Reading block:\", err, \"(\", cSize, \")\", len(b.data))\n+\t\t\tprintf(\"%T\", br)\n+\t\t}\n+\t\treturn err\n+\t}\n+\treturn nil\n+}\n+\n+// sendEOF will make the decoder send EOF on this frame.\n+func (b *blockDec) sendErr(err error) {\n+\tb.Last = true\n+\tb.Type = blockTypeReserved\n+\tb.err = err\n+\tb.input <- struct{}{}\n+}\n+\n+// Close will release resources.\n+// Closed blockDec cannot be reset.\n+func (b *blockDec) Close() {\n+\tclose(b.input)\n+\tclose(b.history)\n+\tclose(b.result)\n+\tb.decWG.Wait()\n+}\n+\n+// decodeAsync will prepare decoding the block when it receives input.\n+// This will separate output and history.\n+func (b *blockDec) startDecoder() {\n+\tdefer b.decWG.Done()\n+\tfor range b.input {\n+\t\t//println(\"blockDec: Got block input\")\n+\t\tswitch b.Type {\n+\t\tcase blockTypeRLE:\n+\t\t\tif cap(b.dst) < int(b.RLESize) {\n+\t\t\t\tif b.lowMem {\n+\t\t\t\t\tb.dst = make([]byte, b.RLESize)\n+\t\t\t\t} else {\n+\t\t\t\t\tb.dst = make([]byte, maxBlockSize)\n+\t\t\t\t}\n+\t\t\t}\n+\t\t\to := decodeOutput{\n+\t\t\t\td:   b,\n+\t\t\t\tb:   b.dst[:b.RLESize],\n+\t\t\t\terr: nil,\n+\t\t\t}\n+\t\t\tv := b.data[0]\n+\t\t\tfor i := range o.b {\n+\t\t\t\to.b[i] = v\n+\t\t\t}\n+\t\t\thist := <-b.history\n+\t\t\thist.append(o.b)\n+\t\t\tb.result <- o\n+\t\tcase blockTypeRaw:\n+\t\t\to := decodeOutput{\n+\t\t\t\td:   b,\n+\t\t\t\tb:   b.data,\n+\t\t\t\terr: nil,\n+\t\t\t}\n+\t\t\thist := <-b.history\n+\t\t\thist.append(o.b)\n+\t\t\tb.result <- o\n+\t\tcase blockTypeCompressed:\n+\t\t\tb.dst = b.dst[:0]\n+\t\t\terr := b.decodeCompressed(nil)\n+\t\t\to := decodeOutput{\n+\t\t\t\td:   b,\n+\t\t\t\tb:   b.dst,\n+\t\t\t\terr: err,\n+\t\t\t}\n+\t\t\tif debug {\n+\t\t\t\tprintln(\"Decompressed to\", len(b.dst), \"bytes, error:\", err)\n+\t\t\t}\n+\t\t\tb.result <- o\n+\t\tcase blockTypeReserved:\n+\t\t\t// Used for returning errors.\n+\t\t\t<-b.history\n+\t\t\tb.result <- decodeOutput{\n+\t\t\t\td:   b,\n+\t\t\t\tb:   nil,\n+\t\t\t\terr: b.err,\n+\t\t\t}\n+\t\tdefault:\n+\t\t\tpanic(\"Invalid block type\")\n+\t\t}\n+\t\tif debug {\n+\t\t\tprintln(\"blockDec: Finished block\")\n+\t\t}\n+\t}\n+}\n+\n+// decodeAsync will prepare decoding the block when it receives the history.\n+// If history is provided, it will not fetch it from the channel.\n+func (b *blockDec) decodeBuf(hist *history) error {\n+\tswitch b.Type {\n+\tcase blockTypeRLE:\n+\t\tif cap(b.dst) < int(b.RLESize) {\n+\t\t\tif b.lowMem {\n+\t\t\t\tb.dst = make([]byte, b.RLESize)\n+\t\t\t} else {\n+\t\t\t\tb.dst = make([]byte, maxBlockSize)\n+\t\t\t}\n+\t\t}\n+\t\tb.dst = b.dst[:b.RLESize]\n+\t\tv := b.data[0]\n+\t\tfor i := range b.dst {\n+\t\t\tb.dst[i] = v\n+\t\t}\n+\t\thist.appendKeep(b.dst)\n+\t\treturn nil\n+\tcase blockTypeRaw:\n+\t\thist.appendKeep(b.data)\n+\t\treturn nil\n+\tcase blockTypeCompressed:\n+\t\tsaved := b.dst\n+\t\tb.dst = hist.b\n+\t\thist.b = nil\n+\t\terr := b.decodeCompressed(hist)\n+\t\tif debug {\n+\t\t\tprintln(\"Decompressed to total\", len(b.dst), \"bytes, hash:\", xxhash.Sum64(b.dst), \"error:\", err)\n+\t\t}\n+\t\thist.b = b.dst\n+\t\tb.dst = saved\n+\t\treturn err\n+\tcase blockTypeReserved:\n+\t\t// Used for returning errors.\n+\t\treturn b.err\n+\tdefault:\n+\t\tpanic(\"Invalid block type\")\n+\t}\n+}\n+\n+// decodeCompressed will start decompressing a block.\n+// If no history is supplied the decoder will decodeAsync as much as possible\n+// before fetching from blockDec.history\n+func (b *blockDec) decodeCompressed(hist *history) error {\n+\tin := b.data\n+\tdelayedHistory := hist == nil\n+\n+\tif delayedHistory {\n+\t\t// We must always grab history.\n+\t\tdefer func() {\n+\t\t\tif hist == nil {\n+\t\t\t\t<-b.history\n+\t\t\t}\n+\t\t}()\n+\t}\n+\t// There must be at least one byte for Literals_Block_Type and one for Sequences_Section_Header\n+\tif len(in) < 2 {\n+\t\treturn ErrBlockTooSmall\n+\t}\n+\tlitType := literalsBlockType(in[0] & 3)\n+\tvar litRegenSize int\n+\tvar litCompSize int\n+\tsizeFormat := (in[0] >> 2) & 3\n+\tvar fourStreams bool\n+\tswitch litType {\n+\tcase literalsBlockRaw, literalsBlockRLE:\n+\t\tswitch sizeFormat {\n+\t\tcase 0, 2:\n+\t\t\t// Regenerated_Size uses 5 bits (0-31). Literals_Section_Header uses 1 byte.\n+\t\t\tlitRegenSize = int(in[0] >> 3)\n+\t\t\tin = in[1:]\n+\t\tcase 1:\n+\t\t\t// Regenerated_Size uses 12 bits (0-4095). Literals_Section_Header uses 2 bytes.\n+\t\t\tlitRegenSize = int(in[0]>>4) + (int(in[1]) << 4)\n+\t\t\tin = in[2:]\n+\t\tcase 3:\n+\t\t\t//  Regenerated_Size uses 20 bits (0-1048575). Literals_Section_Header uses 3 bytes.\n+\t\t\tif len(in) < 3 {\n+\t\t\t\tprintln(\"too small: litType:\", litType, \" sizeFormat\", sizeFormat, len(in))\n+\t\t\t\treturn ErrBlockTooSmall\n+\t\t\t}\n+\t\t\tlitRegenSize = int(in[0]>>4) + (int(in[1]) << 4) + (int(in[2]) << 12)\n+\t\t\tin = in[3:]\n+\t\t}\n+\tcase literalsBlockCompressed, literalsBlockTreeless:\n+\t\tswitch sizeFormat {\n+\t\tcase 0, 1:\n+\t\t\t// Both Regenerated_Size and Compressed_Size use 10 bits (0-1023).\n+\t\t\tif len(in) < 3 {\n+\t\t\t\tprintln(\"too small: litType:\", litType, \" sizeFormat\", sizeFormat, len(in))\n+\t\t\t\treturn ErrBlockTooSmall\n+\t\t\t}\n+\t\t\tn := uint64(in[0]>>4) + (uint64(in[1]) << 4) + (uint64(in[2]) << 12)\n+\t\t\tlitRegenSize = int(n & 1023)\n+\t\t\tlitCompSize = int(n >> 10)\n+\t\t\tfourStreams = sizeFormat == 1\n+\t\t\tin = in[3:]\n+\t\tcase 2:\n+\t\t\tfourStreams = true\n+\t\t\tif len(in) < 4 {\n+\t\t\t\tprintln(\"too small: litType:\", litType, \" sizeFormat\", sizeFormat, len(in))\n+\t\t\t\treturn ErrBlockTooSmall\n+\t\t\t}\n+\t\t\tn := uint64(in[0]>>4) + (uint64(in[1]) << 4) + (uint64(in[2]) << 12) + (uint64(in[3]) << 20)\n+\t\t\tlitRegenSize = int(n & 16383)\n+\t\t\tlitCompSize = int(n >> 14)\n+\t\t\tin = in[4:]\n+\t\tcase 3:\n+\t\t\tfourStreams = true\n+\t\t\tif len(in) < 5 {\n+\t\t\t\tprintln(\"too small: litType:\", litType, \" sizeFormat\", sizeFormat, len(in))\n+\t\t\t\treturn ErrBlockTooSmall\n+\t\t\t}\n+\t\t\tn := uint64(in[0]>>4) + (uint64(in[1]) << 4) + (uint64(in[2]) << 12) + (uint64(in[3]) << 20) + (uint64(in[4]) << 28)\n+\t\t\tlitRegenSize = int(n & 262143)\n+\t\t\tlitCompSize = int(n >> 18)\n+\t\t\tin = in[5:]\n+\t\t}\n+\t}\n+\tif debug {\n+\t\tprintln(\"literals type:\", litType, \"litRegenSize:\", litRegenSize, \"litCompSize:\", litCompSize, \"sizeFormat:\", sizeFormat, \"4X:\", fourStreams)\n+\t}\n+\tvar literals []byte\n+\tvar huff *huff0.Scratch\n+\tswitch litType {\n+\tcase literalsBlockRaw:\n+\t\tif len(in) < litRegenSize {\n+\t\t\tprintln(\"too small: litType:\", litType, \" sizeFormat\", sizeFormat, \"remain:\", len(in), \"want:\", litRegenSize)\n+\t\t\treturn ErrBlockTooSmall\n+\t\t}\n+\t\tliterals = in[:litRegenSize]\n+\t\tin = in[litRegenSize:]\n+\t\t//printf(\"Found %d uncompressed literals\\n\", litRegenSize)\n+\tcase literalsBlockRLE:\n+\t\tif len(in) < 1 {\n+\t\t\tprintln(\"too small: litType:\", litType, \" sizeFormat\", sizeFormat, \"remain:\", len(in), \"want:\", 1)\n+\t\t\treturn ErrBlockTooSmall\n+\t\t}\n+\t\tif cap(b.literalBuf) < litRegenSize {\n+\t\t\tif b.lowMem {\n+\t\t\t\tb.literalBuf = make([]byte, litRegenSize)\n+\t\t\t} else {\n+\t\t\t\tif litRegenSize > maxCompressedLiteralSize {\n+\t\t\t\t\t// Exceptional\n+\t\t\t\t\tb.literalBuf = make([]byte, litRegenSize)\n+\t\t\t\t} else {\n+\t\t\t\t\tb.literalBuf = make([]byte, litRegenSize, maxCompressedLiteralSize)\n+\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t\tliterals = b.literalBuf[:litRegenSize]\n+\t\tv := in[0]\n+\t\tfor i := range literals {\n+\t\t\tliterals[i] = v\n+\t\t}\n+\t\tin = in[1:]\n+\t\tif debug {\n+\t\t\tprintf(\"Found %d RLE compressed literals\\n\", litRegenSize)\n+\t\t}\n+\tcase literalsBlockTreeless:\n+\t\tif len(in) < litCompSize {\n+\t\t\tprintln(\"too small: litType:\", litType, \" sizeFormat\", sizeFormat, \"remain:\", len(in), \"want:\", litCompSize)\n+\t\t\treturn ErrBlockTooSmall\n+\t\t}\n+\t\t// Store compressed literals, so we defer decoding until we get history.\n+\t\tliterals = in[:litCompSize]\n+\t\tin = in[litCompSize:]\n+\t\tif debug {\n+\t\t\tprintf(\"Found %d compressed literals\\n\", litCompSize)\n+\t\t}\n+\tcase literalsBlockCompressed:\n+\t\tif len(in) < litCompSize {\n+\t\t\tprintln(\"too small: litType:\", litType, \" sizeFormat\", sizeFormat, \"remain:\", len(in), \"want:\", litCompSize)\n+\t\t\treturn ErrBlockTooSmall\n+\t\t}\n+\t\tliterals = in[:litCompSize]\n+\t\tin = in[litCompSize:]\n+\t\thuff = huffDecoderPool.Get().(*huff0.Scratch)\n+\t\tvar err error\n+\t\t// Ensure we have space to store it.\n+\t\tif cap(b.literalBuf) < litRegenSize {\n+\t\t\tif b.lowMem {\n+\t\t\t\tb.literalBuf = make([]byte, 0, litRegenSize)\n+\t\t\t} else {\n+\t\t\t\tb.literalBuf = make([]byte, 0, maxCompressedLiteralSize)\n+\t\t\t}\n+\t\t}\n+\t\tif huff == nil {\n+\t\t\thuff = &huff0.Scratch{}\n+\t\t}\n+\t\thuff.Out = b.literalBuf[:0]\n+\t\thuff, literals, err = huff0.ReadTable(literals, huff)\n+\t\tif err != nil {\n+\t\t\tprintln(\"reading huffman table:\", err)\n+\t\t\treturn err\n+\t\t}\n+\t\t// Use our out buffer.\n+\t\thuff.Out = b.literalBuf[:0]\n+\t\thuff.MaxDecodedSize = litRegenSize\n+\t\tif fourStreams {\n+\t\t\tliterals, err = huff.Decompress4X(literals, litRegenSize)\n+\t\t} else {\n+\t\t\tliterals, err = huff.Decompress1X(literals)\n+\t\t}\n+\t\tif err != nil {\n+\t\t\tprintln(\"decoding compressed literals:\", err)\n+\t\t\treturn err\n+\t\t}\n+\t\t// Make sure we don't leak our literals buffer\n+\t\thuff.Out = nil\n+\t\tif len(literals) != litRegenSize {\n+\t\t\treturn fmt.Errorf(\"literal output size mismatch want %d, got %d\", litRegenSize, len(literals))\n+\t\t}\n+\t\tif debug {\n+\t\t\tprintf(\"Decompressed %d literals into %d bytes\\n\", litCompSize, litRegenSize)\n+\t\t}\n+\t}\n+\n+\t// Decode Sequences\n+\t// https://github.com/facebook/zstd/blob/dev/doc/zstd_compression_format.md#sequences-section\n+\tif len(in) < 1 {\n+\t\treturn ErrBlockTooSmall\n+\t}\n+\tseqHeader := in[0]\n+\tnSeqs := 0\n+\tswitch {\n+\tcase seqHeader == 0:\n+\t\tin = in[1:]\n+\tcase seqHeader < 128:\n+\t\tnSeqs = int(seqHeader)\n+\t\tin = in[1:]\n+\tcase seqHeader < 255:\n+\t\tif len(in) < 2 {\n+\t\t\treturn ErrBlockTooSmall\n+\t\t}\n+\t\tnSeqs = int(seqHeader-128)<<8 | int(in[1])\n+\t\tin = in[2:]\n+\tcase seqHeader == 255:\n+\t\tif len(in) < 3 {\n+\t\t\treturn ErrBlockTooSmall\n+\t\t}\n+\t\tnSeqs = 0x7f00 + int(in[1]) + (int(in[2]) << 8)\n+\t\tin = in[3:]\n+\t}\n+\t// Allocate sequences\n+\tif cap(b.sequenceBuf) < nSeqs {\n+\t\tif b.lowMem {\n+\t\t\tb.sequenceBuf = make([]seq, nSeqs)\n+\t\t} else {\n+\t\t\t// Allocate max\n+\t\t\tb.sequenceBuf = make([]seq, nSeqs, maxSequences)\n+\t\t}\n+\t} else {\n+\t\t// Reuse buffer\n+\t\tb.sequenceBuf = b.sequenceBuf[:nSeqs]\n+\t}\n+\tvar seqs = &sequenceDecs{}\n+\tif nSeqs > 0 {\n+\t\tif len(in) < 1 {\n+\t\t\treturn ErrBlockTooSmall\n+\t\t}\n+\t\tbr := byteReader{b: in, off: 0}\n+\t\tcompMode := br.Uint8()\n+\t\tbr.advance(1)\n+\t\tif debug {\n+\t\t\tprintf(\"Compression modes: 0b%b\", compMode)\n+\t\t}\n+\t\tfor i := uint(0); i < 3; i++ {\n+\t\t\tmode := seqCompMode((compMode >> (6 - i*2)) & 3)\n+\t\t\tif debug {\n+\t\t\t\tprintln(\"Table\", tableIndex(i), \"is\", mode)\n+\t\t\t}\n+\t\t\tvar seq *sequenceDec\n+\t\t\tswitch tableIndex(i) {\n+\t\t\tcase tableLiteralLengths:\n+\t\t\t\tseq = &seqs.litLengths\n+\t\t\tcase tableOffsets:\n+\t\t\t\tseq = &seqs.offsets\n+\t\t\tcase tableMatchLengths:\n+\t\t\t\tseq = &seqs.matchLengths\n+\t\t\tdefault:\n+\t\t\t\tpanic(\"unknown table\")\n+\t\t\t}\n+\t\t\tswitch mode {\n+\t\t\tcase compModePredefined:\n+\t\t\t\tseq.fse = &fsePredef[i]\n+\t\t\tcase compModeRLE:\n+\t\t\t\tif br.remain() < 1 {\n+\t\t\t\t\treturn ErrBlockTooSmall\n+\t\t\t\t}\n+\t\t\t\tv := br.Uint8()\n+\t\t\t\tbr.advance(1)\n+\t\t\t\tdec := fseDecoderPool.Get().(*fseDecoder)\n+\t\t\t\tsymb, err := decSymbolValue(v, symbolTableX[i])\n+\t\t\t\tif err != nil {\n+\t\t\t\t\tprintf(\"RLE Transform table (%v) error: %v\", tableIndex(i), err)\n+\t\t\t\t\treturn err\n+\t\t\t\t}\n+\t\t\t\tdec.setRLE(symb)\n+\t\t\t\tseq.fse = dec\n+\t\t\t\tif debug {\n+\t\t\t\t\tprintf(\"RLE set to %+v, code: %v\", symb, v)\n+\t\t\t\t}\n+\t\t\tcase compModeFSE:\n+\t\t\t\tprintln(\"Reading table for\", tableIndex(i))\n+\t\t\t\tdec := fseDecoderPool.Get().(*fseDecoder)\n+\t\t\t\terr := dec.readNCount(&br, uint16(maxTableSymbol[i]))\n+\t\t\t\tif err != nil {\n+\t\t\t\t\tprintln(\"Read table error:\", err)\n+\t\t\t\t\treturn err\n+\t\t\t\t}\n+\t\t\t\terr = dec.transform(symbolTableX[i])\n+\t\t\t\tif err != nil {\n+\t\t\t\t\tprintln(\"Transform table error:\", err)\n+\t\t\t\t\treturn err\n+\t\t\t\t}\n+\t\t\t\tif debug {\n+\t\t\t\t\tprintln(\"Read table ok\", \"symbolLen:\", dec.symbolLen)\n+\t\t\t\t}\n+\t\t\t\tseq.fse = dec\n+\t\t\tcase compModeRepeat:\n+\t\t\t\tseq.repeat = true\n+\t\t\t}\n+\t\t\tif br.overread() {\n+\t\t\t\treturn io.ErrUnexpectedEOF\n+\t\t\t}\n+\t\t}\n+\t\tin = br.unread()\n+\t}\n+\n+\t// Wait for history.\n+\t// All time spent after this is critical since it is strictly sequential.\n+\tif hist == nil {\n+\t\thist = <-b.history\n+\t\tif hist.error {\n+\t\t\treturn ErrDecoderClosed\n+\t\t}\n+\t}\n+\n+\t// Decode treeless literal block.\n+\tif litType == literalsBlockTreeless {\n+\t\t// TODO: We could send the history early WITHOUT the stream history.\n+\t\t//   This would allow decoding treeless literials before the byte history is available.\n+\t\t//   Silencia stats: Treeless 4393, with: 32775, total: 37168, 11% treeless.\n+\t\t//   So not much obvious gain here.\n+\n+\t\tif hist.huffTree == nil {\n+\t\t\treturn errors.New(\"literal block was treeless, but no history was defined\")\n+\t\t}\n+\t\t// Ensure we have space to store it.\n+\t\tif cap(b.literalBuf) < litRegenSize {\n+\t\t\tif b.lowMem {\n+\t\t\t\tb.literalBuf = make([]byte, 0, litRegenSize)\n+\t\t\t} else {\n+\t\t\t\tb.literalBuf = make([]byte, 0, maxCompressedLiteralSize)\n+\t\t\t}\n+\t\t}\n+\t\tvar err error\n+\t\t// Use our out buffer.\n+\t\thuff = hist.huffTree\n+\t\thuff.Out = b.literalBuf[:0]\n+\t\thuff.MaxDecodedSize = litRegenSize\n+\t\tif fourStreams {\n+\t\t\tliterals, err = huff.Decompress4X(literals, litRegenSize)\n+\t\t} else {\n+\t\t\tliterals, err = huff.Decompress1X(literals)\n+\t\t}\n+\t\t// Make sure we don't leak our literals buffer\n+\t\thuff.Out = nil\n+\t\tif err != nil {\n+\t\t\tprintln(\"decompressing literals:\", err)\n+\t\t\treturn err\n+\t\t}\n+\t\tif len(literals) != litRegenSize {\n+\t\t\treturn fmt.Errorf(\"literal output size mismatch want %d, got %d\", litRegenSize, len(literals))\n+\t\t}\n+\t} else {\n+\t\tif hist.huffTree != nil && huff != nil {\n+\t\t\thuffDecoderPool.Put(hist.huffTree)\n+\t\t\thist.huffTree = nil\n+\t\t}\n+\t}\n+\tif huff != nil {\n+\t\thuff.Out = nil\n+\t\thist.huffTree = huff\n+\t}\n+\tif debug {\n+\t\tprintln(\"Final literals:\", len(literals), \"hash:\", xxhash.Sum64(literals), \"and\", nSeqs, \"sequences.\")\n+\t}\n+\n+\tif nSeqs == 0 {\n+\t\t// Decompressed content is defined entirely as Literals Section content.\n+\t\tb.dst = append(b.dst, literals...)\n+\t\tif delayedHistory {\n+\t\t\thist.append(literals)\n+\t\t}\n+\t\treturn nil\n+\t}\n+\n+\tseqs, err := seqs.mergeHistory(&hist.decoders)\n+\tif err != nil {\n+\t\treturn err\n+\t}\n+\tif debug {\n+\t\tprintln(\"History merged ok\")\n+\t}\n+\tbr := &bitReader{}\n+\tif err := br.init(in); err != nil {\n+\t\treturn err\n+\t}\n+\n+\t// TODO: Investigate if sending history without decoders are faster.\n+\t//   This would allow the sequences to be decoded async and only have to construct stream history.\n+\t//   If only recent offsets were not transferred, this would be an obvious win.\n+\t// \t Also, if first 3 sequences don't reference recent offsets, all sequences can be decoded.\n+\n+\tif err := seqs.initialize(br, hist, literals, b.dst); err != nil {\n+\t\tprintln(\"initializing sequences:\", err)\n+\t\treturn err\n+\t}\n+\n+\terr = seqs.decode(nSeqs, br, hist.b)\n+\tif err != nil {\n+\t\treturn err\n+\t}\n+\tif !br.finished() {\n+\t\treturn fmt.Errorf(\"%d extra bits on block, should be 0\", br.remain())\n+\t}\n+\n+\terr = br.close()\n+\tif err != nil {\n+\t\tprintf(\"Closing sequences: %v, %+v\\n\", err, *br)\n+\t}\n+\tif len(b.data) > maxCompressedBlockSize {\n+\t\treturn fmt.Errorf(\"compressed block size too large (%d)\", len(b.data))\n+\t}\n+\t// Set output and release references.\n+\tb.dst = seqs.out\n+\tseqs.out, seqs.literals, seqs.hist = nil, nil, nil\n+\n+\tif !delayedHistory {\n+\t\t// If we don't have delayed history, no need to update.\n+\t\thist.recentOffsets = seqs.prevOffset\n+\t\treturn nil\n+\t}\n+\tif b.Last {\n+\t\t// if last block we don't care about history.\n+\t\tprintln(\"Last block, no history returned\")\n+\t\thist.b = hist.b[:0]\n+\t\treturn nil\n+\t}\n+\thist.append(b.dst)\n+\thist.recentOffsets = seqs.prevOffset\n+\tif debug {\n+\t\tprintln(\"Finished block with literals:\", len(literals), \"and\", nSeqs, \"sequences.\")\n+\t}\n+\n+\treturn nil\n+}"
    },
    {
      "sha": "04c6b638ea2477e25ef94fe3f24de4984e65c4eb",
      "filename": "backend/vendor/github.com/klauspost/compress/zstd/blockenc.go",
      "status": "added",
      "additions": 845,
      "deletions": 0,
      "changes": 845,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/klauspost/compress/zstd/blockenc.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/klauspost/compress/zstd/blockenc.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/github.com/klauspost/compress/zstd/blockenc.go?ref=890d7154e777d0673defcd11db3ef792473d68c9",
      "patch": "@@ -0,0 +1,845 @@\n+// Copyright 2019+ Klaus Post. All rights reserved.\n+// License information can be found in the LICENSE file.\n+// Based on work by Yann Collet, released under BSD License.\n+\n+package zstd\n+\n+import (\n+\t\"errors\"\n+\t\"fmt\"\n+\t\"math\"\n+\t\"math/bits\"\n+\n+\t\"github.com/klauspost/compress/huff0\"\n+)\n+\n+type blockEnc struct {\n+\tsize      int\n+\tliterals  []byte\n+\tsequences []seq\n+\tcoders    seqCoders\n+\tlitEnc    *huff0.Scratch\n+\twr        bitWriter\n+\n+\textraLits int\n+\tlast      bool\n+\n+\toutput            []byte\n+\trecentOffsets     [3]uint32\n+\tprevRecentOffsets [3]uint32\n+}\n+\n+// init should be used once the block has been created.\n+// If called more than once, the effect is the same as calling reset.\n+func (b *blockEnc) init() {\n+\tif cap(b.literals) < maxCompressedLiteralSize {\n+\t\tb.literals = make([]byte, 0, maxCompressedLiteralSize)\n+\t}\n+\tconst defSeqs = 200\n+\tb.literals = b.literals[:0]\n+\tif cap(b.sequences) < defSeqs {\n+\t\tb.sequences = make([]seq, 0, defSeqs)\n+\t}\n+\tif cap(b.output) < maxCompressedBlockSize {\n+\t\tb.output = make([]byte, 0, maxCompressedBlockSize)\n+\t}\n+\tif b.coders.mlEnc == nil {\n+\t\tb.coders.mlEnc = &fseEncoder{}\n+\t\tb.coders.mlPrev = &fseEncoder{}\n+\t\tb.coders.ofEnc = &fseEncoder{}\n+\t\tb.coders.ofPrev = &fseEncoder{}\n+\t\tb.coders.llEnc = &fseEncoder{}\n+\t\tb.coders.llPrev = &fseEncoder{}\n+\t}\n+\tb.litEnc = &huff0.Scratch{WantLogLess: 4}\n+\tb.reset(nil)\n+}\n+\n+// initNewEncode can be used to reset offsets and encoders to the initial state.\n+func (b *blockEnc) initNewEncode() {\n+\tb.recentOffsets = [3]uint32{1, 4, 8}\n+\tb.litEnc.Reuse = huff0.ReusePolicyNone\n+\tb.coders.setPrev(nil, nil, nil)\n+}\n+\n+// reset will reset the block for a new encode, but in the same stream,\n+// meaning that state will be carried over, but the block content is reset.\n+// If a previous block is provided, the recent offsets are carried over.\n+func (b *blockEnc) reset(prev *blockEnc) {\n+\tb.extraLits = 0\n+\tb.literals = b.literals[:0]\n+\tb.size = 0\n+\tb.sequences = b.sequences[:0]\n+\tb.output = b.output[:0]\n+\tb.last = false\n+\tif prev != nil {\n+\t\tb.recentOffsets = prev.prevRecentOffsets\n+\t}\n+}\n+\n+// reset will reset the block for a new encode, but in the same stream,\n+// meaning that state will be carried over, but the block content is reset.\n+// If a previous block is provided, the recent offsets are carried over.\n+func (b *blockEnc) swapEncoders(prev *blockEnc) {\n+\tb.coders.swap(&prev.coders)\n+\tb.litEnc, prev.litEnc = prev.litEnc, b.litEnc\n+}\n+\n+// blockHeader contains the information for a block header.\n+type blockHeader uint32\n+\n+// setLast sets the 'last' indicator on a block.\n+func (h *blockHeader) setLast(b bool) {\n+\tif b {\n+\t\t*h = *h | 1\n+\t} else {\n+\t\tconst mask = (1 << 24) - 2\n+\t\t*h = *h & mask\n+\t}\n+}\n+\n+// setSize will store the compressed size of a block.\n+func (h *blockHeader) setSize(v uint32) {\n+\tconst mask = 7\n+\t*h = (*h)&mask | blockHeader(v<<3)\n+}\n+\n+// setType sets the block type.\n+func (h *blockHeader) setType(t blockType) {\n+\tconst mask = 1 | (((1 << 24) - 1) ^ 7)\n+\t*h = (*h & mask) | blockHeader(t<<1)\n+}\n+\n+// appendTo will append the block header to a slice.\n+func (h blockHeader) appendTo(b []byte) []byte {\n+\treturn append(b, uint8(h), uint8(h>>8), uint8(h>>16))\n+}\n+\n+// String returns a string representation of the block.\n+func (h blockHeader) String() string {\n+\treturn fmt.Sprintf(\"Type: %d, Size: %d, Last:%t\", (h>>1)&3, h>>3, h&1 == 1)\n+}\n+\n+// literalsHeader contains literals header information.\n+type literalsHeader uint64\n+\n+// setType can be used to set the type of literal block.\n+func (h *literalsHeader) setType(t literalsBlockType) {\n+\tconst mask = math.MaxUint64 - 3\n+\t*h = (*h & mask) | literalsHeader(t)\n+}\n+\n+// setSize can be used to set a single size, for uncompressed and RLE content.\n+func (h *literalsHeader) setSize(regenLen int) {\n+\tinBits := bits.Len32(uint32(regenLen))\n+\t// Only retain 2 bits\n+\tconst mask = 3\n+\tlh := uint64(*h & mask)\n+\tswitch {\n+\tcase inBits < 5:\n+\t\tlh |= (uint64(regenLen) << 3) | (1 << 60)\n+\t\tif debug {\n+\t\t\tgot := int(lh>>3) & 0xff\n+\t\t\tif got != regenLen {\n+\t\t\t\tpanic(fmt.Sprint(\"litRegenSize = \", regenLen, \"(want) != \", got, \"(got)\"))\n+\t\t\t}\n+\t\t}\n+\tcase inBits < 12:\n+\t\tlh |= (1 << 2) | (uint64(regenLen) << 4) | (2 << 60)\n+\tcase inBits < 20:\n+\t\tlh |= (3 << 2) | (uint64(regenLen) << 4) | (3 << 60)\n+\tdefault:\n+\t\tpanic(fmt.Errorf(\"internal error: block too big (%d)\", regenLen))\n+\t}\n+\t*h = literalsHeader(lh)\n+}\n+\n+// setSizes will set the size of a compressed literals section and the input length.\n+func (h *literalsHeader) setSizes(compLen, inLen int, single bool) {\n+\tcompBits, inBits := bits.Len32(uint32(compLen)), bits.Len32(uint32(inLen))\n+\t// Only retain 2 bits\n+\tconst mask = 3\n+\tlh := uint64(*h & mask)\n+\tswitch {\n+\tcase compBits <= 10 && inBits <= 10:\n+\t\tif !single {\n+\t\t\tlh |= 1 << 2\n+\t\t}\n+\t\tlh |= (uint64(inLen) << 4) | (uint64(compLen) << (10 + 4)) | (3 << 60)\n+\t\tif debug {\n+\t\t\tconst mmask = (1 << 24) - 1\n+\t\t\tn := (lh >> 4) & mmask\n+\t\t\tif int(n&1023) != inLen {\n+\t\t\t\tpanic(fmt.Sprint(\"regensize:\", int(n&1023), \"!=\", inLen, inBits))\n+\t\t\t}\n+\t\t\tif int(n>>10) != compLen {\n+\t\t\t\tpanic(fmt.Sprint(\"compsize:\", int(n>>10), \"!=\", compLen, compBits))\n+\t\t\t}\n+\t\t}\n+\tcase compBits <= 14 && inBits <= 14:\n+\t\tlh |= (2 << 2) | (uint64(inLen) << 4) | (uint64(compLen) << (14 + 4)) | (4 << 60)\n+\t\tif single {\n+\t\t\tpanic(\"single stream used with more than 10 bits length.\")\n+\t\t}\n+\tcase compBits <= 18 && inBits <= 18:\n+\t\tlh |= (3 << 2) | (uint64(inLen) << 4) | (uint64(compLen) << (18 + 4)) | (5 << 60)\n+\t\tif single {\n+\t\t\tpanic(\"single stream used with more than 10 bits length.\")\n+\t\t}\n+\tdefault:\n+\t\tpanic(\"internal error: block too big\")\n+\t}\n+\t*h = literalsHeader(lh)\n+}\n+\n+// appendTo will append the literals header to a byte slice.\n+func (h literalsHeader) appendTo(b []byte) []byte {\n+\tsize := uint8(h >> 60)\n+\tswitch size {\n+\tcase 1:\n+\t\tb = append(b, uint8(h))\n+\tcase 2:\n+\t\tb = append(b, uint8(h), uint8(h>>8))\n+\tcase 3:\n+\t\tb = append(b, uint8(h), uint8(h>>8), uint8(h>>16))\n+\tcase 4:\n+\t\tb = append(b, uint8(h), uint8(h>>8), uint8(h>>16), uint8(h>>24))\n+\tcase 5:\n+\t\tb = append(b, uint8(h), uint8(h>>8), uint8(h>>16), uint8(h>>24), uint8(h>>32))\n+\tdefault:\n+\t\tpanic(fmt.Errorf(\"internal error: literalsHeader has invalid size (%d)\", size))\n+\t}\n+\treturn b\n+}\n+\n+// size returns the output size with currently set values.\n+func (h literalsHeader) size() int {\n+\treturn int(h >> 60)\n+}\n+\n+func (h literalsHeader) String() string {\n+\treturn fmt.Sprintf(\"Type: %d, SizeFormat: %d, Size: 0x%d, Bytes:%d\", literalsBlockType(h&3), (h>>2)&3, h&((1<<60)-1)>>4, h>>60)\n+}\n+\n+// pushOffsets will push the recent offsets to the backup store.\n+func (b *blockEnc) pushOffsets() {\n+\tb.prevRecentOffsets = b.recentOffsets\n+}\n+\n+// pushOffsets will push the recent offsets to the backup store.\n+func (b *blockEnc) popOffsets() {\n+\tb.recentOffsets = b.prevRecentOffsets\n+}\n+\n+// matchOffset will adjust recent offsets and return the adjusted one,\n+// if it matches a previous offset.\n+func (b *blockEnc) matchOffset(offset, lits uint32) uint32 {\n+\t// Check if offset is one of the recent offsets.\n+\t// Adjusts the output offset accordingly.\n+\t// Gives a tiny bit of compression, typically around 1%.\n+\tif true {\n+\t\tif lits > 0 {\n+\t\t\tswitch offset {\n+\t\t\tcase b.recentOffsets[0]:\n+\t\t\t\toffset = 1\n+\t\t\tcase b.recentOffsets[1]:\n+\t\t\t\tb.recentOffsets[1] = b.recentOffsets[0]\n+\t\t\t\tb.recentOffsets[0] = offset\n+\t\t\t\toffset = 2\n+\t\t\tcase b.recentOffsets[2]:\n+\t\t\t\tb.recentOffsets[2] = b.recentOffsets[1]\n+\t\t\t\tb.recentOffsets[1] = b.recentOffsets[0]\n+\t\t\t\tb.recentOffsets[0] = offset\n+\t\t\t\toffset = 3\n+\t\t\tdefault:\n+\t\t\t\tb.recentOffsets[2] = b.recentOffsets[1]\n+\t\t\t\tb.recentOffsets[1] = b.recentOffsets[0]\n+\t\t\t\tb.recentOffsets[0] = offset\n+\t\t\t\toffset += 3\n+\t\t\t}\n+\t\t} else {\n+\t\t\tswitch offset {\n+\t\t\tcase b.recentOffsets[1]:\n+\t\t\t\tb.recentOffsets[1] = b.recentOffsets[0]\n+\t\t\t\tb.recentOffsets[0] = offset\n+\t\t\t\toffset = 1\n+\t\t\tcase b.recentOffsets[2]:\n+\t\t\t\tb.recentOffsets[2] = b.recentOffsets[1]\n+\t\t\t\tb.recentOffsets[1] = b.recentOffsets[0]\n+\t\t\t\tb.recentOffsets[0] = offset\n+\t\t\t\toffset = 2\n+\t\t\tcase b.recentOffsets[0] - 1:\n+\t\t\t\tb.recentOffsets[2] = b.recentOffsets[1]\n+\t\t\t\tb.recentOffsets[1] = b.recentOffsets[0]\n+\t\t\t\tb.recentOffsets[0] = offset\n+\t\t\t\toffset = 3\n+\t\t\tdefault:\n+\t\t\t\tb.recentOffsets[2] = b.recentOffsets[1]\n+\t\t\t\tb.recentOffsets[1] = b.recentOffsets[0]\n+\t\t\t\tb.recentOffsets[0] = offset\n+\t\t\t\toffset += 3\n+\t\t\t}\n+\t\t}\n+\t} else {\n+\t\toffset += 3\n+\t}\n+\treturn offset\n+}\n+\n+// encodeRaw can be used to set the output to a raw representation of supplied bytes.\n+func (b *blockEnc) encodeRaw(a []byte) {\n+\tvar bh blockHeader\n+\tbh.setLast(b.last)\n+\tbh.setSize(uint32(len(a)))\n+\tbh.setType(blockTypeRaw)\n+\tb.output = bh.appendTo(b.output[:0])\n+\tb.output = append(b.output, a...)\n+\tif debug {\n+\t\tprintln(\"Adding RAW block, length\", len(a))\n+\t}\n+}\n+\n+// encodeRaw can be used to set the output to a raw representation of supplied bytes.\n+func (b *blockEnc) encodeRawTo(dst, src []byte) []byte {\n+\tvar bh blockHeader\n+\tbh.setLast(b.last)\n+\tbh.setSize(uint32(len(src)))\n+\tbh.setType(blockTypeRaw)\n+\tdst = bh.appendTo(dst)\n+\tdst = append(dst, src...)\n+\tif debug {\n+\t\tprintln(\"Adding RAW block, length\", len(src))\n+\t}\n+\treturn dst\n+}\n+\n+// encodeLits can be used if the block is only litLen.\n+func (b *blockEnc) encodeLits(raw bool) error {\n+\tvar bh blockHeader\n+\tbh.setLast(b.last)\n+\tbh.setSize(uint32(len(b.literals)))\n+\n+\t// Don't compress extremely small blocks\n+\tif len(b.literals) < 32 || raw {\n+\t\tif debug {\n+\t\t\tprintln(\"Adding RAW block, length\", len(b.literals))\n+\t\t}\n+\t\tbh.setType(blockTypeRaw)\n+\t\tb.output = bh.appendTo(b.output)\n+\t\tb.output = append(b.output, b.literals...)\n+\t\treturn nil\n+\t}\n+\n+\tvar (\n+\t\tout            []byte\n+\t\treUsed, single bool\n+\t\terr            error\n+\t)\n+\tif len(b.literals) >= 1024 {\n+\t\t// Use 4 Streams.\n+\t\tout, reUsed, err = huff0.Compress4X(b.literals, b.litEnc)\n+\t\tif len(out) > len(b.literals)-len(b.literals)>>4 {\n+\t\t\t// Bail out of compression is too little.\n+\t\t\terr = huff0.ErrIncompressible\n+\t\t}\n+\t} else if len(b.literals) > 32 {\n+\t\t// Use 1 stream\n+\t\tsingle = true\n+\t\tout, reUsed, err = huff0.Compress1X(b.literals, b.litEnc)\n+\t\tif len(out) > len(b.literals)-len(b.literals)>>4 {\n+\t\t\t// Bail out of compression is too little.\n+\t\t\terr = huff0.ErrIncompressible\n+\t\t}\n+\t} else {\n+\t\terr = huff0.ErrIncompressible\n+\t}\n+\n+\tswitch err {\n+\tcase huff0.ErrIncompressible:\n+\t\tif debug {\n+\t\t\tprintln(\"Adding RAW block, length\", len(b.literals))\n+\t\t}\n+\t\tbh.setType(blockTypeRaw)\n+\t\tb.output = bh.appendTo(b.output)\n+\t\tb.output = append(b.output, b.literals...)\n+\t\treturn nil\n+\tcase huff0.ErrUseRLE:\n+\t\tif debug {\n+\t\t\tprintln(\"Adding RLE block, length\", len(b.literals))\n+\t\t}\n+\t\tbh.setType(blockTypeRLE)\n+\t\tb.output = bh.appendTo(b.output)\n+\t\tb.output = append(b.output, b.literals[0])\n+\t\treturn nil\n+\tdefault:\n+\t\treturn err\n+\tcase nil:\n+\t}\n+\t// Compressed...\n+\t// Now, allow reuse\n+\tb.litEnc.Reuse = huff0.ReusePolicyAllow\n+\tbh.setType(blockTypeCompressed)\n+\tvar lh literalsHeader\n+\tif reUsed {\n+\t\tif debug {\n+\t\t\tprintln(\"Reused tree, compressed to\", len(out))\n+\t\t}\n+\t\tlh.setType(literalsBlockTreeless)\n+\t} else {\n+\t\tif debug {\n+\t\t\tprintln(\"New tree, compressed to\", len(out), \"tree size:\", len(b.litEnc.OutTable))\n+\t\t}\n+\t\tlh.setType(literalsBlockCompressed)\n+\t}\n+\t// Set sizes\n+\tlh.setSizes(len(out), len(b.literals), single)\n+\tbh.setSize(uint32(len(out) + lh.size() + 1))\n+\n+\t// Write block headers.\n+\tb.output = bh.appendTo(b.output)\n+\tb.output = lh.appendTo(b.output)\n+\t// Add compressed data.\n+\tb.output = append(b.output, out...)\n+\t// No sequences.\n+\tb.output = append(b.output, 0)\n+\treturn nil\n+}\n+\n+// fuzzFseEncoder can be used to fuzz the FSE encoder.\n+func fuzzFseEncoder(data []byte) int {\n+\tif len(data) > maxSequences || len(data) < 2 {\n+\t\treturn 0\n+\t}\n+\tenc := fseEncoder{}\n+\thist := enc.Histogram()[:256]\n+\tmaxSym := uint8(0)\n+\tfor i, v := range data {\n+\t\tv = v & 63\n+\t\tdata[i] = v\n+\t\thist[v]++\n+\t\tif v > maxSym {\n+\t\t\tmaxSym = v\n+\t\t}\n+\t}\n+\tif maxSym == 0 {\n+\t\t// All 0\n+\t\treturn 0\n+\t}\n+\tmaxCount := func(a []uint32) int {\n+\t\tvar max uint32\n+\t\tfor _, v := range a {\n+\t\t\tif v > max {\n+\t\t\t\tmax = v\n+\t\t\t}\n+\t\t}\n+\t\treturn int(max)\n+\t}\n+\tcnt := maxCount(hist[:maxSym])\n+\tif cnt == len(data) {\n+\t\t// RLE\n+\t\treturn 0\n+\t}\n+\tenc.HistogramFinished(maxSym, cnt)\n+\terr := enc.normalizeCount(len(data))\n+\tif err != nil {\n+\t\treturn 0\n+\t}\n+\t_, err = enc.writeCount(nil)\n+\tif err != nil {\n+\t\tpanic(err)\n+\t}\n+\treturn 1\n+}\n+\n+// encode will encode the block and append the output in b.output.\n+func (b *blockEnc) encode(raw bool) error {\n+\tif len(b.sequences) == 0 {\n+\t\treturn b.encodeLits(raw)\n+\t}\n+\t// We want some difference\n+\tif len(b.literals) > (b.size - (b.size >> 5)) {\n+\t\treturn errIncompressible\n+\t}\n+\n+\tvar bh blockHeader\n+\tvar lh literalsHeader\n+\tbh.setLast(b.last)\n+\tbh.setType(blockTypeCompressed)\n+\t// Store offset of the block header. Needed when we know the size.\n+\tbhOffset := len(b.output)\n+\tb.output = bh.appendTo(b.output)\n+\n+\tvar (\n+\t\tout            []byte\n+\t\treUsed, single bool\n+\t\terr            error\n+\t)\n+\tif len(b.literals) >= 1024 && !raw {\n+\t\t// Use 4 Streams.\n+\t\tout, reUsed, err = huff0.Compress4X(b.literals, b.litEnc)\n+\t} else if len(b.literals) > 32 && !raw {\n+\t\t// Use 1 stream\n+\t\tsingle = true\n+\t\tout, reUsed, err = huff0.Compress1X(b.literals, b.litEnc)\n+\t} else {\n+\t\terr = huff0.ErrIncompressible\n+\t}\n+\n+\tswitch err {\n+\tcase huff0.ErrIncompressible:\n+\t\tlh.setType(literalsBlockRaw)\n+\t\tlh.setSize(len(b.literals))\n+\t\tb.output = lh.appendTo(b.output)\n+\t\tb.output = append(b.output, b.literals...)\n+\t\tif debug {\n+\t\t\tprintln(\"Adding literals RAW, length\", len(b.literals))\n+\t\t}\n+\tcase huff0.ErrUseRLE:\n+\t\tlh.setType(literalsBlockRLE)\n+\t\tlh.setSize(len(b.literals))\n+\t\tb.output = lh.appendTo(b.output)\n+\t\tb.output = append(b.output, b.literals[0])\n+\t\tif debug {\n+\t\t\tprintln(\"Adding literals RLE\")\n+\t\t}\n+\tdefault:\n+\t\tif debug {\n+\t\t\tprintln(\"Adding literals ERROR:\", err)\n+\t\t}\n+\t\treturn err\n+\tcase nil:\n+\t\t// Compressed litLen...\n+\t\tif reUsed {\n+\t\t\tif debug {\n+\t\t\t\tprintln(\"reused tree\")\n+\t\t\t}\n+\t\t\tlh.setType(literalsBlockTreeless)\n+\t\t} else {\n+\t\t\tif debug {\n+\t\t\t\tprintln(\"new tree, size:\", len(b.litEnc.OutTable))\n+\t\t\t}\n+\t\t\tlh.setType(literalsBlockCompressed)\n+\t\t\tif debug {\n+\t\t\t\t_, _, err := huff0.ReadTable(out, nil)\n+\t\t\t\tif err != nil {\n+\t\t\t\t\tpanic(err)\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t\tlh.setSizes(len(out), len(b.literals), single)\n+\t\tif debug {\n+\t\t\tprintf(\"Compressed %d literals to %d bytes\", len(b.literals), len(out))\n+\t\t\tprintln(\"Adding literal header:\", lh)\n+\t\t}\n+\t\tb.output = lh.appendTo(b.output)\n+\t\tb.output = append(b.output, out...)\n+\t\tb.litEnc.Reuse = huff0.ReusePolicyAllow\n+\t\tif debug {\n+\t\t\tprintln(\"Adding literals compressed\")\n+\t\t}\n+\t}\n+\t// Sequence compression\n+\n+\t// Write the number of sequences\n+\tswitch {\n+\tcase len(b.sequences) < 128:\n+\t\tb.output = append(b.output, uint8(len(b.sequences)))\n+\tcase len(b.sequences) < 0x7f00: // TODO: this could be wrong\n+\t\tn := len(b.sequences)\n+\t\tb.output = append(b.output, 128+uint8(n>>8), uint8(n))\n+\tdefault:\n+\t\tn := len(b.sequences) - 0x7f00\n+\t\tb.output = append(b.output, 255, uint8(n), uint8(n>>8))\n+\t}\n+\tif debug {\n+\t\tprintln(\"Encoding\", len(b.sequences), \"sequences\")\n+\t}\n+\tb.genCodes()\n+\tllEnc := b.coders.llEnc\n+\tofEnc := b.coders.ofEnc\n+\tmlEnc := b.coders.mlEnc\n+\terr = llEnc.normalizeCount(len(b.sequences))\n+\tif err != nil {\n+\t\treturn err\n+\t}\n+\terr = ofEnc.normalizeCount(len(b.sequences))\n+\tif err != nil {\n+\t\treturn err\n+\t}\n+\terr = mlEnc.normalizeCount(len(b.sequences))\n+\tif err != nil {\n+\t\treturn err\n+\t}\n+\n+\t// Choose the best compression mode for each type.\n+\t// Will evaluate the new vs predefined and previous.\n+\tchooseComp := func(cur, prev, preDef *fseEncoder) (*fseEncoder, seqCompMode) {\n+\t\t// See if predefined/previous is better\n+\t\thist := cur.count[:cur.symbolLen]\n+\t\tnSize := cur.approxSize(hist) + cur.maxHeaderSize()\n+\t\tpredefSize := preDef.approxSize(hist)\n+\t\tprevSize := prev.approxSize(hist)\n+\n+\t\t// Add a small penalty for new encoders.\n+\t\t// Don't bother with extremely small (<2 byte gains).\n+\t\tnSize = nSize + (nSize+2*8*16)>>4\n+\t\tswitch {\n+\t\tcase predefSize <= prevSize && predefSize <= nSize || forcePreDef:\n+\t\t\tif debug {\n+\t\t\t\tprintln(\"Using predefined\", predefSize>>3, \"<=\", nSize>>3)\n+\t\t\t}\n+\t\t\treturn preDef, compModePredefined\n+\t\tcase prevSize <= nSize:\n+\t\t\tif debug {\n+\t\t\t\tprintln(\"Using previous\", prevSize>>3, \"<=\", nSize>>3)\n+\t\t\t}\n+\t\t\treturn prev, compModeRepeat\n+\t\tdefault:\n+\t\t\tif debug {\n+\t\t\t\tprintln(\"Using new, predef\", predefSize>>3, \". previous:\", prevSize>>3, \">\", nSize>>3, \"header max:\", cur.maxHeaderSize()>>3, \"bytes\")\n+\t\t\t\tprintln(\"tl:\", cur.actualTableLog, \"symbolLen:\", cur.symbolLen, \"norm:\", cur.norm[:cur.symbolLen], \"hist\", cur.count[:cur.symbolLen])\n+\t\t\t}\n+\t\t\treturn cur, compModeFSE\n+\t\t}\n+\t}\n+\n+\t// Write compression mode\n+\tvar mode uint8\n+\tif llEnc.useRLE {\n+\t\tmode |= uint8(compModeRLE) << 6\n+\t\tllEnc.setRLE(b.sequences[0].llCode)\n+\t\tif debug {\n+\t\t\tprintln(\"llEnc.useRLE\")\n+\t\t}\n+\t} else {\n+\t\tvar m seqCompMode\n+\t\tllEnc, m = chooseComp(llEnc, b.coders.llPrev, &fsePredefEnc[tableLiteralLengths])\n+\t\tmode |= uint8(m) << 6\n+\t}\n+\tif ofEnc.useRLE {\n+\t\tmode |= uint8(compModeRLE) << 4\n+\t\tofEnc.setRLE(b.sequences[0].ofCode)\n+\t\tif debug {\n+\t\t\tprintln(\"ofEnc.useRLE\")\n+\t\t}\n+\t} else {\n+\t\tvar m seqCompMode\n+\t\tofEnc, m = chooseComp(ofEnc, b.coders.ofPrev, &fsePredefEnc[tableOffsets])\n+\t\tmode |= uint8(m) << 4\n+\t}\n+\n+\tif mlEnc.useRLE {\n+\t\tmode |= uint8(compModeRLE) << 2\n+\t\tmlEnc.setRLE(b.sequences[0].mlCode)\n+\t\tif debug {\n+\t\t\tprintln(\"mlEnc.useRLE, code: \", b.sequences[0].mlCode, \"value\", b.sequences[0].matchLen)\n+\t\t}\n+\t} else {\n+\t\tvar m seqCompMode\n+\t\tmlEnc, m = chooseComp(mlEnc, b.coders.mlPrev, &fsePredefEnc[tableMatchLengths])\n+\t\tmode |= uint8(m) << 2\n+\t}\n+\tb.output = append(b.output, mode)\n+\tif debug {\n+\t\tprintf(\"Compression modes: 0b%b\", mode)\n+\t}\n+\tb.output, err = llEnc.writeCount(b.output)\n+\tif err != nil {\n+\t\treturn err\n+\t}\n+\tstart := len(b.output)\n+\tb.output, err = ofEnc.writeCount(b.output)\n+\tif err != nil {\n+\t\treturn err\n+\t}\n+\tif false {\n+\t\tprintln(\"block:\", b.output[start:], \"tablelog\", ofEnc.actualTableLog, \"maxcount:\", ofEnc.maxCount)\n+\t\tfmt.Printf(\"selected TableLog: %d, Symbol length: %d\\n\", ofEnc.actualTableLog, ofEnc.symbolLen)\n+\t\tfor i, v := range ofEnc.norm[:ofEnc.symbolLen] {\n+\t\t\tfmt.Printf(\"%3d: %5d -> %4d \\n\", i, ofEnc.count[i], v)\n+\t\t}\n+\t}\n+\tb.output, err = mlEnc.writeCount(b.output)\n+\tif err != nil {\n+\t\treturn err\n+\t}\n+\n+\t// Maybe in block?\n+\twr := &b.wr\n+\twr.reset(b.output)\n+\n+\tvar ll, of, ml cState\n+\n+\t// Current sequence\n+\tseq := len(b.sequences) - 1\n+\ts := b.sequences[seq]\n+\tllEnc.setBits(llBitsTable[:])\n+\tmlEnc.setBits(mlBitsTable[:])\n+\tofEnc.setBits(nil)\n+\n+\tllTT, ofTT, mlTT := llEnc.ct.symbolTT[:256], ofEnc.ct.symbolTT[:256], mlEnc.ct.symbolTT[:256]\n+\n+\t// We have 3 bounds checks here (and in the loop).\n+\t// Since we are iterating backwards it is kinda hard to avoid.\n+\tllB, ofB, mlB := llTT[s.llCode], ofTT[s.ofCode], mlTT[s.mlCode]\n+\tll.init(wr, &llEnc.ct, llB)\n+\tof.init(wr, &ofEnc.ct, ofB)\n+\twr.flush32()\n+\tml.init(wr, &mlEnc.ct, mlB)\n+\n+\t// Each of these lookups also generates a bounds check.\n+\twr.addBits32NC(s.litLen, llB.outBits)\n+\twr.addBits32NC(s.matchLen, mlB.outBits)\n+\twr.flush32()\n+\twr.addBits32NC(s.offset, ofB.outBits)\n+\tif debugSequences {\n+\t\tprintln(\"Encoded seq\", seq, s, \"codes:\", s.llCode, s.mlCode, s.ofCode, \"states:\", ll.state, ml.state, of.state, \"bits:\", llB, mlB, ofB)\n+\t}\n+\tseq--\n+\tif llEnc.maxBits+mlEnc.maxBits+ofEnc.maxBits <= 32 {\n+\t\t// No need to flush (common)\n+\t\tfor seq >= 0 {\n+\t\t\ts = b.sequences[seq]\n+\t\t\twr.flush32()\n+\t\t\tllB, ofB, mlB := llTT[s.llCode], ofTT[s.ofCode], mlTT[s.mlCode]\n+\t\t\t// tabelog max is 8 for all.\n+\t\t\tof.encode(ofB)\n+\t\t\tml.encode(mlB)\n+\t\t\tll.encode(llB)\n+\t\t\twr.flush32()\n+\n+\t\t\t// We checked that all can stay within 32 bits\n+\t\t\twr.addBits32NC(s.litLen, llB.outBits)\n+\t\t\twr.addBits32NC(s.matchLen, mlB.outBits)\n+\t\t\twr.addBits32NC(s.offset, ofB.outBits)\n+\n+\t\t\tif debugSequences {\n+\t\t\t\tprintln(\"Encoded seq\", seq, s)\n+\t\t\t}\n+\n+\t\t\tseq--\n+\t\t}\n+\t} else {\n+\t\tfor seq >= 0 {\n+\t\t\ts = b.sequences[seq]\n+\t\t\twr.flush32()\n+\t\t\tllB, ofB, mlB := llTT[s.llCode], ofTT[s.ofCode], mlTT[s.mlCode]\n+\t\t\t// tabelog max is below 8 for each.\n+\t\t\tof.encode(ofB)\n+\t\t\tml.encode(mlB)\n+\t\t\tll.encode(llB)\n+\t\t\twr.flush32()\n+\n+\t\t\t// ml+ll = max 32 bits total\n+\t\t\twr.addBits32NC(s.litLen, llB.outBits)\n+\t\t\twr.addBits32NC(s.matchLen, mlB.outBits)\n+\t\t\twr.flush32()\n+\t\t\twr.addBits32NC(s.offset, ofB.outBits)\n+\n+\t\t\tif debugSequences {\n+\t\t\t\tprintln(\"Encoded seq\", seq, s)\n+\t\t\t}\n+\n+\t\t\tseq--\n+\t\t}\n+\t}\n+\tml.flush(mlEnc.actualTableLog)\n+\tof.flush(ofEnc.actualTableLog)\n+\tll.flush(llEnc.actualTableLog)\n+\terr = wr.close()\n+\tif err != nil {\n+\t\treturn err\n+\t}\n+\tb.output = wr.out\n+\n+\tif len(b.output)-3-bhOffset >= b.size {\n+\t\t// Maybe even add a bigger margin.\n+\t\tb.litEnc.Reuse = huff0.ReusePolicyNone\n+\t\treturn errIncompressible\n+\t}\n+\n+\t// Size is output minus block header.\n+\tbh.setSize(uint32(len(b.output)-bhOffset) - 3)\n+\tif debug {\n+\t\tprintln(\"Rewriting block header\", bh)\n+\t}\n+\t_ = bh.appendTo(b.output[bhOffset:bhOffset])\n+\tb.coders.setPrev(llEnc, mlEnc, ofEnc)\n+\treturn nil\n+}\n+\n+var errIncompressible = errors.New(\"incompressible\")\n+\n+func (b *blockEnc) genCodes() {\n+\tif len(b.sequences) == 0 {\n+\t\t// nothing to do\n+\t\treturn\n+\t}\n+\n+\tif len(b.sequences) > math.MaxUint16 {\n+\t\tpanic(\"can only encode up to 64K sequences\")\n+\t}\n+\t// No bounds checks after here:\n+\tllH := b.coders.llEnc.Histogram()[:256]\n+\tofH := b.coders.ofEnc.Histogram()[:256]\n+\tmlH := b.coders.mlEnc.Histogram()[:256]\n+\tfor i := range llH {\n+\t\tllH[i] = 0\n+\t}\n+\tfor i := range ofH {\n+\t\tofH[i] = 0\n+\t}\n+\tfor i := range mlH {\n+\t\tmlH[i] = 0\n+\t}\n+\n+\tvar llMax, ofMax, mlMax uint8\n+\tfor i, seq := range b.sequences {\n+\t\tv := llCode(seq.litLen)\n+\t\tseq.llCode = v\n+\t\tllH[v]++\n+\t\tif v > llMax {\n+\t\t\tllMax = v\n+\t\t}\n+\n+\t\tv = ofCode(seq.offset)\n+\t\tseq.ofCode = v\n+\t\tofH[v]++\n+\t\tif v > ofMax {\n+\t\t\tofMax = v\n+\t\t}\n+\n+\t\tv = mlCode(seq.matchLen)\n+\t\tseq.mlCode = v\n+\t\tmlH[v]++\n+\t\tif v > mlMax {\n+\t\t\tmlMax = v\n+\t\t\tif debug && mlMax > maxMatchLengthSymbol {\n+\t\t\t\tpanic(fmt.Errorf(\"mlMax > maxMatchLengthSymbol (%d), matchlen: %d\", mlMax, seq.matchLen))\n+\t\t\t}\n+\t\t}\n+\t\tb.sequences[i] = seq\n+\t}\n+\tmaxCount := func(a []uint32) int {\n+\t\tvar max uint32\n+\t\tfor _, v := range a {\n+\t\t\tif v > max {\n+\t\t\t\tmax = v\n+\t\t\t}\n+\t\t}\n+\t\treturn int(max)\n+\t}\n+\tif mlMax > maxMatchLengthSymbol {\n+\t\tpanic(fmt.Errorf(\"mlMax > maxMatchLengthSymbol (%d)\", mlMax))\n+\t}\n+\tif ofMax > maxOffsetBits {\n+\t\tpanic(fmt.Errorf(\"ofMax > maxOffsetBits (%d)\", ofMax))\n+\t}\n+\tif llMax > maxLiteralLengthSymbol {\n+\t\tpanic(fmt.Errorf(\"llMax > maxLiteralLengthSymbol (%d)\", llMax))\n+\t}\n+\n+\tb.coders.mlEnc.HistogramFinished(mlMax, maxCount(mlH[:mlMax+1]))\n+\tb.coders.ofEnc.HistogramFinished(ofMax, maxCount(ofH[:ofMax+1]))\n+\tb.coders.llEnc.HistogramFinished(llMax, maxCount(llH[:llMax+1]))\n+}"
    },
    {
      "sha": "01a01e486e1886a322e8b7c2ac5dba21f732a383",
      "filename": "backend/vendor/github.com/klauspost/compress/zstd/blocktype_string.go",
      "status": "added",
      "additions": 85,
      "deletions": 0,
      "changes": 85,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/klauspost/compress/zstd/blocktype_string.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/klauspost/compress/zstd/blocktype_string.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/github.com/klauspost/compress/zstd/blocktype_string.go?ref=890d7154e777d0673defcd11db3ef792473d68c9",
      "patch": "@@ -0,0 +1,85 @@\n+// Code generated by \"stringer -type=blockType,literalsBlockType,seqCompMode,tableIndex\"; DO NOT EDIT.\n+\n+package zstd\n+\n+import \"strconv\"\n+\n+func _() {\n+\t// An \"invalid array index\" compiler error signifies that the constant values have changed.\n+\t// Re-run the stringer command to generate them again.\n+\tvar x [1]struct{}\n+\t_ = x[blockTypeRaw-0]\n+\t_ = x[blockTypeRLE-1]\n+\t_ = x[blockTypeCompressed-2]\n+\t_ = x[blockTypeReserved-3]\n+}\n+\n+const _blockType_name = \"blockTypeRawblockTypeRLEblockTypeCompressedblockTypeReserved\"\n+\n+var _blockType_index = [...]uint8{0, 12, 24, 43, 60}\n+\n+func (i blockType) String() string {\n+\tif i >= blockType(len(_blockType_index)-1) {\n+\t\treturn \"blockType(\" + strconv.FormatInt(int64(i), 10) + \")\"\n+\t}\n+\treturn _blockType_name[_blockType_index[i]:_blockType_index[i+1]]\n+}\n+func _() {\n+\t// An \"invalid array index\" compiler error signifies that the constant values have changed.\n+\t// Re-run the stringer command to generate them again.\n+\tvar x [1]struct{}\n+\t_ = x[literalsBlockRaw-0]\n+\t_ = x[literalsBlockRLE-1]\n+\t_ = x[literalsBlockCompressed-2]\n+\t_ = x[literalsBlockTreeless-3]\n+}\n+\n+const _literalsBlockType_name = \"literalsBlockRawliteralsBlockRLEliteralsBlockCompressedliteralsBlockTreeless\"\n+\n+var _literalsBlockType_index = [...]uint8{0, 16, 32, 55, 76}\n+\n+func (i literalsBlockType) String() string {\n+\tif i >= literalsBlockType(len(_literalsBlockType_index)-1) {\n+\t\treturn \"literalsBlockType(\" + strconv.FormatInt(int64(i), 10) + \")\"\n+\t}\n+\treturn _literalsBlockType_name[_literalsBlockType_index[i]:_literalsBlockType_index[i+1]]\n+}\n+func _() {\n+\t// An \"invalid array index\" compiler error signifies that the constant values have changed.\n+\t// Re-run the stringer command to generate them again.\n+\tvar x [1]struct{}\n+\t_ = x[compModePredefined-0]\n+\t_ = x[compModeRLE-1]\n+\t_ = x[compModeFSE-2]\n+\t_ = x[compModeRepeat-3]\n+}\n+\n+const _seqCompMode_name = \"compModePredefinedcompModeRLEcompModeFSEcompModeRepeat\"\n+\n+var _seqCompMode_index = [...]uint8{0, 18, 29, 40, 54}\n+\n+func (i seqCompMode) String() string {\n+\tif i >= seqCompMode(len(_seqCompMode_index)-1) {\n+\t\treturn \"seqCompMode(\" + strconv.FormatInt(int64(i), 10) + \")\"\n+\t}\n+\treturn _seqCompMode_name[_seqCompMode_index[i]:_seqCompMode_index[i+1]]\n+}\n+func _() {\n+\t// An \"invalid array index\" compiler error signifies that the constant values have changed.\n+\t// Re-run the stringer command to generate them again.\n+\tvar x [1]struct{}\n+\t_ = x[tableLiteralLengths-0]\n+\t_ = x[tableOffsets-1]\n+\t_ = x[tableMatchLengths-2]\n+}\n+\n+const _tableIndex_name = \"tableLiteralLengthstableOffsetstableMatchLengths\"\n+\n+var _tableIndex_index = [...]uint8{0, 19, 31, 48}\n+\n+func (i tableIndex) String() string {\n+\tif i >= tableIndex(len(_tableIndex_index)-1) {\n+\t\treturn \"tableIndex(\" + strconv.FormatInt(int64(i), 10) + \")\"\n+\t}\n+\treturn _tableIndex_name[_tableIndex_index[i]:_tableIndex_index[i+1]]\n+}"
    },
    {
      "sha": "07321acb1846c4564ffbee9b64a865ad0b99f9c0",
      "filename": "backend/vendor/github.com/klauspost/compress/zstd/bytebuf.go",
      "status": "added",
      "additions": 127,
      "deletions": 0,
      "changes": 127,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/klauspost/compress/zstd/bytebuf.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/klauspost/compress/zstd/bytebuf.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/github.com/klauspost/compress/zstd/bytebuf.go?ref=890d7154e777d0673defcd11db3ef792473d68c9",
      "patch": "@@ -0,0 +1,127 @@\n+// Copyright 2019+ Klaus Post. All rights reserved.\n+// License information can be found in the LICENSE file.\n+// Based on work by Yann Collet, released under BSD License.\n+\n+package zstd\n+\n+import (\n+\t\"fmt\"\n+\t\"io\"\n+\t\"io/ioutil\"\n+)\n+\n+type byteBuffer interface {\n+\t// Read up to 8 bytes.\n+\t// Returns nil if no more input is available.\n+\treadSmall(n int) []byte\n+\n+\t// Read >8 bytes.\n+\t// MAY use the destination slice.\n+\treadBig(n int, dst []byte) ([]byte, error)\n+\n+\t// Read a single byte.\n+\treadByte() (byte, error)\n+\n+\t// Skip n bytes.\n+\tskipN(n int) error\n+}\n+\n+// in-memory buffer\n+type byteBuf []byte\n+\n+func (b *byteBuf) readSmall(n int) []byte {\n+\tif debug && n > 8 {\n+\t\tpanic(fmt.Errorf(\"small read > 8 (%d). use readBig\", n))\n+\t}\n+\tbb := *b\n+\tif len(bb) < n {\n+\t\treturn nil\n+\t}\n+\tr := bb[:n]\n+\t*b = bb[n:]\n+\treturn r\n+}\n+\n+func (b *byteBuf) readBig(n int, dst []byte) ([]byte, error) {\n+\tbb := *b\n+\tif len(bb) < n {\n+\t\treturn nil, io.ErrUnexpectedEOF\n+\t}\n+\tr := bb[:n]\n+\t*b = bb[n:]\n+\treturn r, nil\n+}\n+\n+func (b *byteBuf) remain() []byte {\n+\treturn *b\n+}\n+\n+func (b *byteBuf) readByte() (byte, error) {\n+\tbb := *b\n+\tif len(bb) < 1 {\n+\t\treturn 0, nil\n+\t}\n+\tr := bb[0]\n+\t*b = bb[1:]\n+\treturn r, nil\n+}\n+\n+func (b *byteBuf) skipN(n int) error {\n+\tbb := *b\n+\tif len(bb) < n {\n+\t\treturn io.ErrUnexpectedEOF\n+\t}\n+\t*b = bb[n:]\n+\treturn nil\n+}\n+\n+// wrapper around a reader.\n+type readerWrapper struct {\n+\tr   io.Reader\n+\ttmp [8]byte\n+}\n+\n+func (r *readerWrapper) readSmall(n int) []byte {\n+\tif debug && n > 8 {\n+\t\tpanic(fmt.Errorf(\"small read > 8 (%d). use readBig\", n))\n+\t}\n+\tn2, err := io.ReadFull(r.r, r.tmp[:n])\n+\t// We only really care about the actual bytes read.\n+\tif n2 != n {\n+\t\tif debug {\n+\t\t\tprintln(\"readSmall: got\", n2, \"want\", n, \"err\", err)\n+\t\t}\n+\t\treturn nil\n+\t}\n+\treturn r.tmp[:n]\n+}\n+\n+func (r *readerWrapper) readBig(n int, dst []byte) ([]byte, error) {\n+\tif cap(dst) < n {\n+\t\tdst = make([]byte, n)\n+\t}\n+\tn2, err := io.ReadFull(r.r, dst[:n])\n+\tif err == io.EOF && n > 0 {\n+\t\terr = io.ErrUnexpectedEOF\n+\t}\n+\treturn dst[:n2], err\n+}\n+\n+func (r *readerWrapper) readByte() (byte, error) {\n+\tn2, err := r.r.Read(r.tmp[:1])\n+\tif err != nil {\n+\t\treturn 0, err\n+\t}\n+\tif n2 != 1 {\n+\t\treturn 0, io.ErrUnexpectedEOF\n+\t}\n+\treturn r.tmp[0], nil\n+}\n+\n+func (r *readerWrapper) skipN(n int) error {\n+\tn2, err := io.CopyN(ioutil.Discard, r.r, int64(n))\n+\tif n2 != int64(n) {\n+\t\terr = io.ErrUnexpectedEOF\n+\t}\n+\treturn err\n+}"
    },
    {
      "sha": "dc4378b64012866143716c3bd982c38894111c32",
      "filename": "backend/vendor/github.com/klauspost/compress/zstd/bytereader.go",
      "status": "added",
      "additions": 74,
      "deletions": 0,
      "changes": 74,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/klauspost/compress/zstd/bytereader.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/klauspost/compress/zstd/bytereader.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/github.com/klauspost/compress/zstd/bytereader.go?ref=890d7154e777d0673defcd11db3ef792473d68c9",
      "patch": "@@ -0,0 +1,74 @@\n+// Copyright 2019+ Klaus Post. All rights reserved.\n+// License information can be found in the LICENSE file.\n+// Based on work by Yann Collet, released under BSD License.\n+\n+package zstd\n+\n+// byteReader provides a byte reader that reads\n+// little endian values from a byte stream.\n+// The input stream is manually advanced.\n+// The reader performs no bounds checks.\n+type byteReader struct {\n+\tb   []byte\n+\toff int\n+}\n+\n+// init will initialize the reader and set the input.\n+func (b *byteReader) init(in []byte) {\n+\tb.b = in\n+\tb.off = 0\n+}\n+\n+// advance the stream b n bytes.\n+func (b *byteReader) advance(n uint) {\n+\tb.off += int(n)\n+}\n+\n+// overread returns whether we have advanced too far.\n+func (b *byteReader) overread() bool {\n+\treturn b.off > len(b.b)\n+}\n+\n+// Int32 returns a little endian int32 starting at current offset.\n+func (b byteReader) Int32() int32 {\n+\tb2 := b.b[b.off : b.off+4 : b.off+4]\n+\tv3 := int32(b2[3])\n+\tv2 := int32(b2[2])\n+\tv1 := int32(b2[1])\n+\tv0 := int32(b2[0])\n+\treturn v0 | (v1 << 8) | (v2 << 16) | (v3 << 24)\n+}\n+\n+// Uint8 returns the next byte\n+func (b *byteReader) Uint8() uint8 {\n+\tv := b.b[b.off]\n+\treturn v\n+}\n+\n+// Uint32 returns a little endian uint32 starting at current offset.\n+func (b byteReader) Uint32() uint32 {\n+\tif r := b.remain(); r < 4 {\n+\t\t// Very rare\n+\t\tv := uint32(0)\n+\t\tfor i := 1; i <= r; i++ {\n+\t\t\tv = (v << 8) | uint32(b.b[len(b.b)-i])\n+\t\t}\n+\t\treturn v\n+\t}\n+\tb2 := b.b[b.off : b.off+4 : b.off+4]\n+\tv3 := uint32(b2[3])\n+\tv2 := uint32(b2[2])\n+\tv1 := uint32(b2[1])\n+\tv0 := uint32(b2[0])\n+\treturn v0 | (v1 << 8) | (v2 << 16) | (v3 << 24)\n+}\n+\n+// unread returns the unread portion of the input.\n+func (b byteReader) unread() []byte {\n+\treturn b.b[b.off:]\n+}\n+\n+// remain will return the number of bytes remaining.\n+func (b byteReader) remain() int {\n+\treturn len(b.b) - b.off\n+}"
    },
    {
      "sha": "35a3cda91407787066c7d6ba4d3f8cd17cbc8e87",
      "filename": "backend/vendor/github.com/klauspost/compress/zstd/decoder.go",
      "status": "added",
      "additions": 513,
      "deletions": 0,
      "changes": 513,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/klauspost/compress/zstd/decoder.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/klauspost/compress/zstd/decoder.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/github.com/klauspost/compress/zstd/decoder.go?ref=890d7154e777d0673defcd11db3ef792473d68c9",
      "patch": "@@ -0,0 +1,513 @@\n+// Copyright 2019+ Klaus Post. All rights reserved.\n+// License information can be found in the LICENSE file.\n+// Based on work by Yann Collet, released under BSD License.\n+\n+package zstd\n+\n+import (\n+\t\"bytes\"\n+\t\"errors\"\n+\t\"io\"\n+\t\"sync\"\n+)\n+\n+// Decoder provides decoding of zstandard streams.\n+// The decoder has been designed to operate without allocations after a warmup.\n+// This means that you should store the decoder for best performance.\n+// To re-use a stream decoder, use the Reset(r io.Reader) error to switch to another stream.\n+// A decoder can safely be re-used even if the previous stream failed.\n+// To release the resources, you must call the Close() function on a decoder.\n+type Decoder struct {\n+\to decoderOptions\n+\n+\t// Unreferenced decoders, ready for use.\n+\tdecoders chan *blockDec\n+\n+\t// Unreferenced decoders, ready for use.\n+\tframes chan *frameDec\n+\n+\t// Streams ready to be decoded.\n+\tstream chan decodeStream\n+\n+\t// Current read position used for Reader functionality.\n+\tcurrent decoderState\n+\n+\t// Custom dictionaries\n+\tdicts map[uint32]struct{}\n+\n+\t// streamWg is the waitgroup for all streams\n+\tstreamWg sync.WaitGroup\n+}\n+\n+// decoderState is used for maintaining state when the decoder\n+// is used for streaming.\n+type decoderState struct {\n+\t// current block being written to stream.\n+\tdecodeOutput\n+\n+\t// output in order to be written to stream.\n+\toutput chan decodeOutput\n+\n+\t// cancel remaining output.\n+\tcancel chan struct{}\n+\n+\tflushed bool\n+}\n+\n+var (\n+\t// Check the interfaces we want to support.\n+\t_ = io.WriterTo(&Decoder{})\n+\t_ = io.Reader(&Decoder{})\n+)\n+\n+// NewReader creates a new decoder.\n+// A nil Reader can be provided in which case Reset can be used to start a decode.\n+//\n+// A Decoder can be used in two modes:\n+//\n+// 1) As a stream, or\n+// 2) For stateless decoding using DecodeAll or DecodeBuffer.\n+//\n+// Only a single stream can be decoded concurrently, but the same decoder\n+// can run multiple concurrent stateless decodes. It is even possible to\n+// use stateless decodes while a stream is being decoded.\n+//\n+// The Reset function can be used to initiate a new stream, which is will considerably\n+// reduce the allocations normally caused by NewReader.\n+func NewReader(r io.Reader, opts ...DOption) (*Decoder, error) {\n+\tinitPredefined()\n+\tvar d Decoder\n+\td.o.setDefault()\n+\tfor _, o := range opts {\n+\t\terr := o(&d.o)\n+\t\tif err != nil {\n+\t\t\treturn nil, err\n+\t\t}\n+\t}\n+\td.current.output = make(chan decodeOutput, d.o.concurrent)\n+\td.current.flushed = true\n+\n+\t// Create decoders\n+\td.decoders = make(chan *blockDec, d.o.concurrent)\n+\td.frames = make(chan *frameDec, d.o.concurrent)\n+\tfor i := 0; i < d.o.concurrent; i++ {\n+\t\td.frames <- newFrameDec(d.o)\n+\t\td.decoders <- newBlockDec(d.o.lowMem)\n+\t}\n+\n+\tif r == nil {\n+\t\treturn &d, nil\n+\t}\n+\treturn &d, d.Reset(r)\n+}\n+\n+// Read bytes from the decompressed stream into p.\n+// Returns the number of bytes written and any error that occurred.\n+// When the stream is done, io.EOF will be returned.\n+func (d *Decoder) Read(p []byte) (int, error) {\n+\tif d.stream == nil {\n+\t\treturn 0, errors.New(\"no input has been initialized\")\n+\t}\n+\tvar n int\n+\tfor {\n+\t\tif len(d.current.b) > 0 {\n+\t\t\tfilled := copy(p, d.current.b)\n+\t\t\tp = p[filled:]\n+\t\t\td.current.b = d.current.b[filled:]\n+\t\t\tn += filled\n+\t\t}\n+\t\tif len(p) == 0 {\n+\t\t\tbreak\n+\t\t}\n+\t\tif len(d.current.b) == 0 {\n+\t\t\t// We have an error and no more data\n+\t\t\tif d.current.err != nil {\n+\t\t\t\tbreak\n+\t\t\t}\n+\t\t\tif !d.nextBlock(n == 0) {\n+\t\t\t\treturn n, nil\n+\t\t\t}\n+\t\t}\n+\t}\n+\tif len(d.current.b) > 0 {\n+\t\tif debug {\n+\t\t\tprintln(\"returning\", n, \"still bytes left:\", len(d.current.b))\n+\t\t}\n+\t\t// Only return error at end of block\n+\t\treturn n, nil\n+\t}\n+\tif d.current.err != nil {\n+\t\td.drainOutput()\n+\t}\n+\tif debug {\n+\t\tprintln(\"returning\", n, d.current.err, len(d.decoders))\n+\t}\n+\treturn n, d.current.err\n+}\n+\n+// Reset will reset the decoder the supplied stream after the current has finished processing.\n+// Note that this functionality cannot be used after Close has been called.\n+func (d *Decoder) Reset(r io.Reader) error {\n+\tif d.current.err == ErrDecoderClosed {\n+\t\treturn d.current.err\n+\t}\n+\tif r == nil {\n+\t\treturn errors.New(\"nil Reader sent as input\")\n+\t}\n+\n+\tif d.stream == nil {\n+\t\td.stream = make(chan decodeStream, 1)\n+\t\td.streamWg.Add(1)\n+\t\tgo d.startStreamDecoder(d.stream)\n+\t}\n+\n+\td.drainOutput()\n+\n+\t// If bytes buffer and < 1MB, do sync decoding anyway.\n+\tif bb, ok := r.(*bytes.Buffer); ok && bb.Len() < 1<<20 {\n+\t\tif debug {\n+\t\t\tprintln(\"*bytes.Buffer detected, doing sync decode, len:\", bb.Len())\n+\t\t}\n+\t\tb := bb.Bytes()\n+\t\tdst, err := d.DecodeAll(b, nil)\n+\t\tif err == nil {\n+\t\t\terr = io.EOF\n+\t\t}\n+\t\td.current.b = dst\n+\t\td.current.err = err\n+\t\td.current.flushed = true\n+\t\tif debug {\n+\t\t\tprintln(\"sync decode to \", len(dst), \"bytes, err:\", err)\n+\t\t}\n+\t\treturn nil\n+\t}\n+\n+\t// Remove current block.\n+\td.current.decodeOutput = decodeOutput{}\n+\td.current.err = nil\n+\td.current.cancel = make(chan struct{})\n+\td.current.flushed = false\n+\td.current.d = nil\n+\n+\td.stream <- decodeStream{\n+\t\tr:      r,\n+\t\toutput: d.current.output,\n+\t\tcancel: d.current.cancel,\n+\t}\n+\treturn nil\n+}\n+\n+// drainOutput will drain the output until errEndOfStream is sent.\n+func (d *Decoder) drainOutput() {\n+\tif d.current.cancel != nil {\n+\t\tprintln(\"cancelling current\")\n+\t\tclose(d.current.cancel)\n+\t\td.current.cancel = nil\n+\t}\n+\tif d.current.d != nil {\n+\t\tif debug {\n+\t\t\tprintf(\"re-adding current decoder %p, decoders: %d\", d.current.d, len(d.decoders))\n+\t\t}\n+\t\td.decoders <- d.current.d\n+\t\td.current.d = nil\n+\t\td.current.b = nil\n+\t}\n+\tif d.current.output == nil || d.current.flushed {\n+\t\tprintln(\"current already flushed\")\n+\t\treturn\n+\t}\n+\tfor {\n+\t\tselect {\n+\t\tcase v := <-d.current.output:\n+\t\t\tif v.d != nil {\n+\t\t\t\tif debug {\n+\t\t\t\t\tprintf(\"re-adding decoder %p\", v.d)\n+\t\t\t\t}\n+\t\t\t\td.decoders <- v.d\n+\t\t\t}\n+\t\t\tif v.err == errEndOfStream {\n+\t\t\t\tprintln(\"current flushed\")\n+\t\t\t\td.current.flushed = true\n+\t\t\t\treturn\n+\t\t\t}\n+\t\t}\n+\t}\n+}\n+\n+// WriteTo writes data to w until there's no more data to write or when an error occurs.\n+// The return value n is the number of bytes written.\n+// Any error encountered during the write is also returned.\n+func (d *Decoder) WriteTo(w io.Writer) (int64, error) {\n+\tif d.stream == nil {\n+\t\treturn 0, errors.New(\"no input has been initialized\")\n+\t}\n+\tvar n int64\n+\tfor {\n+\t\tif len(d.current.b) > 0 {\n+\t\t\tn2, err2 := w.Write(d.current.b)\n+\t\t\tn += int64(n2)\n+\t\t\tif err2 != nil && d.current.err == nil {\n+\t\t\t\td.current.err = err2\n+\t\t\t\tbreak\n+\t\t\t}\n+\t\t}\n+\t\tif d.current.err != nil {\n+\t\t\tbreak\n+\t\t}\n+\t\td.nextBlock(true)\n+\t}\n+\terr := d.current.err\n+\tif err != nil {\n+\t\td.drainOutput()\n+\t}\n+\tif err == io.EOF {\n+\t\terr = nil\n+\t}\n+\treturn n, err\n+}\n+\n+// DecodeAll allows stateless decoding of a blob of bytes.\n+// Output will be appended to dst, so if the destination size is known\n+// you can pre-allocate the destination slice to avoid allocations.\n+// DecodeAll can be used concurrently.\n+// The Decoder concurrency limits will be respected.\n+func (d *Decoder) DecodeAll(input, dst []byte) ([]byte, error) {\n+\tif d.current.err == ErrDecoderClosed {\n+\t\treturn dst, ErrDecoderClosed\n+\t}\n+\n+\t// Grab a block decoder and frame decoder.\n+\tblock, frame := <-d.decoders, <-d.frames\n+\tdefer func() {\n+\t\tif debug {\n+\t\t\tprintf(\"re-adding decoder: %p\", block)\n+\t\t}\n+\t\td.decoders <- block\n+\t\tframe.rawInput = nil\n+\t\tframe.bBuf = nil\n+\t\td.frames <- frame\n+\t}()\n+\tframe.bBuf = input\n+\n+\tfor {\n+\t\terr := frame.reset(&frame.bBuf)\n+\t\tif err == io.EOF {\n+\t\t\treturn dst, nil\n+\t\t}\n+\t\tif err != nil {\n+\t\t\treturn dst, err\n+\t\t}\n+\t\tif frame.FrameContentSize > d.o.maxDecodedSize-uint64(len(dst)) {\n+\t\t\treturn dst, ErrDecoderSizeExceeded\n+\t\t}\n+\t\tif frame.FrameContentSize > 0 && frame.FrameContentSize < 1<<30 {\n+\t\t\t// Never preallocate moe than 1 GB up front.\n+\t\t\tif uint64(cap(dst)) < frame.FrameContentSize {\n+\t\t\t\tdst2 := make([]byte, len(dst), len(dst)+int(frame.FrameContentSize))\n+\t\t\t\tcopy(dst2, dst)\n+\t\t\t\tdst = dst2\n+\t\t\t}\n+\t\t}\n+\t\tif cap(dst) == 0 {\n+\t\t\t// Allocate window size * 2 by default if nothing is provided and we didn't get frame content size.\n+\t\t\tsize := frame.WindowSize * 2\n+\t\t\t// Cap to 1 MB.\n+\t\t\tif size > 1<<20 {\n+\t\t\t\tsize = 1 << 20\n+\t\t\t}\n+\t\t\tdst = make([]byte, 0, frame.WindowSize)\n+\t\t}\n+\n+\t\tdst, err = frame.runDecoder(dst, block)\n+\t\tif err != nil {\n+\t\t\treturn dst, err\n+\t\t}\n+\t\tif len(frame.bBuf) == 0 {\n+\t\t\tbreak\n+\t\t}\n+\t}\n+\treturn dst, nil\n+}\n+\n+// nextBlock returns the next block.\n+// If an error occurs d.err will be set.\n+// Optionally the function can block for new output.\n+// If non-blocking mode is used the returned boolean will be false\n+// if no data was available without blocking.\n+func (d *Decoder) nextBlock(blocking bool) (ok bool) {\n+\tif d.current.d != nil {\n+\t\tif debug {\n+\t\t\tprintf(\"re-adding current decoder %p\", d.current.d)\n+\t\t}\n+\t\td.decoders <- d.current.d\n+\t\td.current.d = nil\n+\t}\n+\tif d.current.err != nil {\n+\t\t// Keep error state.\n+\t\treturn blocking\n+\t}\n+\n+\tif blocking {\n+\t\td.current.decodeOutput = <-d.current.output\n+\t} else {\n+\t\tselect {\n+\t\tcase d.current.decodeOutput = <-d.current.output:\n+\t\tdefault:\n+\t\t\treturn false\n+\t\t}\n+\t}\n+\tif debug {\n+\t\tprintln(\"got\", len(d.current.b), \"bytes, error:\", d.current.err)\n+\t}\n+\treturn true\n+}\n+\n+// Close will release all resources.\n+// It is NOT possible to reuse the decoder after this.\n+func (d *Decoder) Close() {\n+\tif d.current.err == ErrDecoderClosed {\n+\t\treturn\n+\t}\n+\td.drainOutput()\n+\tif d.stream != nil {\n+\t\tclose(d.stream)\n+\t\td.streamWg.Wait()\n+\t\td.stream = nil\n+\t}\n+\tif d.decoders != nil {\n+\t\tclose(d.decoders)\n+\t\tfor dec := range d.decoders {\n+\t\t\tdec.Close()\n+\t\t}\n+\t\td.decoders = nil\n+\t}\n+\tif d.current.d != nil {\n+\t\td.current.d.Close()\n+\t\td.current.d = nil\n+\t}\n+\td.current.err = ErrDecoderClosed\n+}\n+\n+// IOReadCloser returns the decoder as an io.ReadCloser for convenience.\n+// Any changes to the decoder will be reflected, so the returned ReadCloser\n+// can be reused along with the decoder.\n+// io.WriterTo is also supported by the returned ReadCloser.\n+func (d *Decoder) IOReadCloser() io.ReadCloser {\n+\treturn closeWrapper{d: d}\n+}\n+\n+// closeWrapper wraps a function call as a closer.\n+type closeWrapper struct {\n+\td *Decoder\n+}\n+\n+// WriteTo forwards WriteTo calls to the decoder.\n+func (c closeWrapper) WriteTo(w io.Writer) (n int64, err error) {\n+\treturn c.d.WriteTo(w)\n+}\n+\n+// Read forwards read calls to the decoder.\n+func (c closeWrapper) Read(p []byte) (n int, err error) {\n+\treturn c.d.Read(p)\n+}\n+\n+// Close closes the decoder.\n+func (c closeWrapper) Close() error {\n+\tc.d.Close()\n+\treturn nil\n+}\n+\n+type decodeOutput struct {\n+\td   *blockDec\n+\tb   []byte\n+\terr error\n+}\n+\n+type decodeStream struct {\n+\tr io.Reader\n+\n+\t// Blocks ready to be written to output.\n+\toutput chan decodeOutput\n+\n+\t// cancel reading from the input\n+\tcancel chan struct{}\n+}\n+\n+// errEndOfStream indicates that everything from the stream was read.\n+var errEndOfStream = errors.New(\"end-of-stream\")\n+\n+// Create Decoder:\n+// Spawn n block decoders. These accept tasks to decode a block.\n+// Create goroutine that handles stream processing, this will send history to decoders as they are available.\n+// Decoders update the history as they decode.\n+// When a block is returned:\n+// \t\ta) history is sent to the next decoder,\n+// \t\tb) content written to CRC.\n+// \t\tc) return data to WRITER.\n+// \t\td) wait for next block to return data.\n+// Once WRITTEN, the decoders reused by the writer frame decoder for re-use.\n+func (d *Decoder) startStreamDecoder(inStream chan decodeStream) {\n+\tdefer d.streamWg.Done()\n+\tframe := newFrameDec(d.o)\n+\tfor stream := range inStream {\n+\t\tif debug {\n+\t\t\tprintln(\"got new stream\")\n+\t\t}\n+\t\tbr := readerWrapper{r: stream.r}\n+\tdecodeStream:\n+\t\tfor {\n+\t\t\terr := frame.reset(&br)\n+\t\t\tif debug && err != nil {\n+\t\t\t\tprintln(\"Frame decoder returned\", err)\n+\t\t\t}\n+\t\t\tif err != nil {\n+\t\t\t\tstream.output <- decodeOutput{\n+\t\t\t\t\terr: err,\n+\t\t\t\t}\n+\t\t\t\tbreak\n+\t\t\t}\n+\t\t\tif debug {\n+\t\t\t\tprintln(\"starting frame decoder\")\n+\t\t\t}\n+\n+\t\t\t// This goroutine will forward history between frames.\n+\t\t\tframe.frameDone.Add(1)\n+\t\t\tframe.initAsync()\n+\n+\t\t\tgo frame.startDecoder(stream.output)\n+\t\tdecodeFrame:\n+\t\t\t// Go through all blocks of the frame.\n+\t\t\tfor {\n+\t\t\t\tdec := <-d.decoders\n+\t\t\t\tselect {\n+\t\t\t\tcase <-stream.cancel:\n+\t\t\t\t\tif !frame.sendErr(dec, io.EOF) {\n+\t\t\t\t\t\t// To not let the decoder dangle, send it back.\n+\t\t\t\t\t\tstream.output <- decodeOutput{d: dec}\n+\t\t\t\t\t}\n+\t\t\t\t\tbreak decodeStream\n+\t\t\t\tdefault:\n+\t\t\t\t}\n+\t\t\t\terr := frame.next(dec)\n+\t\t\t\tswitch err {\n+\t\t\t\tcase io.EOF:\n+\t\t\t\t\t// End of current frame, no error\n+\t\t\t\t\tprintln(\"EOF on next block\")\n+\t\t\t\t\tbreak decodeFrame\n+\t\t\t\tcase nil:\n+\t\t\t\t\tcontinue\n+\t\t\t\tdefault:\n+\t\t\t\t\tprintln(\"block decoder returned\", err)\n+\t\t\t\t\tbreak decodeStream\n+\t\t\t\t}\n+\t\t\t}\n+\t\t\t// All blocks have started decoding, check if there are more frames.\n+\t\t\tprintln(\"waiting for done\")\n+\t\t\tframe.frameDone.Wait()\n+\t\t\tprintln(\"done waiting...\")\n+\t\t}\n+\t\tframe.frameDone.Wait()\n+\t\tprintln(\"Sending EOS\")\n+\t\tstream.output <- decodeOutput{err: errEndOfStream}\n+\t}\n+}"
    },
    {
      "sha": "2ac9cd2dd30ad21589fd5942851688a08da196db",
      "filename": "backend/vendor/github.com/klauspost/compress/zstd/decoder_options.go",
      "status": "added",
      "additions": 68,
      "deletions": 0,
      "changes": 68,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/klauspost/compress/zstd/decoder_options.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/klauspost/compress/zstd/decoder_options.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/github.com/klauspost/compress/zstd/decoder_options.go?ref=890d7154e777d0673defcd11db3ef792473d68c9",
      "patch": "@@ -0,0 +1,68 @@\n+// Copyright 2019+ Klaus Post. All rights reserved.\n+// License information can be found in the LICENSE file.\n+// Based on work by Yann Collet, released under BSD License.\n+\n+package zstd\n+\n+import (\n+\t\"errors\"\n+\t\"fmt\"\n+\t\"runtime\"\n+)\n+\n+// DOption is an option for creating a decoder.\n+type DOption func(*decoderOptions) error\n+\n+// options retains accumulated state of multiple options.\n+type decoderOptions struct {\n+\tlowMem         bool\n+\tconcurrent     int\n+\tmaxDecodedSize uint64\n+}\n+\n+func (o *decoderOptions) setDefault() {\n+\t*o = decoderOptions{\n+\t\t// use less ram: true for now, but may change.\n+\t\tlowMem:     true,\n+\t\tconcurrent: runtime.GOMAXPROCS(0),\n+\t}\n+\to.maxDecodedSize = 1 << 63\n+}\n+\n+// WithDecoderLowmem will set whether to use a lower amount of memory,\n+// but possibly have to allocate more while running.\n+func WithDecoderLowmem(b bool) DOption {\n+\treturn func(o *decoderOptions) error { o.lowMem = b; return nil }\n+}\n+\n+// WithDecoderConcurrency will set the concurrency,\n+// meaning the maximum number of decoders to run concurrently.\n+// The value supplied must be at least 1.\n+// By default this will be set to GOMAXPROCS.\n+func WithDecoderConcurrency(n int) DOption {\n+\treturn func(o *decoderOptions) error {\n+\t\tif n <= 0 {\n+\t\t\treturn fmt.Errorf(\"Concurrency must be at least 1\")\n+\t\t}\n+\t\to.concurrent = n\n+\t\treturn nil\n+\t}\n+}\n+\n+// WithDecoderMaxMemory allows to set a maximum decoded size for in-memory\n+// non-streaming operations or maximum window size for streaming operations.\n+// This can be used to control memory usage of potentially hostile content.\n+// For streaming operations, the maximum window size is capped at 1<<30 bytes.\n+// Maximum and default is 1 << 63 bytes.\n+func WithDecoderMaxMemory(n uint64) DOption {\n+\treturn func(o *decoderOptions) error {\n+\t\tif n == 0 {\n+\t\t\treturn errors.New(\"WithDecoderMaxMemory must be at least 1\")\n+\t\t}\n+\t\tif n > 1<<63 {\n+\t\t\treturn fmt.Errorf(\"WithDecoderMaxmemory must be less than 1 << 63\")\n+\t\t}\n+\t\to.maxDecodedSize = n\n+\t\treturn nil\n+\t}\n+}"
    },
    {
      "sha": "ee3b09b02a86e05a4702838a82af48f81b0544ed",
      "filename": "backend/vendor/github.com/klauspost/compress/zstd/enc_dfast.go",
      "status": "added",
      "additions": 726,
      "deletions": 0,
      "changes": 726,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/klauspost/compress/zstd/enc_dfast.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/klauspost/compress/zstd/enc_dfast.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/github.com/klauspost/compress/zstd/enc_dfast.go?ref=890d7154e777d0673defcd11db3ef792473d68c9",
      "patch": "@@ -0,0 +1,726 @@\n+// Copyright 2019+ Klaus Post. All rights reserved.\n+// License information can be found in the LICENSE file.\n+// Based on work by Yann Collet, released under BSD License.\n+\n+package zstd\n+\n+const (\n+\tdFastLongTableBits = 17                      // Bits used in the long match table\n+\tdFastLongTableSize = 1 << dFastLongTableBits // Size of the table\n+\tdFastLongTableMask = dFastLongTableSize - 1  // Mask for table indices. Redundant, but can eliminate bounds checks.\n+\n+\tdFastShortTableBits = tableBits                // Bits used in the short match table\n+\tdFastShortTableSize = 1 << dFastShortTableBits // Size of the table\n+\tdFastShortTableMask = dFastShortTableSize - 1  // Mask for table indices. Redundant, but can eliminate bounds checks.\n+)\n+\n+type doubleFastEncoder struct {\n+\tfastEncoder\n+\tlongTable [dFastLongTableSize]tableEntry\n+}\n+\n+// Encode mimmics functionality in zstd_dfast.c\n+func (e *doubleFastEncoder) Encode(blk *blockEnc, src []byte) {\n+\tconst (\n+\t\t// Input margin is the number of bytes we read (8)\n+\t\t// and the maximum we will read ahead (2)\n+\t\tinputMargin            = 8 + 2\n+\t\tminNonLiteralBlockSize = 16\n+\t)\n+\n+\t// Protect against e.cur wraparound.\n+\tfor e.cur > (1<<30)+e.maxMatchOff {\n+\t\tif len(e.hist) == 0 {\n+\t\t\tfor i := range e.table[:] {\n+\t\t\t\te.table[i] = tableEntry{}\n+\t\t\t}\n+\t\t\tfor i := range e.longTable[:] {\n+\t\t\t\te.longTable[i] = tableEntry{}\n+\t\t\t}\n+\t\t\te.cur = e.maxMatchOff\n+\t\t\tbreak\n+\t\t}\n+\t\t// Shift down everything in the table that isn't already too far away.\n+\t\tminOff := e.cur + int32(len(e.hist)) - e.maxMatchOff\n+\t\tfor i := range e.table[:] {\n+\t\t\tv := e.table[i].offset\n+\t\t\tif v < minOff {\n+\t\t\t\tv = 0\n+\t\t\t} else {\n+\t\t\t\tv = v - e.cur + e.maxMatchOff\n+\t\t\t}\n+\t\t\te.table[i].offset = v\n+\t\t}\n+\t\tfor i := range e.longTable[:] {\n+\t\t\tv := e.longTable[i].offset\n+\t\t\tif v < minOff {\n+\t\t\t\tv = 0\n+\t\t\t} else {\n+\t\t\t\tv = v - e.cur + e.maxMatchOff\n+\t\t\t}\n+\t\t\te.longTable[i].offset = v\n+\t\t}\n+\t\te.cur = e.maxMatchOff\n+\t}\n+\n+\ts := e.addBlock(src)\n+\tblk.size = len(src)\n+\tif len(src) < minNonLiteralBlockSize {\n+\t\tblk.extraLits = len(src)\n+\t\tblk.literals = blk.literals[:len(src)]\n+\t\tcopy(blk.literals, src)\n+\t\treturn\n+\t}\n+\n+\t// Override src\n+\tsrc = e.hist\n+\tsLimit := int32(len(src)) - inputMargin\n+\t// stepSize is the number of bytes to skip on every main loop iteration.\n+\t// It should be >= 1.\n+\tstepSize := int32(e.o.targetLength)\n+\tif stepSize == 0 {\n+\t\tstepSize++\n+\t}\n+\n+\tconst kSearchStrength = 8\n+\n+\t// nextEmit is where in src the next emitLiteral should start from.\n+\tnextEmit := s\n+\tcv := load6432(src, s)\n+\n+\t// Relative offsets\n+\toffset1 := int32(blk.recentOffsets[0])\n+\toffset2 := int32(blk.recentOffsets[1])\n+\n+\taddLiterals := func(s *seq, until int32) {\n+\t\tif until == nextEmit {\n+\t\t\treturn\n+\t\t}\n+\t\tblk.literals = append(blk.literals, src[nextEmit:until]...)\n+\t\ts.litLen = uint32(until - nextEmit)\n+\t}\n+\tif debug {\n+\t\tprintln(\"recent offsets:\", blk.recentOffsets)\n+\t}\n+\n+encodeLoop:\n+\tfor {\n+\t\tvar t int32\n+\t\t// We allow the encoder to optionally turn off repeat offsets across blocks\n+\t\tcanRepeat := len(blk.sequences) > 2\n+\n+\t\tfor {\n+\t\t\tif debug && canRepeat && offset1 == 0 {\n+\t\t\t\tpanic(\"offset0 was 0\")\n+\t\t\t}\n+\n+\t\t\tnextHashS := hash5(cv, dFastShortTableBits)\n+\t\t\tnextHashL := hash8(cv, dFastLongTableBits)\n+\t\t\tcandidateL := e.longTable[nextHashL]\n+\t\t\tcandidateS := e.table[nextHashS]\n+\n+\t\t\tconst repOff = 1\n+\t\t\trepIndex := s - offset1 + repOff\n+\t\t\tentry := tableEntry{offset: s + e.cur, val: uint32(cv)}\n+\t\t\te.longTable[nextHashL] = entry\n+\t\t\te.table[nextHashS] = entry\n+\n+\t\t\tif canRepeat {\n+\t\t\t\tif repIndex >= 0 && load3232(src, repIndex) == uint32(cv>>(repOff*8)) {\n+\t\t\t\t\t// Consider history as well.\n+\t\t\t\t\tvar seq seq\n+\t\t\t\t\tlenght := 4 + e.matchlen(s+4+repOff, repIndex+4, src)\n+\n+\t\t\t\t\tseq.matchLen = uint32(lenght - zstdMinMatch)\n+\n+\t\t\t\t\t// We might be able to match backwards.\n+\t\t\t\t\t// Extend as long as we can.\n+\t\t\t\t\tstart := s + repOff\n+\t\t\t\t\t// We end the search early, so we don't risk 0 literals\n+\t\t\t\t\t// and have to do special offset treatment.\n+\t\t\t\t\tstartLimit := nextEmit + 1\n+\n+\t\t\t\t\ttMin := s - e.maxMatchOff\n+\t\t\t\t\tif tMin < 0 {\n+\t\t\t\t\t\ttMin = 0\n+\t\t\t\t\t}\n+\t\t\t\t\tfor repIndex > tMin && start > startLimit && src[repIndex-1] == src[start-1] && seq.matchLen < maxMatchLength-zstdMinMatch-1 {\n+\t\t\t\t\t\trepIndex--\n+\t\t\t\t\t\tstart--\n+\t\t\t\t\t\tseq.matchLen++\n+\t\t\t\t\t}\n+\t\t\t\t\taddLiterals(&seq, start)\n+\n+\t\t\t\t\t// rep 0\n+\t\t\t\t\tseq.offset = 1\n+\t\t\t\t\tif debugSequences {\n+\t\t\t\t\t\tprintln(\"repeat sequence\", seq, \"next s:\", s)\n+\t\t\t\t\t}\n+\t\t\t\t\tblk.sequences = append(blk.sequences, seq)\n+\t\t\t\t\ts += lenght + repOff\n+\t\t\t\t\tnextEmit = s\n+\t\t\t\t\tif s >= sLimit {\n+\t\t\t\t\t\tif debug {\n+\t\t\t\t\t\t\tprintln(\"repeat ended\", s, lenght)\n+\n+\t\t\t\t\t\t}\n+\t\t\t\t\t\tbreak encodeLoop\n+\t\t\t\t\t}\n+\t\t\t\t\tcv = load6432(src, s)\n+\t\t\t\t\tcontinue\n+\t\t\t\t}\n+\t\t\t\tconst repOff2 = 1\n+\t\t\t\t// We deviate from the reference encoder and also check offset 2.\n+\t\t\t\t// Slower and not consistently better, so disabled.\n+\t\t\t\t// repIndex = s - offset2 + repOff2\n+\t\t\t\tif false && repIndex >= 0 && load3232(src, repIndex) == uint32(cv>>(repOff2*8)) {\n+\t\t\t\t\t// Consider history as well.\n+\t\t\t\t\tvar seq seq\n+\t\t\t\t\tlenght := 4 + e.matchlen(s+4+repOff2, repIndex+4, src)\n+\n+\t\t\t\t\tseq.matchLen = uint32(lenght - zstdMinMatch)\n+\n+\t\t\t\t\t// We might be able to match backwards.\n+\t\t\t\t\t// Extend as long as we can.\n+\t\t\t\t\tstart := s + repOff2\n+\t\t\t\t\t// We end the search early, so we don't risk 0 literals\n+\t\t\t\t\t// and have to do special offset treatment.\n+\t\t\t\t\tstartLimit := nextEmit + 1\n+\n+\t\t\t\t\ttMin := s - e.maxMatchOff\n+\t\t\t\t\tif tMin < 0 {\n+\t\t\t\t\t\ttMin = 0\n+\t\t\t\t\t}\n+\t\t\t\t\tfor repIndex > tMin && start > startLimit && src[repIndex-1] == src[start-1] && seq.matchLen < maxMatchLength-zstdMinMatch-1 {\n+\t\t\t\t\t\trepIndex--\n+\t\t\t\t\t\tstart--\n+\t\t\t\t\t\tseq.matchLen++\n+\t\t\t\t\t}\n+\t\t\t\t\taddLiterals(&seq, start)\n+\n+\t\t\t\t\t// rep 2\n+\t\t\t\t\tseq.offset = 2\n+\t\t\t\t\tif debugSequences {\n+\t\t\t\t\t\tprintln(\"repeat sequence 2\", seq, \"next s:\", s)\n+\t\t\t\t\t}\n+\t\t\t\t\tblk.sequences = append(blk.sequences, seq)\n+\t\t\t\t\ts += lenght + repOff2\n+\t\t\t\t\tnextEmit = s\n+\t\t\t\t\tif s >= sLimit {\n+\t\t\t\t\t\tif debug {\n+\t\t\t\t\t\t\tprintln(\"repeat ended\", s, lenght)\n+\n+\t\t\t\t\t\t}\n+\t\t\t\t\t\tbreak encodeLoop\n+\t\t\t\t\t}\n+\t\t\t\t\tcv = load6432(src, s)\n+\t\t\t\t\t// Swap offsets\n+\t\t\t\t\toffset1, offset2 = offset2, offset1\n+\t\t\t\t\tcontinue\n+\t\t\t\t}\n+\t\t\t}\n+\t\t\t// Find the offsets of our two matches.\n+\t\t\tcoffsetL := s - (candidateL.offset - e.cur)\n+\t\t\tcoffsetS := s - (candidateS.offset - e.cur)\n+\n+\t\t\t// Check if we have a long match.\n+\t\t\tif coffsetL < e.maxMatchOff && uint32(cv) == candidateL.val {\n+\t\t\t\t// Found a long match, likely at least 8 bytes.\n+\t\t\t\t// Reference encoder checks all 8 bytes, we only check 4,\n+\t\t\t\t// but the likelihood of both the first 4 bytes and the hash matching should be enough.\n+\t\t\t\tt = candidateL.offset - e.cur\n+\t\t\t\tif debug && s <= t {\n+\t\t\t\t\tpanic(\"s <= t\")\n+\t\t\t\t}\n+\t\t\t\tif debug && s-t > e.maxMatchOff {\n+\t\t\t\t\tpanic(\"s - t >e.maxMatchOff\")\n+\t\t\t\t}\n+\t\t\t\tif debugMatches {\n+\t\t\t\t\tprintln(\"long match\")\n+\t\t\t\t}\n+\t\t\t\tbreak\n+\t\t\t}\n+\n+\t\t\t// Check if we have a short match.\n+\t\t\tif coffsetS < e.maxMatchOff && uint32(cv) == candidateS.val {\n+\t\t\t\t// found a regular match\n+\t\t\t\t// See if we can find a long match at s+1\n+\t\t\t\tconst checkAt = 1\n+\t\t\t\tcv := load6432(src, s+checkAt)\n+\t\t\t\tnextHashL = hash8(cv, dFastLongTableBits)\n+\t\t\t\tcandidateL = e.longTable[nextHashL]\n+\t\t\t\tcoffsetL = s - (candidateL.offset - e.cur) + checkAt\n+\n+\t\t\t\t// We can store it, since we have at least a 4 byte match.\n+\t\t\t\te.longTable[nextHashL] = tableEntry{offset: s + checkAt + e.cur, val: uint32(cv)}\n+\t\t\t\tif coffsetL < e.maxMatchOff && uint32(cv) == candidateL.val {\n+\t\t\t\t\t// Found a long match, likely at least 8 bytes.\n+\t\t\t\t\t// Reference encoder checks all 8 bytes, we only check 4,\n+\t\t\t\t\t// but the likelihood of both the first 4 bytes and the hash matching should be enough.\n+\t\t\t\t\tt = candidateL.offset - e.cur\n+\t\t\t\t\ts += checkAt\n+\t\t\t\t\tif debugMatches {\n+\t\t\t\t\t\tprintln(\"long match (after short)\")\n+\t\t\t\t\t}\n+\t\t\t\t\tbreak\n+\t\t\t\t}\n+\n+\t\t\t\tt = candidateS.offset - e.cur\n+\t\t\t\tif debug && s <= t {\n+\t\t\t\t\tpanic(\"s <= t\")\n+\t\t\t\t}\n+\t\t\t\tif debug && s-t > e.maxMatchOff {\n+\t\t\t\t\tpanic(\"s - t >e.maxMatchOff\")\n+\t\t\t\t}\n+\t\t\t\tif debug && t < 0 {\n+\t\t\t\t\tpanic(\"t<0\")\n+\t\t\t\t}\n+\t\t\t\tif debugMatches {\n+\t\t\t\t\tprintln(\"short match\")\n+\t\t\t\t}\n+\t\t\t\tbreak\n+\t\t\t}\n+\n+\t\t\t// No match found, move forward in input.\n+\t\t\ts += stepSize + ((s - nextEmit) >> (kSearchStrength - 1))\n+\t\t\tif s >= sLimit {\n+\t\t\t\tbreak encodeLoop\n+\t\t\t}\n+\t\t\tcv = load6432(src, s)\n+\t\t}\n+\n+\t\t// A 4-byte match has been found. Update recent offsets.\n+\t\t// We'll later see if more than 4 bytes.\n+\t\toffset2 = offset1\n+\t\toffset1 = s - t\n+\n+\t\tif debug && s <= t {\n+\t\t\tpanic(\"s <= t\")\n+\t\t}\n+\n+\t\tif debug && canRepeat && int(offset1) > len(src) {\n+\t\t\tpanic(\"invalid offset\")\n+\t\t}\n+\n+\t\t// Extend the 4-byte match as long as possible.\n+\t\tl := e.matchlen(s+4, t+4, src) + 4\n+\n+\t\t// Extend backwards\n+\t\ttMin := s - e.maxMatchOff\n+\t\tif tMin < 0 {\n+\t\t\ttMin = 0\n+\t\t}\n+\t\tfor t > tMin && s > nextEmit && src[t-1] == src[s-1] && l < maxMatchLength {\n+\t\t\ts--\n+\t\t\tt--\n+\t\t\tl++\n+\t\t}\n+\n+\t\t// Write our sequence\n+\t\tvar seq seq\n+\t\tseq.litLen = uint32(s - nextEmit)\n+\t\tseq.matchLen = uint32(l - zstdMinMatch)\n+\t\tif seq.litLen > 0 {\n+\t\t\tblk.literals = append(blk.literals, src[nextEmit:s]...)\n+\t\t}\n+\t\tseq.offset = uint32(s-t) + 3\n+\t\ts += l\n+\t\tif debugSequences {\n+\t\t\tprintln(\"sequence\", seq, \"next s:\", s)\n+\t\t}\n+\t\tblk.sequences = append(blk.sequences, seq)\n+\t\tnextEmit = s\n+\t\tif s >= sLimit {\n+\t\t\tbreak encodeLoop\n+\t\t}\n+\n+\t\t// Index match start+1 (long) and start+2 (short)\n+\t\tindex0 := s - l + 1\n+\t\t// Index match end-2 (long) and end-1 (short)\n+\t\tindex1 := s - 2\n+\n+\t\tcv0 := load6432(src, index0)\n+\t\tcv1 := load6432(src, index1)\n+\t\tte0 := tableEntry{offset: index0 + e.cur, val: uint32(cv0)}\n+\t\tte1 := tableEntry{offset: index1 + e.cur, val: uint32(cv1)}\n+\t\te.longTable[hash8(cv0, dFastLongTableBits)] = te0\n+\t\te.longTable[hash8(cv1, dFastLongTableBits)] = te1\n+\t\tcv0 >>= 8\n+\t\tcv1 >>= 8\n+\t\tte0.offset++\n+\t\tte1.offset++\n+\t\tte0.val = uint32(cv0)\n+\t\tte1.val = uint32(cv1)\n+\t\te.table[hash5(cv0, dFastShortTableBits)] = te0\n+\t\te.table[hash5(cv1, dFastShortTableBits)] = te1\n+\n+\t\tcv = load6432(src, s)\n+\n+\t\tif !canRepeat {\n+\t\t\tcontinue\n+\t\t}\n+\n+\t\t// Check offset 2\n+\t\tfor {\n+\t\t\to2 := s - offset2\n+\t\t\tif load3232(src, o2) != uint32(cv) {\n+\t\t\t\t// Do regular search\n+\t\t\t\tbreak\n+\t\t\t}\n+\n+\t\t\t// Store this, since we have it.\n+\t\t\tnextHashS := hash5(cv1>>8, dFastShortTableBits)\n+\t\t\tnextHashL := hash8(cv, dFastLongTableBits)\n+\n+\t\t\t// We have at least 4 byte match.\n+\t\t\t// No need to check backwards. We come straight from a match\n+\t\t\tl := 4 + e.matchlen(s+4, o2+4, src)\n+\n+\t\t\tentry := tableEntry{offset: s + e.cur, val: uint32(cv)}\n+\t\t\te.longTable[nextHashL] = entry\n+\t\t\te.table[nextHashS] = entry\n+\t\t\tseq.matchLen = uint32(l) - zstdMinMatch\n+\t\t\tseq.litLen = 0\n+\n+\t\t\t// Since litlen is always 0, this is offset 1.\n+\t\t\tseq.offset = 1\n+\t\t\ts += l\n+\t\t\tnextEmit = s\n+\t\t\tif debugSequences {\n+\t\t\t\tprintln(\"sequence\", seq, \"next s:\", s)\n+\t\t\t}\n+\t\t\tblk.sequences = append(blk.sequences, seq)\n+\n+\t\t\t// Swap offset 1 and 2.\n+\t\t\toffset1, offset2 = offset2, offset1\n+\t\t\tif s >= sLimit {\n+\t\t\t\t// Finished\n+\t\t\t\tbreak encodeLoop\n+\t\t\t}\n+\t\t\tcv = load6432(src, s)\n+\t\t}\n+\t}\n+\n+\tif int(nextEmit) < len(src) {\n+\t\tblk.literals = append(blk.literals, src[nextEmit:]...)\n+\t\tblk.extraLits = len(src) - int(nextEmit)\n+\t}\n+\tblk.recentOffsets[0] = uint32(offset1)\n+\tblk.recentOffsets[1] = uint32(offset2)\n+\tif debug {\n+\t\tprintln(\"returning, recent offsets:\", blk.recentOffsets, \"extra literals:\", blk.extraLits)\n+\t}\n+}\n+\n+// EncodeNoHist will encode a block with no history and no following blocks.\n+// Most notable difference is that src will not be copied for history and\n+// we do not need to check for max match length.\n+func (e *doubleFastEncoder) EncodeNoHist(blk *blockEnc, src []byte) {\n+\tconst (\n+\t\t// Input margin is the number of bytes we read (8)\n+\t\t// and the maximum we will read ahead (2)\n+\t\tinputMargin            = 8 + 2\n+\t\tminNonLiteralBlockSize = 16\n+\t)\n+\n+\t// Protect against e.cur wraparound.\n+\tif e.cur > (1<<30)+e.maxMatchOff {\n+\t\tfor i := range e.table[:] {\n+\t\t\te.table[i] = tableEntry{}\n+\t\t}\n+\t\tfor i := range e.longTable[:] {\n+\t\t\te.longTable[i] = tableEntry{}\n+\t\t}\n+\t\te.cur = e.maxMatchOff\n+\t}\n+\n+\ts := int32(0)\n+\tblk.size = len(src)\n+\tif len(src) < minNonLiteralBlockSize {\n+\t\tblk.extraLits = len(src)\n+\t\tblk.literals = blk.literals[:len(src)]\n+\t\tcopy(blk.literals, src)\n+\t\treturn\n+\t}\n+\n+\t// Override src\n+\tsLimit := int32(len(src)) - inputMargin\n+\t// stepSize is the number of bytes to skip on every main loop iteration.\n+\t// It should be >= 1.\n+\tstepSize := int32(e.o.targetLength)\n+\tif stepSize == 0 {\n+\t\tstepSize++\n+\t}\n+\n+\tconst kSearchStrength = 8\n+\n+\t// nextEmit is where in src the next emitLiteral should start from.\n+\tnextEmit := s\n+\tcv := load6432(src, s)\n+\n+\t// Relative offsets\n+\toffset1 := int32(blk.recentOffsets[0])\n+\toffset2 := int32(blk.recentOffsets[1])\n+\n+\taddLiterals := func(s *seq, until int32) {\n+\t\tif until == nextEmit {\n+\t\t\treturn\n+\t\t}\n+\t\tblk.literals = append(blk.literals, src[nextEmit:until]...)\n+\t\ts.litLen = uint32(until - nextEmit)\n+\t}\n+\tif debug {\n+\t\tprintln(\"recent offsets:\", blk.recentOffsets)\n+\t}\n+\n+encodeLoop:\n+\tfor {\n+\t\tvar t int32\n+\t\tfor {\n+\n+\t\t\tnextHashS := hash5(cv, dFastShortTableBits)\n+\t\t\tnextHashL := hash8(cv, dFastLongTableBits)\n+\t\t\tcandidateL := e.longTable[nextHashL]\n+\t\t\tcandidateS := e.table[nextHashS]\n+\n+\t\t\tconst repOff = 1\n+\t\t\trepIndex := s - offset1 + repOff\n+\t\t\tentry := tableEntry{offset: s + e.cur, val: uint32(cv)}\n+\t\t\te.longTable[nextHashL] = entry\n+\t\t\te.table[nextHashS] = entry\n+\n+\t\t\tif len(blk.sequences) > 2 {\n+\t\t\t\tif load3232(src, repIndex) == uint32(cv>>(repOff*8)) {\n+\t\t\t\t\t// Consider history as well.\n+\t\t\t\t\tvar seq seq\n+\t\t\t\t\t//length := 4 + e.matchlen(s+4+repOff, repIndex+4, src)\n+\t\t\t\t\tlength := 4 + int32(matchLen(src[s+4+repOff:], src[repIndex+4:]))\n+\n+\t\t\t\t\tseq.matchLen = uint32(length - zstdMinMatch)\n+\n+\t\t\t\t\t// We might be able to match backwards.\n+\t\t\t\t\t// Extend as long as we can.\n+\t\t\t\t\tstart := s + repOff\n+\t\t\t\t\t// We end the search early, so we don't risk 0 literals\n+\t\t\t\t\t// and have to do special offset treatment.\n+\t\t\t\t\tstartLimit := nextEmit + 1\n+\n+\t\t\t\t\ttMin := s - e.maxMatchOff\n+\t\t\t\t\tif tMin < 0 {\n+\t\t\t\t\t\ttMin = 0\n+\t\t\t\t\t}\n+\t\t\t\t\tfor repIndex > tMin && start > startLimit && src[repIndex-1] == src[start-1] {\n+\t\t\t\t\t\trepIndex--\n+\t\t\t\t\t\tstart--\n+\t\t\t\t\t\tseq.matchLen++\n+\t\t\t\t\t}\n+\t\t\t\t\taddLiterals(&seq, start)\n+\n+\t\t\t\t\t// rep 0\n+\t\t\t\t\tseq.offset = 1\n+\t\t\t\t\tif debugSequences {\n+\t\t\t\t\t\tprintln(\"repeat sequence\", seq, \"next s:\", s)\n+\t\t\t\t\t}\n+\t\t\t\t\tblk.sequences = append(blk.sequences, seq)\n+\t\t\t\t\ts += length + repOff\n+\t\t\t\t\tnextEmit = s\n+\t\t\t\t\tif s >= sLimit {\n+\t\t\t\t\t\tif debug {\n+\t\t\t\t\t\t\tprintln(\"repeat ended\", s, length)\n+\n+\t\t\t\t\t\t}\n+\t\t\t\t\t\tbreak encodeLoop\n+\t\t\t\t\t}\n+\t\t\t\t\tcv = load6432(src, s)\n+\t\t\t\t\tcontinue\n+\t\t\t\t}\n+\t\t\t}\n+\t\t\t// Find the offsets of our two matches.\n+\t\t\tcoffsetL := s - (candidateL.offset - e.cur)\n+\t\t\tcoffsetS := s - (candidateS.offset - e.cur)\n+\n+\t\t\t// Check if we have a long match.\n+\t\t\tif coffsetL < e.maxMatchOff && uint32(cv) == candidateL.val {\n+\t\t\t\t// Found a long match, likely at least 8 bytes.\n+\t\t\t\t// Reference encoder checks all 8 bytes, we only check 4,\n+\t\t\t\t// but the likelihood of both the first 4 bytes and the hash matching should be enough.\n+\t\t\t\tt = candidateL.offset - e.cur\n+\t\t\t\tif debug && s <= t {\n+\t\t\t\t\tpanic(\"s <= t\")\n+\t\t\t\t}\n+\t\t\t\tif debug && s-t > e.maxMatchOff {\n+\t\t\t\t\tpanic(\"s - t >e.maxMatchOff\")\n+\t\t\t\t}\n+\t\t\t\tif debugMatches {\n+\t\t\t\t\tprintln(\"long match\")\n+\t\t\t\t}\n+\t\t\t\tbreak\n+\t\t\t}\n+\n+\t\t\t// Check if we have a short match.\n+\t\t\tif coffsetS < e.maxMatchOff && uint32(cv) == candidateS.val {\n+\t\t\t\t// found a regular match\n+\t\t\t\t// See if we can find a long match at s+1\n+\t\t\t\tconst checkAt = 1\n+\t\t\t\tcv := load6432(src, s+checkAt)\n+\t\t\t\tnextHashL = hash8(cv, dFastLongTableBits)\n+\t\t\t\tcandidateL = e.longTable[nextHashL]\n+\t\t\t\tcoffsetL = s - (candidateL.offset - e.cur) + checkAt\n+\n+\t\t\t\t// We can store it, since we have at least a 4 byte match.\n+\t\t\t\te.longTable[nextHashL] = tableEntry{offset: s + checkAt + e.cur, val: uint32(cv)}\n+\t\t\t\tif coffsetL < e.maxMatchOff && uint32(cv) == candidateL.val {\n+\t\t\t\t\t// Found a long match, likely at least 8 bytes.\n+\t\t\t\t\t// Reference encoder checks all 8 bytes, we only check 4,\n+\t\t\t\t\t// but the likelihood of both the first 4 bytes and the hash matching should be enough.\n+\t\t\t\t\tt = candidateL.offset - e.cur\n+\t\t\t\t\ts += checkAt\n+\t\t\t\t\tif debugMatches {\n+\t\t\t\t\t\tprintln(\"long match (after short)\")\n+\t\t\t\t\t}\n+\t\t\t\t\tbreak\n+\t\t\t\t}\n+\n+\t\t\t\tt = candidateS.offset - e.cur\n+\t\t\t\tif debug && s <= t {\n+\t\t\t\t\tpanic(\"s <= t\")\n+\t\t\t\t}\n+\t\t\t\tif debug && s-t > e.maxMatchOff {\n+\t\t\t\t\tpanic(\"s - t >e.maxMatchOff\")\n+\t\t\t\t}\n+\t\t\t\tif debug && t < 0 {\n+\t\t\t\t\tpanic(\"t<0\")\n+\t\t\t\t}\n+\t\t\t\tif debugMatches {\n+\t\t\t\t\tprintln(\"short match\")\n+\t\t\t\t}\n+\t\t\t\tbreak\n+\t\t\t}\n+\n+\t\t\t// No match found, move forward in input.\n+\t\t\ts += stepSize + ((s - nextEmit) >> (kSearchStrength - 1))\n+\t\t\tif s >= sLimit {\n+\t\t\t\tbreak encodeLoop\n+\t\t\t}\n+\t\t\tcv = load6432(src, s)\n+\t\t}\n+\n+\t\t// A 4-byte match has been found. Update recent offsets.\n+\t\t// We'll later see if more than 4 bytes.\n+\t\toffset2 = offset1\n+\t\toffset1 = s - t\n+\n+\t\tif debug && s <= t {\n+\t\t\tpanic(\"s <= t\")\n+\t\t}\n+\n+\t\t// Extend the 4-byte match as long as possible.\n+\t\t//l := e.matchlen(s+4, t+4, src) + 4\n+\t\tl := int32(matchLen(src[s+4:], src[t+4:])) + 4\n+\n+\t\t// Extend backwards\n+\t\ttMin := s - e.maxMatchOff\n+\t\tif tMin < 0 {\n+\t\t\ttMin = 0\n+\t\t}\n+\t\tfor t > tMin && s > nextEmit && src[t-1] == src[s-1] {\n+\t\t\ts--\n+\t\t\tt--\n+\t\t\tl++\n+\t\t}\n+\n+\t\t// Write our sequence\n+\t\tvar seq seq\n+\t\tseq.litLen = uint32(s - nextEmit)\n+\t\tseq.matchLen = uint32(l - zstdMinMatch)\n+\t\tif seq.litLen > 0 {\n+\t\t\tblk.literals = append(blk.literals, src[nextEmit:s]...)\n+\t\t}\n+\t\tseq.offset = uint32(s-t) + 3\n+\t\ts += l\n+\t\tif debugSequences {\n+\t\t\tprintln(\"sequence\", seq, \"next s:\", s)\n+\t\t}\n+\t\tblk.sequences = append(blk.sequences, seq)\n+\t\tnextEmit = s\n+\t\tif s >= sLimit {\n+\t\t\tbreak encodeLoop\n+\t\t}\n+\n+\t\t// Index match start+1 (long) and start+2 (short)\n+\t\tindex0 := s - l + 1\n+\t\t// Index match end-2 (long) and end-1 (short)\n+\t\tindex1 := s - 2\n+\n+\t\tcv0 := load6432(src, index0)\n+\t\tcv1 := load6432(src, index1)\n+\t\tte0 := tableEntry{offset: index0 + e.cur, val: uint32(cv0)}\n+\t\tte1 := tableEntry{offset: index1 + e.cur, val: uint32(cv1)}\n+\t\te.longTable[hash8(cv0, dFastLongTableBits)] = te0\n+\t\te.longTable[hash8(cv1, dFastLongTableBits)] = te1\n+\t\tcv0 >>= 8\n+\t\tcv1 >>= 8\n+\t\tte0.offset++\n+\t\tte1.offset++\n+\t\tte0.val = uint32(cv0)\n+\t\tte1.val = uint32(cv1)\n+\t\te.table[hash5(cv0, dFastShortTableBits)] = te0\n+\t\te.table[hash5(cv1, dFastShortTableBits)] = te1\n+\n+\t\tcv = load6432(src, s)\n+\n+\t\tif len(blk.sequences) <= 2 {\n+\t\t\tcontinue\n+\t\t}\n+\n+\t\t// Check offset 2\n+\t\tfor {\n+\t\t\to2 := s - offset2\n+\t\t\tif load3232(src, o2) != uint32(cv) {\n+\t\t\t\t// Do regular search\n+\t\t\t\tbreak\n+\t\t\t}\n+\n+\t\t\t// Store this, since we have it.\n+\t\t\tnextHashS := hash5(cv1>>8, dFastShortTableBits)\n+\t\t\tnextHashL := hash8(cv, dFastLongTableBits)\n+\n+\t\t\t// We have at least 4 byte match.\n+\t\t\t// No need to check backwards. We come straight from a match\n+\t\t\t//l := 4 + e.matchlen(s+4, o2+4, src)\n+\t\t\tl := 4 + int32(matchLen(src[s+4:], src[o2+4:]))\n+\n+\t\t\tentry := tableEntry{offset: s + e.cur, val: uint32(cv)}\n+\t\t\te.longTable[nextHashL] = entry\n+\t\t\te.table[nextHashS] = entry\n+\t\t\tseq.matchLen = uint32(l) - zstdMinMatch\n+\t\t\tseq.litLen = 0\n+\n+\t\t\t// Since litlen is always 0, this is offset 1.\n+\t\t\tseq.offset = 1\n+\t\t\ts += l\n+\t\t\tnextEmit = s\n+\t\t\tif debugSequences {\n+\t\t\t\tprintln(\"sequence\", seq, \"next s:\", s)\n+\t\t\t}\n+\t\t\tblk.sequences = append(blk.sequences, seq)\n+\n+\t\t\t// Swap offset 1 and 2.\n+\t\t\toffset1, offset2 = offset2, offset1\n+\t\t\tif s >= sLimit {\n+\t\t\t\t// Finished\n+\t\t\t\tbreak encodeLoop\n+\t\t\t}\n+\t\t\tcv = load6432(src, s)\n+\t\t}\n+\t}\n+\n+\tif int(nextEmit) < len(src) {\n+\t\tblk.literals = append(blk.literals, src[nextEmit:]...)\n+\t\tblk.extraLits = len(src) - int(nextEmit)\n+\t}\n+\tif debug {\n+\t\tprintln(\"returning, recent offsets:\", blk.recentOffsets, \"extra literals:\", blk.extraLits)\n+\t}\n+\n+}"
    },
    {
      "sha": "0bdddac5b4df33ec1ab56c373798a73a7595468f",
      "filename": "backend/vendor/github.com/klauspost/compress/zstd/enc_fast.go",
      "status": "added",
      "additions": 656,
      "deletions": 0,
      "changes": 656,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/klauspost/compress/zstd/enc_fast.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/klauspost/compress/zstd/enc_fast.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/github.com/klauspost/compress/zstd/enc_fast.go?ref=890d7154e777d0673defcd11db3ef792473d68c9",
      "patch": "@@ -0,0 +1,656 @@\n+// Copyright 2019+ Klaus Post. All rights reserved.\n+// License information can be found in the LICENSE file.\n+// Based on work by Yann Collet, released under BSD License.\n+\n+package zstd\n+\n+import (\n+\t\"math/bits\"\n+\n+\t\"github.com/klauspost/compress/zstd/internal/xxhash\"\n+)\n+\n+const (\n+\ttableBits      = 15             // Bits used in the table\n+\ttableSize      = 1 << tableBits // Size of the table\n+\ttableMask      = tableSize - 1  // Mask for table indices. Redundant, but can eliminate bounds checks.\n+\tmaxMatchLength = 131074\n+)\n+\n+type tableEntry struct {\n+\tval    uint32\n+\toffset int32\n+}\n+\n+type fastEncoder struct {\n+\to encParams\n+\t// cur is the offset at the start of hist\n+\tcur int32\n+\t// maximum offset. Should be at least 2x block size.\n+\tmaxMatchOff int32\n+\thist        []byte\n+\tcrc         *xxhash.Digest\n+\ttable       [tableSize]tableEntry\n+\ttmp         [8]byte\n+\tblk         *blockEnc\n+}\n+\n+// CRC returns the underlying CRC writer.\n+func (e *fastEncoder) CRC() *xxhash.Digest {\n+\treturn e.crc\n+}\n+\n+// AppendCRC will append the CRC to the destination slice and return it.\n+func (e *fastEncoder) AppendCRC(dst []byte) []byte {\n+\tcrc := e.crc.Sum(e.tmp[:0])\n+\tdst = append(dst, crc[7], crc[6], crc[5], crc[4])\n+\treturn dst\n+}\n+\n+// WindowSize returns the window size of the encoder,\n+// or a window size small enough to contain the input size, if > 0.\n+func (e *fastEncoder) WindowSize(size int) int32 {\n+\tif size > 0 && size < int(e.maxMatchOff) {\n+\t\tb := int32(1) << uint(bits.Len(uint(size)))\n+\t\t// Keep minimum window.\n+\t\tif b < 1024 {\n+\t\t\tb = 1024\n+\t\t}\n+\t\treturn b\n+\t}\n+\treturn e.maxMatchOff\n+}\n+\n+// Block returns the current block.\n+func (e *fastEncoder) Block() *blockEnc {\n+\treturn e.blk\n+}\n+\n+// Encode mimmics functionality in zstd_fast.c\n+func (e *fastEncoder) Encode(blk *blockEnc, src []byte) {\n+\tconst (\n+\t\tinputMargin            = 8\n+\t\tminNonLiteralBlockSize = 1 + 1 + inputMargin\n+\t)\n+\n+\t// Protect against e.cur wraparound.\n+\tfor e.cur > (1<<30)+e.maxMatchOff {\n+\t\tif len(e.hist) == 0 {\n+\t\t\tfor i := range e.table[:] {\n+\t\t\t\te.table[i] = tableEntry{}\n+\t\t\t}\n+\t\t\te.cur = e.maxMatchOff\n+\t\t\tbreak\n+\t\t}\n+\t\t// Shift down everything in the table that isn't already too far away.\n+\t\tminOff := e.cur + int32(len(e.hist)) - e.maxMatchOff\n+\t\tfor i := range e.table[:] {\n+\t\t\tv := e.table[i].offset\n+\t\t\tif v < minOff {\n+\t\t\t\tv = 0\n+\t\t\t} else {\n+\t\t\t\tv = v - e.cur + e.maxMatchOff\n+\t\t\t}\n+\t\t\te.table[i].offset = v\n+\t\t}\n+\t\te.cur = e.maxMatchOff\n+\t}\n+\n+\ts := e.addBlock(src)\n+\tblk.size = len(src)\n+\tif len(src) < minNonLiteralBlockSize {\n+\t\tblk.extraLits = len(src)\n+\t\tblk.literals = blk.literals[:len(src)]\n+\t\tcopy(blk.literals, src)\n+\t\treturn\n+\t}\n+\n+\t// Override src\n+\tsrc = e.hist\n+\tsLimit := int32(len(src)) - inputMargin\n+\t// stepSize is the number of bytes to skip on every main loop iteration.\n+\t// It should be >= 2.\n+\tstepSize := int32(e.o.targetLength)\n+\tif stepSize == 0 {\n+\t\tstepSize++\n+\t}\n+\tstepSize++\n+\n+\t// TEMPLATE\n+\tconst hashLog = tableBits\n+\t// seems global, but would be nice to tweak.\n+\tconst kSearchStrength = 8\n+\n+\t// nextEmit is where in src the next emitLiteral should start from.\n+\tnextEmit := s\n+\tcv := load6432(src, s)\n+\n+\t// Relative offsets\n+\toffset1 := int32(blk.recentOffsets[0])\n+\toffset2 := int32(blk.recentOffsets[1])\n+\n+\taddLiterals := func(s *seq, until int32) {\n+\t\tif until == nextEmit {\n+\t\t\treturn\n+\t\t}\n+\t\tblk.literals = append(blk.literals, src[nextEmit:until]...)\n+\t\ts.litLen = uint32(until - nextEmit)\n+\t}\n+\tif debug {\n+\t\tprintln(\"recent offsets:\", blk.recentOffsets)\n+\t}\n+\n+encodeLoop:\n+\tfor {\n+\t\t// t will contain the match offset when we find one.\n+\t\t// When existing the search loop, we have already checked 4 bytes.\n+\t\tvar t int32\n+\n+\t\t// We will not use repeat offsets across blocks.\n+\t\t// By not using them for the first 3 matches\n+\t\tcanRepeat := len(blk.sequences) > 2\n+\n+\t\tfor {\n+\t\t\tif debug && canRepeat && offset1 == 0 {\n+\t\t\t\tpanic(\"offset0 was 0\")\n+\t\t\t}\n+\n+\t\t\tnextHash := hash6(cv, hashLog)\n+\t\t\tnextHash2 := hash6(cv>>8, hashLog)\n+\t\t\tcandidate := e.table[nextHash]\n+\t\t\tcandidate2 := e.table[nextHash2]\n+\t\t\trepIndex := s - offset1 + 2\n+\n+\t\t\te.table[nextHash] = tableEntry{offset: s + e.cur, val: uint32(cv)}\n+\t\t\te.table[nextHash2] = tableEntry{offset: s + e.cur + 1, val: uint32(cv >> 8)}\n+\n+\t\t\tif canRepeat && repIndex >= 0 && load3232(src, repIndex) == uint32(cv>>16) {\n+\t\t\t\t// Consider history as well.\n+\t\t\t\tvar seq seq\n+\t\t\t\tlenght := 4 + e.matchlen(s+6, repIndex+4, src)\n+\n+\t\t\t\tseq.matchLen = uint32(lenght - zstdMinMatch)\n+\n+\t\t\t\t// We might be able to match backwards.\n+\t\t\t\t// Extend as long as we can.\n+\t\t\t\tstart := s + 2\n+\t\t\t\t// We end the search early, so we don't risk 0 literals\n+\t\t\t\t// and have to do special offset treatment.\n+\t\t\t\tstartLimit := nextEmit + 1\n+\n+\t\t\t\tsMin := s - e.maxMatchOff\n+\t\t\t\tif sMin < 0 {\n+\t\t\t\t\tsMin = 0\n+\t\t\t\t}\n+\t\t\t\tfor repIndex > sMin && start > startLimit && src[repIndex-1] == src[start-1] && seq.matchLen < maxMatchLength-zstdMinMatch {\n+\t\t\t\t\trepIndex--\n+\t\t\t\t\tstart--\n+\t\t\t\t\tseq.matchLen++\n+\t\t\t\t}\n+\t\t\t\taddLiterals(&seq, start)\n+\n+\t\t\t\t// rep 0\n+\t\t\t\tseq.offset = 1\n+\t\t\t\tif debugSequences {\n+\t\t\t\t\tprintln(\"repeat sequence\", seq, \"next s:\", s)\n+\t\t\t\t}\n+\t\t\t\tblk.sequences = append(blk.sequences, seq)\n+\t\t\t\ts += lenght + 2\n+\t\t\t\tnextEmit = s\n+\t\t\t\tif s >= sLimit {\n+\t\t\t\t\tif debug {\n+\t\t\t\t\t\tprintln(\"repeat ended\", s, lenght)\n+\n+\t\t\t\t\t}\n+\t\t\t\t\tbreak encodeLoop\n+\t\t\t\t}\n+\t\t\t\tcv = load6432(src, s)\n+\t\t\t\tcontinue\n+\t\t\t}\n+\t\t\tcoffset0 := s - (candidate.offset - e.cur)\n+\t\t\tcoffset1 := s - (candidate2.offset - e.cur) + 1\n+\t\t\tif coffset0 < e.maxMatchOff && uint32(cv) == candidate.val {\n+\t\t\t\t// found a regular match\n+\t\t\t\tt = candidate.offset - e.cur\n+\t\t\t\tif debug && s <= t {\n+\t\t\t\t\tpanic(\"s <= t\")\n+\t\t\t\t}\n+\t\t\t\tif debug && s-t > e.maxMatchOff {\n+\t\t\t\t\tpanic(\"s - t >e.maxMatchOff\")\n+\t\t\t\t}\n+\t\t\t\tbreak\n+\t\t\t}\n+\n+\t\t\tif coffset1 < e.maxMatchOff && uint32(cv>>8) == candidate2.val {\n+\t\t\t\t// found a regular match\n+\t\t\t\tt = candidate2.offset - e.cur\n+\t\t\t\ts++\n+\t\t\t\tif debug && s <= t {\n+\t\t\t\t\tpanic(\"s <= t\")\n+\t\t\t\t}\n+\t\t\t\tif debug && s-t > e.maxMatchOff {\n+\t\t\t\t\tpanic(\"s - t >e.maxMatchOff\")\n+\t\t\t\t}\n+\t\t\t\tif debug && t < 0 {\n+\t\t\t\t\tpanic(\"t<0\")\n+\t\t\t\t}\n+\t\t\t\tbreak\n+\t\t\t}\n+\t\t\ts += stepSize + ((s - nextEmit) >> (kSearchStrength - 1))\n+\t\t\tif s >= sLimit {\n+\t\t\t\tbreak encodeLoop\n+\t\t\t}\n+\t\t\tcv = load6432(src, s)\n+\t\t}\n+\t\t// A 4-byte match has been found. We'll later see if more than 4 bytes.\n+\t\toffset2 = offset1\n+\t\toffset1 = s - t\n+\n+\t\tif debug && s <= t {\n+\t\t\tpanic(\"s <= t\")\n+\t\t}\n+\n+\t\tif debug && canRepeat && int(offset1) > len(src) {\n+\t\t\tpanic(\"invalid offset\")\n+\t\t}\n+\n+\t\t// Extend the 4-byte match as long as possible.\n+\t\tl := e.matchlen(s+4, t+4, src) + 4\n+\n+\t\t// Extend backwards\n+\t\ttMin := s - e.maxMatchOff\n+\t\tif tMin < 0 {\n+\t\t\ttMin = 0\n+\t\t}\n+\t\tfor t > tMin && s > nextEmit && src[t-1] == src[s-1] && l < maxMatchLength {\n+\t\t\ts--\n+\t\t\tt--\n+\t\t\tl++\n+\t\t}\n+\n+\t\t// Write our sequence.\n+\t\tvar seq seq\n+\t\tseq.litLen = uint32(s - nextEmit)\n+\t\tseq.matchLen = uint32(l - zstdMinMatch)\n+\t\tif seq.litLen > 0 {\n+\t\t\tblk.literals = append(blk.literals, src[nextEmit:s]...)\n+\t\t}\n+\t\t// Don't use repeat offsets\n+\t\tseq.offset = uint32(s-t) + 3\n+\t\ts += l\n+\t\tif debugSequences {\n+\t\t\tprintln(\"sequence\", seq, \"next s:\", s)\n+\t\t}\n+\t\tblk.sequences = append(blk.sequences, seq)\n+\t\tnextEmit = s\n+\t\tif s >= sLimit {\n+\t\t\tbreak encodeLoop\n+\t\t}\n+\t\tcv = load6432(src, s)\n+\n+\t\t// Check offset 2\n+\t\tif o2 := s - offset2; canRepeat && load3232(src, o2) == uint32(cv) {\n+\t\t\t// We have at least 4 byte match.\n+\t\t\t// No need to check backwards. We come straight from a match\n+\t\t\tl := 4 + e.matchlen(s+4, o2+4, src)\n+\n+\t\t\t// Store this, since we have it.\n+\t\t\tnextHash := hash6(cv, hashLog)\n+\t\t\te.table[nextHash] = tableEntry{offset: s + e.cur, val: uint32(cv)}\n+\t\t\tseq.matchLen = uint32(l) - zstdMinMatch\n+\t\t\tseq.litLen = 0\n+\t\t\t// Since litlen is always 0, this is offset 1.\n+\t\t\tseq.offset = 1\n+\t\t\ts += l\n+\t\t\tnextEmit = s\n+\t\t\tif debugSequences {\n+\t\t\t\tprintln(\"sequence\", seq, \"next s:\", s)\n+\t\t\t}\n+\t\t\tblk.sequences = append(blk.sequences, seq)\n+\n+\t\t\t// Swap offset 1 and 2.\n+\t\t\toffset1, offset2 = offset2, offset1\n+\t\t\tif s >= sLimit {\n+\t\t\t\tbreak encodeLoop\n+\t\t\t}\n+\t\t\t// Prepare next loop.\n+\t\t\tcv = load6432(src, s)\n+\t\t}\n+\t}\n+\n+\tif int(nextEmit) < len(src) {\n+\t\tblk.literals = append(blk.literals, src[nextEmit:]...)\n+\t\tblk.extraLits = len(src) - int(nextEmit)\n+\t}\n+\tblk.recentOffsets[0] = uint32(offset1)\n+\tblk.recentOffsets[1] = uint32(offset2)\n+\tif debug {\n+\t\tprintln(\"returning, recent offsets:\", blk.recentOffsets, \"extra literals:\", blk.extraLits)\n+\t}\n+}\n+\n+// EncodeNoHist will encode a block with no history and no following blocks.\n+// Most notable difference is that src will not be copied for history and\n+// we do not need to check for max match length.\n+func (e *fastEncoder) EncodeNoHist(blk *blockEnc, src []byte) {\n+\tconst (\n+\t\tinputMargin            = 8\n+\t\tminNonLiteralBlockSize = 1 + 1 + inputMargin\n+\t)\n+\tif debug {\n+\t\tif len(src) > maxBlockSize {\n+\t\t\tpanic(\"src too big\")\n+\t\t}\n+\t}\n+\t// Protect against e.cur wraparound.\n+\tif e.cur > (1<<30)+e.maxMatchOff {\n+\t\tfor i := range e.table[:] {\n+\t\t\te.table[i] = tableEntry{}\n+\t\t}\n+\t\te.cur = e.maxMatchOff\n+\t}\n+\n+\ts := int32(0)\n+\tblk.size = len(src)\n+\tif len(src) < minNonLiteralBlockSize {\n+\t\tblk.extraLits = len(src)\n+\t\tblk.literals = blk.literals[:len(src)]\n+\t\tcopy(blk.literals, src)\n+\t\treturn\n+\t}\n+\n+\tsLimit := int32(len(src)) - inputMargin\n+\t// stepSize is the number of bytes to skip on every main loop iteration.\n+\t// It should be >= 2.\n+\tconst stepSize = 2\n+\n+\t// TEMPLATE\n+\tconst hashLog = tableBits\n+\t// seems global, but would be nice to tweak.\n+\tconst kSearchStrength = 8\n+\n+\t// nextEmit is where in src the next emitLiteral should start from.\n+\tnextEmit := s\n+\tcv := load6432(src, s)\n+\n+\t// Relative offsets\n+\toffset1 := int32(blk.recentOffsets[0])\n+\toffset2 := int32(blk.recentOffsets[1])\n+\n+\taddLiterals := func(s *seq, until int32) {\n+\t\tif until == nextEmit {\n+\t\t\treturn\n+\t\t}\n+\t\tblk.literals = append(blk.literals, src[nextEmit:until]...)\n+\t\ts.litLen = uint32(until - nextEmit)\n+\t}\n+\tif debug {\n+\t\tprintln(\"recent offsets:\", blk.recentOffsets)\n+\t}\n+\n+encodeLoop:\n+\tfor {\n+\t\t// t will contain the match offset when we find one.\n+\t\t// When existing the search loop, we have already checked 4 bytes.\n+\t\tvar t int32\n+\n+\t\t// We will not use repeat offsets across blocks.\n+\t\t// By not using them for the first 3 matches\n+\n+\t\tfor {\n+\t\t\tnextHash := hash6(cv, hashLog)\n+\t\t\tnextHash2 := hash6(cv>>8, hashLog)\n+\t\t\tcandidate := e.table[nextHash]\n+\t\t\tcandidate2 := e.table[nextHash2]\n+\t\t\trepIndex := s - offset1 + 2\n+\n+\t\t\te.table[nextHash] = tableEntry{offset: s + e.cur, val: uint32(cv)}\n+\t\t\te.table[nextHash2] = tableEntry{offset: s + e.cur + 1, val: uint32(cv >> 8)}\n+\n+\t\t\tif len(blk.sequences) > 2 && load3232(src, repIndex) == uint32(cv>>16) {\n+\t\t\t\t// Consider history as well.\n+\t\t\t\tvar seq seq\n+\t\t\t\t// lenght := 4 + e.matchlen(s+6, repIndex+4, src)\n+\t\t\t\tlenght := 4 + int32(matchLen(src[s+6:], src[repIndex+4:]))\n+\n+\t\t\t\tseq.matchLen = uint32(lenght - zstdMinMatch)\n+\n+\t\t\t\t// We might be able to match backwards.\n+\t\t\t\t// Extend as long as we can.\n+\t\t\t\tstart := s + 2\n+\t\t\t\t// We end the search early, so we don't risk 0 literals\n+\t\t\t\t// and have to do special offset treatment.\n+\t\t\t\tstartLimit := nextEmit + 1\n+\n+\t\t\t\tsMin := s - e.maxMatchOff\n+\t\t\t\tif sMin < 0 {\n+\t\t\t\t\tsMin = 0\n+\t\t\t\t}\n+\t\t\t\tfor repIndex > sMin && start > startLimit && src[repIndex-1] == src[start-1] {\n+\t\t\t\t\trepIndex--\n+\t\t\t\t\tstart--\n+\t\t\t\t\tseq.matchLen++\n+\t\t\t\t}\n+\t\t\t\taddLiterals(&seq, start)\n+\n+\t\t\t\t// rep 0\n+\t\t\t\tseq.offset = 1\n+\t\t\t\tif debugSequences {\n+\t\t\t\t\tprintln(\"repeat sequence\", seq, \"next s:\", s)\n+\t\t\t\t}\n+\t\t\t\tblk.sequences = append(blk.sequences, seq)\n+\t\t\t\ts += lenght + 2\n+\t\t\t\tnextEmit = s\n+\t\t\t\tif s >= sLimit {\n+\t\t\t\t\tif debug {\n+\t\t\t\t\t\tprintln(\"repeat ended\", s, lenght)\n+\n+\t\t\t\t\t}\n+\t\t\t\t\tbreak encodeLoop\n+\t\t\t\t}\n+\t\t\t\tcv = load6432(src, s)\n+\t\t\t\tcontinue\n+\t\t\t}\n+\t\t\tcoffset0 := s - (candidate.offset - e.cur)\n+\t\t\tcoffset1 := s - (candidate2.offset - e.cur) + 1\n+\t\t\tif coffset0 < e.maxMatchOff && uint32(cv) == candidate.val {\n+\t\t\t\t// found a regular match\n+\t\t\t\tt = candidate.offset - e.cur\n+\t\t\t\tif debug && s <= t {\n+\t\t\t\t\tpanic(\"s <= t\")\n+\t\t\t\t}\n+\t\t\t\tif debug && s-t > e.maxMatchOff {\n+\t\t\t\t\tpanic(\"s - t >e.maxMatchOff\")\n+\t\t\t\t}\n+\t\t\t\tbreak\n+\t\t\t}\n+\n+\t\t\tif coffset1 < e.maxMatchOff && uint32(cv>>8) == candidate2.val {\n+\t\t\t\t// found a regular match\n+\t\t\t\tt = candidate2.offset - e.cur\n+\t\t\t\ts++\n+\t\t\t\tif debug && s <= t {\n+\t\t\t\t\tpanic(\"s <= t\")\n+\t\t\t\t}\n+\t\t\t\tif debug && s-t > e.maxMatchOff {\n+\t\t\t\t\tpanic(\"s - t >e.maxMatchOff\")\n+\t\t\t\t}\n+\t\t\t\tif debug && t < 0 {\n+\t\t\t\t\tpanic(\"t<0\")\n+\t\t\t\t}\n+\t\t\t\tbreak\n+\t\t\t}\n+\t\t\ts += stepSize + ((s - nextEmit) >> (kSearchStrength - 1))\n+\t\t\tif s >= sLimit {\n+\t\t\t\tbreak encodeLoop\n+\t\t\t}\n+\t\t\tcv = load6432(src, s)\n+\t\t}\n+\t\t// A 4-byte match has been found. We'll later see if more than 4 bytes.\n+\t\toffset2 = offset1\n+\t\toffset1 = s - t\n+\n+\t\tif debug && s <= t {\n+\t\t\tpanic(\"s <= t\")\n+\t\t}\n+\n+\t\t// Extend the 4-byte match as long as possible.\n+\t\t//l := e.matchlenNoHist(s+4, t+4, src) + 4\n+\t\tl := int32(matchLen(src[s+4:], src[t+4:])) + 4\n+\n+\t\t// Extend backwards\n+\t\ttMin := s - e.maxMatchOff\n+\t\tif tMin < 0 {\n+\t\t\ttMin = 0\n+\t\t}\n+\t\tfor t > tMin && s > nextEmit && src[t-1] == src[s-1] {\n+\t\t\ts--\n+\t\t\tt--\n+\t\t\tl++\n+\t\t}\n+\n+\t\t// Write our sequence.\n+\t\tvar seq seq\n+\t\tseq.litLen = uint32(s - nextEmit)\n+\t\tseq.matchLen = uint32(l - zstdMinMatch)\n+\t\tif seq.litLen > 0 {\n+\t\t\tblk.literals = append(blk.literals, src[nextEmit:s]...)\n+\t\t}\n+\t\t// Don't use repeat offsets\n+\t\tseq.offset = uint32(s-t) + 3\n+\t\ts += l\n+\t\tif debugSequences {\n+\t\t\tprintln(\"sequence\", seq, \"next s:\", s)\n+\t\t}\n+\t\tblk.sequences = append(blk.sequences, seq)\n+\t\tnextEmit = s\n+\t\tif s >= sLimit {\n+\t\t\tbreak encodeLoop\n+\t\t}\n+\t\tcv = load6432(src, s)\n+\n+\t\t// Check offset 2\n+\t\tif o2 := s - offset2; len(blk.sequences) > 2 && load3232(src, o2) == uint32(cv) {\n+\t\t\t// We have at least 4 byte match.\n+\t\t\t// No need to check backwards. We come straight from a match\n+\t\t\t//l := 4 + e.matchlenNoHist(s+4, o2+4, src)\n+\t\t\tl := 4 + int32(matchLen(src[s+4:], src[o2+4:]))\n+\n+\t\t\t// Store this, since we have it.\n+\t\t\tnextHash := hash6(cv, hashLog)\n+\t\t\te.table[nextHash] = tableEntry{offset: s + e.cur, val: uint32(cv)}\n+\t\t\tseq.matchLen = uint32(l) - zstdMinMatch\n+\t\t\tseq.litLen = 0\n+\t\t\t// Since litlen is always 0, this is offset 1.\n+\t\t\tseq.offset = 1\n+\t\t\ts += l\n+\t\t\tnextEmit = s\n+\t\t\tif debugSequences {\n+\t\t\t\tprintln(\"sequence\", seq, \"next s:\", s)\n+\t\t\t}\n+\t\t\tblk.sequences = append(blk.sequences, seq)\n+\n+\t\t\t// Swap offset 1 and 2.\n+\t\t\toffset1, offset2 = offset2, offset1\n+\t\t\tif s >= sLimit {\n+\t\t\t\tbreak encodeLoop\n+\t\t\t}\n+\t\t\t// Prepare next loop.\n+\t\t\tcv = load6432(src, s)\n+\t\t}\n+\t}\n+\n+\tif int(nextEmit) < len(src) {\n+\t\tblk.literals = append(blk.literals, src[nextEmit:]...)\n+\t\tblk.extraLits = len(src) - int(nextEmit)\n+\t}\n+\tif debug {\n+\t\tprintln(\"returning, recent offsets:\", blk.recentOffsets, \"extra literals:\", blk.extraLits)\n+\t}\n+}\n+\n+func (e *fastEncoder) addBlock(src []byte) int32 {\n+\t// check if we have space already\n+\tif len(e.hist)+len(src) > cap(e.hist) {\n+\t\tif cap(e.hist) == 0 {\n+\t\t\tl := e.maxMatchOff * 2\n+\t\t\t// Make it at least 1MB.\n+\t\t\tif l < 1<<20 {\n+\t\t\t\tl = 1 << 20\n+\t\t\t}\n+\t\t\te.hist = make([]byte, 0, l)\n+\t\t} else {\n+\t\t\tif cap(e.hist) < int(e.maxMatchOff*2) {\n+\t\t\t\tpanic(\"unexpected buffer size\")\n+\t\t\t}\n+\t\t\t// Move down\n+\t\t\toffset := int32(len(e.hist)) - e.maxMatchOff\n+\t\t\tcopy(e.hist[0:e.maxMatchOff], e.hist[offset:])\n+\t\t\te.cur += offset\n+\t\t\te.hist = e.hist[:e.maxMatchOff]\n+\t\t}\n+\t}\n+\ts := int32(len(e.hist))\n+\te.hist = append(e.hist, src...)\n+\treturn s\n+}\n+\n+// useBlock will replace the block with the provided one,\n+// but transfer recent offsets from the previous.\n+func (e *fastEncoder) UseBlock(enc *blockEnc) {\n+\tenc.reset(e.blk)\n+\te.blk = enc\n+}\n+\n+func (e *fastEncoder) matchlenNoHist(s, t int32, src []byte) int32 {\n+\t// Extend the match to be as long as possible.\n+\treturn int32(matchLen(src[s:], src[t:]))\n+}\n+\n+func (e *fastEncoder) matchlen(s, t int32, src []byte) int32 {\n+\tif debug {\n+\t\tif s < 0 {\n+\t\t\tpanic(\"s<0\")\n+\t\t}\n+\t\tif t < 0 {\n+\t\t\tpanic(\"t<0\")\n+\t\t}\n+\t\tif s-t > e.maxMatchOff {\n+\t\t\tpanic(s - t)\n+\t\t}\n+\t}\n+\ts1 := int(s) + maxMatchLength - 4\n+\tif s1 > len(src) {\n+\t\ts1 = len(src)\n+\t}\n+\n+\t// Extend the match to be as long as possible.\n+\treturn int32(matchLen(src[s:s1], src[t:]))\n+}\n+\n+// Reset the encoding table.\n+func (e *fastEncoder) Reset() {\n+\tif e.blk == nil {\n+\t\te.blk = &blockEnc{}\n+\t\te.blk.init()\n+\t} else {\n+\t\te.blk.reset(nil)\n+\t}\n+\te.blk.initNewEncode()\n+\tif e.crc == nil {\n+\t\te.crc = xxhash.New()\n+\t} else {\n+\t\te.crc.Reset()\n+\t}\n+\tif cap(e.hist) < int(e.maxMatchOff*2) {\n+\t\tl := e.maxMatchOff * 2\n+\t\t// Make it at least 1MB.\n+\t\tif l < 1<<20 {\n+\t\t\tl = 1 << 20\n+\t\t}\n+\t\te.hist = make([]byte, 0, l)\n+\t}\n+\t// We offset current position so everything will be out of reach\n+\te.cur += e.maxMatchOff + int32(len(e.hist))\n+\te.hist = e.hist[:0]\n+}"
    },
    {
      "sha": "b6779ecb6d72299ff7e83f4a85034511399fbc9e",
      "filename": "backend/vendor/github.com/klauspost/compress/zstd/enc_params.go",
      "status": "added",
      "additions": 154,
      "deletions": 0,
      "changes": 154,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/klauspost/compress/zstd/enc_params.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/klauspost/compress/zstd/enc_params.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/github.com/klauspost/compress/zstd/enc_params.go?ref=890d7154e777d0673defcd11db3ef792473d68c9",
      "patch": "@@ -0,0 +1,154 @@\n+// Copyright 2019+ Klaus Post. All rights reserved.\n+// License information can be found in the LICENSE file.\n+// Based on work by Yann Collet, released under BSD License.\n+\n+package zstd\n+\n+type encParams struct {\n+\t// largest match distance : larger == more compression, more memory needed during decompression\n+\twindowLog uint8\n+\n+\t// fully searched segment : larger == more compression, slower, more memory (useless for fast)\n+\tchainLog uint8\n+\n+\t//  dispatch table : larger == faster, more memory\n+\thashLog uint8\n+\n+\t// < nb of searches : larger == more compression, slower\n+\tsearchLog uint8\n+\n+\t// < match length searched : larger == faster decompression, sometimes less compression\n+\tminMatch uint8\n+\n+\t// acceptable match size for optimal parser (only) : larger == more compression, slower\n+\ttargetLength uint32\n+\n+\t// see ZSTD_strategy definition above\n+\tstrategy strategy\n+}\n+\n+// strategy defines the algorithm to use when generating sequences.\n+type strategy uint8\n+\n+const (\n+\t// Compression strategies, listed from fastest to strongest\n+\tstrategyFast strategy = iota + 1\n+\tstrategyDfast\n+\tstrategyGreedy\n+\tstrategyLazy\n+\tstrategyLazy2\n+\tstrategyBtlazy2\n+\tstrategyBtopt\n+\tstrategyBtultra\n+\tstrategyBtultra2\n+\t// note : new strategies _might_ be added in the future.\n+\t//   Only the order (from fast to strong) is guaranteed\n+\n+)\n+\n+var defEncParams = [4][]encParams{\n+\t{ // \"default\" - for any srcSize > 256 KB\n+\t\t// W,  C,  H,  S,  L, TL, strat\n+\t\t{19, 12, 13, 1, 6, 1, strategyFast},       // base for negative levels\n+\t\t{19, 13, 14, 1, 7, 0, strategyFast},       // level  1\n+\t\t{20, 15, 16, 1, 6, 0, strategyFast},       // level  2\n+\t\t{21, 16, 17, 1, 5, 1, strategyDfast},      // level  3\n+\t\t{21, 18, 18, 1, 5, 1, strategyDfast},      // level  4\n+\t\t{21, 18, 19, 2, 5, 2, strategyGreedy},     // level  5\n+\t\t{21, 19, 19, 3, 5, 4, strategyGreedy},     // level  6\n+\t\t{21, 19, 19, 3, 5, 8, strategyLazy},       // level  7\n+\t\t{21, 19, 19, 3, 5, 16, strategyLazy2},     // level  8\n+\t\t{21, 19, 20, 4, 5, 16, strategyLazy2},     // level  9\n+\t\t{22, 20, 21, 4, 5, 16, strategyLazy2},     // level 10\n+\t\t{22, 21, 22, 4, 5, 16, strategyLazy2},     // level 11\n+\t\t{22, 21, 22, 5, 5, 16, strategyLazy2},     // level 12\n+\t\t{22, 21, 22, 5, 5, 32, strategyBtlazy2},   // level 13\n+\t\t{22, 22, 23, 5, 5, 32, strategyBtlazy2},   // level 14\n+\t\t{22, 23, 23, 6, 5, 32, strategyBtlazy2},   // level 15\n+\t\t{22, 22, 22, 5, 5, 48, strategyBtopt},     // level 16\n+\t\t{23, 23, 22, 5, 4, 64, strategyBtopt},     // level 17\n+\t\t{23, 23, 22, 6, 3, 64, strategyBtultra},   // level 18\n+\t\t{23, 24, 22, 7, 3, 256, strategyBtultra2}, // level 19\n+\t\t{25, 25, 23, 7, 3, 256, strategyBtultra2}, // level 20\n+\t\t{26, 26, 24, 7, 3, 512, strategyBtultra2}, // level 21\n+\t\t{27, 27, 25, 9, 3, 999, strategyBtultra2}, // level 22\n+\t},\n+\t{ // for srcSize <= 256 KB\n+\t\t// W,  C,  H,  S,  L,  T, strat\n+\t\t{18, 12, 13, 1, 5, 1, strategyFast},        // base for negative levels\n+\t\t{18, 13, 14, 1, 6, 0, strategyFast},        // level  1\n+\t\t{18, 14, 14, 1, 5, 1, strategyDfast},       // level  2\n+\t\t{18, 16, 16, 1, 4, 1, strategyDfast},       // level  3\n+\t\t{18, 16, 17, 2, 5, 2, strategyGreedy},      // level  4.\n+\t\t{18, 18, 18, 3, 5, 2, strategyGreedy},      // level  5.\n+\t\t{18, 18, 19, 3, 5, 4, strategyLazy},        // level  6.\n+\t\t{18, 18, 19, 4, 4, 4, strategyLazy},        // level  7\n+\t\t{18, 18, 19, 4, 4, 8, strategyLazy2},       // level  8\n+\t\t{18, 18, 19, 5, 4, 8, strategyLazy2},       // level  9\n+\t\t{18, 18, 19, 6, 4, 8, strategyLazy2},       // level 10\n+\t\t{18, 18, 19, 5, 4, 12, strategyBtlazy2},    // level 11.\n+\t\t{18, 19, 19, 7, 4, 12, strategyBtlazy2},    // level 12.\n+\t\t{18, 18, 19, 4, 4, 16, strategyBtopt},      // level 13\n+\t\t{18, 18, 19, 4, 3, 32, strategyBtopt},      // level 14.\n+\t\t{18, 18, 19, 6, 3, 128, strategyBtopt},     // level 15.\n+\t\t{18, 19, 19, 6, 3, 128, strategyBtultra},   // level 16.\n+\t\t{18, 19, 19, 8, 3, 256, strategyBtultra},   // level 17.\n+\t\t{18, 19, 19, 6, 3, 128, strategyBtultra2},  // level 18.\n+\t\t{18, 19, 19, 8, 3, 256, strategyBtultra2},  // level 19.\n+\t\t{18, 19, 19, 10, 3, 512, strategyBtultra2}, // level 20.\n+\t\t{18, 19, 19, 12, 3, 512, strategyBtultra2}, // level 21.\n+\t\t{18, 19, 19, 13, 3, 999, strategyBtultra2}, // level 22.\n+\t},\n+\t{ // for srcSize <= 128 KB\n+\t\t// W,  C,  H,  S,  L,  T, strat\n+\t\t{17, 12, 12, 1, 5, 1, strategyFast},        // base for negative levels\n+\t\t{17, 12, 13, 1, 6, 0, strategyFast},        // level  1\n+\t\t{17, 13, 15, 1, 5, 0, strategyFast},        // level  2\n+\t\t{17, 15, 16, 2, 5, 1, strategyDfast},       // level  3\n+\t\t{17, 17, 17, 2, 4, 1, strategyDfast},       // level  4\n+\t\t{17, 16, 17, 3, 4, 2, strategyGreedy},      // level  5\n+\t\t{17, 17, 17, 3, 4, 4, strategyLazy},        // level  6\n+\t\t{17, 17, 17, 3, 4, 8, strategyLazy2},       // level  7\n+\t\t{17, 17, 17, 4, 4, 8, strategyLazy2},       // level  8\n+\t\t{17, 17, 17, 5, 4, 8, strategyLazy2},       // level  9\n+\t\t{17, 17, 17, 6, 4, 8, strategyLazy2},       // level 10\n+\t\t{17, 17, 17, 5, 4, 8, strategyBtlazy2},     // level 11\n+\t\t{17, 18, 17, 7, 4, 12, strategyBtlazy2},    // level 12\n+\t\t{17, 18, 17, 3, 4, 12, strategyBtopt},      // level 13.\n+\t\t{17, 18, 17, 4, 3, 32, strategyBtopt},      // level 14.\n+\t\t{17, 18, 17, 6, 3, 256, strategyBtopt},     // level 15.\n+\t\t{17, 18, 17, 6, 3, 128, strategyBtultra},   // level 16.\n+\t\t{17, 18, 17, 8, 3, 256, strategyBtultra},   // level 17.\n+\t\t{17, 18, 17, 10, 3, 512, strategyBtultra},  // level 18.\n+\t\t{17, 18, 17, 5, 3, 256, strategyBtultra2},  // level 19.\n+\t\t{17, 18, 17, 7, 3, 512, strategyBtultra2},  // level 20.\n+\t\t{17, 18, 17, 9, 3, 512, strategyBtultra2},  // level 21.\n+\t\t{17, 18, 17, 11, 3, 999, strategyBtultra2}, // level 22.\n+\t},\n+\t{ // for srcSize <= 16 KB\n+\t\t// W,  C,  H,  S,  L,  T, strat\n+\t\t{14, 12, 13, 1, 5, 1, strategyFast},        // base for negative levels\n+\t\t{14, 14, 15, 1, 5, 0, strategyFast},        // level  1\n+\t\t{14, 14, 15, 1, 4, 0, strategyFast},        // level  2\n+\t\t{14, 14, 15, 2, 4, 1, strategyDfast},       // level  3\n+\t\t{14, 14, 14, 4, 4, 2, strategyGreedy},      // level  4\n+\t\t{14, 14, 14, 3, 4, 4, strategyLazy},        // level  5.\n+\t\t{14, 14, 14, 4, 4, 8, strategyLazy2},       // level  6\n+\t\t{14, 14, 14, 6, 4, 8, strategyLazy2},       // level  7\n+\t\t{14, 14, 14, 8, 4, 8, strategyLazy2},       // level  8.\n+\t\t{14, 15, 14, 5, 4, 8, strategyBtlazy2},     // level  9.\n+\t\t{14, 15, 14, 9, 4, 8, strategyBtlazy2},     // level 10.\n+\t\t{14, 15, 14, 3, 4, 12, strategyBtopt},      // level 11.\n+\t\t{14, 15, 14, 4, 3, 24, strategyBtopt},      // level 12.\n+\t\t{14, 15, 14, 5, 3, 32, strategyBtultra},    // level 13.\n+\t\t{14, 15, 15, 6, 3, 64, strategyBtultra},    // level 14.\n+\t\t{14, 15, 15, 7, 3, 256, strategyBtultra},   // level 15.\n+\t\t{14, 15, 15, 5, 3, 48, strategyBtultra2},   // level 16.\n+\t\t{14, 15, 15, 6, 3, 128, strategyBtultra2},  // level 17.\n+\t\t{14, 15, 15, 7, 3, 256, strategyBtultra2},  // level 18.\n+\t\t{14, 15, 15, 8, 3, 256, strategyBtultra2},  // level 19.\n+\t\t{14, 15, 15, 8, 3, 512, strategyBtultra2},  // level 20.\n+\t\t{14, 15, 15, 9, 3, 512, strategyBtultra2},  // level 21.\n+\t\t{14, 15, 15, 10, 3, 999, strategyBtultra2}, // level 22.\n+\t},\n+}"
    },
    {
      "sha": "366dd66bde9bad27d3da087a3e47ffb3f6c91755",
      "filename": "backend/vendor/github.com/klauspost/compress/zstd/encoder.go",
      "status": "added",
      "additions": 539,
      "deletions": 0,
      "changes": 539,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/klauspost/compress/zstd/encoder.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/klauspost/compress/zstd/encoder.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/github.com/klauspost/compress/zstd/encoder.go?ref=890d7154e777d0673defcd11db3ef792473d68c9",
      "patch": "@@ -0,0 +1,539 @@\n+// Copyright 2019+ Klaus Post. All rights reserved.\n+// License information can be found in the LICENSE file.\n+// Based on work by Yann Collet, released under BSD License.\n+\n+package zstd\n+\n+import (\n+\t\"crypto/rand\"\n+\t\"fmt\"\n+\t\"io\"\n+\trdebug \"runtime/debug\"\n+\t\"sync\"\n+\n+\t\"github.com/klauspost/compress/zstd/internal/xxhash\"\n+)\n+\n+// Encoder provides encoding to Zstandard.\n+// An Encoder can be used for either compressing a stream via the\n+// io.WriteCloser interface supported by the Encoder or as multiple independent\n+// tasks via the EncodeAll function.\n+// Smaller encodes are encouraged to use the EncodeAll function.\n+// Use NewWriter to create a new instance.\n+type Encoder struct {\n+\to        encoderOptions\n+\tencoders chan encoder\n+\tstate    encoderState\n+\tinit     sync.Once\n+}\n+\n+type encoder interface {\n+\tEncode(blk *blockEnc, src []byte)\n+\tEncodeNoHist(blk *blockEnc, src []byte)\n+\tBlock() *blockEnc\n+\tCRC() *xxhash.Digest\n+\tAppendCRC([]byte) []byte\n+\tWindowSize(size int) int32\n+\tUseBlock(*blockEnc)\n+\tReset()\n+}\n+\n+type encoderState struct {\n+\tw             io.Writer\n+\tfilling       []byte\n+\tcurrent       []byte\n+\tprevious      []byte\n+\tencoder       encoder\n+\twriting       *blockEnc\n+\terr           error\n+\twriteErr      error\n+\tnWritten      int64\n+\theaderWritten bool\n+\teofWritten    bool\n+\n+\t// This waitgroup indicates an encode is running.\n+\twg sync.WaitGroup\n+\t// This waitgroup indicates we have a block encoding/writing.\n+\twWg sync.WaitGroup\n+}\n+\n+// NewWriter will create a new Zstandard encoder.\n+// If the encoder will be used for encoding blocks a nil writer can be used.\n+func NewWriter(w io.Writer, opts ...EOption) (*Encoder, error) {\n+\tinitPredefined()\n+\tvar e Encoder\n+\te.o.setDefault()\n+\tfor _, o := range opts {\n+\t\terr := o(&e.o)\n+\t\tif err != nil {\n+\t\t\treturn nil, err\n+\t\t}\n+\t}\n+\tif w != nil {\n+\t\te.Reset(w)\n+\t} else {\n+\t\te.init.Do(func() {\n+\t\t\te.initialize()\n+\t\t})\n+\t}\n+\treturn &e, nil\n+}\n+\n+func (e *Encoder) initialize() {\n+\te.encoders = make(chan encoder, e.o.concurrent)\n+\tfor i := 0; i < e.o.concurrent; i++ {\n+\t\te.encoders <- e.o.encoder()\n+\t}\n+}\n+\n+// Reset will re-initialize the writer and new writes will encode to the supplied writer\n+// as a new, independent stream.\n+func (e *Encoder) Reset(w io.Writer) {\n+\te.init.Do(func() {\n+\t\te.initialize()\n+\t})\n+\ts := &e.state\n+\ts.wg.Wait()\n+\ts.wWg.Wait()\n+\tif cap(s.filling) == 0 {\n+\t\ts.filling = make([]byte, 0, e.o.blockSize)\n+\t}\n+\tif cap(s.current) == 0 {\n+\t\ts.current = make([]byte, 0, e.o.blockSize)\n+\t}\n+\tif cap(s.previous) == 0 {\n+\t\ts.previous = make([]byte, 0, e.o.blockSize)\n+\t}\n+\tif s.encoder == nil {\n+\t\ts.encoder = e.o.encoder()\n+\t}\n+\tif s.writing == nil {\n+\t\ts.writing = &blockEnc{}\n+\t\ts.writing.init()\n+\t}\n+\ts.writing.initNewEncode()\n+\ts.filling = s.filling[:0]\n+\ts.current = s.current[:0]\n+\ts.previous = s.previous[:0]\n+\ts.encoder.Reset()\n+\ts.headerWritten = false\n+\ts.eofWritten = false\n+\ts.w = w\n+\ts.err = nil\n+\ts.nWritten = 0\n+\ts.writeErr = nil\n+}\n+\n+// Write data to the encoder.\n+// Input data will be buffered and as the buffer fills up\n+// content will be compressed and written to the output.\n+// When done writing, use Close to flush the remaining output\n+// and write CRC if requested.\n+func (e *Encoder) Write(p []byte) (n int, err error) {\n+\ts := &e.state\n+\tfor len(p) > 0 {\n+\t\tif len(p)+len(s.filling) < e.o.blockSize {\n+\t\t\tif e.o.crc {\n+\t\t\t\t_, _ = s.encoder.CRC().Write(p)\n+\t\t\t}\n+\t\t\ts.filling = append(s.filling, p...)\n+\t\t\treturn n + len(p), nil\n+\t\t}\n+\t\tadd := p\n+\t\tif len(p)+len(s.filling) > e.o.blockSize {\n+\t\t\tadd = add[:e.o.blockSize-len(s.filling)]\n+\t\t}\n+\t\tif e.o.crc {\n+\t\t\t_, _ = s.encoder.CRC().Write(add)\n+\t\t}\n+\t\ts.filling = append(s.filling, add...)\n+\t\tp = p[len(add):]\n+\t\tn += len(add)\n+\t\tif len(s.filling) < e.o.blockSize {\n+\t\t\treturn n, nil\n+\t\t}\n+\t\terr := e.nextBlock(false)\n+\t\tif err != nil {\n+\t\t\treturn n, err\n+\t\t}\n+\t\tif debug && len(s.filling) > 0 {\n+\t\t\tpanic(len(s.filling))\n+\t\t}\n+\t}\n+\treturn n, nil\n+}\n+\n+// nextBlock will synchronize and start compressing input in e.state.filling.\n+// If an error has occurred during encoding it will be returned.\n+func (e *Encoder) nextBlock(final bool) error {\n+\ts := &e.state\n+\t// Wait for current block.\n+\ts.wg.Wait()\n+\tif s.err != nil {\n+\t\treturn s.err\n+\t}\n+\tif len(s.filling) > e.o.blockSize {\n+\t\treturn fmt.Errorf(\"block > maxStoreBlockSize\")\n+\t}\n+\tif !s.headerWritten {\n+\t\tvar tmp [maxHeaderSize]byte\n+\t\tfh := frameHeader{\n+\t\t\tContentSize:   0,\n+\t\t\tWindowSize:    uint32(s.encoder.WindowSize(0)),\n+\t\t\tSingleSegment: false,\n+\t\t\tChecksum:      e.o.crc,\n+\t\t\tDictID:        0,\n+\t\t}\n+\t\tdst, err := fh.appendTo(tmp[:0])\n+\t\tif err != nil {\n+\t\t\treturn err\n+\t\t}\n+\t\ts.headerWritten = true\n+\t\ts.wWg.Wait()\n+\t\tvar n2 int\n+\t\tn2, s.err = s.w.Write(dst)\n+\t\tif s.err != nil {\n+\t\t\treturn s.err\n+\t\t}\n+\t\ts.nWritten += int64(n2)\n+\t}\n+\tif s.eofWritten {\n+\t\t// Ensure we only write it once.\n+\t\tfinal = false\n+\t}\n+\n+\tif len(s.filling) == 0 {\n+\t\t// Final block, but no data.\n+\t\tif final {\n+\t\t\tenc := s.encoder\n+\t\t\tblk := enc.Block()\n+\t\t\tblk.reset(nil)\n+\t\t\tblk.last = true\n+\t\t\tblk.encodeRaw(nil)\n+\t\t\ts.wWg.Wait()\n+\t\t\t_, s.err = s.w.Write(blk.output)\n+\t\t\ts.nWritten += int64(len(blk.output))\n+\t\t\ts.eofWritten = true\n+\t\t}\n+\t\treturn s.err\n+\t}\n+\n+\t// Move blocks forward.\n+\ts.filling, s.current, s.previous = s.previous[:0], s.filling, s.current\n+\ts.wg.Add(1)\n+\tgo func(src []byte) {\n+\t\tif debug {\n+\t\t\tprintln(\"Adding block,\", len(src), \"bytes, final:\", final)\n+\t\t}\n+\t\tdefer func() {\n+\t\t\tif r := recover(); r != nil {\n+\t\t\t\ts.err = fmt.Errorf(\"panic while encoding: %v\", r)\n+\t\t\t\trdebug.PrintStack()\n+\t\t\t}\n+\t\t\ts.wg.Done()\n+\t\t}()\n+\t\tenc := s.encoder\n+\t\tblk := enc.Block()\n+\t\tenc.Encode(blk, src)\n+\t\tblk.last = final\n+\t\tif final {\n+\t\t\ts.eofWritten = true\n+\t\t}\n+\t\t// Wait for pending writes.\n+\t\ts.wWg.Wait()\n+\t\tif s.writeErr != nil {\n+\t\t\ts.err = s.writeErr\n+\t\t\treturn\n+\t\t}\n+\t\t// Transfer encoders from previous write block.\n+\t\tblk.swapEncoders(s.writing)\n+\t\t// Transfer recent offsets to next.\n+\t\tenc.UseBlock(s.writing)\n+\t\ts.writing = blk\n+\t\ts.wWg.Add(1)\n+\t\tgo func() {\n+\t\t\tdefer func() {\n+\t\t\t\tif r := recover(); r != nil {\n+\t\t\t\t\ts.writeErr = fmt.Errorf(\"panic while encoding/writing: %v\", r)\n+\t\t\t\t\trdebug.PrintStack()\n+\t\t\t\t}\n+\t\t\t\ts.wWg.Done()\n+\t\t\t}()\n+\t\t\terr := errIncompressible\n+\t\t\t// If we got the exact same number of literals as input,\n+\t\t\t// assume the literals cannot be compressed.\n+\t\t\tif len(src) != len(blk.literals) || len(src) != e.o.blockSize {\n+\t\t\t\terr = blk.encode(e.o.noEntropy)\n+\t\t\t}\n+\t\t\tswitch err {\n+\t\t\tcase errIncompressible:\n+\t\t\t\tif debug {\n+\t\t\t\t\tprintln(\"Storing incompressible block as raw\")\n+\t\t\t\t}\n+\t\t\t\tblk.encodeRaw(src)\n+\t\t\t\t// In fast mode, we do not transfer offsets, so we don't have to deal with changing the.\n+\t\t\tcase nil:\n+\t\t\tdefault:\n+\t\t\t\ts.writeErr = err\n+\t\t\t\treturn\n+\t\t\t}\n+\t\t\t_, s.writeErr = s.w.Write(blk.output)\n+\t\t\ts.nWritten += int64(len(blk.output))\n+\t\t}()\n+\t}(s.current)\n+\treturn nil\n+}\n+\n+// ReadFrom reads data from r until EOF or error.\n+// The return value n is the number of bytes read.\n+// Any error except io.EOF encountered during the read is also returned.\n+//\n+// The Copy function uses ReaderFrom if available.\n+func (e *Encoder) ReadFrom(r io.Reader) (n int64, err error) {\n+\tif debug {\n+\t\tprintln(\"Using ReadFrom\")\n+\t}\n+\t// Maybe handle stuff queued?\n+\te.state.filling = e.state.filling[:e.o.blockSize]\n+\tsrc := e.state.filling\n+\tfor {\n+\t\tn2, err := r.Read(src)\n+\t\t_, _ = e.state.encoder.CRC().Write(src[:n2])\n+\t\t// src is now the unfilled part...\n+\t\tsrc = src[n2:]\n+\t\tn += int64(n2)\n+\t\tswitch err {\n+\t\tcase io.EOF:\n+\t\t\te.state.filling = e.state.filling[:len(e.state.filling)-len(src)]\n+\t\t\tif debug {\n+\t\t\t\tprintln(\"ReadFrom: got EOF final block:\", len(e.state.filling))\n+\t\t\t}\n+\t\t\treturn n, e.nextBlock(true)\n+\t\tdefault:\n+\t\t\tif debug {\n+\t\t\t\tprintln(\"ReadFrom: got error:\", err)\n+\t\t\t}\n+\t\t\te.state.err = err\n+\t\t\treturn n, err\n+\t\tcase nil:\n+\t\t}\n+\t\tif len(src) > 0 {\n+\t\t\tif debug {\n+\t\t\t\tprintln(\"ReadFrom: got space left in source:\", len(src))\n+\t\t\t}\n+\t\t\tcontinue\n+\t\t}\n+\t\terr = e.nextBlock(false)\n+\t\tif err != nil {\n+\t\t\treturn n, err\n+\t\t}\n+\t\te.state.filling = e.state.filling[:e.o.blockSize]\n+\t\tsrc = e.state.filling\n+\t}\n+}\n+\n+// Flush will send the currently written data to output\n+// and block until everything has been written.\n+// This should only be used on rare occasions where pushing the currently queued data is critical.\n+func (e *Encoder) Flush() error {\n+\ts := &e.state\n+\tif len(s.filling) > 0 {\n+\t\terr := e.nextBlock(false)\n+\t\tif err != nil {\n+\t\t\treturn err\n+\t\t}\n+\t}\n+\ts.wg.Wait()\n+\ts.wWg.Wait()\n+\tif s.err != nil {\n+\t\treturn s.err\n+\t}\n+\treturn s.writeErr\n+}\n+\n+// Close will flush the final output and close the stream.\n+// The function will block until everything has been written.\n+// The Encoder can still be re-used after calling this.\n+func (e *Encoder) Close() error {\n+\ts := &e.state\n+\tif s.encoder == nil {\n+\t\treturn nil\n+\t}\n+\terr := e.nextBlock(true)\n+\tif err != nil {\n+\t\treturn err\n+\t}\n+\ts.wg.Wait()\n+\ts.wWg.Wait()\n+\n+\tif s.err != nil {\n+\t\treturn s.err\n+\t}\n+\tif s.writeErr != nil {\n+\t\treturn s.writeErr\n+\t}\n+\n+\t// Write CRC\n+\tif e.o.crc && s.err == nil {\n+\t\t// heap alloc.\n+\t\tvar tmp [4]byte\n+\t\t_, s.err = s.w.Write(s.encoder.AppendCRC(tmp[:0]))\n+\t\ts.nWritten += 4\n+\t}\n+\n+\t// Add padding with content from crypto/rand.Reader\n+\tif s.err == nil && e.o.pad > 0 {\n+\t\tadd := calcSkippableFrame(s.nWritten, int64(e.o.pad))\n+\t\tframe, err := skippableFrame(s.filling[:0], add, rand.Reader)\n+\t\tif err != nil {\n+\t\t\treturn err\n+\t\t}\n+\t\t_, s.err = s.w.Write(frame)\n+\t}\n+\treturn s.err\n+}\n+\n+// EncodeAll will encode all input in src and append it to dst.\n+// This function can be called concurrently, but each call will only run on a single goroutine.\n+// If empty input is given, nothing is returned, unless WithZeroFrames is specified.\n+// Encoded blocks can be concatenated and the result will be the combined input stream.\n+// Data compressed with EncodeAll can be decoded with the Decoder,\n+// using either a stream or DecodeAll.\n+func (e *Encoder) EncodeAll(src, dst []byte) []byte {\n+\tif len(src) == 0 {\n+\t\tif e.o.fullZero {\n+\t\t\t// Add frame header.\n+\t\t\tfh := frameHeader{\n+\t\t\t\tContentSize:   0,\n+\t\t\t\tWindowSize:    MinWindowSize,\n+\t\t\t\tSingleSegment: true,\n+\t\t\t\t// Adding a checksum would be a waste of space.\n+\t\t\t\tChecksum: false,\n+\t\t\t\tDictID:   0,\n+\t\t\t}\n+\t\t\tdst, _ = fh.appendTo(dst)\n+\n+\t\t\t// Write raw block as last one only.\n+\t\t\tvar blk blockHeader\n+\t\t\tblk.setSize(0)\n+\t\t\tblk.setType(blockTypeRaw)\n+\t\t\tblk.setLast(true)\n+\t\t\tdst = blk.appendTo(dst)\n+\t\t}\n+\t\treturn dst\n+\t}\n+\te.init.Do(func() {\n+\t\te.o.setDefault()\n+\t\te.initialize()\n+\t})\n+\tenc := <-e.encoders\n+\tdefer func() {\n+\t\t// Release encoder reference to last block.\n+\t\tenc.Reset()\n+\t\te.encoders <- enc\n+\t}()\n+\tenc.Reset()\n+\tblk := enc.Block()\n+\t// Use single segments when above minimum window and below 1MB.\n+\tsingle := len(src) < 1<<20 && len(src) > MinWindowSize\n+\tif e.o.single != nil {\n+\t\tsingle = *e.o.single\n+\t}\n+\tfh := frameHeader{\n+\t\tContentSize:   uint64(len(src)),\n+\t\tWindowSize:    uint32(enc.WindowSize(len(src))),\n+\t\tSingleSegment: single,\n+\t\tChecksum:      e.o.crc,\n+\t\tDictID:        0,\n+\t}\n+\n+\t// If less than 1MB, allocate a buffer up front.\n+\tif len(dst) == 0 && cap(dst) == 0 && len(src) < 1<<20 {\n+\t\tdst = make([]byte, 0, len(src))\n+\t}\n+\tdst, err := fh.appendTo(dst)\n+\tif err != nil {\n+\t\tpanic(err)\n+\t}\n+\n+\tif len(src) <= e.o.blockSize && len(src) <= maxBlockSize {\n+\t\t// Slightly faster with no history and everything in one block.\n+\t\tif e.o.crc {\n+\t\t\t_, _ = enc.CRC().Write(src)\n+\t\t}\n+\t\tblk.reset(nil)\n+\t\tblk.last = true\n+\t\tenc.EncodeNoHist(blk, src)\n+\n+\t\t// If we got the exact same number of literals as input,\n+\t\t// assume the literals cannot be compressed.\n+\t\terr := errIncompressible\n+\t\toldout := blk.output\n+\t\tif len(blk.literals) != len(src) || len(src) != e.o.blockSize {\n+\t\t\t// Output directly to dst\n+\t\t\tblk.output = dst\n+\t\t\terr = blk.encode(e.o.noEntropy)\n+\t\t}\n+\n+\t\tswitch err {\n+\t\tcase errIncompressible:\n+\t\t\tif debug {\n+\t\t\t\tprintln(\"Storing incompressible block as raw\")\n+\t\t\t}\n+\t\t\tdst = blk.encodeRawTo(dst, src)\n+\t\tcase nil:\n+\t\t\tdst = blk.output\n+\t\tdefault:\n+\t\t\tpanic(err)\n+\t\t}\n+\t\tblk.output = oldout\n+\t} else {\n+\t\tfor len(src) > 0 {\n+\t\t\ttodo := src\n+\t\t\tif len(todo) > e.o.blockSize {\n+\t\t\t\ttodo = todo[:e.o.blockSize]\n+\t\t\t}\n+\t\t\tsrc = src[len(todo):]\n+\t\t\tif e.o.crc {\n+\t\t\t\t_, _ = enc.CRC().Write(todo)\n+\t\t\t}\n+\t\t\tblk.reset(nil)\n+\t\t\tblk.pushOffsets()\n+\t\t\tenc.Encode(blk, todo)\n+\t\t\tif len(src) == 0 {\n+\t\t\t\tblk.last = true\n+\t\t\t}\n+\t\t\terr := errIncompressible\n+\t\t\t// If we got the exact same number of literals as input,\n+\t\t\t// assume the literals cannot be compressed.\n+\t\t\tif len(blk.literals) != len(todo) || len(todo) != e.o.blockSize {\n+\t\t\t\terr = blk.encode(e.o.noEntropy)\n+\t\t\t}\n+\n+\t\t\tswitch err {\n+\t\t\tcase errIncompressible:\n+\t\t\t\tif debug {\n+\t\t\t\t\tprintln(\"Storing incompressible block as raw\")\n+\t\t\t\t}\n+\t\t\t\tdst = blk.encodeRawTo(dst, todo)\n+\t\t\t\tblk.popOffsets()\n+\t\t\tcase nil:\n+\t\t\t\tdst = append(dst, blk.output...)\n+\t\t\tdefault:\n+\t\t\t\tpanic(err)\n+\t\t\t}\n+\t\t}\n+\t}\n+\tif e.o.crc {\n+\t\tdst = enc.AppendCRC(dst)\n+\t}\n+\t// Add padding with content from crypto/rand.Reader\n+\tif e.o.pad > 0 {\n+\t\tadd := calcSkippableFrame(int64(len(dst)), int64(e.o.pad))\n+\t\tdst, err = skippableFrame(dst, add, rand.Reader)\n+\t\tif err != nil {\n+\t\t\tpanic(err)\n+\t\t}\n+\t}\n+\treturn dst\n+}"
    },
    {
      "sha": "40eb457331a689374d1ac240f194379112deeea6",
      "filename": "backend/vendor/github.com/klauspost/compress/zstd/encoder_options.go",
      "status": "added",
      "additions": 231,
      "deletions": 0,
      "changes": 231,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/klauspost/compress/zstd/encoder_options.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/klauspost/compress/zstd/encoder_options.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/github.com/klauspost/compress/zstd/encoder_options.go?ref=890d7154e777d0673defcd11db3ef792473d68c9",
      "patch": "@@ -0,0 +1,231 @@\n+package zstd\n+\n+import (\n+\t\"errors\"\n+\t\"fmt\"\n+\t\"runtime\"\n+\t\"strings\"\n+)\n+\n+// EOption is an option for creating a encoder.\n+type EOption func(*encoderOptions) error\n+\n+// options retains accumulated state of multiple options.\n+type encoderOptions struct {\n+\tconcurrent int\n+\tcrc        bool\n+\tsingle     *bool\n+\tpad        int\n+\tblockSize  int\n+\twindowSize int\n+\tlevel      EncoderLevel\n+\tfullZero   bool\n+\tnoEntropy  bool\n+}\n+\n+func (o *encoderOptions) setDefault() {\n+\t*o = encoderOptions{\n+\t\t// use less ram: true for now, but may change.\n+\t\tconcurrent: runtime.GOMAXPROCS(0),\n+\t\tcrc:        true,\n+\t\tsingle:     nil,\n+\t\tblockSize:  1 << 16,\n+\t\twindowSize: 1 << 22,\n+\t\tlevel:      SpeedDefault,\n+\t}\n+}\n+\n+// encoder returns an encoder with the selected options.\n+func (o encoderOptions) encoder() encoder {\n+\tswitch o.level {\n+\tcase SpeedDefault:\n+\t\treturn &doubleFastEncoder{fastEncoder: fastEncoder{maxMatchOff: int32(o.windowSize)}}\n+\tcase SpeedFastest:\n+\t\treturn &fastEncoder{maxMatchOff: int32(o.windowSize)}\n+\t}\n+\tpanic(\"unknown compression level\")\n+}\n+\n+// WithEncoderCRC will add CRC value to output.\n+// Output will be 4 bytes larger.\n+func WithEncoderCRC(b bool) EOption {\n+\treturn func(o *encoderOptions) error { o.crc = b; return nil }\n+}\n+\n+// WithEncoderConcurrency will set the concurrency,\n+// meaning the maximum number of decoders to run concurrently.\n+// The value supplied must be at least 1.\n+// By default this will be set to GOMAXPROCS.\n+func WithEncoderConcurrency(n int) EOption {\n+\treturn func(o *encoderOptions) error {\n+\t\tif n <= 0 {\n+\t\t\treturn fmt.Errorf(\"concurrency must be at least 1\")\n+\t\t}\n+\t\to.concurrent = n\n+\t\treturn nil\n+\t}\n+}\n+\n+// WithWindowSize will set the maximum allowed back-reference distance.\n+// The value must be a power of two between WindowSizeMin and WindowSizeMax.\n+// A larger value will enable better compression but allocate more memory and,\n+// for above-default values, take considerably longer.\n+// The default value is determined by the compression level.\n+func WithWindowSize(n int) EOption {\n+\treturn func(o *encoderOptions) error {\n+\t\tswitch {\n+\t\tcase n < MinWindowSize:\n+\t\t\treturn fmt.Errorf(\"window size must be at least %d\", MinWindowSize)\n+\t\tcase n > MaxWindowSize:\n+\t\t\treturn fmt.Errorf(\"window size must be at most %d\", MaxWindowSize)\n+\t\tcase (n & (n - 1)) != 0:\n+\t\t\treturn errors.New(\"window size must be a power of 2\")\n+\t\t}\n+\n+\t\to.windowSize = n\n+\t\tif o.blockSize > o.windowSize {\n+\t\t\to.blockSize = o.windowSize\n+\t\t}\n+\t\treturn nil\n+\t}\n+}\n+\n+// WithEncoderPadding will add padding to all output so the size will be a multiple of n.\n+// This can be used to obfuscate the exact output size or make blocks of a certain size.\n+// The contents will be a skippable frame, so it will be invisible by the decoder.\n+// n must be > 0 and <= 1GB, 1<<30 bytes.\n+// The padded area will be filled with data from crypto/rand.Reader.\n+// If `EncodeAll` is used with data already in the destination, the total size will be multiple of this.\n+func WithEncoderPadding(n int) EOption {\n+\treturn func(o *encoderOptions) error {\n+\t\tif n <= 0 {\n+\t\t\treturn fmt.Errorf(\"padding must be at least 1\")\n+\t\t}\n+\t\t// No need to waste our time.\n+\t\tif n == 1 {\n+\t\t\to.pad = 0\n+\t\t}\n+\t\tif n > 1<<30 {\n+\t\t\treturn fmt.Errorf(\"padding must less than 1GB (1<<30 bytes) \")\n+\t\t}\n+\t\to.pad = n\n+\t\treturn nil\n+\t}\n+}\n+\n+// EncoderLevel predefines encoder compression levels.\n+// Only use the constants made available, since the actual mapping\n+// of these values are very likely to change and your compression could change\n+// unpredictably when upgrading the library.\n+type EncoderLevel int\n+\n+const (\n+\tspeedNotSet EncoderLevel = iota\n+\n+\t// SpeedFastest will choose the fastest reasonable compression.\n+\t// This is roughly equivalent to the fastest Zstandard mode.\n+\tSpeedFastest\n+\n+\t// SpeedDefault is the default \"pretty fast\" compression option.\n+\t// This is roughly equivalent to the default Zstandard mode (level 3).\n+\tSpeedDefault\n+\n+\t// speedLast should be kept as the last actual compression option.\n+\t// The is not for external usage, but is used to keep track of the valid options.\n+\tspeedLast\n+\n+\t// SpeedBetterCompression will (in the future) yield better compression than the default,\n+\t// but at approximately 4x the CPU usage of the default.\n+\t// For now this is not implemented.\n+\tSpeedBetterCompression = SpeedDefault\n+\n+\t// SpeedBestCompression will choose the best available compression option.\n+\t// For now this is not implemented.\n+\tSpeedBestCompression = SpeedDefault\n+)\n+\n+// EncoderLevelFromString will convert a string representation of an encoding level back\n+// to a compression level. The compare is not case sensitive.\n+// If the string wasn't recognized, (false, SpeedDefault) will be returned.\n+func EncoderLevelFromString(s string) (bool, EncoderLevel) {\n+\tfor l := EncoderLevel(speedNotSet + 1); l < speedLast; l++ {\n+\t\tif strings.EqualFold(s, l.String()) {\n+\t\t\treturn true, l\n+\t\t}\n+\t}\n+\treturn false, SpeedDefault\n+}\n+\n+// EncoderLevelFromZstd will return an encoder level that closest matches the compression\n+// ratio of a specific zstd compression level.\n+// Many input values will provide the same compression level.\n+func EncoderLevelFromZstd(level int) EncoderLevel {\n+\tswitch {\n+\tcase level < 3:\n+\t\treturn SpeedFastest\n+\tcase level >= 3:\n+\t\treturn SpeedDefault\n+\t}\n+\treturn SpeedDefault\n+}\n+\n+// String provides a string representation of the compression level.\n+func (e EncoderLevel) String() string {\n+\tswitch e {\n+\tcase SpeedFastest:\n+\t\treturn \"fastest\"\n+\tcase SpeedDefault:\n+\t\treturn \"default\"\n+\tdefault:\n+\t\treturn \"invalid\"\n+\t}\n+}\n+\n+// WithEncoderLevel specifies a predefined compression level.\n+func WithEncoderLevel(l EncoderLevel) EOption {\n+\treturn func(o *encoderOptions) error {\n+\t\tswitch {\n+\t\tcase l <= speedNotSet || l >= speedLast:\n+\t\t\treturn fmt.Errorf(\"unknown encoder level\")\n+\t\t}\n+\t\to.level = l\n+\t\treturn nil\n+\t}\n+}\n+\n+// WithZeroFrames will encode 0 length input as full frames.\n+// This can be needed for compatibility with zstandard usage,\n+// but is not needed for this package.\n+func WithZeroFrames(b bool) EOption {\n+\treturn func(o *encoderOptions) error {\n+\t\to.fullZero = b\n+\t\treturn nil\n+\t}\n+}\n+\n+// WithNoEntropyCompression will always skip entropy compression of literals.\n+// This can be useful if content has matches, but unlikely to benefit from entropy\n+// compression. Usually the slight speed improvement is not worth enabling this.\n+func WithNoEntropyCompression(b bool) EOption {\n+\treturn func(o *encoderOptions) error {\n+\t\to.noEntropy = b\n+\t\treturn nil\n+\t}\n+}\n+\n+// WithSingleSegment will set the \"single segment\" flag when EncodeAll is used.\n+// If this flag is set, data must be regenerated within a single continuous memory segment.\n+// In this case, Window_Descriptor byte is skipped, but Frame_Content_Size is necessarily present.\n+// As a consequence, the decoder must allocate a memory segment of size equal or larger than size of your content.\n+// In order to preserve the decoder from unreasonable memory requirements,\n+// a decoder is allowed to reject a compressed frame which requests a memory size beyond decoder's authorized range.\n+// For broader compatibility, decoders are recommended to support memory sizes of at least 8 MB.\n+// This is only a recommendation, each decoder is free to support higher or lower limits, depending on local limitations.\n+// If this is not specified, block encodes will automatically choose this based on the input size.\n+// This setting has no effect on streamed encodes.\n+func WithSingleSegment(b bool) EOption {\n+\treturn func(o *encoderOptions) error {\n+\t\to.single = &b\n+\t\treturn nil\n+\t}\n+}"
    },
    {
      "sha": "40790747a3789688f2e35439111c053437c106a8",
      "filename": "backend/vendor/github.com/klauspost/compress/zstd/framedec.go",
      "status": "added",
      "additions": 489,
      "deletions": 0,
      "changes": 489,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/klauspost/compress/zstd/framedec.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/klauspost/compress/zstd/framedec.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/github.com/klauspost/compress/zstd/framedec.go?ref=890d7154e777d0673defcd11db3ef792473d68c9",
      "patch": "@@ -0,0 +1,489 @@\n+// Copyright 2019+ Klaus Post. All rights reserved.\n+// License information can be found in the LICENSE file.\n+// Based on work by Yann Collet, released under BSD License.\n+\n+package zstd\n+\n+import (\n+\t\"bytes\"\n+\t\"encoding/hex\"\n+\t\"errors\"\n+\t\"hash\"\n+\t\"io\"\n+\t\"sync\"\n+\n+\t\"github.com/klauspost/compress/zstd/internal/xxhash\"\n+)\n+\n+type frameDec struct {\n+\to         decoderOptions\n+\tcrc       hash.Hash64\n+\tframeDone sync.WaitGroup\n+\toffset    int64\n+\n+\tWindowSize       uint64\n+\tDictionaryID     uint32\n+\tFrameContentSize uint64\n+\tHasCheckSum      bool\n+\tSingleSegment    bool\n+\n+\t// maxWindowSize is the maximum windows size to support.\n+\t// should never be bigger than max-int.\n+\tmaxWindowSize uint64\n+\n+\t// In order queue of blocks being decoded.\n+\tdecoding chan *blockDec\n+\n+\t// Frame history passed between blocks\n+\thistory history\n+\n+\trawInput byteBuffer\n+\n+\t// Byte buffer that can be reused for small input blocks.\n+\tbBuf byteBuf\n+\n+\t// asyncRunning indicates whether the async routine processes input on 'decoding'.\n+\tasyncRunning   bool\n+\tasyncRunningMu sync.Mutex\n+}\n+\n+const (\n+\t// The minimum Window_Size is 1 KB.\n+\tMinWindowSize = 1 << 10\n+\tMaxWindowSize = 1 << 30\n+)\n+\n+var (\n+\tframeMagic          = []byte{0x28, 0xb5, 0x2f, 0xfd}\n+\tskippableFrameMagic = []byte{0x2a, 0x4d, 0x18}\n+)\n+\n+func newFrameDec(o decoderOptions) *frameDec {\n+\td := frameDec{\n+\t\to:             o,\n+\t\tmaxWindowSize: MaxWindowSize,\n+\t}\n+\tif d.maxWindowSize > o.maxDecodedSize {\n+\t\td.maxWindowSize = o.maxDecodedSize\n+\t}\n+\treturn &d\n+}\n+\n+// reset will read the frame header and prepare for block decoding.\n+// If nothing can be read from the input, io.EOF will be returned.\n+// Any other error indicated that the stream contained data, but\n+// there was a problem.\n+func (d *frameDec) reset(br byteBuffer) error {\n+\td.HasCheckSum = false\n+\td.WindowSize = 0\n+\tvar b []byte\n+\tfor {\n+\t\tb = br.readSmall(4)\n+\t\tif b == nil {\n+\t\t\treturn io.EOF\n+\t\t}\n+\t\tif !bytes.Equal(b[1:4], skippableFrameMagic) || b[0]&0xf0 != 0x50 {\n+\t\t\tif debug {\n+\t\t\t\tprintln(\"Not skippable\", hex.EncodeToString(b), hex.EncodeToString(skippableFrameMagic))\n+\t\t\t}\n+\t\t\t// Break if not skippable frame.\n+\t\t\tbreak\n+\t\t}\n+\t\t// Read size to skip\n+\t\tb = br.readSmall(4)\n+\t\tif b == nil {\n+\t\t\tprintln(\"Reading Frame Size EOF\")\n+\t\t\treturn io.ErrUnexpectedEOF\n+\t\t}\n+\t\tn := uint32(b[0]) | (uint32(b[1]) << 8) | (uint32(b[2]) << 16) | (uint32(b[3]) << 24)\n+\t\tprintln(\"Skipping frame with\", n, \"bytes.\")\n+\t\terr := br.skipN(int(n))\n+\t\tif err != nil {\n+\t\t\tif debug {\n+\t\t\t\tprintln(\"Reading discarded frame\", err)\n+\t\t\t}\n+\t\t\treturn err\n+\t\t}\n+\t}\n+\tif !bytes.Equal(b, frameMagic) {\n+\t\tprintln(\"Got magic numbers: \", b, \"want:\", frameMagic)\n+\t\treturn ErrMagicMismatch\n+\t}\n+\n+\t// Read Frame_Header_Descriptor\n+\tfhd, err := br.readByte()\n+\tif err != nil {\n+\t\tprintln(\"Reading Frame_Header_Descriptor\", err)\n+\t\treturn err\n+\t}\n+\td.SingleSegment = fhd&(1<<5) != 0\n+\n+\tif fhd&(1<<3) != 0 {\n+\t\treturn errors.New(\"Reserved bit set on frame header\")\n+\t}\n+\n+\t// Read Window_Descriptor\n+\t// https://github.com/facebook/zstd/blob/dev/doc/zstd_compression_format.md#window_descriptor\n+\td.WindowSize = 0\n+\tif !d.SingleSegment {\n+\t\twd, err := br.readByte()\n+\t\tif err != nil {\n+\t\t\tprintln(\"Reading Window_Descriptor\", err)\n+\t\t\treturn err\n+\t\t}\n+\t\tprintf(\"raw: %x, mantissa: %d, exponent: %d\\n\", wd, wd&7, wd>>3)\n+\t\twindowLog := 10 + (wd >> 3)\n+\t\twindowBase := uint64(1) << windowLog\n+\t\twindowAdd := (windowBase / 8) * uint64(wd&0x7)\n+\t\td.WindowSize = windowBase + windowAdd\n+\t}\n+\n+\t// Read Dictionary_ID\n+\t// https://github.com/facebook/zstd/blob/dev/doc/zstd_compression_format.md#dictionary_id\n+\td.DictionaryID = 0\n+\tif size := fhd & 3; size != 0 {\n+\t\tif size == 3 {\n+\t\t\tsize = 4\n+\t\t}\n+\t\tb = br.readSmall(int(size))\n+\t\tif b == nil {\n+\t\t\tif debug {\n+\t\t\t\tprintln(\"Reading Dictionary_ID\", io.ErrUnexpectedEOF)\n+\t\t\t}\n+\t\t\treturn io.ErrUnexpectedEOF\n+\t\t}\n+\t\tswitch size {\n+\t\tcase 1:\n+\t\t\td.DictionaryID = uint32(b[0])\n+\t\tcase 2:\n+\t\t\td.DictionaryID = uint32(b[0]) | (uint32(b[1]) << 8)\n+\t\tcase 4:\n+\t\t\td.DictionaryID = uint32(b[0]) | (uint32(b[1]) << 8) | (uint32(b[2]) << 16) | (uint32(b[3]) << 24)\n+\t\t}\n+\t\tif debug {\n+\t\t\tprintln(\"Dict size\", size, \"ID:\", d.DictionaryID)\n+\t\t}\n+\t\tif d.DictionaryID != 0 {\n+\t\t\treturn ErrUnknownDictionary\n+\t\t}\n+\t}\n+\n+\t// Read Frame_Content_Size\n+\t// https://github.com/facebook/zstd/blob/dev/doc/zstd_compression_format.md#frame_content_size\n+\tvar fcsSize int\n+\tv := fhd >> 6\n+\tswitch v {\n+\tcase 0:\n+\t\tif d.SingleSegment {\n+\t\t\tfcsSize = 1\n+\t\t}\n+\tdefault:\n+\t\tfcsSize = 1 << v\n+\t}\n+\td.FrameContentSize = 0\n+\tif fcsSize > 0 {\n+\t\tb := br.readSmall(fcsSize)\n+\t\tif b == nil {\n+\t\t\tprintln(\"Reading Frame content\", io.ErrUnexpectedEOF)\n+\t\t\treturn io.ErrUnexpectedEOF\n+\t\t}\n+\t\tswitch fcsSize {\n+\t\tcase 1:\n+\t\t\td.FrameContentSize = uint64(b[0])\n+\t\tcase 2:\n+\t\t\t// When FCS_Field_Size is 2, the offset of 256 is added.\n+\t\t\td.FrameContentSize = uint64(b[0]) | (uint64(b[1]) << 8) + 256\n+\t\tcase 4:\n+\t\t\td.FrameContentSize = uint64(b[0]) | (uint64(b[1]) << 8) | (uint64(b[2]) << 16) | (uint64(b[3]) << 24)\n+\t\tcase 8:\n+\t\t\td1 := uint32(b[0]) | (uint32(b[1]) << 8) | (uint32(b[2]) << 16) | (uint32(b[3]) << 24)\n+\t\t\td2 := uint32(b[4]) | (uint32(b[5]) << 8) | (uint32(b[6]) << 16) | (uint32(b[7]) << 24)\n+\t\t\td.FrameContentSize = uint64(d1) | (uint64(d2) << 32)\n+\t\t}\n+\t\tif debug {\n+\t\t\tprintln(\"field size bits:\", v, \"fcsSize:\", fcsSize, \"FrameContentSize:\", d.FrameContentSize, hex.EncodeToString(b[:fcsSize]), \"singleseg:\", d.SingleSegment, \"window:\", d.WindowSize)\n+\t\t}\n+\t}\n+\t// Move this to shared.\n+\td.HasCheckSum = fhd&(1<<2) != 0\n+\tif d.HasCheckSum {\n+\t\tif d.crc == nil {\n+\t\t\td.crc = xxhash.New()\n+\t\t}\n+\t\td.crc.Reset()\n+\t}\n+\n+\tif d.WindowSize == 0 && d.SingleSegment {\n+\t\t// We may not need window in this case.\n+\t\td.WindowSize = d.FrameContentSize\n+\t\tif d.WindowSize < MinWindowSize {\n+\t\t\td.WindowSize = MinWindowSize\n+\t\t}\n+\t}\n+\n+\tif d.WindowSize > d.maxWindowSize {\n+\t\tprintf(\"window size %d > max %d\\n\", d.WindowSize, d.maxWindowSize)\n+\t\treturn ErrWindowSizeExceeded\n+\t}\n+\t// The minimum Window_Size is 1 KB.\n+\tif d.WindowSize < MinWindowSize {\n+\t\tprintln(\"got window size: \", d.WindowSize)\n+\t\treturn ErrWindowSizeTooSmall\n+\t}\n+\td.history.windowSize = int(d.WindowSize)\n+\td.history.maxSize = d.history.windowSize + maxBlockSize\n+\t// history contains input - maybe we do something\n+\td.rawInput = br\n+\treturn nil\n+}\n+\n+// next will start decoding the next block from stream.\n+func (d *frameDec) next(block *blockDec) error {\n+\tif debug {\n+\t\tprintf(\"decoding new block %p:%p\", block, block.data)\n+\t}\n+\terr := block.reset(d.rawInput, d.WindowSize)\n+\tif err != nil {\n+\t\tprintln(\"block error:\", err)\n+\t\t// Signal the frame decoder we have a problem.\n+\t\td.sendErr(block, err)\n+\t\treturn err\n+\t}\n+\tblock.input <- struct{}{}\n+\tif debug {\n+\t\tprintln(\"next block:\", block)\n+\t}\n+\td.asyncRunningMu.Lock()\n+\tdefer d.asyncRunningMu.Unlock()\n+\tif !d.asyncRunning {\n+\t\treturn nil\n+\t}\n+\tif block.Last {\n+\t\t// We indicate the frame is done by sending io.EOF\n+\t\td.decoding <- block\n+\t\treturn io.EOF\n+\t}\n+\td.decoding <- block\n+\treturn nil\n+}\n+\n+// sendEOF will queue an error block on the frame.\n+// This will cause the frame decoder to return when it encounters the block.\n+// Returns true if the decoder was added.\n+func (d *frameDec) sendErr(block *blockDec, err error) bool {\n+\td.asyncRunningMu.Lock()\n+\tdefer d.asyncRunningMu.Unlock()\n+\tif !d.asyncRunning {\n+\t\treturn false\n+\t}\n+\n+\tprintln(\"sending error\", err.Error())\n+\tblock.sendErr(err)\n+\td.decoding <- block\n+\treturn true\n+}\n+\n+// checkCRC will check the checksum if the frame has one.\n+// Will return ErrCRCMismatch if crc check failed, otherwise nil.\n+func (d *frameDec) checkCRC() error {\n+\tif !d.HasCheckSum {\n+\t\treturn nil\n+\t}\n+\tvar tmp [4]byte\n+\tgot := d.crc.Sum64()\n+\t// Flip to match file order.\n+\ttmp[0] = byte(got >> 0)\n+\ttmp[1] = byte(got >> 8)\n+\ttmp[2] = byte(got >> 16)\n+\ttmp[3] = byte(got >> 24)\n+\n+\t// We can overwrite upper tmp now\n+\twant := d.rawInput.readSmall(4)\n+\tif want == nil {\n+\t\tprintln(\"CRC missing?\")\n+\t\treturn io.ErrUnexpectedEOF\n+\t}\n+\n+\tif !bytes.Equal(tmp[:], want) {\n+\t\tif debug {\n+\t\t\tprintln(\"CRC Check Failed:\", tmp[:], \"!=\", want)\n+\t\t}\n+\t\treturn ErrCRCMismatch\n+\t}\n+\tif debug {\n+\t\tprintln(\"CRC ok\", tmp[:])\n+\t}\n+\treturn nil\n+}\n+\n+func (d *frameDec) initAsync() {\n+\tif !d.o.lowMem && !d.SingleSegment {\n+\t\t// set max extra size history to 20MB.\n+\t\td.history.maxSize = d.history.windowSize + maxBlockSize*10\n+\t}\n+\t// re-alloc if more than one extra block size.\n+\tif d.o.lowMem && cap(d.history.b) > d.history.maxSize+maxBlockSize {\n+\t\td.history.b = make([]byte, 0, d.history.maxSize)\n+\t}\n+\tif cap(d.history.b) < d.history.maxSize {\n+\t\td.history.b = make([]byte, 0, d.history.maxSize)\n+\t}\n+\tif cap(d.decoding) < d.o.concurrent {\n+\t\td.decoding = make(chan *blockDec, d.o.concurrent)\n+\t}\n+\tif debug {\n+\t\th := d.history\n+\t\tprintf(\"history init. len: %d, cap: %d\", len(h.b), cap(h.b))\n+\t}\n+\td.asyncRunningMu.Lock()\n+\td.asyncRunning = true\n+\td.asyncRunningMu.Unlock()\n+}\n+\n+// startDecoder will start decoding blocks and write them to the writer.\n+// The decoder will stop as soon as an error occurs or at end of frame.\n+// When the frame has finished decoding the *bufio.Reader\n+// containing the remaining input will be sent on frameDec.frameDone.\n+func (d *frameDec) startDecoder(output chan decodeOutput) {\n+\t// TODO: Init to dictionary\n+\td.history.reset()\n+\twritten := int64(0)\n+\n+\tdefer func() {\n+\t\td.asyncRunningMu.Lock()\n+\t\td.asyncRunning = false\n+\t\td.asyncRunningMu.Unlock()\n+\n+\t\t// Drain the currently decoding.\n+\t\td.history.error = true\n+\tflushdone:\n+\t\tfor {\n+\t\t\tselect {\n+\t\t\tcase b := <-d.decoding:\n+\t\t\t\tb.history <- &d.history\n+\t\t\t\toutput <- <-b.result\n+\t\t\tdefault:\n+\t\t\t\tbreak flushdone\n+\t\t\t}\n+\t\t}\n+\t\tprintln(\"frame decoder done, signalling done\")\n+\t\td.frameDone.Done()\n+\t}()\n+\t// Get decoder for first block.\n+\tblock := <-d.decoding\n+\tblock.history <- &d.history\n+\tfor {\n+\t\tvar next *blockDec\n+\t\t// Get result\n+\t\tr := <-block.result\n+\t\tif r.err != nil {\n+\t\t\tprintln(\"Result contained error\", r.err)\n+\t\t\toutput <- r\n+\t\t\treturn\n+\t\t}\n+\t\tif debug {\n+\t\t\tprintln(\"got result, from \", d.offset, \"to\", d.offset+int64(len(r.b)))\n+\t\t\td.offset += int64(len(r.b))\n+\t\t}\n+\t\tif !block.Last {\n+\t\t\t// Send history to next block\n+\t\t\tselect {\n+\t\t\tcase next = <-d.decoding:\n+\t\t\t\tif debug {\n+\t\t\t\t\tprintln(\"Sending \", len(d.history.b), \"bytes as history\")\n+\t\t\t\t}\n+\t\t\t\tnext.history <- &d.history\n+\t\t\tdefault:\n+\t\t\t\t// Wait until we have sent the block, so\n+\t\t\t\t// other decoders can potentially get the decoder.\n+\t\t\t\tnext = nil\n+\t\t\t}\n+\t\t}\n+\n+\t\t// Add checksum, async to decoding.\n+\t\tif d.HasCheckSum {\n+\t\t\tn, err := d.crc.Write(r.b)\n+\t\t\tif err != nil {\n+\t\t\t\tr.err = err\n+\t\t\t\tif n != len(r.b) {\n+\t\t\t\t\tr.err = io.ErrShortWrite\n+\t\t\t\t}\n+\t\t\t\toutput <- r\n+\t\t\t\treturn\n+\t\t\t}\n+\t\t}\n+\t\twritten += int64(len(r.b))\n+\t\tif d.SingleSegment && uint64(written) > d.FrameContentSize {\n+\t\t\tprintln(\"runDecoder: single segment and\", uint64(written), \">\", d.FrameContentSize)\n+\t\t\tr.err = ErrFrameSizeExceeded\n+\t\t\toutput <- r\n+\t\t\treturn\n+\t\t}\n+\t\tif block.Last {\n+\t\t\tr.err = d.checkCRC()\n+\t\t\toutput <- r\n+\t\t\treturn\n+\t\t}\n+\t\toutput <- r\n+\t\tif next == nil {\n+\t\t\t// There was no decoder available, we wait for one now that we have sent to the writer.\n+\t\t\tif debug {\n+\t\t\t\tprintln(\"Sending \", len(d.history.b), \" bytes as history\")\n+\t\t\t}\n+\t\t\tnext = <-d.decoding\n+\t\t\tnext.history <- &d.history\n+\t\t}\n+\t\tblock = next\n+\t}\n+}\n+\n+// runDecoder will create a sync decoder that will decode a block of data.\n+func (d *frameDec) runDecoder(dst []byte, dec *blockDec) ([]byte, error) {\n+\t// TODO: Init to dictionary\n+\td.history.reset()\n+\tsaved := d.history.b\n+\n+\t// We use the history for output to avoid copying it.\n+\td.history.b = dst\n+\t// Store input length, so we only check new data.\n+\tcrcStart := len(dst)\n+\tvar err error\n+\tfor {\n+\t\terr = dec.reset(d.rawInput, d.WindowSize)\n+\t\tif err != nil {\n+\t\t\tbreak\n+\t\t}\n+\t\tif debug {\n+\t\t\tprintln(\"next block:\", dec)\n+\t\t}\n+\t\terr = dec.decodeBuf(&d.history)\n+\t\tif err != nil || dec.Last {\n+\t\t\tbreak\n+\t\t}\n+\t\tif uint64(len(d.history.b)) > d.o.maxDecodedSize {\n+\t\t\terr = ErrDecoderSizeExceeded\n+\t\t\tbreak\n+\t\t}\n+\t\tif d.SingleSegment && uint64(len(d.history.b)) > d.o.maxDecodedSize {\n+\t\t\tprintln(\"runDecoder: single segment and\", uint64(len(d.history.b)), \">\", d.o.maxDecodedSize)\n+\t\t\terr = ErrFrameSizeExceeded\n+\t\t\tbreak\n+\t\t}\n+\t}\n+\tdst = d.history.b\n+\tif err == nil {\n+\t\tif d.HasCheckSum {\n+\t\t\tvar n int\n+\t\t\tn, err = d.crc.Write(dst[crcStart:])\n+\t\t\tif err == nil {\n+\t\t\t\tif n != len(dst)-crcStart {\n+\t\t\t\t\terr = io.ErrShortWrite\n+\t\t\t\t} else {\n+\t\t\t\t\terr = d.checkCRC()\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t}\n+\td.history.b = saved\n+\treturn dst, err\n+}"
    },
    {
      "sha": "4479cfe18b25513511cc1b098adc9c2f24d54ea6",
      "filename": "backend/vendor/github.com/klauspost/compress/zstd/frameenc.go",
      "status": "added",
      "additions": 115,
      "deletions": 0,
      "changes": 115,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/klauspost/compress/zstd/frameenc.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/klauspost/compress/zstd/frameenc.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/github.com/klauspost/compress/zstd/frameenc.go?ref=890d7154e777d0673defcd11db3ef792473d68c9",
      "patch": "@@ -0,0 +1,115 @@\n+// Copyright 2019+ Klaus Post. All rights reserved.\n+// License information can be found in the LICENSE file.\n+// Based on work by Yann Collet, released under BSD License.\n+\n+package zstd\n+\n+import (\n+\t\"fmt\"\n+\t\"io\"\n+\t\"math\"\n+\t\"math/bits\"\n+)\n+\n+type frameHeader struct {\n+\tContentSize   uint64\n+\tWindowSize    uint32\n+\tSingleSegment bool\n+\tChecksum      bool\n+\tDictID        uint32 // Not stored.\n+}\n+\n+const maxHeaderSize = 14\n+\n+func (f frameHeader) appendTo(dst []byte) ([]byte, error) {\n+\tdst = append(dst, frameMagic...)\n+\tvar fhd uint8\n+\tif f.Checksum {\n+\t\tfhd |= 1 << 2\n+\t}\n+\tif f.SingleSegment {\n+\t\tfhd |= 1 << 5\n+\t}\n+\tvar fcs uint8\n+\tif f.ContentSize >= 256 {\n+\t\tfcs++\n+\t}\n+\tif f.ContentSize >= 65536+256 {\n+\t\tfcs++\n+\t}\n+\tif f.ContentSize >= 0xffffffff {\n+\t\tfcs++\n+\t}\n+\tfhd |= fcs << 6\n+\n+\tdst = append(dst, fhd)\n+\tif !f.SingleSegment {\n+\t\tconst winLogMin = 10\n+\t\twindowLog := (bits.Len32(f.WindowSize-1) - winLogMin) << 3\n+\t\tdst = append(dst, uint8(windowLog))\n+\t}\n+\n+\tswitch fcs {\n+\tcase 0:\n+\t\tif f.SingleSegment {\n+\t\t\tdst = append(dst, uint8(f.ContentSize))\n+\t\t}\n+\t\t// Unless SingleSegment is set, framessizes < 256 are nto stored.\n+\tcase 1:\n+\t\tf.ContentSize -= 256\n+\t\tdst = append(dst, uint8(f.ContentSize), uint8(f.ContentSize>>8))\n+\tcase 2:\n+\t\tdst = append(dst, uint8(f.ContentSize), uint8(f.ContentSize>>8), uint8(f.ContentSize>>16), uint8(f.ContentSize>>24))\n+\tcase 3:\n+\t\tdst = append(dst, uint8(f.ContentSize), uint8(f.ContentSize>>8), uint8(f.ContentSize>>16), uint8(f.ContentSize>>24),\n+\t\t\tuint8(f.ContentSize>>32), uint8(f.ContentSize>>40), uint8(f.ContentSize>>48), uint8(f.ContentSize>>56))\n+\tdefault:\n+\t\tpanic(\"invalid fcs\")\n+\t}\n+\treturn dst, nil\n+}\n+\n+const skippableFrameHeader = 4 + 4\n+\n+// calcSkippableFrame will return a total size to be added for written\n+// to be divisible by multiple.\n+// The value will always be > skippableFrameHeader.\n+// The function will panic if written < 0 or wantMultiple <= 0.\n+func calcSkippableFrame(written, wantMultiple int64) int {\n+\tif wantMultiple <= 0 {\n+\t\tpanic(\"wantMultiple <= 0\")\n+\t}\n+\tif written < 0 {\n+\t\tpanic(\"written < 0\")\n+\t}\n+\tleftOver := written % wantMultiple\n+\tif leftOver == 0 {\n+\t\treturn 0\n+\t}\n+\ttoAdd := wantMultiple - leftOver\n+\tfor toAdd < skippableFrameHeader {\n+\t\ttoAdd += wantMultiple\n+\t}\n+\treturn int(toAdd)\n+}\n+\n+// skippableFrame will add a skippable frame with a total size of bytes.\n+// total should be >= skippableFrameHeader and < math.MaxUint32.\n+func skippableFrame(dst []byte, total int, r io.Reader) ([]byte, error) {\n+\tif total == 0 {\n+\t\treturn dst, nil\n+\t}\n+\tif total < skippableFrameHeader {\n+\t\treturn dst, fmt.Errorf(\"requested skippable frame (%d) < 8\", total)\n+\t}\n+\tif int64(total) > math.MaxUint32 {\n+\t\treturn dst, fmt.Errorf(\"requested skippable frame (%d) > max uint32\", total)\n+\t}\n+\tdst = append(dst, 0x50, 0x2a, 0x4d, 0x18)\n+\tf := uint32(total - skippableFrameHeader)\n+\tdst = append(dst, uint8(f), uint8(f>>8), uint8(f>>16), uint8(f>>24))\n+\tstart := len(dst)\n+\tdst = append(dst, make([]byte, f)...)\n+\t_, err := io.ReadFull(r, dst[start:])\n+\treturn dst, err\n+}"
    },
    {
      "sha": "9efe34feb34815ad118ef6ebeb51121f39d91e25",
      "filename": "backend/vendor/github.com/klauspost/compress/zstd/fse_decoder.go",
      "status": "added",
      "additions": 384,
      "deletions": 0,
      "changes": 384,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/klauspost/compress/zstd/fse_decoder.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/klauspost/compress/zstd/fse_decoder.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/github.com/klauspost/compress/zstd/fse_decoder.go?ref=890d7154e777d0673defcd11db3ef792473d68c9",
      "patch": "@@ -0,0 +1,384 @@\n+// Copyright 2019+ Klaus Post. All rights reserved.\n+// License information can be found in the LICENSE file.\n+// Based on work by Yann Collet, released under BSD License.\n+\n+package zstd\n+\n+import (\n+\t\"errors\"\n+\t\"fmt\"\n+)\n+\n+const (\n+\ttablelogAbsoluteMax = 9\n+)\n+\n+const (\n+\t/*!MEMORY_USAGE :\n+\t *  Memory usage formula : N->2^N Bytes (examples : 10 -> 1KB; 12 -> 4KB ; 16 -> 64KB; 20 -> 1MB; etc.)\n+\t *  Increasing memory usage improves compression ratio\n+\t *  Reduced memory usage can improve speed, due to cache effect\n+\t *  Recommended max value is 14, for 16KB, which nicely fits into Intel x86 L1 cache */\n+\tmaxMemoryUsage = 11\n+\n+\tmaxTableLog    = maxMemoryUsage - 2\n+\tmaxTablesize   = 1 << maxTableLog\n+\tmaxTableMask   = (1 << maxTableLog) - 1\n+\tminTablelog    = 5\n+\tmaxSymbolValue = 255\n+)\n+\n+// fseDecoder provides temporary storage for compression and decompression.\n+type fseDecoder struct {\n+\tdt             [maxTablesize]decSymbol // Decompression table.\n+\tsymbolLen      uint16                  // Length of active part of the symbol table.\n+\tactualTableLog uint8                   // Selected tablelog.\n+\tmaxBits        uint8                   // Maximum number of additional bits\n+\n+\t// used for table creation to avoid allocations.\n+\tstateTable [256]uint16\n+\tnorm       [maxSymbolValue + 1]int16\n+\tpreDefined bool\n+}\n+\n+// tableStep returns the next table index.\n+func tableStep(tableSize uint32) uint32 {\n+\treturn (tableSize >> 1) + (tableSize >> 3) + 3\n+}\n+\n+// readNCount will read the symbol distribution so decoding tables can be constructed.\n+func (s *fseDecoder) readNCount(b *byteReader, maxSymbol uint16) error {\n+\tvar (\n+\t\tcharnum   uint16\n+\t\tprevious0 bool\n+\t)\n+\tif b.remain() < 4 {\n+\t\treturn errors.New(\"input too small\")\n+\t}\n+\tbitStream := b.Uint32()\n+\tnbBits := uint((bitStream & 0xF) + minTablelog) // extract tableLog\n+\tif nbBits > tablelogAbsoluteMax {\n+\t\tprintln(\"Invalid tablelog:\", nbBits)\n+\t\treturn errors.New(\"tableLog too large\")\n+\t}\n+\tbitStream >>= 4\n+\tbitCount := uint(4)\n+\n+\ts.actualTableLog = uint8(nbBits)\n+\tremaining := int32((1 << nbBits) + 1)\n+\tthreshold := int32(1 << nbBits)\n+\tgotTotal := int32(0)\n+\tnbBits++\n+\n+\tfor remaining > 1 && charnum <= maxSymbol {\n+\t\tif previous0 {\n+\t\t\t//println(\"prev0\")\n+\t\t\tn0 := charnum\n+\t\t\tfor (bitStream & 0xFFFF) == 0xFFFF {\n+\t\t\t\t//println(\"24 x 0\")\n+\t\t\t\tn0 += 24\n+\t\t\t\tif r := b.remain(); r > 5 {\n+\t\t\t\t\tb.advance(2)\n+\t\t\t\t\tbitStream = b.Uint32() >> bitCount\n+\t\t\t\t} else {\n+\t\t\t\t\t// end of bit stream\n+\t\t\t\t\tbitStream >>= 16\n+\t\t\t\t\tbitCount += 16\n+\t\t\t\t}\n+\t\t\t}\n+\t\t\t//printf(\"bitstream: %d, 0b%b\", bitStream&3, bitStream)\n+\t\t\tfor (bitStream & 3) == 3 {\n+\t\t\t\tn0 += 3\n+\t\t\t\tbitStream >>= 2\n+\t\t\t\tbitCount += 2\n+\t\t\t}\n+\t\t\tn0 += uint16(bitStream & 3)\n+\t\t\tbitCount += 2\n+\n+\t\t\tif n0 > maxSymbolValue {\n+\t\t\t\treturn errors.New(\"maxSymbolValue too small\")\n+\t\t\t}\n+\t\t\t//println(\"inserting \", n0-charnum, \"zeroes from idx\", charnum, \"ending before\", n0)\n+\t\t\tfor charnum < n0 {\n+\t\t\t\ts.norm[uint8(charnum)] = 0\n+\t\t\t\tcharnum++\n+\t\t\t}\n+\n+\t\t\tif r := b.remain(); r >= 7 || r+int(bitCount>>3) >= 4 {\n+\t\t\t\tb.advance(bitCount >> 3)\n+\t\t\t\tbitCount &= 7\n+\t\t\t\tbitStream = b.Uint32() >> bitCount\n+\t\t\t} else {\n+\t\t\t\tbitStream >>= 2\n+\t\t\t}\n+\t\t}\n+\n+\t\tmax := (2*threshold - 1) - remaining\n+\t\tvar count int32\n+\n+\t\tif int32(bitStream)&(threshold-1) < max {\n+\t\t\tcount = int32(bitStream) & (threshold - 1)\n+\t\t\tif debug && nbBits < 1 {\n+\t\t\t\tpanic(\"nbBits underflow\")\n+\t\t\t}\n+\t\t\tbitCount += nbBits - 1\n+\t\t} else {\n+\t\t\tcount = int32(bitStream) & (2*threshold - 1)\n+\t\t\tif count >= threshold {\n+\t\t\t\tcount -= max\n+\t\t\t}\n+\t\t\tbitCount += nbBits\n+\t\t}\n+\n+\t\t// extra accuracy\n+\t\tcount--\n+\t\tif count < 0 {\n+\t\t\t// -1 means +1\n+\t\t\tremaining += count\n+\t\t\tgotTotal -= count\n+\t\t} else {\n+\t\t\tremaining -= count\n+\t\t\tgotTotal += count\n+\t\t}\n+\t\ts.norm[charnum&0xff] = int16(count)\n+\t\tcharnum++\n+\t\tprevious0 = count == 0\n+\t\tfor remaining < threshold {\n+\t\t\tnbBits--\n+\t\t\tthreshold >>= 1\n+\t\t}\n+\n+\t\t//println(\"b.off:\", b.off, \"len:\", len(b.b), \"bc:\", bitCount, \"remain:\", b.remain())\n+\t\tif r := b.remain(); r >= 7 || r+int(bitCount>>3) >= 4 {\n+\t\t\tb.advance(bitCount >> 3)\n+\t\t\tbitCount &= 7\n+\t\t} else {\n+\t\t\tbitCount -= (uint)(8 * (len(b.b) - 4 - b.off))\n+\t\t\tb.off = len(b.b) - 4\n+\t\t\t//println(\"b.off:\", b.off, \"len:\", len(b.b), \"bc:\", bitCount, \"iend\", iend)\n+\t\t}\n+\t\tbitStream = b.Uint32() >> (bitCount & 31)\n+\t\t//printf(\"bitstream is now: 0b%b\", bitStream)\n+\t}\n+\ts.symbolLen = charnum\n+\tif s.symbolLen <= 1 {\n+\t\treturn fmt.Errorf(\"symbolLen (%d) too small\", s.symbolLen)\n+\t}\n+\tif s.symbolLen > maxSymbolValue+1 {\n+\t\treturn fmt.Errorf(\"symbolLen (%d) too big\", s.symbolLen)\n+\t}\n+\tif remaining != 1 {\n+\t\treturn fmt.Errorf(\"corruption detected (remaining %d != 1)\", remaining)\n+\t}\n+\tif bitCount > 32 {\n+\t\treturn fmt.Errorf(\"corruption detected (bitCount %d > 32)\", bitCount)\n+\t}\n+\tif gotTotal != 1<<s.actualTableLog {\n+\t\treturn fmt.Errorf(\"corruption detected (total %d != %d)\", gotTotal, 1<<s.actualTableLog)\n+\t}\n+\tb.advance((bitCount + 7) >> 3)\n+\t// println(s.norm[:s.symbolLen], s.symbolLen)\n+\treturn s.buildDtable()\n+}\n+\n+// decSymbol contains information about a state entry,\n+// Including the state offset base, the output symbol and\n+// the number of bits to read for the low part of the destination state.\n+// Using a composite uint64 is faster than a struct with separate members.\n+type decSymbol uint64\n+\n+func newDecSymbol(nbits, addBits uint8, newState uint16, baseline uint32) decSymbol {\n+\treturn decSymbol(nbits) | (decSymbol(addBits) << 8) | (decSymbol(newState) << 16) | (decSymbol(baseline) << 32)\n+}\n+\n+func (d decSymbol) nbBits() uint8 {\n+\treturn uint8(d)\n+}\n+\n+func (d decSymbol) addBits() uint8 {\n+\treturn uint8(d >> 8)\n+}\n+\n+func (d decSymbol) newState() uint16 {\n+\treturn uint16(d >> 16)\n+}\n+\n+func (d decSymbol) baseline() uint32 {\n+\treturn uint32(d >> 32)\n+}\n+\n+func (d decSymbol) baselineInt() int {\n+\treturn int(d >> 32)\n+}\n+\n+func (d *decSymbol) set(nbits, addBits uint8, newState uint16, baseline uint32) {\n+\t*d = decSymbol(nbits) | (decSymbol(addBits) << 8) | (decSymbol(newState) << 16) | (decSymbol(baseline) << 32)\n+}\n+\n+func (d *decSymbol) setNBits(nBits uint8) {\n+\tconst mask = 0xffffffffffffff00\n+\t*d = (*d & mask) | decSymbol(nBits)\n+}\n+\n+func (d *decSymbol) setAddBits(addBits uint8) {\n+\tconst mask = 0xffffffffffff00ff\n+\t*d = (*d & mask) | (decSymbol(addBits) << 8)\n+}\n+\n+func (d *decSymbol) setNewState(state uint16) {\n+\tconst mask = 0xffffffff0000ffff\n+\t*d = (*d & mask) | decSymbol(state)<<16\n+}\n+\n+func (d *decSymbol) setBaseline(baseline uint32) {\n+\tconst mask = 0xffffffff\n+\t*d = (*d & mask) | decSymbol(baseline)<<32\n+}\n+\n+func (d *decSymbol) setExt(addBits uint8, baseline uint32) {\n+\tconst mask = 0xffff00ff\n+\t*d = (*d & mask) | (decSymbol(addBits) << 8) | (decSymbol(baseline) << 32)\n+}\n+\n+// decSymbolValue returns the transformed decSymbol for the given symbol.\n+func decSymbolValue(symb uint8, t []baseOffset) (decSymbol, error) {\n+\tif int(symb) >= len(t) {\n+\t\treturn 0, fmt.Errorf(\"rle symbol %d >= max %d\", symb, len(t))\n+\t}\n+\tlu := t[symb]\n+\treturn newDecSymbol(0, lu.addBits, 0, lu.baseLine), nil\n+}\n+\n+// setRLE will set the decoder til RLE mode.\n+func (s *fseDecoder) setRLE(symbol decSymbol) {\n+\ts.actualTableLog = 0\n+\ts.maxBits = symbol.addBits()\n+\ts.dt[0] = symbol\n+}\n+\n+// buildDtable will build the decoding table.\n+func (s *fseDecoder) buildDtable() error {\n+\ttableSize := uint32(1 << s.actualTableLog)\n+\thighThreshold := tableSize - 1\n+\tsymbolNext := s.stateTable[:256]\n+\n+\t// Init, lay down lowprob symbols\n+\t{\n+\t\tfor i, v := range s.norm[:s.symbolLen] {\n+\t\t\tif v == -1 {\n+\t\t\t\ts.dt[highThreshold].setAddBits(uint8(i))\n+\t\t\t\thighThreshold--\n+\t\t\t\tsymbolNext[i] = 1\n+\t\t\t} else {\n+\t\t\t\tsymbolNext[i] = uint16(v)\n+\t\t\t}\n+\t\t}\n+\t}\n+\t// Spread symbols\n+\t{\n+\t\ttableMask := tableSize - 1\n+\t\tstep := tableStep(tableSize)\n+\t\tposition := uint32(0)\n+\t\tfor ss, v := range s.norm[:s.symbolLen] {\n+\t\t\tfor i := 0; i < int(v); i++ {\n+\t\t\t\ts.dt[position].setAddBits(uint8(ss))\n+\t\t\t\tposition = (position + step) & tableMask\n+\t\t\t\tfor position > highThreshold {\n+\t\t\t\t\t// lowprob area\n+\t\t\t\t\tposition = (position + step) & tableMask\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t\tif position != 0 {\n+\t\t\t// position must reach all cells once, otherwise normalizedCounter is incorrect\n+\t\t\treturn errors.New(\"corrupted input (position != 0)\")\n+\t\t}\n+\t}\n+\n+\t// Build Decoding table\n+\t{\n+\t\ttableSize := uint16(1 << s.actualTableLog)\n+\t\tfor u, v := range s.dt[:tableSize] {\n+\t\t\tsymbol := v.addBits()\n+\t\t\tnextState := symbolNext[symbol]\n+\t\t\tsymbolNext[symbol] = nextState + 1\n+\t\t\tnBits := s.actualTableLog - byte(highBits(uint32(nextState)))\n+\t\t\ts.dt[u&maxTableMask].setNBits(nBits)\n+\t\t\tnewState := (nextState << nBits) - tableSize\n+\t\t\tif newState > tableSize {\n+\t\t\t\treturn fmt.Errorf(\"newState (%d) outside table size (%d)\", newState, tableSize)\n+\t\t\t}\n+\t\t\tif newState == uint16(u) && nBits == 0 {\n+\t\t\t\t// Seems weird that this is possible with nbits > 0.\n+\t\t\t\treturn fmt.Errorf(\"newState (%d) == oldState (%d) and no bits\", newState, u)\n+\t\t\t}\n+\t\t\ts.dt[u&maxTableMask].setNewState(newState)\n+\t\t}\n+\t}\n+\treturn nil\n+}\n+\n+// transform will transform the decoder table into a table usable for\n+// decoding without having to apply the transformation while decoding.\n+// The state will contain the base value and the number of bits to read.\n+func (s *fseDecoder) transform(t []baseOffset) error {\n+\ttableSize := uint16(1 << s.actualTableLog)\n+\ts.maxBits = 0\n+\tfor i, v := range s.dt[:tableSize] {\n+\t\tadd := v.addBits()\n+\t\tif int(add) >= len(t) {\n+\t\t\treturn fmt.Errorf(\"invalid decoding table entry %d, symbol %d >= max (%d)\", i, v.addBits(), len(t))\n+\t\t}\n+\t\tlu := t[add]\n+\t\tif lu.addBits > s.maxBits {\n+\t\t\ts.maxBits = lu.addBits\n+\t\t}\n+\t\tv.setExt(lu.addBits, lu.baseLine)\n+\t\ts.dt[i] = v\n+\t}\n+\treturn nil\n+}\n+\n+type fseState struct {\n+\tdt    []decSymbol\n+\tstate decSymbol\n+}\n+\n+// Initialize and decodeAsync first state and symbol.\n+func (s *fseState) init(br *bitReader, tableLog uint8, dt []decSymbol) {\n+\ts.dt = dt\n+\tbr.fill()\n+\ts.state = dt[br.getBits(tableLog)]\n+}\n+\n+// next returns the current symbol and sets the next state.\n+// At least tablelog bits must be available in the bit reader.\n+func (s *fseState) next(br *bitReader) {\n+\tlowBits := uint16(br.getBits(s.state.nbBits()))\n+\ts.state = s.dt[s.state.newState()+lowBits]\n+}\n+\n+// finished returns true if all bits have been read from the bitstream\n+// and the next state would require reading bits from the input.\n+func (s *fseState) finished(br *bitReader) bool {\n+\treturn br.finished() && s.state.nbBits() > 0\n+}\n+\n+// final returns the current state symbol without decoding the next.\n+func (s *fseState) final() (int, uint8) {\n+\treturn s.state.baselineInt(), s.state.addBits()\n+}\n+\n+// final returns the current state symbol without decoding the next.\n+func (s decSymbol) final() (int, uint8) {\n+\treturn s.baselineInt(), s.addBits()\n+}\n+\n+// nextFast returns the next symbol and sets the next state.\n+// This can only be used if no symbols are 0 bits.\n+// At least tablelog bits must be available in the bit reader.\n+func (s *fseState) nextFast(br *bitReader) (uint32, uint8) {\n+\tlowBits := uint16(br.getBitsFast(s.state.nbBits()))\n+\ts.state = s.dt[s.state.newState()+lowBits]\n+\treturn s.state.baseline(), s.state.addBits()\n+}"
    },
    {
      "sha": "619836f52f0433f14eb5ae122a4009d6d4efcec4",
      "filename": "backend/vendor/github.com/klauspost/compress/zstd/fse_encoder.go",
      "status": "added",
      "additions": 726,
      "deletions": 0,
      "changes": 726,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/klauspost/compress/zstd/fse_encoder.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/klauspost/compress/zstd/fse_encoder.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/github.com/klauspost/compress/zstd/fse_encoder.go?ref=890d7154e777d0673defcd11db3ef792473d68c9",
      "patch": "@@ -0,0 +1,726 @@\n+// Copyright 2019+ Klaus Post. All rights reserved.\n+// License information can be found in the LICENSE file.\n+// Based on work by Yann Collet, released under BSD License.\n+\n+package zstd\n+\n+import (\n+\t\"errors\"\n+\t\"fmt\"\n+\t\"math\"\n+)\n+\n+const (\n+\t// For encoding we only support up to\n+\tmaxEncTableLog    = 8\n+\tmaxEncTablesize   = 1 << maxTableLog\n+\tmaxEncTableMask   = (1 << maxTableLog) - 1\n+\tminEncTablelog    = 5\n+\tmaxEncSymbolValue = maxMatchLengthSymbol\n+)\n+\n+// Scratch provides temporary storage for compression and decompression.\n+type fseEncoder struct {\n+\tsymbolLen      uint16 // Length of active part of the symbol table.\n+\tactualTableLog uint8  // Selected tablelog.\n+\tct             cTable // Compression tables.\n+\tmaxCount       int    // count of the most probable symbol\n+\tzeroBits       bool   // no bits has prob > 50%.\n+\tclearCount     bool   // clear count\n+\tuseRLE         bool   // This encoder is for RLE\n+\tpreDefined     bool   // This encoder is predefined.\n+\treUsed         bool   // Set to know when the encoder has been reused.\n+\trleVal         uint8  // RLE Symbol\n+\tmaxBits        uint8  // Maximum output bits after transform.\n+\n+\t// TODO: Technically zstd should be fine with 64 bytes.\n+\tcount [256]uint32\n+\tnorm  [256]int16\n+}\n+\n+// cTable contains tables used for compression.\n+type cTable struct {\n+\ttableSymbol []byte\n+\tstateTable  []uint16\n+\tsymbolTT    []symbolTransform\n+}\n+\n+// symbolTransform contains the state transform for a symbol.\n+type symbolTransform struct {\n+\tdeltaNbBits    uint32\n+\tdeltaFindState int16\n+\toutBits        uint8\n+}\n+\n+// String prints values as a human readable string.\n+func (s symbolTransform) String() string {\n+\treturn fmt.Sprintf(\"{deltabits: %08x, findstate:%d outbits:%d}\", s.deltaNbBits, s.deltaFindState, s.outBits)\n+}\n+\n+// Histogram allows to populate the histogram and skip that step in the compression,\n+// It otherwise allows to inspect the histogram when compression is done.\n+// To indicate that you have populated the histogram call HistogramFinished\n+// with the value of the highest populated symbol, as well as the number of entries\n+// in the most populated entry. These are accepted at face value.\n+// The returned slice will always be length 256.\n+func (s *fseEncoder) Histogram() []uint32 {\n+\treturn s.count[:]\n+}\n+\n+// HistogramFinished can be called to indicate that the histogram has been populated.\n+// maxSymbol is the index of the highest set symbol of the next data segment.\n+// maxCount is the number of entries in the most populated entry.\n+// These are accepted at face value.\n+func (s *fseEncoder) HistogramFinished(maxSymbol uint8, maxCount int) {\n+\ts.maxCount = maxCount\n+\ts.symbolLen = uint16(maxSymbol) + 1\n+\ts.clearCount = maxCount != 0\n+}\n+\n+// prepare will prepare and allocate scratch tables used for both compression and decompression.\n+func (s *fseEncoder) prepare() (*fseEncoder, error) {\n+\tif s == nil {\n+\t\ts = &fseEncoder{}\n+\t}\n+\ts.useRLE = false\n+\tif s.clearCount && s.maxCount == 0 {\n+\t\tfor i := range s.count {\n+\t\t\ts.count[i] = 0\n+\t\t}\n+\t\ts.clearCount = false\n+\t}\n+\treturn s, nil\n+}\n+\n+// allocCtable will allocate tables needed for compression.\n+// If existing tables a re big enough, they are simply re-used.\n+func (s *fseEncoder) allocCtable() {\n+\ttableSize := 1 << s.actualTableLog\n+\t// get tableSymbol that is big enough.\n+\tif cap(s.ct.tableSymbol) < int(tableSize) {\n+\t\ts.ct.tableSymbol = make([]byte, tableSize)\n+\t}\n+\ts.ct.tableSymbol = s.ct.tableSymbol[:tableSize]\n+\n+\tctSize := tableSize\n+\tif cap(s.ct.stateTable) < ctSize {\n+\t\ts.ct.stateTable = make([]uint16, ctSize)\n+\t}\n+\ts.ct.stateTable = s.ct.stateTable[:ctSize]\n+\n+\tif cap(s.ct.symbolTT) < 256 {\n+\t\ts.ct.symbolTT = make([]symbolTransform, 256)\n+\t}\n+\ts.ct.symbolTT = s.ct.symbolTT[:256]\n+}\n+\n+// buildCTable will populate the compression table so it is ready to be used.\n+func (s *fseEncoder) buildCTable() error {\n+\ttableSize := uint32(1 << s.actualTableLog)\n+\thighThreshold := tableSize - 1\n+\tvar cumul [256]int16\n+\n+\ts.allocCtable()\n+\ttableSymbol := s.ct.tableSymbol[:tableSize]\n+\t// symbol start positions\n+\t{\n+\t\tcumul[0] = 0\n+\t\tfor ui, v := range s.norm[:s.symbolLen-1] {\n+\t\t\tu := byte(ui) // one less than reference\n+\t\t\tif v == -1 {\n+\t\t\t\t// Low proba symbol\n+\t\t\t\tcumul[u+1] = cumul[u] + 1\n+\t\t\t\ttableSymbol[highThreshold] = u\n+\t\t\t\thighThreshold--\n+\t\t\t} else {\n+\t\t\t\tcumul[u+1] = cumul[u] + v\n+\t\t\t}\n+\t\t}\n+\t\t// Encode last symbol separately to avoid overflowing u\n+\t\tu := int(s.symbolLen - 1)\n+\t\tv := s.norm[s.symbolLen-1]\n+\t\tif v == -1 {\n+\t\t\t// Low proba symbol\n+\t\t\tcumul[u+1] = cumul[u] + 1\n+\t\t\ttableSymbol[highThreshold] = byte(u)\n+\t\t\thighThreshold--\n+\t\t} else {\n+\t\t\tcumul[u+1] = cumul[u] + v\n+\t\t}\n+\t\tif uint32(cumul[s.symbolLen]) != tableSize {\n+\t\t\treturn fmt.Errorf(\"internal error: expected cumul[s.symbolLen] (%d) == tableSize (%d)\", cumul[s.symbolLen], tableSize)\n+\t\t}\n+\t\tcumul[s.symbolLen] = int16(tableSize) + 1\n+\t}\n+\t// Spread symbols\n+\ts.zeroBits = false\n+\t{\n+\t\tstep := tableStep(tableSize)\n+\t\ttableMask := tableSize - 1\n+\t\tvar position uint32\n+\t\t// if any symbol > largeLimit, we may have 0 bits output.\n+\t\tlargeLimit := int16(1 << (s.actualTableLog - 1))\n+\t\tfor ui, v := range s.norm[:s.symbolLen] {\n+\t\t\tsymbol := byte(ui)\n+\t\t\tif v > largeLimit {\n+\t\t\t\ts.zeroBits = true\n+\t\t\t}\n+\t\t\tfor nbOccurrences := int16(0); nbOccurrences < v; nbOccurrences++ {\n+\t\t\t\ttableSymbol[position] = symbol\n+\t\t\t\tposition = (position + step) & tableMask\n+\t\t\t\tfor position > highThreshold {\n+\t\t\t\t\tposition = (position + step) & tableMask\n+\t\t\t\t} /* Low proba area */\n+\t\t\t}\n+\t\t}\n+\n+\t\t// Check if we have gone through all positions\n+\t\tif position != 0 {\n+\t\t\treturn errors.New(\"position!=0\")\n+\t\t}\n+\t}\n+\n+\t// Build table\n+\ttable := s.ct.stateTable\n+\t{\n+\t\ttsi := int(tableSize)\n+\t\tfor u, v := range tableSymbol {\n+\t\t\t// TableU16 : sorted by symbol order; gives next state value\n+\t\t\ttable[cumul[v]] = uint16(tsi + u)\n+\t\t\tcumul[v]++\n+\t\t}\n+\t}\n+\n+\t// Build Symbol Transformation Table\n+\t{\n+\t\ttotal := int16(0)\n+\t\tsymbolTT := s.ct.symbolTT[:s.symbolLen]\n+\t\ttableLog := s.actualTableLog\n+\t\ttl := (uint32(tableLog) << 16) - (1 << tableLog)\n+\t\tfor i, v := range s.norm[:s.symbolLen] {\n+\t\t\tswitch v {\n+\t\t\tcase 0:\n+\t\t\tcase -1, 1:\n+\t\t\t\tsymbolTT[i].deltaNbBits = tl\n+\t\t\t\tsymbolTT[i].deltaFindState = int16(total - 1)\n+\t\t\t\ttotal++\n+\t\t\tdefault:\n+\t\t\t\tmaxBitsOut := uint32(tableLog) - highBit(uint32(v-1))\n+\t\t\t\tminStatePlus := uint32(v) << maxBitsOut\n+\t\t\t\tsymbolTT[i].deltaNbBits = (maxBitsOut << 16) - minStatePlus\n+\t\t\t\tsymbolTT[i].deltaFindState = int16(total - v)\n+\t\t\t\ttotal += v\n+\t\t\t}\n+\t\t}\n+\t\tif total != int16(tableSize) {\n+\t\t\treturn fmt.Errorf(\"total mismatch %d (got) != %d (want)\", total, tableSize)\n+\t\t}\n+\t}\n+\treturn nil\n+}\n+\n+var rtbTable = [...]uint32{0, 473195, 504333, 520860, 550000, 700000, 750000, 830000}\n+\n+func (s *fseEncoder) setRLE(val byte) {\n+\ts.allocCtable()\n+\ts.actualTableLog = 0\n+\ts.ct.stateTable = s.ct.stateTable[:1]\n+\ts.ct.symbolTT[val] = symbolTransform{\n+\t\tdeltaFindState: 0,\n+\t\tdeltaNbBits:    0,\n+\t}\n+\tif debug {\n+\t\tprintln(\"setRLE: val\", val, \"symbolTT\", s.ct.symbolTT[val])\n+\t}\n+\ts.rleVal = val\n+\ts.useRLE = true\n+}\n+\n+// setBits will set output bits for the transform.\n+// if nil is provided, the number of bits is equal to the index.\n+func (s *fseEncoder) setBits(transform []byte) {\n+\tif s.reUsed || s.preDefined {\n+\t\treturn\n+\t}\n+\tif s.useRLE {\n+\t\tif transform == nil {\n+\t\t\ts.ct.symbolTT[s.rleVal].outBits = s.rleVal\n+\t\t\ts.maxBits = s.rleVal\n+\t\t\treturn\n+\t\t}\n+\t\ts.maxBits = transform[s.rleVal]\n+\t\ts.ct.symbolTT[s.rleVal].outBits = s.maxBits\n+\t\treturn\n+\t}\n+\tif transform == nil {\n+\t\tfor i := range s.ct.symbolTT[:s.symbolLen] {\n+\t\t\ts.ct.symbolTT[i].outBits = uint8(i)\n+\t\t}\n+\t\ts.maxBits = uint8(s.symbolLen - 1)\n+\t\treturn\n+\t}\n+\ts.maxBits = 0\n+\tfor i, v := range transform[:s.symbolLen] {\n+\t\ts.ct.symbolTT[i].outBits = v\n+\t\tif v > s.maxBits {\n+\t\t\t// We could assume bits always going up, but we play safe.\n+\t\t\ts.maxBits = v\n+\t\t}\n+\t}\n+}\n+\n+// normalizeCount will normalize the count of the symbols so\n+// the total is equal to the table size.\n+// If successful, compression tables will also be made ready.\n+func (s *fseEncoder) normalizeCount(length int) error {\n+\tif s.reUsed {\n+\t\treturn nil\n+\t}\n+\ts.optimalTableLog(length)\n+\tvar (\n+\t\ttableLog          = s.actualTableLog\n+\t\tscale             = 62 - uint64(tableLog)\n+\t\tstep              = (1 << 62) / uint64(length)\n+\t\tvStep             = uint64(1) << (scale - 20)\n+\t\tstillToDistribute = int16(1 << tableLog)\n+\t\tlargest           int\n+\t\tlargestP          int16\n+\t\tlowThreshold      = (uint32)(length >> tableLog)\n+\t)\n+\tif s.maxCount == length {\n+\t\ts.useRLE = true\n+\t\treturn nil\n+\t}\n+\ts.useRLE = false\n+\tfor i, cnt := range s.count[:s.symbolLen] {\n+\t\t// already handled\n+\t\t// if (count[s] == s.length) return 0;   /* rle special case */\n+\n+\t\tif cnt == 0 {\n+\t\t\ts.norm[i] = 0\n+\t\t\tcontinue\n+\t\t}\n+\t\tif cnt <= lowThreshold {\n+\t\t\ts.norm[i] = -1\n+\t\t\tstillToDistribute--\n+\t\t} else {\n+\t\t\tproba := (int16)((uint64(cnt) * step) >> scale)\n+\t\t\tif proba < 8 {\n+\t\t\t\trestToBeat := vStep * uint64(rtbTable[proba])\n+\t\t\t\tv := uint64(cnt)*step - (uint64(proba) << scale)\n+\t\t\t\tif v > restToBeat {\n+\t\t\t\t\tproba++\n+\t\t\t\t}\n+\t\t\t}\n+\t\t\tif proba > largestP {\n+\t\t\t\tlargestP = proba\n+\t\t\t\tlargest = i\n+\t\t\t}\n+\t\t\ts.norm[i] = proba\n+\t\t\tstillToDistribute -= proba\n+\t\t}\n+\t}\n+\n+\tif -stillToDistribute >= (s.norm[largest] >> 1) {\n+\t\t// corner case, need another normalization method\n+\t\terr := s.normalizeCount2(length)\n+\t\tif err != nil {\n+\t\t\treturn err\n+\t\t}\n+\t\tif debug {\n+\t\t\terr = s.validateNorm()\n+\t\t\tif err != nil {\n+\t\t\t\treturn err\n+\t\t\t}\n+\t\t}\n+\t\treturn s.buildCTable()\n+\t}\n+\ts.norm[largest] += stillToDistribute\n+\tif debug {\n+\t\terr := s.validateNorm()\n+\t\tif err != nil {\n+\t\t\treturn err\n+\t\t}\n+\t}\n+\treturn s.buildCTable()\n+}\n+\n+// Secondary normalization method.\n+// To be used when primary method fails.\n+func (s *fseEncoder) normalizeCount2(length int) error {\n+\tconst notYetAssigned = -2\n+\tvar (\n+\t\tdistributed  uint32\n+\t\ttotal        = uint32(length)\n+\t\ttableLog     = s.actualTableLog\n+\t\tlowThreshold = uint32(total >> tableLog)\n+\t\tlowOne       = uint32((total * 3) >> (tableLog + 1))\n+\t)\n+\tfor i, cnt := range s.count[:s.symbolLen] {\n+\t\tif cnt == 0 {\n+\t\t\ts.norm[i] = 0\n+\t\t\tcontinue\n+\t\t}\n+\t\tif cnt <= lowThreshold {\n+\t\t\ts.norm[i] = -1\n+\t\t\tdistributed++\n+\t\t\ttotal -= cnt\n+\t\t\tcontinue\n+\t\t}\n+\t\tif cnt <= lowOne {\n+\t\t\ts.norm[i] = 1\n+\t\t\tdistributed++\n+\t\t\ttotal -= cnt\n+\t\t\tcontinue\n+\t\t}\n+\t\ts.norm[i] = notYetAssigned\n+\t}\n+\ttoDistribute := (1 << tableLog) - distributed\n+\n+\tif (total / toDistribute) > lowOne {\n+\t\t// risk of rounding to zero\n+\t\tlowOne = uint32((total * 3) / (toDistribute * 2))\n+\t\tfor i, cnt := range s.count[:s.symbolLen] {\n+\t\t\tif (s.norm[i] == notYetAssigned) && (cnt <= lowOne) {\n+\t\t\t\ts.norm[i] = 1\n+\t\t\t\tdistributed++\n+\t\t\t\ttotal -= cnt\n+\t\t\t\tcontinue\n+\t\t\t}\n+\t\t}\n+\t\ttoDistribute = (1 << tableLog) - distributed\n+\t}\n+\tif distributed == uint32(s.symbolLen)+1 {\n+\t\t// all values are pretty poor;\n+\t\t//   probably incompressible data (should have already been detected);\n+\t\t//   find max, then give all remaining points to max\n+\t\tvar maxV int\n+\t\tvar maxC uint32\n+\t\tfor i, cnt := range s.count[:s.symbolLen] {\n+\t\t\tif cnt > maxC {\n+\t\t\t\tmaxV = i\n+\t\t\t\tmaxC = cnt\n+\t\t\t}\n+\t\t}\n+\t\ts.norm[maxV] += int16(toDistribute)\n+\t\treturn nil\n+\t}\n+\n+\tif total == 0 {\n+\t\t// all of the symbols were low enough for the lowOne or lowThreshold\n+\t\tfor i := uint32(0); toDistribute > 0; i = (i + 1) % (uint32(s.symbolLen)) {\n+\t\t\tif s.norm[i] > 0 {\n+\t\t\t\ttoDistribute--\n+\t\t\t\ts.norm[i]++\n+\t\t\t}\n+\t\t}\n+\t\treturn nil\n+\t}\n+\n+\tvar (\n+\t\tvStepLog = 62 - uint64(tableLog)\n+\t\tmid      = uint64((1 << (vStepLog - 1)) - 1)\n+\t\trStep    = (((1 << vStepLog) * uint64(toDistribute)) + mid) / uint64(total) // scale on remaining\n+\t\ttmpTotal = mid\n+\t)\n+\tfor i, cnt := range s.count[:s.symbolLen] {\n+\t\tif s.norm[i] == notYetAssigned {\n+\t\t\tvar (\n+\t\t\t\tend    = tmpTotal + uint64(cnt)*rStep\n+\t\t\t\tsStart = uint32(tmpTotal >> vStepLog)\n+\t\t\t\tsEnd   = uint32(end >> vStepLog)\n+\t\t\t\tweight = sEnd - sStart\n+\t\t\t)\n+\t\t\tif weight < 1 {\n+\t\t\t\treturn errors.New(\"weight < 1\")\n+\t\t\t}\n+\t\t\ts.norm[i] = int16(weight)\n+\t\t\ttmpTotal = end\n+\t\t}\n+\t}\n+\treturn nil\n+}\n+\n+// optimalTableLog calculates and sets the optimal tableLog in s.actualTableLog\n+func (s *fseEncoder) optimalTableLog(length int) {\n+\ttableLog := uint8(maxEncTableLog)\n+\tminBitsSrc := highBit(uint32(length)) + 1\n+\tminBitsSymbols := highBit(uint32(s.symbolLen-1)) + 2\n+\tminBits := uint8(minBitsSymbols)\n+\tif minBitsSrc < minBitsSymbols {\n+\t\tminBits = uint8(minBitsSrc)\n+\t}\n+\n+\tmaxBitsSrc := uint8(highBit(uint32(length-1))) - 2\n+\tif maxBitsSrc < tableLog {\n+\t\t// Accuracy can be reduced\n+\t\ttableLog = maxBitsSrc\n+\t}\n+\tif minBits > tableLog {\n+\t\ttableLog = minBits\n+\t}\n+\t// Need a minimum to safely represent all symbol values\n+\tif tableLog < minEncTablelog {\n+\t\ttableLog = minEncTablelog\n+\t}\n+\tif tableLog > maxEncTableLog {\n+\t\ttableLog = maxEncTableLog\n+\t}\n+\ts.actualTableLog = tableLog\n+}\n+\n+// validateNorm validates the normalized histogram table.\n+func (s *fseEncoder) validateNorm() (err error) {\n+\tvar total int\n+\tfor _, v := range s.norm[:s.symbolLen] {\n+\t\tif v >= 0 {\n+\t\t\ttotal += int(v)\n+\t\t} else {\n+\t\t\ttotal -= int(v)\n+\t\t}\n+\t}\n+\tdefer func() {\n+\t\tif err == nil {\n+\t\t\treturn\n+\t\t}\n+\t\tfmt.Printf(\"selected TableLog: %d, Symbol length: %d\\n\", s.actualTableLog, s.symbolLen)\n+\t\tfor i, v := range s.norm[:s.symbolLen] {\n+\t\t\tfmt.Printf(\"%3d: %5d -> %4d \\n\", i, s.count[i], v)\n+\t\t}\n+\t}()\n+\tif total != (1 << s.actualTableLog) {\n+\t\treturn fmt.Errorf(\"warning: Total == %d != %d\", total, 1<<s.actualTableLog)\n+\t}\n+\tfor i, v := range s.count[s.symbolLen:] {\n+\t\tif v != 0 {\n+\t\t\treturn fmt.Errorf(\"warning: Found symbol out of range, %d after cut\", i)\n+\t\t}\n+\t}\n+\treturn nil\n+}\n+\n+// writeCount will write the normalized histogram count to header.\n+// This is read back by readNCount.\n+func (s *fseEncoder) writeCount(out []byte) ([]byte, error) {\n+\tif s.useRLE {\n+\t\treturn append(out, s.rleVal), nil\n+\t}\n+\tif s.preDefined || s.reUsed {\n+\t\t// Never write predefined.\n+\t\treturn out, nil\n+\t}\n+\n+\tvar (\n+\t\ttableLog  = s.actualTableLog\n+\t\ttableSize = 1 << tableLog\n+\t\tprevious0 bool\n+\t\tcharnum   uint16\n+\n+\t\t// maximum header size plus 2 extra bytes for final output if bitCount == 0.\n+\t\tmaxHeaderSize = ((int(s.symbolLen) * int(tableLog)) >> 3) + 3 + 2\n+\n+\t\t// Write Table Size\n+\t\tbitStream = uint32(tableLog - minEncTablelog)\n+\t\tbitCount  = uint(4)\n+\t\tremaining = int16(tableSize + 1) /* +1 for extra accuracy */\n+\t\tthreshold = int16(tableSize)\n+\t\tnbBits    = uint(tableLog + 1)\n+\t\toutP      = len(out)\n+\t)\n+\tif cap(out) < outP+maxHeaderSize {\n+\t\tout = append(out, make([]byte, maxHeaderSize*3)...)\n+\t\tout = out[:len(out)-maxHeaderSize*3]\n+\t}\n+\tout = out[:outP+maxHeaderSize]\n+\n+\t// stops at 1\n+\tfor remaining > 1 {\n+\t\tif previous0 {\n+\t\t\tstart := charnum\n+\t\t\tfor s.norm[charnum] == 0 {\n+\t\t\t\tcharnum++\n+\t\t\t}\n+\t\t\tfor charnum >= start+24 {\n+\t\t\t\tstart += 24\n+\t\t\t\tbitStream += uint32(0xFFFF) << bitCount\n+\t\t\t\tout[outP] = byte(bitStream)\n+\t\t\t\tout[outP+1] = byte(bitStream >> 8)\n+\t\t\t\toutP += 2\n+\t\t\t\tbitStream >>= 16\n+\t\t\t}\n+\t\t\tfor charnum >= start+3 {\n+\t\t\t\tstart += 3\n+\t\t\t\tbitStream += 3 << bitCount\n+\t\t\t\tbitCount += 2\n+\t\t\t}\n+\t\t\tbitStream += uint32(charnum-start) << bitCount\n+\t\t\tbitCount += 2\n+\t\t\tif bitCount > 16 {\n+\t\t\t\tout[outP] = byte(bitStream)\n+\t\t\t\tout[outP+1] = byte(bitStream >> 8)\n+\t\t\t\toutP += 2\n+\t\t\t\tbitStream >>= 16\n+\t\t\t\tbitCount -= 16\n+\t\t\t}\n+\t\t}\n+\n+\t\tcount := s.norm[charnum]\n+\t\tcharnum++\n+\t\tmax := (2*threshold - 1) - remaining\n+\t\tif count < 0 {\n+\t\t\tremaining += count\n+\t\t} else {\n+\t\t\tremaining -= count\n+\t\t}\n+\t\tcount++ // +1 for extra accuracy\n+\t\tif count >= threshold {\n+\t\t\tcount += max // [0..max[ [max..threshold[ (...) [threshold+max 2*threshold[\n+\t\t}\n+\t\tbitStream += uint32(count) << bitCount\n+\t\tbitCount += nbBits\n+\t\tif count < max {\n+\t\t\tbitCount--\n+\t\t}\n+\n+\t\tprevious0 = count == 1\n+\t\tif remaining < 1 {\n+\t\t\treturn nil, errors.New(\"internal error: remaining < 1\")\n+\t\t}\n+\t\tfor remaining < threshold {\n+\t\t\tnbBits--\n+\t\t\tthreshold >>= 1\n+\t\t}\n+\n+\t\tif bitCount > 16 {\n+\t\t\tout[outP] = byte(bitStream)\n+\t\t\tout[outP+1] = byte(bitStream >> 8)\n+\t\t\toutP += 2\n+\t\t\tbitStream >>= 16\n+\t\t\tbitCount -= 16\n+\t\t}\n+\t}\n+\n+\tif outP+2 > len(out) {\n+\t\treturn nil, fmt.Errorf(\"internal error: %d > %d, maxheader: %d, sl: %d, tl: %d, normcount: %v\", outP+2, len(out), maxHeaderSize, s.symbolLen, int(tableLog), s.norm[:s.symbolLen])\n+\t}\n+\tout[outP] = byte(bitStream)\n+\tout[outP+1] = byte(bitStream >> 8)\n+\toutP += int((bitCount + 7) / 8)\n+\n+\tif charnum > s.symbolLen {\n+\t\treturn nil, errors.New(\"internal error: charnum > s.symbolLen\")\n+\t}\n+\treturn out[:outP], nil\n+}\n+\n+// Approximate symbol cost, as fractional value, using fixed-point format (accuracyLog fractional bits)\n+// note 1 : assume symbolValue is valid (<= maxSymbolValue)\n+// note 2 : if freq[symbolValue]==0, @return a fake cost of tableLog+1 bits *\n+func (s *fseEncoder) bitCost(symbolValue uint8, accuracyLog uint32) uint32 {\n+\tminNbBits := s.ct.symbolTT[symbolValue].deltaNbBits >> 16\n+\tthreshold := (minNbBits + 1) << 16\n+\tif debug {\n+\t\tif !(s.actualTableLog < 16) {\n+\t\t\tpanic(\"!s.actualTableLog < 16\")\n+\t\t}\n+\t\t// ensure enough room for renormalization double shift\n+\t\tif !(uint8(accuracyLog) < 31-s.actualTableLog) {\n+\t\t\tpanic(\"!uint8(accuracyLog) < 31-s.actualTableLog\")\n+\t\t}\n+\t}\n+\ttableSize := uint32(1) << s.actualTableLog\n+\tdeltaFromThreshold := threshold - (s.ct.symbolTT[symbolValue].deltaNbBits + tableSize)\n+\t// linear interpolation (very approximate)\n+\tnormalizedDeltaFromThreshold := (deltaFromThreshold << accuracyLog) >> s.actualTableLog\n+\tbitMultiplier := uint32(1) << accuracyLog\n+\tif debug {\n+\t\tif s.ct.symbolTT[symbolValue].deltaNbBits+tableSize > threshold {\n+\t\t\tpanic(\"s.ct.symbolTT[symbolValue].deltaNbBits+tableSize > threshold\")\n+\t\t}\n+\t\tif normalizedDeltaFromThreshold > bitMultiplier {\n+\t\t\tpanic(\"normalizedDeltaFromThreshold > bitMultiplier\")\n+\t\t}\n+\t}\n+\treturn (minNbBits+1)*bitMultiplier - normalizedDeltaFromThreshold\n+}\n+\n+// Returns the cost in bits of encoding the distribution in count using ctable.\n+// Histogram should only be up to the last non-zero symbol.\n+// Returns an -1 if ctable cannot represent all the symbols in count.\n+func (s *fseEncoder) approxSize(hist []uint32) uint32 {\n+\tif int(s.symbolLen) < len(hist) {\n+\t\t// More symbols than we have.\n+\t\treturn math.MaxUint32\n+\t}\n+\tif s.useRLE {\n+\t\t// We will never reuse RLE encoders.\n+\t\treturn math.MaxUint32\n+\t}\n+\tconst kAccuracyLog = 8\n+\tbadCost := (uint32(s.actualTableLog) + 1) << kAccuracyLog\n+\tvar cost uint32\n+\tfor i, v := range hist {\n+\t\tif v == 0 {\n+\t\t\tcontinue\n+\t\t}\n+\t\tif s.norm[i] == 0 {\n+\t\t\treturn math.MaxUint32\n+\t\t}\n+\t\tbitCost := s.bitCost(uint8(i), kAccuracyLog)\n+\t\tif bitCost > badCost {\n+\t\t\treturn math.MaxUint32\n+\t\t}\n+\t\tcost += v * bitCost\n+\t}\n+\treturn cost >> kAccuracyLog\n+}\n+\n+// maxHeaderSize returns the maximum header size in bits.\n+// This is not exact size, but we want a penalty for new tables anyway.\n+func (s *fseEncoder) maxHeaderSize() uint32 {\n+\tif s.preDefined {\n+\t\treturn 0\n+\t}\n+\tif s.useRLE {\n+\t\treturn 8\n+\t}\n+\treturn (((uint32(s.symbolLen) * uint32(s.actualTableLog)) >> 3) + 3) * 8\n+}\n+\n+// cState contains the compression state of a stream.\n+type cState struct {\n+\tbw         *bitWriter\n+\tstateTable []uint16\n+\tstate      uint16\n+}\n+\n+// init will initialize the compression state to the first symbol of the stream.\n+func (c *cState) init(bw *bitWriter, ct *cTable, first symbolTransform) {\n+\tc.bw = bw\n+\tc.stateTable = ct.stateTable\n+\tif len(c.stateTable) == 1 {\n+\t\t// RLE\n+\t\tc.stateTable[0] = uint16(0)\n+\t\tc.state = 0\n+\t\treturn\n+\t}\n+\tnbBitsOut := (first.deltaNbBits + (1 << 15)) >> 16\n+\tim := int32((nbBitsOut << 16) - first.deltaNbBits)\n+\tlu := (im >> nbBitsOut) + int32(first.deltaFindState)\n+\tc.state = c.stateTable[lu]\n+\treturn\n+}\n+\n+// encode the output symbol provided and write it to the bitstream.\n+func (c *cState) encode(symbolTT symbolTransform) {\n+\tnbBitsOut := (uint32(c.state) + symbolTT.deltaNbBits) >> 16\n+\tdstState := int32(c.state>>(nbBitsOut&15)) + int32(symbolTT.deltaFindState)\n+\tc.bw.addBits16NC(c.state, uint8(nbBitsOut))\n+\tc.state = c.stateTable[dstState]\n+}\n+\n+// flush will write the tablelog to the output and flush the remaining full bytes.\n+func (c *cState) flush(tableLog uint8) {\n+\tc.bw.flush32()\n+\tc.bw.addBits16NC(c.state, tableLog)\n+}"
    },
    {
      "sha": "6c17dc17f4fd1982f77f2c8f3a67f8850d8a69aa",
      "filename": "backend/vendor/github.com/klauspost/compress/zstd/fse_predefined.go",
      "status": "added",
      "additions": 158,
      "deletions": 0,
      "changes": 158,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/klauspost/compress/zstd/fse_predefined.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/klauspost/compress/zstd/fse_predefined.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/github.com/klauspost/compress/zstd/fse_predefined.go?ref=890d7154e777d0673defcd11db3ef792473d68c9",
      "patch": "@@ -0,0 +1,158 @@\n+// Copyright 2019+ Klaus Post. All rights reserved.\n+// License information can be found in the LICENSE file.\n+// Based on work by Yann Collet, released under BSD License.\n+\n+package zstd\n+\n+import (\n+\t\"fmt\"\n+\t\"math\"\n+\t\"sync\"\n+)\n+\n+var (\n+\t// fsePredef are the predefined fse tables as defined here:\n+\t// https://github.com/facebook/zstd/blob/dev/doc/zstd_compression_format.md#default-distributions\n+\t// These values are already transformed.\n+\tfsePredef [3]fseDecoder\n+\n+\t// fsePredefEnc are the predefined encoder based on fse tables as defined here:\n+\t// https://github.com/facebook/zstd/blob/dev/doc/zstd_compression_format.md#default-distributions\n+\t// These values are already transformed.\n+\tfsePredefEnc [3]fseEncoder\n+\n+\t// symbolTableX contain the transformations needed for each type as defined in\n+\t// https://github.com/facebook/zstd/blob/dev/doc/zstd_compression_format.md#the-codes-for-literals-lengths-match-lengths-and-offsets\n+\tsymbolTableX [3][]baseOffset\n+\n+\t// maxTableSymbol is the biggest supported symbol for each table type\n+\t// https://github.com/facebook/zstd/blob/dev/doc/zstd_compression_format.md#the-codes-for-literals-lengths-match-lengths-and-offsets\n+\tmaxTableSymbol = [3]uint8{tableLiteralLengths: maxLiteralLengthSymbol, tableOffsets: maxOffsetLengthSymbol, tableMatchLengths: maxMatchLengthSymbol}\n+\n+\t// bitTables is the bits table for each table.\n+\tbitTables = [3][]byte{tableLiteralLengths: llBitsTable[:], tableOffsets: nil, tableMatchLengths: mlBitsTable[:]}\n+)\n+\n+type tableIndex uint8\n+\n+const (\n+\t// indexes for fsePredef and symbolTableX\n+\ttableLiteralLengths tableIndex = 0\n+\ttableOffsets        tableIndex = 1\n+\ttableMatchLengths   tableIndex = 2\n+\n+\tmaxLiteralLengthSymbol = 35\n+\tmaxOffsetLengthSymbol  = 30\n+\tmaxMatchLengthSymbol   = 52\n+)\n+\n+// baseOffset is used for calculating transformations.\n+type baseOffset struct {\n+\tbaseLine uint32\n+\taddBits  uint8\n+}\n+\n+// fillBase will precalculate base offsets with the given bit distributions.\n+func fillBase(dst []baseOffset, base uint32, bits ...uint8) {\n+\tif len(bits) != len(dst) {\n+\t\tpanic(fmt.Sprintf(\"len(dst) (%d) != len(bits) (%d)\", len(dst), len(bits)))\n+\t}\n+\tfor i, bit := range bits {\n+\t\tif base > math.MaxInt32 {\n+\t\t\tpanic(fmt.Sprintf(\"invalid decoding table, base overflows int32\"))\n+\t\t}\n+\n+\t\tdst[i] = baseOffset{\n+\t\t\tbaseLine: base,\n+\t\t\taddBits:  bit,\n+\t\t}\n+\t\tbase += 1 << bit\n+\t}\n+}\n+\n+var predef sync.Once\n+\n+func initPredefined() {\n+\tpredef.Do(func() {\n+\t\t// Literals length codes\n+\t\ttmp := make([]baseOffset, 36)\n+\t\tfor i := range tmp[:16] {\n+\t\t\ttmp[i] = baseOffset{\n+\t\t\t\tbaseLine: uint32(i),\n+\t\t\t\taddBits:  0,\n+\t\t\t}\n+\t\t}\n+\t\tfillBase(tmp[16:], 16, 1, 1, 1, 1, 2, 2, 3, 3, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16)\n+\t\tsymbolTableX[tableLiteralLengths] = tmp\n+\n+\t\t// Match length codes\n+\t\ttmp = make([]baseOffset, 53)\n+\t\tfor i := range tmp[:32] {\n+\t\t\ttmp[i] = baseOffset{\n+\t\t\t\t// The transformation adds the 3 length.\n+\t\t\t\tbaseLine: uint32(i) + 3,\n+\t\t\t\taddBits:  0,\n+\t\t\t}\n+\t\t}\n+\t\tfillBase(tmp[32:], 35, 1, 1, 1, 1, 2, 2, 3, 3, 4, 4, 5, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16)\n+\t\tsymbolTableX[tableMatchLengths] = tmp\n+\n+\t\t// Offset codes\n+\t\ttmp = make([]baseOffset, maxOffsetBits+1)\n+\t\ttmp[1] = baseOffset{\n+\t\t\tbaseLine: 1,\n+\t\t\taddBits:  1,\n+\t\t}\n+\t\tfillBase(tmp[2:], 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30)\n+\t\tsymbolTableX[tableOffsets] = tmp\n+\n+\t\t// Fill predefined tables and transform them.\n+\t\t// https://github.com/facebook/zstd/blob/dev/doc/zstd_compression_format.md#default-distributions\n+\t\tfor i := range fsePredef[:] {\n+\t\t\tf := &fsePredef[i]\n+\t\t\tswitch tableIndex(i) {\n+\t\t\tcase tableLiteralLengths:\n+\t\t\t\t// https://github.com/facebook/zstd/blob/ededcfca57366461021c922720878c81a5854a0a/lib/decompress/zstd_decompress_block.c#L243\n+\t\t\t\tf.actualTableLog = 6\n+\t\t\t\tcopy(f.norm[:], []int16{4, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1,\n+\t\t\t\t\t2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 1, 1, 1, 1, 1,\n+\t\t\t\t\t-1, -1, -1, -1})\n+\t\t\t\tf.symbolLen = 36\n+\t\t\tcase tableOffsets:\n+\t\t\t\t// https://github.com/facebook/zstd/blob/ededcfca57366461021c922720878c81a5854a0a/lib/decompress/zstd_decompress_block.c#L281\n+\t\t\t\tf.actualTableLog = 5\n+\t\t\t\tcopy(f.norm[:], []int16{\n+\t\t\t\t\t1, 1, 1, 1, 1, 1, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1,\n+\t\t\t\t\t1, 1, 1, 1, 1, 1, 1, 1, -1, -1, -1, -1, -1})\n+\t\t\t\tf.symbolLen = 29\n+\t\t\tcase tableMatchLengths:\n+\t\t\t\t//https://github.com/facebook/zstd/blob/ededcfca57366461021c922720878c81a5854a0a/lib/decompress/zstd_decompress_block.c#L304\n+\t\t\t\tf.actualTableLog = 6\n+\t\t\t\tcopy(f.norm[:], []int16{\n+\t\t\t\t\t1, 4, 3, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1,\n+\t\t\t\t\t1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n+\t\t\t\t\t1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, -1,\n+\t\t\t\t\t-1, -1, -1, -1, -1})\n+\t\t\t\tf.symbolLen = 53\n+\t\t\t}\n+\t\t\tif err := f.buildDtable(); err != nil {\n+\t\t\t\tpanic(fmt.Errorf(\"building table %v: %v\", tableIndex(i), err))\n+\t\t\t}\n+\t\t\tif err := f.transform(symbolTableX[i]); err != nil {\n+\t\t\t\tpanic(fmt.Errorf(\"building table %v: %v\", tableIndex(i), err))\n+\t\t\t}\n+\t\t\tf.preDefined = true\n+\n+\t\t\t// Create encoder as well\n+\t\t\tenc := &fsePredefEnc[i]\n+\t\t\tcopy(enc.norm[:], f.norm[:])\n+\t\t\tenc.symbolLen = f.symbolLen\n+\t\t\tenc.actualTableLog = f.actualTableLog\n+\t\t\tif err := enc.buildCTable(); err != nil {\n+\t\t\t\tpanic(fmt.Errorf(\"building encoding table %v: %v\", tableIndex(i), err))\n+\t\t\t}\n+\t\t\tenc.setBits(bitTables[i])\n+\t\t\tenc.preDefined = true\n+\t\t}\n+\t})\n+}"
    },
    {
      "sha": "4a752067fc905c02e8d43af3758d64ea3169dee6",
      "filename": "backend/vendor/github.com/klauspost/compress/zstd/hash.go",
      "status": "added",
      "additions": 77,
      "deletions": 0,
      "changes": 77,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/klauspost/compress/zstd/hash.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/klauspost/compress/zstd/hash.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/github.com/klauspost/compress/zstd/hash.go?ref=890d7154e777d0673defcd11db3ef792473d68c9",
      "patch": "@@ -0,0 +1,77 @@\n+// Copyright 2019+ Klaus Post. All rights reserved.\n+// License information can be found in the LICENSE file.\n+// Based on work by Yann Collet, released under BSD License.\n+\n+package zstd\n+\n+const (\n+\tprime3bytes = 506832829\n+\tprime4bytes = 2654435761\n+\tprime5bytes = 889523592379\n+\tprime6bytes = 227718039650203\n+\tprime7bytes = 58295818150454627\n+\tprime8bytes = 0xcf1bbcdcb7a56463\n+)\n+\n+// hashLen returns a hash of the lowest l bytes of u for a size size of h bytes.\n+// l must be >=4 and <=8. Any other value will return hash for 4 bytes.\n+// h should always be <32.\n+// Preferably h and l should be a constant.\n+// FIXME: This does NOT get resolved, if 'mls' is constant,\n+//  so this cannot be used.\n+func hashLen(u uint64, hashLog, mls uint8) uint32 {\n+\tswitch mls {\n+\tcase 5:\n+\t\treturn hash5(u, hashLog)\n+\tcase 6:\n+\t\treturn hash6(u, hashLog)\n+\tcase 7:\n+\t\treturn hash7(u, hashLog)\n+\tcase 8:\n+\t\treturn hash8(u, hashLog)\n+\tdefault:\n+\t\treturn hash4x64(u, hashLog)\n+\t}\n+}\n+\n+// hash3 returns the hash of the lower 3 bytes of u to fit in a hash table with h bits.\n+// Preferably h should be a constant and should always be <32.\n+func hash3(u uint32, h uint8) uint32 {\n+\treturn ((u << (32 - 24)) * prime3bytes) >> ((32 - h) & 31)\n+}\n+\n+// hash4 returns the hash of u to fit in a hash table with h bits.\n+// Preferably h should be a constant and should always be <32.\n+func hash4(u uint32, h uint8) uint32 {\n+\treturn (u * prime4bytes) >> ((32 - h) & 31)\n+}\n+\n+// hash4x64 returns the hash of the lowest 4 bytes of u to fit in a hash table with h bits.\n+// Preferably h should be a constant and should always be <32.\n+func hash4x64(u uint64, h uint8) uint32 {\n+\treturn (uint32(u) * prime4bytes) >> ((32 - h) & 31)\n+}\n+\n+// hash5 returns the hash of the lowest 5 bytes of u to fit in a hash table with h bits.\n+// Preferably h should be a constant and should always be <64.\n+func hash5(u uint64, h uint8) uint32 {\n+\treturn uint32(((u << (64 - 40)) * prime5bytes) >> ((64 - h) & 63))\n+}\n+\n+// hash6 returns the hash of the lowest 6 bytes of u to fit in a hash table with h bits.\n+// Preferably h should be a constant and should always be <64.\n+func hash6(u uint64, h uint8) uint32 {\n+\treturn uint32(((u << (64 - 48)) * prime6bytes) >> ((64 - h) & 63))\n+}\n+\n+// hash7 returns the hash of the lowest 7 bytes of u to fit in a hash table with h bits.\n+// Preferably h should be a constant and should always be <64.\n+func hash7(u uint64, h uint8) uint32 {\n+\treturn uint32(((u << (64 - 56)) * prime7bytes) >> ((64 - h) & 63))\n+}\n+\n+// hash8 returns the hash of u to fit in a hash table with h bits.\n+// Preferably h should be a constant and should always be <64.\n+func hash8(u uint64, h uint8) uint32 {\n+\treturn uint32((u * prime8bytes) >> ((64 - h) & 63))\n+}"
    },
    {
      "sha": "e8c419bd533808491fcc82e158d9928addbc3371",
      "filename": "backend/vendor/github.com/klauspost/compress/zstd/history.go",
      "status": "added",
      "additions": 73,
      "deletions": 0,
      "changes": 73,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/klauspost/compress/zstd/history.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/klauspost/compress/zstd/history.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/github.com/klauspost/compress/zstd/history.go?ref=890d7154e777d0673defcd11db3ef792473d68c9",
      "patch": "@@ -0,0 +1,73 @@\n+// Copyright 2019+ Klaus Post. All rights reserved.\n+// License information can be found in the LICENSE file.\n+// Based on work by Yann Collet, released under BSD License.\n+\n+package zstd\n+\n+import (\n+\t\"github.com/klauspost/compress/huff0\"\n+)\n+\n+// history contains the information transferred between blocks.\n+type history struct {\n+\tb             []byte\n+\thuffTree      *huff0.Scratch\n+\trecentOffsets [3]int\n+\tdecoders      sequenceDecs\n+\twindowSize    int\n+\tmaxSize       int\n+\terror         bool\n+}\n+\n+// reset will reset the history to initial state of a frame.\n+// The history must already have been initialized to the desired size.\n+func (h *history) reset() {\n+\th.b = h.b[:0]\n+\th.error = false\n+\th.recentOffsets = [3]int{1, 4, 8}\n+\tif f := h.decoders.litLengths.fse; f != nil && !f.preDefined {\n+\t\tfseDecoderPool.Put(f)\n+\t}\n+\tif f := h.decoders.offsets.fse; f != nil && !f.preDefined {\n+\t\tfseDecoderPool.Put(f)\n+\t}\n+\tif f := h.decoders.matchLengths.fse; f != nil && !f.preDefined {\n+\t\tfseDecoderPool.Put(f)\n+\t}\n+\th.decoders = sequenceDecs{}\n+\tif h.huffTree != nil {\n+\t\thuffDecoderPool.Put(h.huffTree)\n+\t}\n+\th.huffTree = nil\n+\t//printf(\"history created: %+v (l: %d, c: %d)\", *h, len(h.b), cap(h.b))\n+}\n+\n+// append bytes to history.\n+// This function will make sure there is space for it,\n+// if the buffer has been allocated with enough extra space.\n+func (h *history) append(b []byte) {\n+\tif len(b) >= h.windowSize {\n+\t\t// Discard all history by simply overwriting\n+\t\th.b = h.b[:h.windowSize]\n+\t\tcopy(h.b, b[len(b)-h.windowSize:])\n+\t\treturn\n+\t}\n+\n+\t// If there is space, append it.\n+\tif len(b) < cap(h.b)-len(h.b) {\n+\t\th.b = append(h.b, b...)\n+\t\treturn\n+\t}\n+\n+\t// Move data down so we only have window size left.\n+\t// We know we have less than window size in b at this point.\n+\tdiscard := len(b) + len(h.b) - h.windowSize\n+\tcopy(h.b, h.b[discard:])\n+\th.b = h.b[:h.windowSize]\n+\tcopy(h.b[h.windowSize-len(b):], b)\n+}\n+\n+// append bytes to history without ever discarding anything.\n+func (h *history) appendKeep(b []byte) {\n+\th.b = append(h.b, b...)\n+}"
    },
    {
      "sha": "24b53065f40b5d7d277a64375956ec19cb2123c5",
      "filename": "backend/vendor/github.com/klauspost/compress/zstd/internal/xxhash/LICENSE.txt",
      "status": "added",
      "additions": 22,
      "deletions": 0,
      "changes": 22,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/klauspost/compress/zstd/internal/xxhash/LICENSE.txt",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/klauspost/compress/zstd/internal/xxhash/LICENSE.txt",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/github.com/klauspost/compress/zstd/internal/xxhash/LICENSE.txt?ref=890d7154e777d0673defcd11db3ef792473d68c9",
      "patch": "@@ -0,0 +1,22 @@\n+Copyright (c) 2016 Caleb Spare\n+\n+MIT License\n+\n+Permission is hereby granted, free of charge, to any person obtaining\n+a copy of this software and associated documentation files (the\n+\"Software\"), to deal in the Software without restriction, including\n+without limitation the rights to use, copy, modify, merge, publish,\n+distribute, sublicense, and/or sell copies of the Software, and to\n+permit persons to whom the Software is furnished to do so, subject to\n+the following conditions:\n+\n+The above copyright notice and this permission notice shall be\n+included in all copies or substantial portions of the Software.\n+\n+THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\n+EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\n+MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n+NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE\n+LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION\n+OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION\n+WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
    },
    {
      "sha": "69aa3bb587c8d9d316c1658de4d262a7ddfb1532",
      "filename": "backend/vendor/github.com/klauspost/compress/zstd/internal/xxhash/README.md",
      "status": "added",
      "additions": 58,
      "deletions": 0,
      "changes": 58,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/klauspost/compress/zstd/internal/xxhash/README.md",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/klauspost/compress/zstd/internal/xxhash/README.md",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/github.com/klauspost/compress/zstd/internal/xxhash/README.md?ref=890d7154e777d0673defcd11db3ef792473d68c9",
      "patch": "@@ -0,0 +1,58 @@\n+# xxhash\n+\n+VENDORED: Go to [github.com/cespare/xxhash](https://github.com/cespare/xxhash) for original package.\n+\n+\n+[![GoDoc](https://godoc.org/github.com/cespare/xxhash?status.svg)](https://godoc.org/github.com/cespare/xxhash)\n+[![Build Status](https://travis-ci.org/cespare/xxhash.svg?branch=master)](https://travis-ci.org/cespare/xxhash)\n+\n+xxhash is a Go implementation of the 64-bit\n+[xxHash](http://cyan4973.github.io/xxHash/) algorithm, XXH64. This is a\n+high-quality hashing algorithm that is much faster than anything in the Go\n+standard library.\n+\n+This package provides a straightforward API:\n+\n+```\n+func Sum64(b []byte) uint64\n+func Sum64String(s string) uint64\n+type Digest struct{ ... }\n+    func New() *Digest\n+```\n+\n+The `Digest` type implements hash.Hash64. Its key methods are:\n+\n+```\n+func (*Digest) Write([]byte) (int, error)\n+func (*Digest) WriteString(string) (int, error)\n+func (*Digest) Sum64() uint64\n+```\n+\n+This implementation provides a fast pure-Go implementation and an even faster\n+assembly implementation for amd64.\n+\n+## Benchmarks\n+\n+Here are some quick benchmarks comparing the pure-Go and assembly\n+implementations of Sum64.\n+\n+| input size | purego | asm |\n+| --- | --- | --- |\n+| 5 B   |  979.66 MB/s |  1291.17 MB/s  |\n+| 100 B | 7475.26 MB/s | 7973.40 MB/s  |\n+| 4 KB  | 17573.46 MB/s | 17602.65 MB/s |\n+| 10 MB | 17131.46 MB/s | 17142.16 MB/s |\n+\n+These numbers were generated on Ubuntu 18.04 with an Intel i7-8700K CPU using\n+the following commands under Go 1.11.2:\n+\n+```\n+$ go test -tags purego -benchtime 10s -bench '/xxhash,direct,bytes'\n+$ go test -benchtime 10s -bench '/xxhash,direct,bytes'\n+```\n+\n+## Projects using this package\n+\n+- [InfluxDB](https://github.com/influxdata/influxdb)\n+- [Prometheus](https://github.com/prometheus/prometheus)\n+- [FreeCache](https://github.com/coocood/freecache)"
    },
    {
      "sha": "426b9cac7869e2ee448fa1659dabea50f2cffc04",
      "filename": "backend/vendor/github.com/klauspost/compress/zstd/internal/xxhash/xxhash.go",
      "status": "added",
      "additions": 238,
      "deletions": 0,
      "changes": 238,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/klauspost/compress/zstd/internal/xxhash/xxhash.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/klauspost/compress/zstd/internal/xxhash/xxhash.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/github.com/klauspost/compress/zstd/internal/xxhash/xxhash.go?ref=890d7154e777d0673defcd11db3ef792473d68c9",
      "patch": "@@ -0,0 +1,238 @@\n+// Package xxhash implements the 64-bit variant of xxHash (XXH64) as described\n+// at http://cyan4973.github.io/xxHash/.\n+// THIS IS VENDORED: Go to github.com/cespare/xxhash for original package.\n+\n+package xxhash\n+\n+import (\n+\t\"encoding/binary\"\n+\t\"errors\"\n+\t\"math/bits\"\n+)\n+\n+const (\n+\tprime1 uint64 = 11400714785074694791\n+\tprime2 uint64 = 14029467366897019727\n+\tprime3 uint64 = 1609587929392839161\n+\tprime4 uint64 = 9650029242287828579\n+\tprime5 uint64 = 2870177450012600261\n+)\n+\n+// NOTE(caleb): I'm using both consts and vars of the primes. Using consts where\n+// possible in the Go code is worth a small (but measurable) performance boost\n+// by avoiding some MOVQs. Vars are needed for the asm and also are useful for\n+// convenience in the Go code in a few places where we need to intentionally\n+// avoid constant arithmetic (e.g., v1 := prime1 + prime2 fails because the\n+// result overflows a uint64).\n+var (\n+\tprime1v = prime1\n+\tprime2v = prime2\n+\tprime3v = prime3\n+\tprime4v = prime4\n+\tprime5v = prime5\n+)\n+\n+// Digest implements hash.Hash64.\n+type Digest struct {\n+\tv1    uint64\n+\tv2    uint64\n+\tv3    uint64\n+\tv4    uint64\n+\ttotal uint64\n+\tmem   [32]byte\n+\tn     int // how much of mem is used\n+}\n+\n+// New creates a new Digest that computes the 64-bit xxHash algorithm.\n+func New() *Digest {\n+\tvar d Digest\n+\td.Reset()\n+\treturn &d\n+}\n+\n+// Reset clears the Digest's state so that it can be reused.\n+func (d *Digest) Reset() {\n+\td.v1 = prime1v + prime2\n+\td.v2 = prime2\n+\td.v3 = 0\n+\td.v4 = -prime1v\n+\td.total = 0\n+\td.n = 0\n+}\n+\n+// Size always returns 8 bytes.\n+func (d *Digest) Size() int { return 8 }\n+\n+// BlockSize always returns 32 bytes.\n+func (d *Digest) BlockSize() int { return 32 }\n+\n+// Write adds more data to d. It always returns len(b), nil.\n+func (d *Digest) Write(b []byte) (n int, err error) {\n+\tn = len(b)\n+\td.total += uint64(n)\n+\n+\tif d.n+n < 32 {\n+\t\t// This new data doesn't even fill the current block.\n+\t\tcopy(d.mem[d.n:], b)\n+\t\td.n += n\n+\t\treturn\n+\t}\n+\n+\tif d.n > 0 {\n+\t\t// Finish off the partial block.\n+\t\tcopy(d.mem[d.n:], b)\n+\t\td.v1 = round(d.v1, u64(d.mem[0:8]))\n+\t\td.v2 = round(d.v2, u64(d.mem[8:16]))\n+\t\td.v3 = round(d.v3, u64(d.mem[16:24]))\n+\t\td.v4 = round(d.v4, u64(d.mem[24:32]))\n+\t\tb = b[32-d.n:]\n+\t\td.n = 0\n+\t}\n+\n+\tif len(b) >= 32 {\n+\t\t// One or more full blocks left.\n+\t\tnw := writeBlocks(d, b)\n+\t\tb = b[nw:]\n+\t}\n+\n+\t// Store any remaining partial block.\n+\tcopy(d.mem[:], b)\n+\td.n = len(b)\n+\n+\treturn\n+}\n+\n+// Sum appends the current hash to b and returns the resulting slice.\n+func (d *Digest) Sum(b []byte) []byte {\n+\ts := d.Sum64()\n+\treturn append(\n+\t\tb,\n+\t\tbyte(s>>56),\n+\t\tbyte(s>>48),\n+\t\tbyte(s>>40),\n+\t\tbyte(s>>32),\n+\t\tbyte(s>>24),\n+\t\tbyte(s>>16),\n+\t\tbyte(s>>8),\n+\t\tbyte(s),\n+\t)\n+}\n+\n+// Sum64 returns the current hash.\n+func (d *Digest) Sum64() uint64 {\n+\tvar h uint64\n+\n+\tif d.total >= 32 {\n+\t\tv1, v2, v3, v4 := d.v1, d.v2, d.v3, d.v4\n+\t\th = rol1(v1) + rol7(v2) + rol12(v3) + rol18(v4)\n+\t\th = mergeRound(h, v1)\n+\t\th = mergeRound(h, v2)\n+\t\th = mergeRound(h, v3)\n+\t\th = mergeRound(h, v4)\n+\t} else {\n+\t\th = d.v3 + prime5\n+\t}\n+\n+\th += d.total\n+\n+\ti, end := 0, d.n\n+\tfor ; i+8 <= end; i += 8 {\n+\t\tk1 := round(0, u64(d.mem[i:i+8]))\n+\t\th ^= k1\n+\t\th = rol27(h)*prime1 + prime4\n+\t}\n+\tif i+4 <= end {\n+\t\th ^= uint64(u32(d.mem[i:i+4])) * prime1\n+\t\th = rol23(h)*prime2 + prime3\n+\t\ti += 4\n+\t}\n+\tfor i < end {\n+\t\th ^= uint64(d.mem[i]) * prime5\n+\t\th = rol11(h) * prime1\n+\t\ti++\n+\t}\n+\n+\th ^= h >> 33\n+\th *= prime2\n+\th ^= h >> 29\n+\th *= prime3\n+\th ^= h >> 32\n+\n+\treturn h\n+}\n+\n+const (\n+\tmagic         = \"xxh\\x06\"\n+\tmarshaledSize = len(magic) + 8*5 + 32\n+)\n+\n+// MarshalBinary implements the encoding.BinaryMarshaler interface.\n+func (d *Digest) MarshalBinary() ([]byte, error) {\n+\tb := make([]byte, 0, marshaledSize)\n+\tb = append(b, magic...)\n+\tb = appendUint64(b, d.v1)\n+\tb = appendUint64(b, d.v2)\n+\tb = appendUint64(b, d.v3)\n+\tb = appendUint64(b, d.v4)\n+\tb = appendUint64(b, d.total)\n+\tb = append(b, d.mem[:d.n]...)\n+\tb = b[:len(b)+len(d.mem)-d.n]\n+\treturn b, nil\n+}\n+\n+// UnmarshalBinary implements the encoding.BinaryUnmarshaler interface.\n+func (d *Digest) UnmarshalBinary(b []byte) error {\n+\tif len(b) < len(magic) || string(b[:len(magic)]) != magic {\n+\t\treturn errors.New(\"xxhash: invalid hash state identifier\")\n+\t}\n+\tif len(b) != marshaledSize {\n+\t\treturn errors.New(\"xxhash: invalid hash state size\")\n+\t}\n+\tb = b[len(magic):]\n+\tb, d.v1 = consumeUint64(b)\n+\tb, d.v2 = consumeUint64(b)\n+\tb, d.v3 = consumeUint64(b)\n+\tb, d.v4 = consumeUint64(b)\n+\tb, d.total = consumeUint64(b)\n+\tcopy(d.mem[:], b)\n+\tb = b[len(d.mem):]\n+\td.n = int(d.total % uint64(len(d.mem)))\n+\treturn nil\n+}\n+\n+func appendUint64(b []byte, x uint64) []byte {\n+\tvar a [8]byte\n+\tbinary.LittleEndian.PutUint64(a[:], x)\n+\treturn append(b, a[:]...)\n+}\n+\n+func consumeUint64(b []byte) ([]byte, uint64) {\n+\tx := u64(b)\n+\treturn b[8:], x\n+}\n+\n+func u64(b []byte) uint64 { return binary.LittleEndian.Uint64(b) }\n+func u32(b []byte) uint32 { return binary.LittleEndian.Uint32(b) }\n+\n+func round(acc, input uint64) uint64 {\n+\tacc += input * prime2\n+\tacc = rol31(acc)\n+\tacc *= prime1\n+\treturn acc\n+}\n+\n+func mergeRound(acc, val uint64) uint64 {\n+\tval = round(0, val)\n+\tacc ^= val\n+\tacc = acc*prime1 + prime4\n+\treturn acc\n+}\n+\n+func rol1(x uint64) uint64  { return bits.RotateLeft64(x, 1) }\n+func rol7(x uint64) uint64  { return bits.RotateLeft64(x, 7) }\n+func rol11(x uint64) uint64 { return bits.RotateLeft64(x, 11) }\n+func rol12(x uint64) uint64 { return bits.RotateLeft64(x, 12) }\n+func rol18(x uint64) uint64 { return bits.RotateLeft64(x, 18) }\n+func rol23(x uint64) uint64 { return bits.RotateLeft64(x, 23) }\n+func rol27(x uint64) uint64 { return bits.RotateLeft64(x, 27) }\n+func rol31(x uint64) uint64 { return bits.RotateLeft64(x, 31) }"
    },
    {
      "sha": "35318d7c46cbd533e194f946894e37eeaa1d50a6",
      "filename": "backend/vendor/github.com/klauspost/compress/zstd/internal/xxhash/xxhash_amd64.go",
      "status": "added",
      "additions": 13,
      "deletions": 0,
      "changes": 13,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/klauspost/compress/zstd/internal/xxhash/xxhash_amd64.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/klauspost/compress/zstd/internal/xxhash/xxhash_amd64.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/github.com/klauspost/compress/zstd/internal/xxhash/xxhash_amd64.go?ref=890d7154e777d0673defcd11db3ef792473d68c9",
      "patch": "@@ -0,0 +1,13 @@\n+// +build !appengine\n+// +build gc\n+// +build !purego\n+\n+package xxhash\n+\n+// Sum64 computes the 64-bit xxHash digest of b.\n+//\n+//go:noescape\n+func Sum64(b []byte) uint64\n+\n+//go:noescape\n+func writeBlocks(*Digest, []byte) int"
    },
    {
      "sha": "d580e32aed4afb344b5a652d1654878cfb9f2abc",
      "filename": "backend/vendor/github.com/klauspost/compress/zstd/internal/xxhash/xxhash_amd64.s",
      "status": "added",
      "additions": 215,
      "deletions": 0,
      "changes": 215,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/klauspost/compress/zstd/internal/xxhash/xxhash_amd64.s",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/klauspost/compress/zstd/internal/xxhash/xxhash_amd64.s",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/github.com/klauspost/compress/zstd/internal/xxhash/xxhash_amd64.s?ref=890d7154e777d0673defcd11db3ef792473d68c9",
      "patch": "@@ -0,0 +1,215 @@\n+// +build !appengine\n+// +build gc\n+// +build !purego\n+\n+#include \"textflag.h\"\n+\n+// Register allocation:\n+// AX\th\n+// CX\tpointer to advance through b\n+// DX\tn\n+// BX\tloop end\n+// R8\tv1, k1\n+// R9\tv2\n+// R10\tv3\n+// R11\tv4\n+// R12\ttmp\n+// R13\tprime1v\n+// R14\tprime2v\n+// R15\tprime4v\n+\n+// round reads from and advances the buffer pointer in CX.\n+// It assumes that R13 has prime1v and R14 has prime2v.\n+#define round(r) \\\n+\tMOVQ  (CX), R12 \\\n+\tADDQ  $8, CX    \\\n+\tIMULQ R14, R12  \\\n+\tADDQ  R12, r    \\\n+\tROLQ  $31, r    \\\n+\tIMULQ R13, r\n+\n+// mergeRound applies a merge round on the two registers acc and val.\n+// It assumes that R13 has prime1v, R14 has prime2v, and R15 has prime4v.\n+#define mergeRound(acc, val) \\\n+\tIMULQ R14, val \\\n+\tROLQ  $31, val \\\n+\tIMULQ R13, val \\\n+\tXORQ  val, acc \\\n+\tIMULQ R13, acc \\\n+\tADDQ  R15, acc\n+\n+// func Sum64(b []byte) uint64\n+TEXT ·Sum64(SB), NOSPLIT, $0-32\n+\t// Load fixed primes.\n+\tMOVQ ·prime1v(SB), R13\n+\tMOVQ ·prime2v(SB), R14\n+\tMOVQ ·prime4v(SB), R15\n+\n+\t// Load slice.\n+\tMOVQ b_base+0(FP), CX\n+\tMOVQ b_len+8(FP), DX\n+\tLEAQ (CX)(DX*1), BX\n+\n+\t// The first loop limit will be len(b)-32.\n+\tSUBQ $32, BX\n+\n+\t// Check whether we have at least one block.\n+\tCMPQ DX, $32\n+\tJLT  noBlocks\n+\n+\t// Set up initial state (v1, v2, v3, v4).\n+\tMOVQ R13, R8\n+\tADDQ R14, R8\n+\tMOVQ R14, R9\n+\tXORQ R10, R10\n+\tXORQ R11, R11\n+\tSUBQ R13, R11\n+\n+\t// Loop until CX > BX.\n+blockLoop:\n+\tround(R8)\n+\tround(R9)\n+\tround(R10)\n+\tround(R11)\n+\n+\tCMPQ CX, BX\n+\tJLE  blockLoop\n+\n+\tMOVQ R8, AX\n+\tROLQ $1, AX\n+\tMOVQ R9, R12\n+\tROLQ $7, R12\n+\tADDQ R12, AX\n+\tMOVQ R10, R12\n+\tROLQ $12, R12\n+\tADDQ R12, AX\n+\tMOVQ R11, R12\n+\tROLQ $18, R12\n+\tADDQ R12, AX\n+\n+\tmergeRound(AX, R8)\n+\tmergeRound(AX, R9)\n+\tmergeRound(AX, R10)\n+\tmergeRound(AX, R11)\n+\n+\tJMP afterBlocks\n+\n+noBlocks:\n+\tMOVQ ·prime5v(SB), AX\n+\n+afterBlocks:\n+\tADDQ DX, AX\n+\n+\t// Right now BX has len(b)-32, and we want to loop until CX > len(b)-8.\n+\tADDQ $24, BX\n+\n+\tCMPQ CX, BX\n+\tJG   fourByte\n+\n+wordLoop:\n+\t// Calculate k1.\n+\tMOVQ  (CX), R8\n+\tADDQ  $8, CX\n+\tIMULQ R14, R8\n+\tROLQ  $31, R8\n+\tIMULQ R13, R8\n+\n+\tXORQ  R8, AX\n+\tROLQ  $27, AX\n+\tIMULQ R13, AX\n+\tADDQ  R15, AX\n+\n+\tCMPQ CX, BX\n+\tJLE  wordLoop\n+\n+fourByte:\n+\tADDQ $4, BX\n+\tCMPQ CX, BX\n+\tJG   singles\n+\n+\tMOVL  (CX), R8\n+\tADDQ  $4, CX\n+\tIMULQ R13, R8\n+\tXORQ  R8, AX\n+\n+\tROLQ  $23, AX\n+\tIMULQ R14, AX\n+\tADDQ  ·prime3v(SB), AX\n+\n+singles:\n+\tADDQ $4, BX\n+\tCMPQ CX, BX\n+\tJGE  finalize\n+\n+singlesLoop:\n+\tMOVBQZX (CX), R12\n+\tADDQ    $1, CX\n+\tIMULQ   ·prime5v(SB), R12\n+\tXORQ    R12, AX\n+\n+\tROLQ  $11, AX\n+\tIMULQ R13, AX\n+\n+\tCMPQ CX, BX\n+\tJL   singlesLoop\n+\n+finalize:\n+\tMOVQ  AX, R12\n+\tSHRQ  $33, R12\n+\tXORQ  R12, AX\n+\tIMULQ R14, AX\n+\tMOVQ  AX, R12\n+\tSHRQ  $29, R12\n+\tXORQ  R12, AX\n+\tIMULQ ·prime3v(SB), AX\n+\tMOVQ  AX, R12\n+\tSHRQ  $32, R12\n+\tXORQ  R12, AX\n+\n+\tMOVQ AX, ret+24(FP)\n+\tRET\n+\n+// writeBlocks uses the same registers as above except that it uses AX to store\n+// the d pointer.\n+\n+// func writeBlocks(d *Digest, b []byte) int\n+TEXT ·writeBlocks(SB), NOSPLIT, $0-40\n+\t// Load fixed primes needed for round.\n+\tMOVQ ·prime1v(SB), R13\n+\tMOVQ ·prime2v(SB), R14\n+\n+\t// Load slice.\n+\tMOVQ b_base+8(FP), CX\n+\tMOVQ b_len+16(FP), DX\n+\tLEAQ (CX)(DX*1), BX\n+\tSUBQ $32, BX\n+\n+\t// Load vN from d.\n+\tMOVQ d+0(FP), AX\n+\tMOVQ 0(AX), R8   // v1\n+\tMOVQ 8(AX), R9   // v2\n+\tMOVQ 16(AX), R10 // v3\n+\tMOVQ 24(AX), R11 // v4\n+\n+\t// We don't need to check the loop condition here; this function is\n+\t// always called with at least one block of data to process.\n+blockLoop:\n+\tround(R8)\n+\tround(R9)\n+\tround(R10)\n+\tround(R11)\n+\n+\tCMPQ CX, BX\n+\tJLE  blockLoop\n+\n+\t// Copy vN back to d.\n+\tMOVQ R8, 0(AX)\n+\tMOVQ R9, 8(AX)\n+\tMOVQ R10, 16(AX)\n+\tMOVQ R11, 24(AX)\n+\n+\t// The number of bytes written is CX minus the old base pointer.\n+\tSUBQ b_base+8(FP), CX\n+\tMOVQ CX, ret+32(FP)\n+\n+\tRET"
    },
    {
      "sha": "4a5a821603e5b8d876d07219882c5e127619f1a3",
      "filename": "backend/vendor/github.com/klauspost/compress/zstd/internal/xxhash/xxhash_other.go",
      "status": "added",
      "additions": 76,
      "deletions": 0,
      "changes": 76,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/klauspost/compress/zstd/internal/xxhash/xxhash_other.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/klauspost/compress/zstd/internal/xxhash/xxhash_other.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/github.com/klauspost/compress/zstd/internal/xxhash/xxhash_other.go?ref=890d7154e777d0673defcd11db3ef792473d68c9",
      "patch": "@@ -0,0 +1,76 @@\n+// +build !amd64 appengine !gc purego\n+\n+package xxhash\n+\n+// Sum64 computes the 64-bit xxHash digest of b.\n+func Sum64(b []byte) uint64 {\n+\t// A simpler version would be\n+\t//   d := New()\n+\t//   d.Write(b)\n+\t//   return d.Sum64()\n+\t// but this is faster, particularly for small inputs.\n+\n+\tn := len(b)\n+\tvar h uint64\n+\n+\tif n >= 32 {\n+\t\tv1 := prime1v + prime2\n+\t\tv2 := prime2\n+\t\tv3 := uint64(0)\n+\t\tv4 := -prime1v\n+\t\tfor len(b) >= 32 {\n+\t\t\tv1 = round(v1, u64(b[0:8:len(b)]))\n+\t\t\tv2 = round(v2, u64(b[8:16:len(b)]))\n+\t\t\tv3 = round(v3, u64(b[16:24:len(b)]))\n+\t\t\tv4 = round(v4, u64(b[24:32:len(b)]))\n+\t\t\tb = b[32:len(b):len(b)]\n+\t\t}\n+\t\th = rol1(v1) + rol7(v2) + rol12(v3) + rol18(v4)\n+\t\th = mergeRound(h, v1)\n+\t\th = mergeRound(h, v2)\n+\t\th = mergeRound(h, v3)\n+\t\th = mergeRound(h, v4)\n+\t} else {\n+\t\th = prime5\n+\t}\n+\n+\th += uint64(n)\n+\n+\ti, end := 0, len(b)\n+\tfor ; i+8 <= end; i += 8 {\n+\t\tk1 := round(0, u64(b[i:i+8:len(b)]))\n+\t\th ^= k1\n+\t\th = rol27(h)*prime1 + prime4\n+\t}\n+\tif i+4 <= end {\n+\t\th ^= uint64(u32(b[i:i+4:len(b)])) * prime1\n+\t\th = rol23(h)*prime2 + prime3\n+\t\ti += 4\n+\t}\n+\tfor ; i < end; i++ {\n+\t\th ^= uint64(b[i]) * prime5\n+\t\th = rol11(h) * prime1\n+\t}\n+\n+\th ^= h >> 33\n+\th *= prime2\n+\th ^= h >> 29\n+\th *= prime3\n+\th ^= h >> 32\n+\n+\treturn h\n+}\n+\n+func writeBlocks(d *Digest, b []byte) int {\n+\tv1, v2, v3, v4 := d.v1, d.v2, d.v3, d.v4\n+\tn := len(b)\n+\tfor len(b) >= 32 {\n+\t\tv1 = round(v1, u64(b[0:8:len(b)]))\n+\t\tv2 = round(v2, u64(b[8:16:len(b)]))\n+\t\tv3 = round(v3, u64(b[16:24:len(b)]))\n+\t\tv4 = round(v4, u64(b[24:32:len(b)]))\n+\t\tb = b[32:len(b):len(b)]\n+\t}\n+\td.v1, d.v2, d.v3, d.v4 = v1, v2, v3, v4\n+\treturn n - len(b)\n+}"
    },
    {
      "sha": "6f3b0cb10264c553f9bbf0bcab8ca0fc0158d8ca",
      "filename": "backend/vendor/github.com/klauspost/compress/zstd/internal/xxhash/xxhash_safe.go",
      "status": "added",
      "additions": 11,
      "deletions": 0,
      "changes": 11,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/klauspost/compress/zstd/internal/xxhash/xxhash_safe.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/klauspost/compress/zstd/internal/xxhash/xxhash_safe.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/github.com/klauspost/compress/zstd/internal/xxhash/xxhash_safe.go?ref=890d7154e777d0673defcd11db3ef792473d68c9",
      "patch": "@@ -0,0 +1,11 @@\n+package xxhash\n+\n+// Sum64String computes the 64-bit xxHash digest of s.\n+func Sum64String(s string) uint64 {\n+\treturn Sum64([]byte(s))\n+}\n+\n+// WriteString adds more data to d. It always returns len(s), nil.\n+func (d *Digest) WriteString(s string) (n int, err error) {\n+\treturn d.Write([]byte(s))\n+}"
    },
    {
      "sha": "15a45f7b5012ab8d9c3e5949cf925541ab6fda8e",
      "filename": "backend/vendor/github.com/klauspost/compress/zstd/seqdec.go",
      "status": "added",
      "additions": 402,
      "deletions": 0,
      "changes": 402,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/klauspost/compress/zstd/seqdec.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/klauspost/compress/zstd/seqdec.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/github.com/klauspost/compress/zstd/seqdec.go?ref=890d7154e777d0673defcd11db3ef792473d68c9",
      "patch": "@@ -0,0 +1,402 @@\n+// Copyright 2019+ Klaus Post. All rights reserved.\n+// License information can be found in the LICENSE file.\n+// Based on work by Yann Collet, released under BSD License.\n+\n+package zstd\n+\n+import (\n+\t\"errors\"\n+\t\"fmt\"\n+\t\"io\"\n+)\n+\n+type seq struct {\n+\tlitLen   uint32\n+\tmatchLen uint32\n+\toffset   uint32\n+\n+\t// Codes are stored here for the encoder\n+\t// so they only have to be looked up once.\n+\tllCode, mlCode, ofCode uint8\n+}\n+\n+func (s seq) String() string {\n+\tif s.offset <= 3 {\n+\t\tif s.offset == 0 {\n+\t\t\treturn fmt.Sprint(\"litLen:\", s.litLen, \", matchLen:\", s.matchLen+zstdMinMatch, \", offset: INVALID (0)\")\n+\t\t}\n+\t\treturn fmt.Sprint(\"litLen:\", s.litLen, \", matchLen:\", s.matchLen+zstdMinMatch, \", offset:\", s.offset, \" (repeat)\")\n+\t}\n+\treturn fmt.Sprint(\"litLen:\", s.litLen, \", matchLen:\", s.matchLen+zstdMinMatch, \", offset:\", s.offset-3, \" (new)\")\n+}\n+\n+type seqCompMode uint8\n+\n+const (\n+\tcompModePredefined seqCompMode = iota\n+\tcompModeRLE\n+\tcompModeFSE\n+\tcompModeRepeat\n+)\n+\n+type sequenceDec struct {\n+\t// decoder keeps track of the current state and updates it from the bitstream.\n+\tfse    *fseDecoder\n+\tstate  fseState\n+\trepeat bool\n+}\n+\n+// init the state of the decoder with input from stream.\n+func (s *sequenceDec) init(br *bitReader) error {\n+\tif s.fse == nil {\n+\t\treturn errors.New(\"sequence decoder not defined\")\n+\t}\n+\ts.state.init(br, s.fse.actualTableLog, s.fse.dt[:1<<s.fse.actualTableLog])\n+\treturn nil\n+}\n+\n+// sequenceDecs contains all 3 sequence decoders and their state.\n+type sequenceDecs struct {\n+\tlitLengths   sequenceDec\n+\toffsets      sequenceDec\n+\tmatchLengths sequenceDec\n+\tprevOffset   [3]int\n+\thist         []byte\n+\tliterals     []byte\n+\tout          []byte\n+\tmaxBits      uint8\n+}\n+\n+// initialize all 3 decoders from the stream input.\n+func (s *sequenceDecs) initialize(br *bitReader, hist *history, literals, out []byte) error {\n+\tif err := s.litLengths.init(br); err != nil {\n+\t\treturn errors.New(\"litLengths:\" + err.Error())\n+\t}\n+\tif err := s.offsets.init(br); err != nil {\n+\t\treturn errors.New(\"offsets:\" + err.Error())\n+\t}\n+\tif err := s.matchLengths.init(br); err != nil {\n+\t\treturn errors.New(\"matchLengths:\" + err.Error())\n+\t}\n+\ts.literals = literals\n+\ts.hist = hist.b\n+\ts.prevOffset = hist.recentOffsets\n+\ts.maxBits = s.litLengths.fse.maxBits + s.offsets.fse.maxBits + s.matchLengths.fse.maxBits\n+\ts.out = out\n+\treturn nil\n+}\n+\n+// decode sequences from the stream with the provided history.\n+func (s *sequenceDecs) decode(seqs int, br *bitReader, hist []byte) error {\n+\tstartSize := len(s.out)\n+\t// Grab full sizes tables, to avoid bounds checks.\n+\tllTable, mlTable, ofTable := s.litLengths.fse.dt[:maxTablesize], s.matchLengths.fse.dt[:maxTablesize], s.offsets.fse.dt[:maxTablesize]\n+\tllState, mlState, ofState := s.litLengths.state.state, s.matchLengths.state.state, s.offsets.state.state\n+\n+\tfor i := seqs - 1; i >= 0; i-- {\n+\t\tif br.overread() {\n+\t\t\tprintf(\"reading sequence %d, exceeded available data\\n\", seqs-i)\n+\t\t\treturn io.ErrUnexpectedEOF\n+\t\t}\n+\t\tvar litLen, matchOff, matchLen int\n+\t\tif br.off > 4+((maxOffsetBits+16+16)>>3) {\n+\t\t\tlitLen, matchOff, matchLen = s.nextFast(br, llState, mlState, ofState)\n+\t\t\tbr.fillFast()\n+\t\t} else {\n+\t\t\tlitLen, matchOff, matchLen = s.next(br, llState, mlState, ofState)\n+\t\t\tbr.fill()\n+\t\t}\n+\n+\t\tif debugSequences {\n+\t\t\tprintln(\"Seq\", seqs-i-1, \"Litlen:\", litLen, \"matchOff:\", matchOff, \"(abs) matchLen:\", matchLen)\n+\t\t}\n+\n+\t\tif litLen > len(s.literals) {\n+\t\t\treturn fmt.Errorf(\"unexpected literal count, want %d bytes, but only %d is available\", litLen, len(s.literals))\n+\t\t}\n+\t\tsize := litLen + matchLen + len(s.out)\n+\t\tif size-startSize > maxBlockSize {\n+\t\t\treturn fmt.Errorf(\"output (%d) bigger than max block size\", size)\n+\t\t}\n+\t\tif size > cap(s.out) {\n+\t\t\t// Not enough size, will be extremely rarely triggered,\n+\t\t\t// but could be if destination slice is too small for sync operations.\n+\t\t\t// We add maxBlockSize to the capacity.\n+\t\t\ts.out = append(s.out, make([]byte, maxBlockSize)...)\n+\t\t\ts.out = s.out[:len(s.out)-maxBlockSize]\n+\t\t}\n+\t\tif matchLen > maxMatchLen {\n+\t\t\treturn fmt.Errorf(\"match len (%d) bigger than max allowed length\", matchLen)\n+\t\t}\n+\t\tif matchOff > len(s.out)+len(hist)+litLen {\n+\t\t\treturn fmt.Errorf(\"match offset (%d) bigger than current history (%d)\", matchOff, len(s.out)+len(hist)+litLen)\n+\t\t}\n+\t\tif matchOff == 0 && matchLen > 0 {\n+\t\t\treturn fmt.Errorf(\"zero matchoff and matchlen > 0\")\n+\t\t}\n+\n+\t\ts.out = append(s.out, s.literals[:litLen]...)\n+\t\ts.literals = s.literals[litLen:]\n+\t\tout := s.out\n+\n+\t\t// Copy from history.\n+\t\t// TODO: Blocks without history could be made to ignore this completely.\n+\t\tif v := matchOff - len(s.out); v > 0 {\n+\t\t\t// v is the start position in history from end.\n+\t\t\tstart := len(s.hist) - v\n+\t\t\tif matchLen > v {\n+\t\t\t\t// Some goes into current block.\n+\t\t\t\t// Copy remainder of history\n+\t\t\t\tout = append(out, s.hist[start:]...)\n+\t\t\t\tmatchOff -= v\n+\t\t\t\tmatchLen -= v\n+\t\t\t} else {\n+\t\t\t\tout = append(out, s.hist[start:start+matchLen]...)\n+\t\t\t\tmatchLen = 0\n+\t\t\t}\n+\t\t}\n+\t\t// We must be in current buffer now\n+\t\tif matchLen > 0 {\n+\t\t\tstart := len(s.out) - matchOff\n+\t\t\tif matchLen <= len(s.out)-start {\n+\t\t\t\t// No overlap\n+\t\t\t\tout = append(out, s.out[start:start+matchLen]...)\n+\t\t\t} else {\n+\t\t\t\t// Overlapping copy\n+\t\t\t\t// Extend destination slice and copy one byte at the time.\n+\t\t\t\tout = out[:len(out)+matchLen]\n+\t\t\t\tsrc := out[start : start+matchLen]\n+\t\t\t\t// Destination is the space we just added.\n+\t\t\t\tdst := out[len(out)-matchLen:]\n+\t\t\t\tdst = dst[:len(src)]\n+\t\t\t\tfor i := range src {\n+\t\t\t\t\tdst[i] = src[i]\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t\ts.out = out\n+\t\tif i == 0 {\n+\t\t\t// This is the last sequence, so we shouldn't update state.\n+\t\t\tbreak\n+\t\t}\n+\n+\t\t// Manually inlined, ~ 5-20% faster\n+\t\t// Update all 3 states at once. Approx 20% faster.\n+\t\tnBits := llState.nbBits() + mlState.nbBits() + ofState.nbBits()\n+\t\tif nBits == 0 {\n+\t\t\tllState = llTable[llState.newState()&maxTableMask]\n+\t\t\tmlState = mlTable[mlState.newState()&maxTableMask]\n+\t\t\tofState = ofTable[ofState.newState()&maxTableMask]\n+\t\t} else {\n+\t\t\tbits := br.getBitsFast(nBits)\n+\t\t\tlowBits := uint16(bits >> ((ofState.nbBits() + mlState.nbBits()) & 31))\n+\t\t\tllState = llTable[(llState.newState()+lowBits)&maxTableMask]\n+\n+\t\t\tlowBits = uint16(bits >> (ofState.nbBits() & 31))\n+\t\t\tlowBits &= bitMask[mlState.nbBits()&15]\n+\t\t\tmlState = mlTable[(mlState.newState()+lowBits)&maxTableMask]\n+\n+\t\t\tlowBits = uint16(bits) & bitMask[ofState.nbBits()&15]\n+\t\t\tofState = ofTable[(ofState.newState()+lowBits)&maxTableMask]\n+\t\t}\n+\t}\n+\n+\t// Add final literals\n+\ts.out = append(s.out, s.literals...)\n+\treturn nil\n+}\n+\n+// update states, at least 27 bits must be available.\n+func (s *sequenceDecs) update(br *bitReader) {\n+\t// Max 8 bits\n+\ts.litLengths.state.next(br)\n+\t// Max 9 bits\n+\ts.matchLengths.state.next(br)\n+\t// Max 8 bits\n+\ts.offsets.state.next(br)\n+}\n+\n+var bitMask [16]uint16\n+\n+func init() {\n+\tfor i := range bitMask[:] {\n+\t\tbitMask[i] = uint16((1 << uint(i)) - 1)\n+\t}\n+}\n+\n+// update states, at least 27 bits must be available.\n+func (s *sequenceDecs) updateAlt(br *bitReader) {\n+\t// Update all 3 states at once. Approx 20% faster.\n+\ta, b, c := s.litLengths.state.state, s.matchLengths.state.state, s.offsets.state.state\n+\n+\tnBits := a.nbBits() + b.nbBits() + c.nbBits()\n+\tif nBits == 0 {\n+\t\ts.litLengths.state.state = s.litLengths.state.dt[a.newState()]\n+\t\ts.matchLengths.state.state = s.matchLengths.state.dt[b.newState()]\n+\t\ts.offsets.state.state = s.offsets.state.dt[c.newState()]\n+\t\treturn\n+\t}\n+\tbits := br.getBitsFast(nBits)\n+\tlowBits := uint16(bits >> ((c.nbBits() + b.nbBits()) & 31))\n+\ts.litLengths.state.state = s.litLengths.state.dt[a.newState()+lowBits]\n+\n+\tlowBits = uint16(bits >> (c.nbBits() & 31))\n+\tlowBits &= bitMask[b.nbBits()&15]\n+\ts.matchLengths.state.state = s.matchLengths.state.dt[b.newState()+lowBits]\n+\n+\tlowBits = uint16(bits) & bitMask[c.nbBits()&15]\n+\ts.offsets.state.state = s.offsets.state.dt[c.newState()+lowBits]\n+}\n+\n+// nextFast will return new states when there are at least 4 unused bytes left on the stream when done.\n+func (s *sequenceDecs) nextFast(br *bitReader, llState, mlState, ofState decSymbol) (ll, mo, ml int) {\n+\t// Final will not read from stream.\n+\tll, llB := llState.final()\n+\tml, mlB := mlState.final()\n+\tmo, moB := ofState.final()\n+\n+\t// extra bits are stored in reverse order.\n+\tbr.fillFast()\n+\tmo += br.getBits(moB)\n+\tif s.maxBits > 32 {\n+\t\tbr.fillFast()\n+\t}\n+\tml += br.getBits(mlB)\n+\tll += br.getBits(llB)\n+\n+\tif moB > 1 {\n+\t\ts.prevOffset[2] = s.prevOffset[1]\n+\t\ts.prevOffset[1] = s.prevOffset[0]\n+\t\ts.prevOffset[0] = mo\n+\t\treturn\n+\t}\n+\t// mo = s.adjustOffset(mo, ll, moB)\n+\t// Inlined for rather big speedup\n+\tif ll == 0 {\n+\t\t// There is an exception though, when current sequence's literals_length = 0.\n+\t\t// In this case, repeated offsets are shifted by one, so an offset_value of 1 means Repeated_Offset2,\n+\t\t// an offset_value of 2 means Repeated_Offset3, and an offset_value of 3 means Repeated_Offset1 - 1_byte.\n+\t\tmo++\n+\t}\n+\n+\tif mo == 0 {\n+\t\tmo = s.prevOffset[0]\n+\t\treturn\n+\t}\n+\tvar temp int\n+\tif mo == 3 {\n+\t\ttemp = s.prevOffset[0] - 1\n+\t} else {\n+\t\ttemp = s.prevOffset[mo]\n+\t}\n+\n+\tif temp == 0 {\n+\t\t// 0 is not valid; input is corrupted; force offset to 1\n+\t\tprintln(\"temp was 0\")\n+\t\ttemp = 1\n+\t}\n+\n+\tif mo != 1 {\n+\t\ts.prevOffset[2] = s.prevOffset[1]\n+\t}\n+\ts.prevOffset[1] = s.prevOffset[0]\n+\ts.prevOffset[0] = temp\n+\tmo = temp\n+\treturn\n+}\n+\n+func (s *sequenceDecs) next(br *bitReader, llState, mlState, ofState decSymbol) (ll, mo, ml int) {\n+\t// Final will not read from stream.\n+\tll, llB := llState.final()\n+\tml, mlB := mlState.final()\n+\tmo, moB := ofState.final()\n+\n+\t// extra bits are stored in reverse order.\n+\tbr.fill()\n+\tif s.maxBits <= 32 {\n+\t\tmo += br.getBits(moB)\n+\t\tml += br.getBits(mlB)\n+\t\tll += br.getBits(llB)\n+\t} else {\n+\t\tmo += br.getBits(moB)\n+\t\tbr.fill()\n+\t\t// matchlength+literal length, max 32 bits\n+\t\tml += br.getBits(mlB)\n+\t\tll += br.getBits(llB)\n+\n+\t}\n+\tmo = s.adjustOffset(mo, ll, moB)\n+\treturn\n+}\n+\n+func (s *sequenceDecs) adjustOffset(offset, litLen int, offsetB uint8) int {\n+\tif offsetB > 1 {\n+\t\ts.prevOffset[2] = s.prevOffset[1]\n+\t\ts.prevOffset[1] = s.prevOffset[0]\n+\t\ts.prevOffset[0] = offset\n+\t\treturn offset\n+\t}\n+\n+\tif litLen == 0 {\n+\t\t// There is an exception though, when current sequence's literals_length = 0.\n+\t\t// In this case, repeated offsets are shifted by one, so an offset_value of 1 means Repeated_Offset2,\n+\t\t// an offset_value of 2 means Repeated_Offset3, and an offset_value of 3 means Repeated_Offset1 - 1_byte.\n+\t\toffset++\n+\t}\n+\n+\tif offset == 0 {\n+\t\treturn s.prevOffset[0]\n+\t}\n+\tvar temp int\n+\tif offset == 3 {\n+\t\ttemp = s.prevOffset[0] - 1\n+\t} else {\n+\t\ttemp = s.prevOffset[offset]\n+\t}\n+\n+\tif temp == 0 {\n+\t\t// 0 is not valid; input is corrupted; force offset to 1\n+\t\tprintln(\"temp was 0\")\n+\t\ttemp = 1\n+\t}\n+\n+\tif offset != 1 {\n+\t\ts.prevOffset[2] = s.prevOffset[1]\n+\t}\n+\ts.prevOffset[1] = s.prevOffset[0]\n+\ts.prevOffset[0] = temp\n+\treturn temp\n+}\n+\n+// mergeHistory will merge history.\n+func (s *sequenceDecs) mergeHistory(hist *sequenceDecs) (*sequenceDecs, error) {\n+\tfor i := uint(0); i < 3; i++ {\n+\t\tvar sNew, sHist *sequenceDec\n+\t\tswitch i {\n+\t\tdefault:\n+\t\t\t// same as \"case 0\":\n+\t\t\tsNew = &s.litLengths\n+\t\t\tsHist = &hist.litLengths\n+\t\tcase 1:\n+\t\t\tsNew = &s.offsets\n+\t\t\tsHist = &hist.offsets\n+\t\tcase 2:\n+\t\t\tsNew = &s.matchLengths\n+\t\t\tsHist = &hist.matchLengths\n+\t\t}\n+\t\tif sNew.repeat {\n+\t\t\tif sHist.fse == nil {\n+\t\t\t\treturn nil, fmt.Errorf(\"sequence stream %d, repeat requested, but no history\", i)\n+\t\t\t}\n+\t\t\tcontinue\n+\t\t}\n+\t\tif sNew.fse == nil {\n+\t\t\treturn nil, fmt.Errorf(\"sequence stream %d, no fse found\", i)\n+\t\t}\n+\t\tif sHist.fse != nil && !sHist.fse.preDefined {\n+\t\t\tfseDecoderPool.Put(sHist.fse)\n+\t\t}\n+\t\tsHist.fse = sNew.fse\n+\t}\n+\treturn hist, nil\n+}"
    },
    {
      "sha": "36bcc3cc02eb945dc692ab3c7e41d270c6fd8e5a",
      "filename": "backend/vendor/github.com/klauspost/compress/zstd/seqenc.go",
      "status": "added",
      "additions": 115,
      "deletions": 0,
      "changes": 115,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/klauspost/compress/zstd/seqenc.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/klauspost/compress/zstd/seqenc.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/github.com/klauspost/compress/zstd/seqenc.go?ref=890d7154e777d0673defcd11db3ef792473d68c9",
      "patch": "@@ -0,0 +1,115 @@\n+// Copyright 2019+ Klaus Post. All rights reserved.\n+// License information can be found in the LICENSE file.\n+// Based on work by Yann Collet, released under BSD License.\n+\n+package zstd\n+\n+import \"math/bits\"\n+\n+type seqCoders struct {\n+\tllEnc, ofEnc, mlEnc    *fseEncoder\n+\tllPrev, ofPrev, mlPrev *fseEncoder\n+}\n+\n+// swap coders with another (block).\n+func (s *seqCoders) swap(other *seqCoders) {\n+\t*s, *other = *other, *s\n+}\n+\n+// setPrev will update the previous encoders to the actually used ones\n+// and make sure a fresh one is in the main slot.\n+func (s *seqCoders) setPrev(ll, ml, of *fseEncoder) {\n+\tcompareSwap := func(used *fseEncoder, current, prev **fseEncoder) {\n+\t\t// We used the new one, more current to history and reuse the previous history\n+\t\tif *current == used {\n+\t\t\t*prev, *current = *current, *prev\n+\t\t\tc := *current\n+\t\t\tp := *prev\n+\t\t\tc.reUsed = false\n+\t\t\tp.reUsed = true\n+\t\t\treturn\n+\t\t}\n+\t\tif used == *prev {\n+\t\t\treturn\n+\t\t}\n+\t\t// Ensure we cannot reuse by accident\n+\t\tprevEnc := *prev\n+\t\tprevEnc.symbolLen = 0\n+\t\treturn\n+\t}\n+\tcompareSwap(ll, &s.llEnc, &s.llPrev)\n+\tcompareSwap(ml, &s.mlEnc, &s.mlPrev)\n+\tcompareSwap(of, &s.ofEnc, &s.ofPrev)\n+}\n+\n+func highBit(val uint32) (n uint32) {\n+\treturn uint32(bits.Len32(val) - 1)\n+}\n+\n+var llCodeTable = [64]byte{0, 1, 2, 3, 4, 5, 6, 7,\n+\t8, 9, 10, 11, 12, 13, 14, 15,\n+\t16, 16, 17, 17, 18, 18, 19, 19,\n+\t20, 20, 20, 20, 21, 21, 21, 21,\n+\t22, 22, 22, 22, 22, 22, 22, 22,\n+\t23, 23, 23, 23, 23, 23, 23, 23,\n+\t24, 24, 24, 24, 24, 24, 24, 24,\n+\t24, 24, 24, 24, 24, 24, 24, 24}\n+\n+// Up to 6 bits\n+const maxLLCode = 35\n+\n+// llBitsTable translates from ll code to number of bits.\n+var llBitsTable = [maxLLCode + 1]byte{\n+\t0, 0, 0, 0, 0, 0, 0, 0,\n+\t0, 0, 0, 0, 0, 0, 0, 0,\n+\t1, 1, 1, 1, 2, 2, 3, 3,\n+\t4, 6, 7, 8, 9, 10, 11, 12,\n+\t13, 14, 15, 16}\n+\n+// llCode returns the code that represents the literal length requested.\n+func llCode(litLength uint32) uint8 {\n+\tconst llDeltaCode = 19\n+\tif litLength <= 63 {\n+\t\t// Compiler insists on bounds check (Go 1.12)\n+\t\treturn llCodeTable[litLength&63]\n+\t}\n+\treturn uint8(highBit(litLength)) + llDeltaCode\n+}\n+\n+var mlCodeTable = [128]byte{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15,\n+\t16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31,\n+\t32, 32, 33, 33, 34, 34, 35, 35, 36, 36, 36, 36, 37, 37, 37, 37,\n+\t38, 38, 38, 38, 38, 38, 38, 38, 39, 39, 39, 39, 39, 39, 39, 39,\n+\t40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40,\n+\t41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41,\n+\t42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42,\n+\t42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42}\n+\n+// Up to 6 bits\n+const maxMLCode = 52\n+\n+// mlBitsTable translates from ml code to number of bits.\n+var mlBitsTable = [maxMLCode + 1]byte{\n+\t0, 0, 0, 0, 0, 0, 0, 0,\n+\t0, 0, 0, 0, 0, 0, 0, 0,\n+\t0, 0, 0, 0, 0, 0, 0, 0,\n+\t0, 0, 0, 0, 0, 0, 0, 0,\n+\t1, 1, 1, 1, 2, 2, 3, 3,\n+\t4, 4, 5, 7, 8, 9, 10, 11,\n+\t12, 13, 14, 15, 16}\n+\n+// note : mlBase = matchLength - MINMATCH;\n+// because it's the format it's stored in seqStore->sequences\n+func mlCode(mlBase uint32) uint8 {\n+\tconst mlDeltaCode = 36\n+\tif mlBase <= 127 {\n+\t\t// Compiler insists on bounds check (Go 1.12)\n+\t\treturn mlCodeTable[mlBase&127]\n+\t}\n+\treturn uint8(highBit(mlBase)) + mlDeltaCode\n+}\n+\n+func ofCode(offset uint32) uint8 {\n+\t// A valid offset will always be > 0.\n+\treturn uint8(bits.Len32(offset) - 1)\n+}"
    },
    {
      "sha": "356956ba256dc2fd30773480f595beaf3bec4bf6",
      "filename": "backend/vendor/github.com/klauspost/compress/zstd/snappy.go",
      "status": "added",
      "additions": 436,
      "deletions": 0,
      "changes": 436,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/klauspost/compress/zstd/snappy.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/klauspost/compress/zstd/snappy.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/github.com/klauspost/compress/zstd/snappy.go?ref=890d7154e777d0673defcd11db3ef792473d68c9",
      "patch": "@@ -0,0 +1,436 @@\n+// Copyright 2019+ Klaus Post. All rights reserved.\n+// License information can be found in the LICENSE file.\n+// Based on work by Yann Collet, released under BSD License.\n+\n+package zstd\n+\n+import (\n+\t\"encoding/binary\"\n+\t\"errors\"\n+\t\"hash/crc32\"\n+\t\"io\"\n+\n+\t\"github.com/klauspost/compress/huff0\"\n+\t\"github.com/klauspost/compress/snappy\"\n+)\n+\n+const (\n+\tsnappyTagLiteral = 0x00\n+\tsnappyTagCopy1   = 0x01\n+\tsnappyTagCopy2   = 0x02\n+\tsnappyTagCopy4   = 0x03\n+)\n+\n+const (\n+\tsnappyChecksumSize = 4\n+\tsnappyMagicBody    = \"sNaPpY\"\n+\n+\t// snappyMaxBlockSize is the maximum size of the input to encodeBlock. It is not\n+\t// part of the wire format per se, but some parts of the encoder assume\n+\t// that an offset fits into a uint16.\n+\t//\n+\t// Also, for the framing format (Writer type instead of Encode function),\n+\t// https://github.com/google/snappy/blob/master/framing_format.txt says\n+\t// that \"the uncompressed data in a chunk must be no longer than 65536\n+\t// bytes\".\n+\tsnappyMaxBlockSize = 65536\n+\n+\t// snappyMaxEncodedLenOfMaxBlockSize equals MaxEncodedLen(snappyMaxBlockSize), but is\n+\t// hard coded to be a const instead of a variable, so that obufLen can also\n+\t// be a const. Their equivalence is confirmed by\n+\t// TestMaxEncodedLenOfMaxBlockSize.\n+\tsnappyMaxEncodedLenOfMaxBlockSize = 76490\n+)\n+\n+const (\n+\tchunkTypeCompressedData   = 0x00\n+\tchunkTypeUncompressedData = 0x01\n+\tchunkTypePadding          = 0xfe\n+\tchunkTypeStreamIdentifier = 0xff\n+)\n+\n+var (\n+\t// ErrSnappyCorrupt reports that the input is invalid.\n+\tErrSnappyCorrupt = errors.New(\"snappy: corrupt input\")\n+\t// ErrSnappyTooLarge reports that the uncompressed length is too large.\n+\tErrSnappyTooLarge = errors.New(\"snappy: decoded block is too large\")\n+\t// ErrSnappyUnsupported reports that the input isn't supported.\n+\tErrSnappyUnsupported = errors.New(\"snappy: unsupported input\")\n+\n+\terrUnsupportedLiteralLength = errors.New(\"snappy: unsupported literal length\")\n+)\n+\n+// SnappyConverter can read SnappyConverter-compressed streams and convert them to zstd.\n+// Conversion is done by converting the stream directly from Snappy without intermediate\n+// full decoding.\n+// Therefore the compression ratio is much less than what can be done by a full decompression\n+// and compression, and a faulty Snappy stream may lead to a faulty Zstandard stream without\n+// any errors being generated.\n+// No CRC value is being generated and not all CRC values of the Snappy stream are checked.\n+// However, it provides really fast recompression of Snappy streams.\n+// The converter can be reused to avoid allocations, even after errors.\n+type SnappyConverter struct {\n+\tr     io.Reader\n+\terr   error\n+\tbuf   []byte\n+\tblock *blockEnc\n+}\n+\n+// Convert the Snappy stream supplied in 'in' and write the zStandard stream to 'w'.\n+// If any error is detected on the Snappy stream it is returned.\n+// The number of bytes written is returned.\n+func (r *SnappyConverter) Convert(in io.Reader, w io.Writer) (int64, error) {\n+\tinitPredefined()\n+\tr.err = nil\n+\tr.r = in\n+\tif r.block == nil {\n+\t\tr.block = &blockEnc{}\n+\t\tr.block.init()\n+\t}\n+\tr.block.initNewEncode()\n+\tif len(r.buf) != snappyMaxEncodedLenOfMaxBlockSize+snappyChecksumSize {\n+\t\tr.buf = make([]byte, snappyMaxEncodedLenOfMaxBlockSize+snappyChecksumSize)\n+\t}\n+\tr.block.litEnc.Reuse = huff0.ReusePolicyNone\n+\tvar written int64\n+\tvar readHeader bool\n+\t{\n+\t\tvar header []byte\n+\t\tvar n int\n+\t\theader, r.err = frameHeader{WindowSize: snappyMaxBlockSize}.appendTo(r.buf[:0])\n+\n+\t\tn, r.err = w.Write(header)\n+\t\tif r.err != nil {\n+\t\t\treturn written, r.err\n+\t\t}\n+\t\twritten += int64(n)\n+\t}\n+\n+\tfor {\n+\t\tif !r.readFull(r.buf[:4], true) {\n+\t\t\t// Add empty last block\n+\t\t\tr.block.reset(nil)\n+\t\t\tr.block.last = true\n+\t\t\terr := r.block.encodeLits(false)\n+\t\t\tif err != nil {\n+\t\t\t\treturn written, err\n+\t\t\t}\n+\t\t\tn, err := w.Write(r.block.output)\n+\t\t\tif err != nil {\n+\t\t\t\treturn written, err\n+\t\t\t}\n+\t\t\twritten += int64(n)\n+\n+\t\t\treturn written, r.err\n+\t\t}\n+\t\tchunkType := r.buf[0]\n+\t\tif !readHeader {\n+\t\t\tif chunkType != chunkTypeStreamIdentifier {\n+\t\t\t\tprintln(\"chunkType != chunkTypeStreamIdentifier\", chunkType)\n+\t\t\t\tr.err = ErrSnappyCorrupt\n+\t\t\t\treturn written, r.err\n+\t\t\t}\n+\t\t\treadHeader = true\n+\t\t}\n+\t\tchunkLen := int(r.buf[1]) | int(r.buf[2])<<8 | int(r.buf[3])<<16\n+\t\tif chunkLen > len(r.buf) {\n+\t\t\tprintln(\"chunkLen > len(r.buf)\", chunkType)\n+\t\t\tr.err = ErrSnappyUnsupported\n+\t\t\treturn written, r.err\n+\t\t}\n+\n+\t\t// The chunk types are specified at\n+\t\t// https://github.com/google/snappy/blob/master/framing_format.txt\n+\t\tswitch chunkType {\n+\t\tcase chunkTypeCompressedData:\n+\t\t\t// Section 4.2. Compressed data (chunk type 0x00).\n+\t\t\tif chunkLen < snappyChecksumSize {\n+\t\t\t\tprintln(\"chunkLen < snappyChecksumSize\", chunkLen, snappyChecksumSize)\n+\t\t\t\tr.err = ErrSnappyCorrupt\n+\t\t\t\treturn written, r.err\n+\t\t\t}\n+\t\t\tbuf := r.buf[:chunkLen]\n+\t\t\tif !r.readFull(buf, false) {\n+\t\t\t\treturn written, r.err\n+\t\t\t}\n+\t\t\t//checksum := uint32(buf[0]) | uint32(buf[1])<<8 | uint32(buf[2])<<16 | uint32(buf[3])<<24\n+\t\t\tbuf = buf[snappyChecksumSize:]\n+\n+\t\t\tn, hdr, err := snappyDecodedLen(buf)\n+\t\t\tif err != nil {\n+\t\t\t\tr.err = err\n+\t\t\t\treturn written, r.err\n+\t\t\t}\n+\t\t\tbuf = buf[hdr:]\n+\t\t\tif n > snappyMaxBlockSize {\n+\t\t\t\tprintln(\"n > snappyMaxBlockSize\", n, snappyMaxBlockSize)\n+\t\t\t\tr.err = ErrSnappyCorrupt\n+\t\t\t\treturn written, r.err\n+\t\t\t}\n+\t\t\tr.block.reset(nil)\n+\t\t\tr.block.pushOffsets()\n+\t\t\tif err := decodeSnappy(r.block, buf); err != nil {\n+\t\t\t\tr.err = err\n+\t\t\t\treturn written, r.err\n+\t\t\t}\n+\t\t\tif r.block.size+r.block.extraLits != n {\n+\t\t\t\tprintf(\"invalid size, want %d, got %d\\n\", n, r.block.size+r.block.extraLits)\n+\t\t\t\tr.err = ErrSnappyCorrupt\n+\t\t\t\treturn written, r.err\n+\t\t\t}\n+\t\t\terr = r.block.encode(false)\n+\t\t\tswitch err {\n+\t\t\tcase errIncompressible:\n+\t\t\t\tr.block.popOffsets()\n+\t\t\t\tr.block.reset(nil)\n+\t\t\t\tr.block.literals, err = snappy.Decode(r.block.literals[:n], r.buf[snappyChecksumSize:chunkLen])\n+\t\t\t\tif err != nil {\n+\t\t\t\t\tprintln(\"snappy.Decode:\", err)\n+\t\t\t\t\treturn written, err\n+\t\t\t\t}\n+\t\t\t\terr = r.block.encodeLits(false)\n+\t\t\t\tif err != nil {\n+\t\t\t\t\treturn written, err\n+\t\t\t\t}\n+\t\t\tcase nil:\n+\t\t\tdefault:\n+\t\t\t\treturn written, err\n+\t\t\t}\n+\n+\t\t\tn, r.err = w.Write(r.block.output)\n+\t\t\tif r.err != nil {\n+\t\t\t\treturn written, err\n+\t\t\t}\n+\t\t\twritten += int64(n)\n+\t\t\tcontinue\n+\t\tcase chunkTypeUncompressedData:\n+\t\t\tif debug {\n+\t\t\t\tprintln(\"Uncompressed, chunklen\", chunkLen)\n+\t\t\t}\n+\t\t\t// Section 4.3. Uncompressed data (chunk type 0x01).\n+\t\t\tif chunkLen < snappyChecksumSize {\n+\t\t\t\tprintln(\"chunkLen < snappyChecksumSize\", chunkLen, snappyChecksumSize)\n+\t\t\t\tr.err = ErrSnappyCorrupt\n+\t\t\t\treturn written, r.err\n+\t\t\t}\n+\t\t\tr.block.reset(nil)\n+\t\t\tbuf := r.buf[:snappyChecksumSize]\n+\t\t\tif !r.readFull(buf, false) {\n+\t\t\t\treturn written, r.err\n+\t\t\t}\n+\t\t\tchecksum := uint32(buf[0]) | uint32(buf[1])<<8 | uint32(buf[2])<<16 | uint32(buf[3])<<24\n+\t\t\t// Read directly into r.decoded instead of via r.buf.\n+\t\t\tn := chunkLen - snappyChecksumSize\n+\t\t\tif n > snappyMaxBlockSize {\n+\t\t\t\tprintln(\"n > snappyMaxBlockSize\", n, snappyMaxBlockSize)\n+\t\t\t\tr.err = ErrSnappyCorrupt\n+\t\t\t\treturn written, r.err\n+\t\t\t}\n+\t\t\tr.block.literals = r.block.literals[:n]\n+\t\t\tif !r.readFull(r.block.literals, false) {\n+\t\t\t\treturn written, r.err\n+\t\t\t}\n+\t\t\tif snappyCRC(r.block.literals) != checksum {\n+\t\t\t\tprintln(\"literals crc mismatch\")\n+\t\t\t\tr.err = ErrSnappyCorrupt\n+\t\t\t\treturn written, r.err\n+\t\t\t}\n+\t\t\terr := r.block.encodeLits(false)\n+\t\t\tif err != nil {\n+\t\t\t\treturn written, err\n+\t\t\t}\n+\t\t\tn, r.err = w.Write(r.block.output)\n+\t\t\tif r.err != nil {\n+\t\t\t\treturn written, err\n+\t\t\t}\n+\t\t\twritten += int64(n)\n+\t\t\tcontinue\n+\n+\t\tcase chunkTypeStreamIdentifier:\n+\t\t\tif debug {\n+\t\t\t\tprintln(\"stream id\", chunkLen, len(snappyMagicBody))\n+\t\t\t}\n+\t\t\t// Section 4.1. Stream identifier (chunk type 0xff).\n+\t\t\tif chunkLen != len(snappyMagicBody) {\n+\t\t\t\tprintln(\"chunkLen != len(snappyMagicBody)\", chunkLen, len(snappyMagicBody))\n+\t\t\t\tr.err = ErrSnappyCorrupt\n+\t\t\t\treturn written, r.err\n+\t\t\t}\n+\t\t\tif !r.readFull(r.buf[:len(snappyMagicBody)], false) {\n+\t\t\t\treturn written, r.err\n+\t\t\t}\n+\t\t\tfor i := 0; i < len(snappyMagicBody); i++ {\n+\t\t\t\tif r.buf[i] != snappyMagicBody[i] {\n+\t\t\t\t\tprintln(\"r.buf[i] != snappyMagicBody[i]\", r.buf[i], snappyMagicBody[i], i)\n+\t\t\t\t\tr.err = ErrSnappyCorrupt\n+\t\t\t\t\treturn written, r.err\n+\t\t\t\t}\n+\t\t\t}\n+\t\t\tcontinue\n+\t\t}\n+\n+\t\tif chunkType <= 0x7f {\n+\t\t\t// Section 4.5. Reserved unskippable chunks (chunk types 0x02-0x7f).\n+\t\t\tprintln(\"chunkType <= 0x7f\")\n+\t\t\tr.err = ErrSnappyUnsupported\n+\t\t\treturn written, r.err\n+\t\t}\n+\t\t// Section 4.4 Padding (chunk type 0xfe).\n+\t\t// Section 4.6. Reserved skippable chunks (chunk types 0x80-0xfd).\n+\t\tif !r.readFull(r.buf[:chunkLen], false) {\n+\t\t\treturn written, r.err\n+\t\t}\n+\t}\n+}\n+\n+// decodeSnappy writes the decoding of src to dst. It assumes that the varint-encoded\n+// length of the decompressed bytes has already been read.\n+func decodeSnappy(blk *blockEnc, src []byte) error {\n+\t//decodeRef(make([]byte, snappyMaxBlockSize), src)\n+\tvar s, length int\n+\tlits := blk.extraLits\n+\tvar offset uint32\n+\tfor s < len(src) {\n+\t\tswitch src[s] & 0x03 {\n+\t\tcase snappyTagLiteral:\n+\t\t\tx := uint32(src[s] >> 2)\n+\t\t\tswitch {\n+\t\t\tcase x < 60:\n+\t\t\t\ts++\n+\t\t\tcase x == 60:\n+\t\t\t\ts += 2\n+\t\t\t\tif uint(s) > uint(len(src)) { // The uint conversions catch overflow from the previous line.\n+\t\t\t\t\tprintln(\"uint(s) > uint(len(src)\", s, src)\n+\t\t\t\t\treturn ErrSnappyCorrupt\n+\t\t\t\t}\n+\t\t\t\tx = uint32(src[s-1])\n+\t\t\tcase x == 61:\n+\t\t\t\ts += 3\n+\t\t\t\tif uint(s) > uint(len(src)) { // The uint conversions catch overflow from the previous line.\n+\t\t\t\t\tprintln(\"uint(s) > uint(len(src)\", s, src)\n+\t\t\t\t\treturn ErrSnappyCorrupt\n+\t\t\t\t}\n+\t\t\t\tx = uint32(src[s-2]) | uint32(src[s-1])<<8\n+\t\t\tcase x == 62:\n+\t\t\t\ts += 4\n+\t\t\t\tif uint(s) > uint(len(src)) { // The uint conversions catch overflow from the previous line.\n+\t\t\t\t\tprintln(\"uint(s) > uint(len(src)\", s, src)\n+\t\t\t\t\treturn ErrSnappyCorrupt\n+\t\t\t\t}\n+\t\t\t\tx = uint32(src[s-3]) | uint32(src[s-2])<<8 | uint32(src[s-1])<<16\n+\t\t\tcase x == 63:\n+\t\t\t\ts += 5\n+\t\t\t\tif uint(s) > uint(len(src)) { // The uint conversions catch overflow from the previous line.\n+\t\t\t\t\tprintln(\"uint(s) > uint(len(src)\", s, src)\n+\t\t\t\t\treturn ErrSnappyCorrupt\n+\t\t\t\t}\n+\t\t\t\tx = uint32(src[s-4]) | uint32(src[s-3])<<8 | uint32(src[s-2])<<16 | uint32(src[s-1])<<24\n+\t\t\t}\n+\t\t\tif x > snappyMaxBlockSize {\n+\t\t\t\tprintln(\"x > snappyMaxBlockSize\", x, snappyMaxBlockSize)\n+\t\t\t\treturn ErrSnappyCorrupt\n+\t\t\t}\n+\t\t\tlength = int(x) + 1\n+\t\t\tif length <= 0 {\n+\t\t\t\tprintln(\"length <= 0 \", length)\n+\n+\t\t\t\treturn errUnsupportedLiteralLength\n+\t\t\t}\n+\t\t\t//if length > snappyMaxBlockSize-d || uint32(length) > len(src)-s {\n+\t\t\t//\treturn ErrSnappyCorrupt\n+\t\t\t//}\n+\n+\t\t\tblk.literals = append(blk.literals, src[s:s+length]...)\n+\t\t\t//println(length, \"litLen\")\n+\t\t\tlits += length\n+\t\t\ts += length\n+\t\t\tcontinue\n+\n+\t\tcase snappyTagCopy1:\n+\t\t\ts += 2\n+\t\t\tif uint(s) > uint(len(src)) { // The uint conversions catch overflow from the previous line.\n+\t\t\t\tprintln(\"uint(s) > uint(len(src)\", s, len(src))\n+\t\t\t\treturn ErrSnappyCorrupt\n+\t\t\t}\n+\t\t\tlength = 4 + int(src[s-2])>>2&0x7\n+\t\t\toffset = uint32(src[s-2])&0xe0<<3 | uint32(src[s-1])\n+\n+\t\tcase snappyTagCopy2:\n+\t\t\ts += 3\n+\t\t\tif uint(s) > uint(len(src)) { // The uint conversions catch overflow from the previous line.\n+\t\t\t\tprintln(\"uint(s) > uint(len(src)\", s, len(src))\n+\t\t\t\treturn ErrSnappyCorrupt\n+\t\t\t}\n+\t\t\tlength = 1 + int(src[s-3])>>2\n+\t\t\toffset = uint32(src[s-2]) | uint32(src[s-1])<<8\n+\n+\t\tcase snappyTagCopy4:\n+\t\t\ts += 5\n+\t\t\tif uint(s) > uint(len(src)) { // The uint conversions catch overflow from the previous line.\n+\t\t\t\tprintln(\"uint(s) > uint(len(src)\", s, len(src))\n+\t\t\t\treturn ErrSnappyCorrupt\n+\t\t\t}\n+\t\t\tlength = 1 + int(src[s-5])>>2\n+\t\t\toffset = uint32(src[s-4]) | uint32(src[s-3])<<8 | uint32(src[s-2])<<16 | uint32(src[s-1])<<24\n+\t\t}\n+\n+\t\tif offset <= 0 || blk.size+lits < int(offset) /*|| length > len(blk)-d */ {\n+\t\t\tprintln(\"offset <= 0 || blk.size+lits < int(offset)\", offset, blk.size+lits, int(offset), blk.size, lits)\n+\n+\t\t\treturn ErrSnappyCorrupt\n+\t\t}\n+\n+\t\t// Check if offset is one of the recent offsets.\n+\t\t// Adjusts the output offset accordingly.\n+\t\t// Gives a tiny bit of compression, typically around 1%.\n+\t\tif false {\n+\t\t\toffset = blk.matchOffset(offset, uint32(lits))\n+\t\t} else {\n+\t\t\toffset += 3\n+\t\t}\n+\n+\t\tblk.sequences = append(blk.sequences, seq{\n+\t\t\tlitLen:   uint32(lits),\n+\t\t\toffset:   offset,\n+\t\t\tmatchLen: uint32(length) - zstdMinMatch,\n+\t\t})\n+\t\tblk.size += length + lits\n+\t\tlits = 0\n+\t}\n+\tblk.extraLits = lits\n+\treturn nil\n+}\n+\n+func (r *SnappyConverter) readFull(p []byte, allowEOF bool) (ok bool) {\n+\tif _, r.err = io.ReadFull(r.r, p); r.err != nil {\n+\t\tif r.err == io.ErrUnexpectedEOF || (r.err == io.EOF && !allowEOF) {\n+\t\t\tr.err = ErrSnappyCorrupt\n+\t\t}\n+\t\treturn false\n+\t}\n+\treturn true\n+}\n+\n+var crcTable = crc32.MakeTable(crc32.Castagnoli)\n+\n+// crc implements the checksum specified in section 3 of\n+// https://github.com/google/snappy/blob/master/framing_format.txt\n+func snappyCRC(b []byte) uint32 {\n+\tc := crc32.Update(0, crcTable, b)\n+\treturn uint32(c>>15|c<<17) + 0xa282ead8\n+}\n+\n+// snappyDecodedLen returns the length of the decoded block and the number of bytes\n+// that the length header occupied.\n+func snappyDecodedLen(src []byte) (blockLen, headerLen int, err error) {\n+\tv, n := binary.Uvarint(src)\n+\tif n <= 0 || v > 0xffffffff {\n+\t\treturn 0, 0, ErrSnappyCorrupt\n+\t}\n+\n+\tconst wordSize = 32 << (^uint(0) >> 32 & 1)\n+\tif wordSize == 32 && v > 0x7fffffff {\n+\t\treturn 0, 0, ErrSnappyTooLarge\n+\t}\n+\treturn int(v), n, nil\n+}"
    },
    {
      "sha": "57a8a2f5bb82e7e2ef8052fb9c43d6988ea50d65",
      "filename": "backend/vendor/github.com/klauspost/compress/zstd/zstd.go",
      "status": "added",
      "additions": 136,
      "deletions": 0,
      "changes": 136,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/klauspost/compress/zstd/zstd.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/klauspost/compress/zstd/zstd.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/github.com/klauspost/compress/zstd/zstd.go?ref=890d7154e777d0673defcd11db3ef792473d68c9",
      "patch": "@@ -0,0 +1,136 @@\n+// Package zstd provides decompression of zstandard files.\n+//\n+// For advanced usage and examples, go to the README: https://github.com/klauspost/compress/tree/master/zstd#zstd\n+package zstd\n+\n+import (\n+\t\"errors\"\n+\t\"log\"\n+\t\"math/bits\"\n+)\n+\n+const debug = false\n+const debugSequences = false\n+const debugMatches = false\n+\n+// force encoder to use predefined tables.\n+const forcePreDef = false\n+\n+// zstdMinMatch is the minimum zstd match length.\n+const zstdMinMatch = 3\n+\n+var (\n+\t// ErrReservedBlockType is returned when a reserved block type is found.\n+\t// Typically this indicates wrong or corrupted input.\n+\tErrReservedBlockType = errors.New(\"invalid input: reserved block type encountered\")\n+\n+\t// ErrCompressedSizeTooBig is returned when a block is bigger than allowed.\n+\t// Typically this indicates wrong or corrupted input.\n+\tErrCompressedSizeTooBig = errors.New(\"invalid input: compressed size too big\")\n+\n+\t// ErrBlockTooSmall is returned when a block is too small to be decoded.\n+\t// Typically returned on invalid input.\n+\tErrBlockTooSmall = errors.New(\"block too small\")\n+\n+\t// ErrMagicMismatch is returned when a \"magic\" number isn't what is expected.\n+\t// Typically this indicates wrong or corrupted input.\n+\tErrMagicMismatch = errors.New(\"invalid input: magic number mismatch\")\n+\n+\t// ErrWindowSizeExceeded is returned when a reference exceeds the valid window size.\n+\t// Typically this indicates wrong or corrupted input.\n+\tErrWindowSizeExceeded = errors.New(\"window size exceeded\")\n+\n+\t// ErrWindowSizeTooSmall is returned when no window size is specified.\n+\t// Typically this indicates wrong or corrupted input.\n+\tErrWindowSizeTooSmall = errors.New(\"invalid input: window size was too small\")\n+\n+\t// ErrDecoderSizeExceeded is returned if decompressed size exceeds the configured limit.\n+\tErrDecoderSizeExceeded = errors.New(\"decompressed size exceeds configured limit\")\n+\n+\t// ErrUnknownDictionary is returned if the dictionary ID is unknown.\n+\t// For the time being dictionaries are not supported.\n+\tErrUnknownDictionary = errors.New(\"unknown dictionary\")\n+\n+\t// ErrFrameSizeExceeded is returned if the stated frame size is exceeded.\n+\t// This is only returned if SingleSegment is specified on the frame.\n+\tErrFrameSizeExceeded = errors.New(\"frame size exceeded\")\n+\n+\t// ErrCRCMismatch is returned if CRC mismatches.\n+\tErrCRCMismatch = errors.New(\"CRC check failed\")\n+\n+\t// ErrDecoderClosed will be returned if the Decoder was used after\n+\t// Close has been called.\n+\tErrDecoderClosed = errors.New(\"decoder used after Close\")\n+)\n+\n+func println(a ...interface{}) {\n+\tif debug {\n+\t\tlog.Println(a...)\n+\t}\n+}\n+\n+func printf(format string, a ...interface{}) {\n+\tif debug {\n+\t\tlog.Printf(format, a...)\n+\t}\n+}\n+\n+// matchLen returns the maximum length.\n+// a must be the shortest of the two.\n+// The function also returns whether all bytes matched.\n+func matchLen(a, b []byte) int {\n+\tb = b[:len(a)]\n+\tfor i := 0; i < len(a)-7; i += 8 {\n+\t\tif diff := load64(a, i) ^ load64(b, i); diff != 0 {\n+\t\t\treturn i + (bits.TrailingZeros64(diff) >> 3)\n+\t\t}\n+\t}\n+\tchecked := (len(a) >> 3) << 3\n+\ta = a[checked:]\n+\tb = b[checked:]\n+\t// TODO: We could do a 4 check.\n+\tfor i := range a {\n+\t\tif a[i] != b[i] {\n+\t\t\treturn int(i) + checked\n+\t\t}\n+\t}\n+\treturn len(a) + checked\n+}\n+\n+// matchLen returns a match length in src between index s and t\n+func matchLenIn(src []byte, s, t int32) int32 {\n+\ts1 := len(src)\n+\tb := src[t:]\n+\ta := src[s:s1]\n+\tb = b[:len(a)]\n+\t// Extend the match to be as long as possible.\n+\tfor i := range a {\n+\t\tif a[i] != b[i] {\n+\t\t\treturn int32(i)\n+\t\t}\n+\t}\n+\treturn int32(len(a))\n+}\n+\n+func load3232(b []byte, i int32) uint32 {\n+\t// Help the compiler eliminate bounds checks on the read so it can be done in a single read.\n+\tb = b[i:]\n+\tb = b[:4]\n+\treturn uint32(b[0]) | uint32(b[1])<<8 | uint32(b[2])<<16 | uint32(b[3])<<24\n+}\n+\n+func load6432(b []byte, i int32) uint64 {\n+\t// Help the compiler eliminate bounds checks on the read so it can be done in a single read.\n+\tb = b[i:]\n+\tb = b[:8]\n+\treturn uint64(b[0]) | uint64(b[1])<<8 | uint64(b[2])<<16 | uint64(b[3])<<24 |\n+\t\tuint64(b[4])<<32 | uint64(b[5])<<40 | uint64(b[6])<<48 | uint64(b[7])<<56\n+}\n+\n+func load64(b []byte, i int) uint64 {\n+\t// Help the compiler eliminate bounds checks on the read so it can be done in a single read.\n+\tb = b[i:]\n+\tb = b[:8]\n+\treturn uint64(b[0]) | uint64(b[1])<<8 | uint64(b[2])<<16 | uint64(b[3])<<24 |\n+\t\tuint64(b[4])<<32 | uint64(b[5])<<40 | uint64(b[6])<<48 | uint64(b[7])<<56\n+}"
    },
    {
      "sha": "2b16e997415f3e2fb451cb140b4e30cf073e7c00",
      "filename": "backend/vendor/github.com/patrickmn/go-cache/CONTRIBUTORS",
      "status": "removed",
      "additions": 0,
      "deletions": 9,
      "changes": 9,
      "blob_url": "https://github.com/umputun/remark42/blob/0d67f7e53d4c07b9d299d2dd2d8d33b75fc83a23/backend/vendor/github.com/patrickmn/go-cache/CONTRIBUTORS",
      "raw_url": "https://github.com/umputun/remark42/raw/0d67f7e53d4c07b9d299d2dd2d8d33b75fc83a23/backend/vendor/github.com/patrickmn/go-cache/CONTRIBUTORS",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/github.com/patrickmn/go-cache/CONTRIBUTORS?ref=0d67f7e53d4c07b9d299d2dd2d8d33b75fc83a23",
      "patch": "@@ -1,9 +0,0 @@\n-This is a list of people who have contributed code to go-cache. They, or their\n-employers, are the copyright holders of the contributed code. Contributed code\n-is subject to the license restrictions listed in LICENSE (as they were when the\n-code was contributed.)\n-\n-Dustin Sallings <dustin@spy.net>\n-Jason Mooberry <jasonmoo@me.com>\n-Sergey Shepelev <temotor@gmail.com>\n-Alex Edwards <ajmedwards@gmail.com>"
    },
    {
      "sha": "c5789cc66cc8a7a4beaafb54c81771bf1e014e1e",
      "filename": "backend/vendor/github.com/patrickmn/go-cache/README.md",
      "status": "removed",
      "additions": 0,
      "deletions": 83,
      "changes": 83,
      "blob_url": "https://github.com/umputun/remark42/blob/0d67f7e53d4c07b9d299d2dd2d8d33b75fc83a23/backend/vendor/github.com/patrickmn/go-cache/README.md",
      "raw_url": "https://github.com/umputun/remark42/raw/0d67f7e53d4c07b9d299d2dd2d8d33b75fc83a23/backend/vendor/github.com/patrickmn/go-cache/README.md",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/github.com/patrickmn/go-cache/README.md?ref=0d67f7e53d4c07b9d299d2dd2d8d33b75fc83a23",
      "patch": "@@ -1,83 +0,0 @@\n-# go-cache\n-\n-go-cache is an in-memory key:value store/cache similar to memcached that is\n-suitable for applications running on a single machine. Its major advantage is\n-that, being essentially a thread-safe `map[string]interface{}` with expiration\n-times, it doesn't need to serialize or transmit its contents over the network.\n-\n-Any object can be stored, for a given duration or forever, and the cache can be\n-safely used by multiple goroutines.\n-\n-Although go-cache isn't meant to be used as a persistent datastore, the entire\n-cache can be saved to and loaded from a file (using `c.Items()` to retrieve the\n-items map to serialize, and `NewFrom()` to create a cache from a deserialized\n-one) to recover from downtime quickly. (See the docs for `NewFrom()` for caveats.)\n-\n-### Installation\n-\n-`go get github.com/patrickmn/go-cache`\n-\n-### Usage\n-\n-```go\n-import (\n-\t\"fmt\"\n-\t\"github.com/patrickmn/go-cache\"\n-\t\"time\"\n-)\n-\n-func main() {\n-\t// Create a cache with a default expiration time of 5 minutes, and which\n-\t// purges expired items every 10 minutes\n-\tc := cache.New(5*time.Minute, 10*time.Minute)\n-\n-\t// Set the value of the key \"foo\" to \"bar\", with the default expiration time\n-\tc.Set(\"foo\", \"bar\", cache.DefaultExpiration)\n-\n-\t// Set the value of the key \"baz\" to 42, with no expiration time\n-\t// (the item won't be removed until it is re-set, or removed using\n-\t// c.Delete(\"baz\")\n-\tc.Set(\"baz\", 42, cache.NoExpiration)\n-\n-\t// Get the string associated with the key \"foo\" from the cache\n-\tfoo, found := c.Get(\"foo\")\n-\tif found {\n-\t\tfmt.Println(foo)\n-\t}\n-\n-\t// Since Go is statically typed, and cache values can be anything, type\n-\t// assertion is needed when values are being passed to functions that don't\n-\t// take arbitrary types, (i.e. interface{}). The simplest way to do this for\n-\t// values which will only be used once--e.g. for passing to another\n-\t// function--is:\n-\tfoo, found := c.Get(\"foo\")\n-\tif found {\n-\t\tMyFunction(foo.(string))\n-\t}\n-\n-\t// This gets tedious if the value is used several times in the same function.\n-\t// You might do either of the following instead:\n-\tif x, found := c.Get(\"foo\"); found {\n-\t\tfoo := x.(string)\n-\t\t// ...\n-\t}\n-\t// or\n-\tvar foo string\n-\tif x, found := c.Get(\"foo\"); found {\n-\t\tfoo = x.(string)\n-\t}\n-\t// ...\n-\t// foo can then be passed around freely as a string\n-\n-\t// Want performance? Store pointers!\n-\tc.Set(\"foo\", &MyStruct, cache.DefaultExpiration)\n-\tif x, found := c.Get(\"foo\"); found {\n-\t\tfoo := x.(*MyStruct)\n-\t\t\t// ...\n-\t}\n-}\n-```\n-\n-### Reference\n-\n-`godoc` or [http://godoc.org/github.com/patrickmn/go-cache](http://godoc.org/github.com/patrickmn/go-cache)"
    },
    {
      "sha": "db88d2f2cb1924cc263690409b4d8298d2fa7ed6",
      "filename": "backend/vendor/github.com/patrickmn/go-cache/cache.go",
      "status": "removed",
      "additions": 0,
      "deletions": 1161,
      "changes": 1161,
      "blob_url": "https://github.com/umputun/remark42/blob/0d67f7e53d4c07b9d299d2dd2d8d33b75fc83a23/backend/vendor/github.com/patrickmn/go-cache/cache.go",
      "raw_url": "https://github.com/umputun/remark42/raw/0d67f7e53d4c07b9d299d2dd2d8d33b75fc83a23/backend/vendor/github.com/patrickmn/go-cache/cache.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/github.com/patrickmn/go-cache/cache.go?ref=0d67f7e53d4c07b9d299d2dd2d8d33b75fc83a23",
      "patch": "@@ -1,1161 +0,0 @@\n-package cache\n-\n-import (\n-\t\"encoding/gob\"\n-\t\"fmt\"\n-\t\"io\"\n-\t\"os\"\n-\t\"runtime\"\n-\t\"sync\"\n-\t\"time\"\n-)\n-\n-type Item struct {\n-\tObject     interface{}\n-\tExpiration int64\n-}\n-\n-// Returns true if the item has expired.\n-func (item Item) Expired() bool {\n-\tif item.Expiration == 0 {\n-\t\treturn false\n-\t}\n-\treturn time.Now().UnixNano() > item.Expiration\n-}\n-\n-const (\n-\t// For use with functions that take an expiration time.\n-\tNoExpiration time.Duration = -1\n-\t// For use with functions that take an expiration time. Equivalent to\n-\t// passing in the same expiration duration as was given to New() or\n-\t// NewFrom() when the cache was created (e.g. 5 minutes.)\n-\tDefaultExpiration time.Duration = 0\n-)\n-\n-type Cache struct {\n-\t*cache\n-\t// If this is confusing, see the comment at the bottom of New()\n-}\n-\n-type cache struct {\n-\tdefaultExpiration time.Duration\n-\titems             map[string]Item\n-\tmu                sync.RWMutex\n-\tonEvicted         func(string, interface{})\n-\tjanitor           *janitor\n-}\n-\n-// Add an item to the cache, replacing any existing item. If the duration is 0\n-// (DefaultExpiration), the cache's default expiration time is used. If it is -1\n-// (NoExpiration), the item never expires.\n-func (c *cache) Set(k string, x interface{}, d time.Duration) {\n-\t// \"Inlining\" of set\n-\tvar e int64\n-\tif d == DefaultExpiration {\n-\t\td = c.defaultExpiration\n-\t}\n-\tif d > 0 {\n-\t\te = time.Now().Add(d).UnixNano()\n-\t}\n-\tc.mu.Lock()\n-\tc.items[k] = Item{\n-\t\tObject:     x,\n-\t\tExpiration: e,\n-\t}\n-\t// TODO: Calls to mu.Unlock are currently not deferred because defer\n-\t// adds ~200 ns (as of go1.)\n-\tc.mu.Unlock()\n-}\n-\n-func (c *cache) set(k string, x interface{}, d time.Duration) {\n-\tvar e int64\n-\tif d == DefaultExpiration {\n-\t\td = c.defaultExpiration\n-\t}\n-\tif d > 0 {\n-\t\te = time.Now().Add(d).UnixNano()\n-\t}\n-\tc.items[k] = Item{\n-\t\tObject:     x,\n-\t\tExpiration: e,\n-\t}\n-}\n-\n-// Add an item to the cache, replacing any existing item, using the default\n-// expiration.\n-func (c *cache) SetDefault(k string, x interface{}) {\n-\tc.Set(k, x, DefaultExpiration)\n-}\n-\n-// Add an item to the cache only if an item doesn't already exist for the given\n-// key, or if the existing item has expired. Returns an error otherwise.\n-func (c *cache) Add(k string, x interface{}, d time.Duration) error {\n-\tc.mu.Lock()\n-\t_, found := c.get(k)\n-\tif found {\n-\t\tc.mu.Unlock()\n-\t\treturn fmt.Errorf(\"Item %s already exists\", k)\n-\t}\n-\tc.set(k, x, d)\n-\tc.mu.Unlock()\n-\treturn nil\n-}\n-\n-// Set a new value for the cache key only if it already exists, and the existing\n-// item hasn't expired. Returns an error otherwise.\n-func (c *cache) Replace(k string, x interface{}, d time.Duration) error {\n-\tc.mu.Lock()\n-\t_, found := c.get(k)\n-\tif !found {\n-\t\tc.mu.Unlock()\n-\t\treturn fmt.Errorf(\"Item %s doesn't exist\", k)\n-\t}\n-\tc.set(k, x, d)\n-\tc.mu.Unlock()\n-\treturn nil\n-}\n-\n-// Get an item from the cache. Returns the item or nil, and a bool indicating\n-// whether the key was found.\n-func (c *cache) Get(k string) (interface{}, bool) {\n-\tc.mu.RLock()\n-\t// \"Inlining\" of get and Expired\n-\titem, found := c.items[k]\n-\tif !found {\n-\t\tc.mu.RUnlock()\n-\t\treturn nil, false\n-\t}\n-\tif item.Expiration > 0 {\n-\t\tif time.Now().UnixNano() > item.Expiration {\n-\t\t\tc.mu.RUnlock()\n-\t\t\treturn nil, false\n-\t\t}\n-\t}\n-\tc.mu.RUnlock()\n-\treturn item.Object, true\n-}\n-\n-// GetWithExpiration returns an item and its expiration time from the cache.\n-// It returns the item or nil, the expiration time if one is set (if the item\n-// never expires a zero value for time.Time is returned), and a bool indicating\n-// whether the key was found.\n-func (c *cache) GetWithExpiration(k string) (interface{}, time.Time, bool) {\n-\tc.mu.RLock()\n-\t// \"Inlining\" of get and Expired\n-\titem, found := c.items[k]\n-\tif !found {\n-\t\tc.mu.RUnlock()\n-\t\treturn nil, time.Time{}, false\n-\t}\n-\n-\tif item.Expiration > 0 {\n-\t\tif time.Now().UnixNano() > item.Expiration {\n-\t\t\tc.mu.RUnlock()\n-\t\t\treturn nil, time.Time{}, false\n-\t\t}\n-\n-\t\t// Return the item and the expiration time\n-\t\tc.mu.RUnlock()\n-\t\treturn item.Object, time.Unix(0, item.Expiration), true\n-\t}\n-\n-\t// If expiration <= 0 (i.e. no expiration time set) then return the item\n-\t// and a zeroed time.Time\n-\tc.mu.RUnlock()\n-\treturn item.Object, time.Time{}, true\n-}\n-\n-func (c *cache) get(k string) (interface{}, bool) {\n-\titem, found := c.items[k]\n-\tif !found {\n-\t\treturn nil, false\n-\t}\n-\t// \"Inlining\" of Expired\n-\tif item.Expiration > 0 {\n-\t\tif time.Now().UnixNano() > item.Expiration {\n-\t\t\treturn nil, false\n-\t\t}\n-\t}\n-\treturn item.Object, true\n-}\n-\n-// Increment an item of type int, int8, int16, int32, int64, uintptr, uint,\n-// uint8, uint32, or uint64, float32 or float64 by n. Returns an error if the\n-// item's value is not an integer, if it was not found, or if it is not\n-// possible to increment it by n. To retrieve the incremented value, use one\n-// of the specialized methods, e.g. IncrementInt64.\n-func (c *cache) Increment(k string, n int64) error {\n-\tc.mu.Lock()\n-\tv, found := c.items[k]\n-\tif !found || v.Expired() {\n-\t\tc.mu.Unlock()\n-\t\treturn fmt.Errorf(\"Item %s not found\", k)\n-\t}\n-\tswitch v.Object.(type) {\n-\tcase int:\n-\t\tv.Object = v.Object.(int) + int(n)\n-\tcase int8:\n-\t\tv.Object = v.Object.(int8) + int8(n)\n-\tcase int16:\n-\t\tv.Object = v.Object.(int16) + int16(n)\n-\tcase int32:\n-\t\tv.Object = v.Object.(int32) + int32(n)\n-\tcase int64:\n-\t\tv.Object = v.Object.(int64) + n\n-\tcase uint:\n-\t\tv.Object = v.Object.(uint) + uint(n)\n-\tcase uintptr:\n-\t\tv.Object = v.Object.(uintptr) + uintptr(n)\n-\tcase uint8:\n-\t\tv.Object = v.Object.(uint8) + uint8(n)\n-\tcase uint16:\n-\t\tv.Object = v.Object.(uint16) + uint16(n)\n-\tcase uint32:\n-\t\tv.Object = v.Object.(uint32) + uint32(n)\n-\tcase uint64:\n-\t\tv.Object = v.Object.(uint64) + uint64(n)\n-\tcase float32:\n-\t\tv.Object = v.Object.(float32) + float32(n)\n-\tcase float64:\n-\t\tv.Object = v.Object.(float64) + float64(n)\n-\tdefault:\n-\t\tc.mu.Unlock()\n-\t\treturn fmt.Errorf(\"The value for %s is not an integer\", k)\n-\t}\n-\tc.items[k] = v\n-\tc.mu.Unlock()\n-\treturn nil\n-}\n-\n-// Increment an item of type float32 or float64 by n. Returns an error if the\n-// item's value is not floating point, if it was not found, or if it is not\n-// possible to increment it by n. Pass a negative number to decrement the\n-// value. To retrieve the incremented value, use one of the specialized methods,\n-// e.g. IncrementFloat64.\n-func (c *cache) IncrementFloat(k string, n float64) error {\n-\tc.mu.Lock()\n-\tv, found := c.items[k]\n-\tif !found || v.Expired() {\n-\t\tc.mu.Unlock()\n-\t\treturn fmt.Errorf(\"Item %s not found\", k)\n-\t}\n-\tswitch v.Object.(type) {\n-\tcase float32:\n-\t\tv.Object = v.Object.(float32) + float32(n)\n-\tcase float64:\n-\t\tv.Object = v.Object.(float64) + n\n-\tdefault:\n-\t\tc.mu.Unlock()\n-\t\treturn fmt.Errorf(\"The value for %s does not have type float32 or float64\", k)\n-\t}\n-\tc.items[k] = v\n-\tc.mu.Unlock()\n-\treturn nil\n-}\n-\n-// Increment an item of type int by n. Returns an error if the item's value is\n-// not an int, or if it was not found. If there is no error, the incremented\n-// value is returned.\n-func (c *cache) IncrementInt(k string, n int) (int, error) {\n-\tc.mu.Lock()\n-\tv, found := c.items[k]\n-\tif !found || v.Expired() {\n-\t\tc.mu.Unlock()\n-\t\treturn 0, fmt.Errorf(\"Item %s not found\", k)\n-\t}\n-\trv, ok := v.Object.(int)\n-\tif !ok {\n-\t\tc.mu.Unlock()\n-\t\treturn 0, fmt.Errorf(\"The value for %s is not an int\", k)\n-\t}\n-\tnv := rv + n\n-\tv.Object = nv\n-\tc.items[k] = v\n-\tc.mu.Unlock()\n-\treturn nv, nil\n-}\n-\n-// Increment an item of type int8 by n. Returns an error if the item's value is\n-// not an int8, or if it was not found. If there is no error, the incremented\n-// value is returned.\n-func (c *cache) IncrementInt8(k string, n int8) (int8, error) {\n-\tc.mu.Lock()\n-\tv, found := c.items[k]\n-\tif !found || v.Expired() {\n-\t\tc.mu.Unlock()\n-\t\treturn 0, fmt.Errorf(\"Item %s not found\", k)\n-\t}\n-\trv, ok := v.Object.(int8)\n-\tif !ok {\n-\t\tc.mu.Unlock()\n-\t\treturn 0, fmt.Errorf(\"The value for %s is not an int8\", k)\n-\t}\n-\tnv := rv + n\n-\tv.Object = nv\n-\tc.items[k] = v\n-\tc.mu.Unlock()\n-\treturn nv, nil\n-}\n-\n-// Increment an item of type int16 by n. Returns an error if the item's value is\n-// not an int16, or if it was not found. If there is no error, the incremented\n-// value is returned.\n-func (c *cache) IncrementInt16(k string, n int16) (int16, error) {\n-\tc.mu.Lock()\n-\tv, found := c.items[k]\n-\tif !found || v.Expired() {\n-\t\tc.mu.Unlock()\n-\t\treturn 0, fmt.Errorf(\"Item %s not found\", k)\n-\t}\n-\trv, ok := v.Object.(int16)\n-\tif !ok {\n-\t\tc.mu.Unlock()\n-\t\treturn 0, fmt.Errorf(\"The value for %s is not an int16\", k)\n-\t}\n-\tnv := rv + n\n-\tv.Object = nv\n-\tc.items[k] = v\n-\tc.mu.Unlock()\n-\treturn nv, nil\n-}\n-\n-// Increment an item of type int32 by n. Returns an error if the item's value is\n-// not an int32, or if it was not found. If there is no error, the incremented\n-// value is returned.\n-func (c *cache) IncrementInt32(k string, n int32) (int32, error) {\n-\tc.mu.Lock()\n-\tv, found := c.items[k]\n-\tif !found || v.Expired() {\n-\t\tc.mu.Unlock()\n-\t\treturn 0, fmt.Errorf(\"Item %s not found\", k)\n-\t}\n-\trv, ok := v.Object.(int32)\n-\tif !ok {\n-\t\tc.mu.Unlock()\n-\t\treturn 0, fmt.Errorf(\"The value for %s is not an int32\", k)\n-\t}\n-\tnv := rv + n\n-\tv.Object = nv\n-\tc.items[k] = v\n-\tc.mu.Unlock()\n-\treturn nv, nil\n-}\n-\n-// Increment an item of type int64 by n. Returns an error if the item's value is\n-// not an int64, or if it was not found. If there is no error, the incremented\n-// value is returned.\n-func (c *cache) IncrementInt64(k string, n int64) (int64, error) {\n-\tc.mu.Lock()\n-\tv, found := c.items[k]\n-\tif !found || v.Expired() {\n-\t\tc.mu.Unlock()\n-\t\treturn 0, fmt.Errorf(\"Item %s not found\", k)\n-\t}\n-\trv, ok := v.Object.(int64)\n-\tif !ok {\n-\t\tc.mu.Unlock()\n-\t\treturn 0, fmt.Errorf(\"The value for %s is not an int64\", k)\n-\t}\n-\tnv := rv + n\n-\tv.Object = nv\n-\tc.items[k] = v\n-\tc.mu.Unlock()\n-\treturn nv, nil\n-}\n-\n-// Increment an item of type uint by n. Returns an error if the item's value is\n-// not an uint, or if it was not found. If there is no error, the incremented\n-// value is returned.\n-func (c *cache) IncrementUint(k string, n uint) (uint, error) {\n-\tc.mu.Lock()\n-\tv, found := c.items[k]\n-\tif !found || v.Expired() {\n-\t\tc.mu.Unlock()\n-\t\treturn 0, fmt.Errorf(\"Item %s not found\", k)\n-\t}\n-\trv, ok := v.Object.(uint)\n-\tif !ok {\n-\t\tc.mu.Unlock()\n-\t\treturn 0, fmt.Errorf(\"The value for %s is not an uint\", k)\n-\t}\n-\tnv := rv + n\n-\tv.Object = nv\n-\tc.items[k] = v\n-\tc.mu.Unlock()\n-\treturn nv, nil\n-}\n-\n-// Increment an item of type uintptr by n. Returns an error if the item's value\n-// is not an uintptr, or if it was not found. If there is no error, the\n-// incremented value is returned.\n-func (c *cache) IncrementUintptr(k string, n uintptr) (uintptr, error) {\n-\tc.mu.Lock()\n-\tv, found := c.items[k]\n-\tif !found || v.Expired() {\n-\t\tc.mu.Unlock()\n-\t\treturn 0, fmt.Errorf(\"Item %s not found\", k)\n-\t}\n-\trv, ok := v.Object.(uintptr)\n-\tif !ok {\n-\t\tc.mu.Unlock()\n-\t\treturn 0, fmt.Errorf(\"The value for %s is not an uintptr\", k)\n-\t}\n-\tnv := rv + n\n-\tv.Object = nv\n-\tc.items[k] = v\n-\tc.mu.Unlock()\n-\treturn nv, nil\n-}\n-\n-// Increment an item of type uint8 by n. Returns an error if the item's value\n-// is not an uint8, or if it was not found. If there is no error, the\n-// incremented value is returned.\n-func (c *cache) IncrementUint8(k string, n uint8) (uint8, error) {\n-\tc.mu.Lock()\n-\tv, found := c.items[k]\n-\tif !found || v.Expired() {\n-\t\tc.mu.Unlock()\n-\t\treturn 0, fmt.Errorf(\"Item %s not found\", k)\n-\t}\n-\trv, ok := v.Object.(uint8)\n-\tif !ok {\n-\t\tc.mu.Unlock()\n-\t\treturn 0, fmt.Errorf(\"The value for %s is not an uint8\", k)\n-\t}\n-\tnv := rv + n\n-\tv.Object = nv\n-\tc.items[k] = v\n-\tc.mu.Unlock()\n-\treturn nv, nil\n-}\n-\n-// Increment an item of type uint16 by n. Returns an error if the item's value\n-// is not an uint16, or if it was not found. If there is no error, the\n-// incremented value is returned.\n-func (c *cache) IncrementUint16(k string, n uint16) (uint16, error) {\n-\tc.mu.Lock()\n-\tv, found := c.items[k]\n-\tif !found || v.Expired() {\n-\t\tc.mu.Unlock()\n-\t\treturn 0, fmt.Errorf(\"Item %s not found\", k)\n-\t}\n-\trv, ok := v.Object.(uint16)\n-\tif !ok {\n-\t\tc.mu.Unlock()\n-\t\treturn 0, fmt.Errorf(\"The value for %s is not an uint16\", k)\n-\t}\n-\tnv := rv + n\n-\tv.Object = nv\n-\tc.items[k] = v\n-\tc.mu.Unlock()\n-\treturn nv, nil\n-}\n-\n-// Increment an item of type uint32 by n. Returns an error if the item's value\n-// is not an uint32, or if it was not found. If there is no error, the\n-// incremented value is returned.\n-func (c *cache) IncrementUint32(k string, n uint32) (uint32, error) {\n-\tc.mu.Lock()\n-\tv, found := c.items[k]\n-\tif !found || v.Expired() {\n-\t\tc.mu.Unlock()\n-\t\treturn 0, fmt.Errorf(\"Item %s not found\", k)\n-\t}\n-\trv, ok := v.Object.(uint32)\n-\tif !ok {\n-\t\tc.mu.Unlock()\n-\t\treturn 0, fmt.Errorf(\"The value for %s is not an uint32\", k)\n-\t}\n-\tnv := rv + n\n-\tv.Object = nv\n-\tc.items[k] = v\n-\tc.mu.Unlock()\n-\treturn nv, nil\n-}\n-\n-// Increment an item of type uint64 by n. Returns an error if the item's value\n-// is not an uint64, or if it was not found. If there is no error, the\n-// incremented value is returned.\n-func (c *cache) IncrementUint64(k string, n uint64) (uint64, error) {\n-\tc.mu.Lock()\n-\tv, found := c.items[k]\n-\tif !found || v.Expired() {\n-\t\tc.mu.Unlock()\n-\t\treturn 0, fmt.Errorf(\"Item %s not found\", k)\n-\t}\n-\trv, ok := v.Object.(uint64)\n-\tif !ok {\n-\t\tc.mu.Unlock()\n-\t\treturn 0, fmt.Errorf(\"The value for %s is not an uint64\", k)\n-\t}\n-\tnv := rv + n\n-\tv.Object = nv\n-\tc.items[k] = v\n-\tc.mu.Unlock()\n-\treturn nv, nil\n-}\n-\n-// Increment an item of type float32 by n. Returns an error if the item's value\n-// is not an float32, or if it was not found. If there is no error, the\n-// incremented value is returned.\n-func (c *cache) IncrementFloat32(k string, n float32) (float32, error) {\n-\tc.mu.Lock()\n-\tv, found := c.items[k]\n-\tif !found || v.Expired() {\n-\t\tc.mu.Unlock()\n-\t\treturn 0, fmt.Errorf(\"Item %s not found\", k)\n-\t}\n-\trv, ok := v.Object.(float32)\n-\tif !ok {\n-\t\tc.mu.Unlock()\n-\t\treturn 0, fmt.Errorf(\"The value for %s is not an float32\", k)\n-\t}\n-\tnv := rv + n\n-\tv.Object = nv\n-\tc.items[k] = v\n-\tc.mu.Unlock()\n-\treturn nv, nil\n-}\n-\n-// Increment an item of type float64 by n. Returns an error if the item's value\n-// is not an float64, or if it was not found. If there is no error, the\n-// incremented value is returned.\n-func (c *cache) IncrementFloat64(k string, n float64) (float64, error) {\n-\tc.mu.Lock()\n-\tv, found := c.items[k]\n-\tif !found || v.Expired() {\n-\t\tc.mu.Unlock()\n-\t\treturn 0, fmt.Errorf(\"Item %s not found\", k)\n-\t}\n-\trv, ok := v.Object.(float64)\n-\tif !ok {\n-\t\tc.mu.Unlock()\n-\t\treturn 0, fmt.Errorf(\"The value for %s is not an float64\", k)\n-\t}\n-\tnv := rv + n\n-\tv.Object = nv\n-\tc.items[k] = v\n-\tc.mu.Unlock()\n-\treturn nv, nil\n-}\n-\n-// Decrement an item of type int, int8, int16, int32, int64, uintptr, uint,\n-// uint8, uint32, or uint64, float32 or float64 by n. Returns an error if the\n-// item's value is not an integer, if it was not found, or if it is not\n-// possible to decrement it by n. To retrieve the decremented value, use one\n-// of the specialized methods, e.g. DecrementInt64.\n-func (c *cache) Decrement(k string, n int64) error {\n-\t// TODO: Implement Increment and Decrement more cleanly.\n-\t// (Cannot do Increment(k, n*-1) for uints.)\n-\tc.mu.Lock()\n-\tv, found := c.items[k]\n-\tif !found || v.Expired() {\n-\t\tc.mu.Unlock()\n-\t\treturn fmt.Errorf(\"Item not found\")\n-\t}\n-\tswitch v.Object.(type) {\n-\tcase int:\n-\t\tv.Object = v.Object.(int) - int(n)\n-\tcase int8:\n-\t\tv.Object = v.Object.(int8) - int8(n)\n-\tcase int16:\n-\t\tv.Object = v.Object.(int16) - int16(n)\n-\tcase int32:\n-\t\tv.Object = v.Object.(int32) - int32(n)\n-\tcase int64:\n-\t\tv.Object = v.Object.(int64) - n\n-\tcase uint:\n-\t\tv.Object = v.Object.(uint) - uint(n)\n-\tcase uintptr:\n-\t\tv.Object = v.Object.(uintptr) - uintptr(n)\n-\tcase uint8:\n-\t\tv.Object = v.Object.(uint8) - uint8(n)\n-\tcase uint16:\n-\t\tv.Object = v.Object.(uint16) - uint16(n)\n-\tcase uint32:\n-\t\tv.Object = v.Object.(uint32) - uint32(n)\n-\tcase uint64:\n-\t\tv.Object = v.Object.(uint64) - uint64(n)\n-\tcase float32:\n-\t\tv.Object = v.Object.(float32) - float32(n)\n-\tcase float64:\n-\t\tv.Object = v.Object.(float64) - float64(n)\n-\tdefault:\n-\t\tc.mu.Unlock()\n-\t\treturn fmt.Errorf(\"The value for %s is not an integer\", k)\n-\t}\n-\tc.items[k] = v\n-\tc.mu.Unlock()\n-\treturn nil\n-}\n-\n-// Decrement an item of type float32 or float64 by n. Returns an error if the\n-// item's value is not floating point, if it was not found, or if it is not\n-// possible to decrement it by n. Pass a negative number to decrement the\n-// value. To retrieve the decremented value, use one of the specialized methods,\n-// e.g. DecrementFloat64.\n-func (c *cache) DecrementFloat(k string, n float64) error {\n-\tc.mu.Lock()\n-\tv, found := c.items[k]\n-\tif !found || v.Expired() {\n-\t\tc.mu.Unlock()\n-\t\treturn fmt.Errorf(\"Item %s not found\", k)\n-\t}\n-\tswitch v.Object.(type) {\n-\tcase float32:\n-\t\tv.Object = v.Object.(float32) - float32(n)\n-\tcase float64:\n-\t\tv.Object = v.Object.(float64) - n\n-\tdefault:\n-\t\tc.mu.Unlock()\n-\t\treturn fmt.Errorf(\"The value for %s does not have type float32 or float64\", k)\n-\t}\n-\tc.items[k] = v\n-\tc.mu.Unlock()\n-\treturn nil\n-}\n-\n-// Decrement an item of type int by n. Returns an error if the item's value is\n-// not an int, or if it was not found. If there is no error, the decremented\n-// value is returned.\n-func (c *cache) DecrementInt(k string, n int) (int, error) {\n-\tc.mu.Lock()\n-\tv, found := c.items[k]\n-\tif !found || v.Expired() {\n-\t\tc.mu.Unlock()\n-\t\treturn 0, fmt.Errorf(\"Item %s not found\", k)\n-\t}\n-\trv, ok := v.Object.(int)\n-\tif !ok {\n-\t\tc.mu.Unlock()\n-\t\treturn 0, fmt.Errorf(\"The value for %s is not an int\", k)\n-\t}\n-\tnv := rv - n\n-\tv.Object = nv\n-\tc.items[k] = v\n-\tc.mu.Unlock()\n-\treturn nv, nil\n-}\n-\n-// Decrement an item of type int8 by n. Returns an error if the item's value is\n-// not an int8, or if it was not found. If there is no error, the decremented\n-// value is returned.\n-func (c *cache) DecrementInt8(k string, n int8) (int8, error) {\n-\tc.mu.Lock()\n-\tv, found := c.items[k]\n-\tif !found || v.Expired() {\n-\t\tc.mu.Unlock()\n-\t\treturn 0, fmt.Errorf(\"Item %s not found\", k)\n-\t}\n-\trv, ok := v.Object.(int8)\n-\tif !ok {\n-\t\tc.mu.Unlock()\n-\t\treturn 0, fmt.Errorf(\"The value for %s is not an int8\", k)\n-\t}\n-\tnv := rv - n\n-\tv.Object = nv\n-\tc.items[k] = v\n-\tc.mu.Unlock()\n-\treturn nv, nil\n-}\n-\n-// Decrement an item of type int16 by n. Returns an error if the item's value is\n-// not an int16, or if it was not found. If there is no error, the decremented\n-// value is returned.\n-func (c *cache) DecrementInt16(k string, n int16) (int16, error) {\n-\tc.mu.Lock()\n-\tv, found := c.items[k]\n-\tif !found || v.Expired() {\n-\t\tc.mu.Unlock()\n-\t\treturn 0, fmt.Errorf(\"Item %s not found\", k)\n-\t}\n-\trv, ok := v.Object.(int16)\n-\tif !ok {\n-\t\tc.mu.Unlock()\n-\t\treturn 0, fmt.Errorf(\"The value for %s is not an int16\", k)\n-\t}\n-\tnv := rv - n\n-\tv.Object = nv\n-\tc.items[k] = v\n-\tc.mu.Unlock()\n-\treturn nv, nil\n-}\n-\n-// Decrement an item of type int32 by n. Returns an error if the item's value is\n-// not an int32, or if it was not found. If there is no error, the decremented\n-// value is returned.\n-func (c *cache) DecrementInt32(k string, n int32) (int32, error) {\n-\tc.mu.Lock()\n-\tv, found := c.items[k]\n-\tif !found || v.Expired() {\n-\t\tc.mu.Unlock()\n-\t\treturn 0, fmt.Errorf(\"Item %s not found\", k)\n-\t}\n-\trv, ok := v.Object.(int32)\n-\tif !ok {\n-\t\tc.mu.Unlock()\n-\t\treturn 0, fmt.Errorf(\"The value for %s is not an int32\", k)\n-\t}\n-\tnv := rv - n\n-\tv.Object = nv\n-\tc.items[k] = v\n-\tc.mu.Unlock()\n-\treturn nv, nil\n-}\n-\n-// Decrement an item of type int64 by n. Returns an error if the item's value is\n-// not an int64, or if it was not found. If there is no error, the decremented\n-// value is returned.\n-func (c *cache) DecrementInt64(k string, n int64) (int64, error) {\n-\tc.mu.Lock()\n-\tv, found := c.items[k]\n-\tif !found || v.Expired() {\n-\t\tc.mu.Unlock()\n-\t\treturn 0, fmt.Errorf(\"Item %s not found\", k)\n-\t}\n-\trv, ok := v.Object.(int64)\n-\tif !ok {\n-\t\tc.mu.Unlock()\n-\t\treturn 0, fmt.Errorf(\"The value for %s is not an int64\", k)\n-\t}\n-\tnv := rv - n\n-\tv.Object = nv\n-\tc.items[k] = v\n-\tc.mu.Unlock()\n-\treturn nv, nil\n-}\n-\n-// Decrement an item of type uint by n. Returns an error if the item's value is\n-// not an uint, or if it was not found. If there is no error, the decremented\n-// value is returned.\n-func (c *cache) DecrementUint(k string, n uint) (uint, error) {\n-\tc.mu.Lock()\n-\tv, found := c.items[k]\n-\tif !found || v.Expired() {\n-\t\tc.mu.Unlock()\n-\t\treturn 0, fmt.Errorf(\"Item %s not found\", k)\n-\t}\n-\trv, ok := v.Object.(uint)\n-\tif !ok {\n-\t\tc.mu.Unlock()\n-\t\treturn 0, fmt.Errorf(\"The value for %s is not an uint\", k)\n-\t}\n-\tnv := rv - n\n-\tv.Object = nv\n-\tc.items[k] = v\n-\tc.mu.Unlock()\n-\treturn nv, nil\n-}\n-\n-// Decrement an item of type uintptr by n. Returns an error if the item's value\n-// is not an uintptr, or if it was not found. If there is no error, the\n-// decremented value is returned.\n-func (c *cache) DecrementUintptr(k string, n uintptr) (uintptr, error) {\n-\tc.mu.Lock()\n-\tv, found := c.items[k]\n-\tif !found || v.Expired() {\n-\t\tc.mu.Unlock()\n-\t\treturn 0, fmt.Errorf(\"Item %s not found\", k)\n-\t}\n-\trv, ok := v.Object.(uintptr)\n-\tif !ok {\n-\t\tc.mu.Unlock()\n-\t\treturn 0, fmt.Errorf(\"The value for %s is not an uintptr\", k)\n-\t}\n-\tnv := rv - n\n-\tv.Object = nv\n-\tc.items[k] = v\n-\tc.mu.Unlock()\n-\treturn nv, nil\n-}\n-\n-// Decrement an item of type uint8 by n. Returns an error if the item's value is\n-// not an uint8, or if it was not found. If there is no error, the decremented\n-// value is returned.\n-func (c *cache) DecrementUint8(k string, n uint8) (uint8, error) {\n-\tc.mu.Lock()\n-\tv, found := c.items[k]\n-\tif !found || v.Expired() {\n-\t\tc.mu.Unlock()\n-\t\treturn 0, fmt.Errorf(\"Item %s not found\", k)\n-\t}\n-\trv, ok := v.Object.(uint8)\n-\tif !ok {\n-\t\tc.mu.Unlock()\n-\t\treturn 0, fmt.Errorf(\"The value for %s is not an uint8\", k)\n-\t}\n-\tnv := rv - n\n-\tv.Object = nv\n-\tc.items[k] = v\n-\tc.mu.Unlock()\n-\treturn nv, nil\n-}\n-\n-// Decrement an item of type uint16 by n. Returns an error if the item's value\n-// is not an uint16, or if it was not found. If there is no error, the\n-// decremented value is returned.\n-func (c *cache) DecrementUint16(k string, n uint16) (uint16, error) {\n-\tc.mu.Lock()\n-\tv, found := c.items[k]\n-\tif !found || v.Expired() {\n-\t\tc.mu.Unlock()\n-\t\treturn 0, fmt.Errorf(\"Item %s not found\", k)\n-\t}\n-\trv, ok := v.Object.(uint16)\n-\tif !ok {\n-\t\tc.mu.Unlock()\n-\t\treturn 0, fmt.Errorf(\"The value for %s is not an uint16\", k)\n-\t}\n-\tnv := rv - n\n-\tv.Object = nv\n-\tc.items[k] = v\n-\tc.mu.Unlock()\n-\treturn nv, nil\n-}\n-\n-// Decrement an item of type uint32 by n. Returns an error if the item's value\n-// is not an uint32, or if it was not found. If there is no error, the\n-// decremented value is returned.\n-func (c *cache) DecrementUint32(k string, n uint32) (uint32, error) {\n-\tc.mu.Lock()\n-\tv, found := c.items[k]\n-\tif !found || v.Expired() {\n-\t\tc.mu.Unlock()\n-\t\treturn 0, fmt.Errorf(\"Item %s not found\", k)\n-\t}\n-\trv, ok := v.Object.(uint32)\n-\tif !ok {\n-\t\tc.mu.Unlock()\n-\t\treturn 0, fmt.Errorf(\"The value for %s is not an uint32\", k)\n-\t}\n-\tnv := rv - n\n-\tv.Object = nv\n-\tc.items[k] = v\n-\tc.mu.Unlock()\n-\treturn nv, nil\n-}\n-\n-// Decrement an item of type uint64 by n. Returns an error if the item's value\n-// is not an uint64, or if it was not found. If there is no error, the\n-// decremented value is returned.\n-func (c *cache) DecrementUint64(k string, n uint64) (uint64, error) {\n-\tc.mu.Lock()\n-\tv, found := c.items[k]\n-\tif !found || v.Expired() {\n-\t\tc.mu.Unlock()\n-\t\treturn 0, fmt.Errorf(\"Item %s not found\", k)\n-\t}\n-\trv, ok := v.Object.(uint64)\n-\tif !ok {\n-\t\tc.mu.Unlock()\n-\t\treturn 0, fmt.Errorf(\"The value for %s is not an uint64\", k)\n-\t}\n-\tnv := rv - n\n-\tv.Object = nv\n-\tc.items[k] = v\n-\tc.mu.Unlock()\n-\treturn nv, nil\n-}\n-\n-// Decrement an item of type float32 by n. Returns an error if the item's value\n-// is not an float32, or if it was not found. If there is no error, the\n-// decremented value is returned.\n-func (c *cache) DecrementFloat32(k string, n float32) (float32, error) {\n-\tc.mu.Lock()\n-\tv, found := c.items[k]\n-\tif !found || v.Expired() {\n-\t\tc.mu.Unlock()\n-\t\treturn 0, fmt.Errorf(\"Item %s not found\", k)\n-\t}\n-\trv, ok := v.Object.(float32)\n-\tif !ok {\n-\t\tc.mu.Unlock()\n-\t\treturn 0, fmt.Errorf(\"The value for %s is not an float32\", k)\n-\t}\n-\tnv := rv - n\n-\tv.Object = nv\n-\tc.items[k] = v\n-\tc.mu.Unlock()\n-\treturn nv, nil\n-}\n-\n-// Decrement an item of type float64 by n. Returns an error if the item's value\n-// is not an float64, or if it was not found. If there is no error, the\n-// decremented value is returned.\n-func (c *cache) DecrementFloat64(k string, n float64) (float64, error) {\n-\tc.mu.Lock()\n-\tv, found := c.items[k]\n-\tif !found || v.Expired() {\n-\t\tc.mu.Unlock()\n-\t\treturn 0, fmt.Errorf(\"Item %s not found\", k)\n-\t}\n-\trv, ok := v.Object.(float64)\n-\tif !ok {\n-\t\tc.mu.Unlock()\n-\t\treturn 0, fmt.Errorf(\"The value for %s is not an float64\", k)\n-\t}\n-\tnv := rv - n\n-\tv.Object = nv\n-\tc.items[k] = v\n-\tc.mu.Unlock()\n-\treturn nv, nil\n-}\n-\n-// Delete an item from the cache. Does nothing if the key is not in the cache.\n-func (c *cache) Delete(k string) {\n-\tc.mu.Lock()\n-\tv, evicted := c.delete(k)\n-\tc.mu.Unlock()\n-\tif evicted {\n-\t\tc.onEvicted(k, v)\n-\t}\n-}\n-\n-func (c *cache) delete(k string) (interface{}, bool) {\n-\tif c.onEvicted != nil {\n-\t\tif v, found := c.items[k]; found {\n-\t\t\tdelete(c.items, k)\n-\t\t\treturn v.Object, true\n-\t\t}\n-\t}\n-\tdelete(c.items, k)\n-\treturn nil, false\n-}\n-\n-type keyAndValue struct {\n-\tkey   string\n-\tvalue interface{}\n-}\n-\n-// Delete all expired items from the cache.\n-func (c *cache) DeleteExpired() {\n-\tvar evictedItems []keyAndValue\n-\tnow := time.Now().UnixNano()\n-\tc.mu.Lock()\n-\tfor k, v := range c.items {\n-\t\t// \"Inlining\" of expired\n-\t\tif v.Expiration > 0 && now > v.Expiration {\n-\t\t\tov, evicted := c.delete(k)\n-\t\t\tif evicted {\n-\t\t\t\tevictedItems = append(evictedItems, keyAndValue{k, ov})\n-\t\t\t}\n-\t\t}\n-\t}\n-\tc.mu.Unlock()\n-\tfor _, v := range evictedItems {\n-\t\tc.onEvicted(v.key, v.value)\n-\t}\n-}\n-\n-// Sets an (optional) function that is called with the key and value when an\n-// item is evicted from the cache. (Including when it is deleted manually, but\n-// not when it is overwritten.) Set to nil to disable.\n-func (c *cache) OnEvicted(f func(string, interface{})) {\n-\tc.mu.Lock()\n-\tc.onEvicted = f\n-\tc.mu.Unlock()\n-}\n-\n-// Write the cache's items (using Gob) to an io.Writer.\n-//\n-// NOTE: This method is deprecated in favor of c.Items() and NewFrom() (see the\n-// documentation for NewFrom().)\n-func (c *cache) Save(w io.Writer) (err error) {\n-\tenc := gob.NewEncoder(w)\n-\tdefer func() {\n-\t\tif x := recover(); x != nil {\n-\t\t\terr = fmt.Errorf(\"Error registering item types with Gob library\")\n-\t\t}\n-\t}()\n-\tc.mu.RLock()\n-\tdefer c.mu.RUnlock()\n-\tfor _, v := range c.items {\n-\t\tgob.Register(v.Object)\n-\t}\n-\terr = enc.Encode(&c.items)\n-\treturn\n-}\n-\n-// Save the cache's items to the given filename, creating the file if it\n-// doesn't exist, and overwriting it if it does.\n-//\n-// NOTE: This method is deprecated in favor of c.Items() and NewFrom() (see the\n-// documentation for NewFrom().)\n-func (c *cache) SaveFile(fname string) error {\n-\tfp, err := os.Create(fname)\n-\tif err != nil {\n-\t\treturn err\n-\t}\n-\terr = c.Save(fp)\n-\tif err != nil {\n-\t\tfp.Close()\n-\t\treturn err\n-\t}\n-\treturn fp.Close()\n-}\n-\n-// Add (Gob-serialized) cache items from an io.Reader, excluding any items with\n-// keys that already exist (and haven't expired) in the current cache.\n-//\n-// NOTE: This method is deprecated in favor of c.Items() and NewFrom() (see the\n-// documentation for NewFrom().)\n-func (c *cache) Load(r io.Reader) error {\n-\tdec := gob.NewDecoder(r)\n-\titems := map[string]Item{}\n-\terr := dec.Decode(&items)\n-\tif err == nil {\n-\t\tc.mu.Lock()\n-\t\tdefer c.mu.Unlock()\n-\t\tfor k, v := range items {\n-\t\t\tov, found := c.items[k]\n-\t\t\tif !found || ov.Expired() {\n-\t\t\t\tc.items[k] = v\n-\t\t\t}\n-\t\t}\n-\t}\n-\treturn err\n-}\n-\n-// Load and add cache items from the given filename, excluding any items with\n-// keys that already exist in the current cache.\n-//\n-// NOTE: This method is deprecated in favor of c.Items() and NewFrom() (see the\n-// documentation for NewFrom().)\n-func (c *cache) LoadFile(fname string) error {\n-\tfp, err := os.Open(fname)\n-\tif err != nil {\n-\t\treturn err\n-\t}\n-\terr = c.Load(fp)\n-\tif err != nil {\n-\t\tfp.Close()\n-\t\treturn err\n-\t}\n-\treturn fp.Close()\n-}\n-\n-// Copies all unexpired items in the cache into a new map and returns it.\n-func (c *cache) Items() map[string]Item {\n-\tc.mu.RLock()\n-\tdefer c.mu.RUnlock()\n-\tm := make(map[string]Item, len(c.items))\n-\tnow := time.Now().UnixNano()\n-\tfor k, v := range c.items {\n-\t\t// \"Inlining\" of Expired\n-\t\tif v.Expiration > 0 {\n-\t\t\tif now > v.Expiration {\n-\t\t\t\tcontinue\n-\t\t\t}\n-\t\t}\n-\t\tm[k] = v\n-\t}\n-\treturn m\n-}\n-\n-// Returns the number of items in the cache. This may include items that have\n-// expired, but have not yet been cleaned up.\n-func (c *cache) ItemCount() int {\n-\tc.mu.RLock()\n-\tn := len(c.items)\n-\tc.mu.RUnlock()\n-\treturn n\n-}\n-\n-// Delete all items from the cache.\n-func (c *cache) Flush() {\n-\tc.mu.Lock()\n-\tc.items = map[string]Item{}\n-\tc.mu.Unlock()\n-}\n-\n-type janitor struct {\n-\tInterval time.Duration\n-\tstop     chan bool\n-}\n-\n-func (j *janitor) Run(c *cache) {\n-\tticker := time.NewTicker(j.Interval)\n-\tfor {\n-\t\tselect {\n-\t\tcase <-ticker.C:\n-\t\t\tc.DeleteExpired()\n-\t\tcase <-j.stop:\n-\t\t\tticker.Stop()\n-\t\t\treturn\n-\t\t}\n-\t}\n-}\n-\n-func stopJanitor(c *Cache) {\n-\tc.janitor.stop <- true\n-}\n-\n-func runJanitor(c *cache, ci time.Duration) {\n-\tj := &janitor{\n-\t\tInterval: ci,\n-\t\tstop:     make(chan bool),\n-\t}\n-\tc.janitor = j\n-\tgo j.Run(c)\n-}\n-\n-func newCache(de time.Duration, m map[string]Item) *cache {\n-\tif de == 0 {\n-\t\tde = -1\n-\t}\n-\tc := &cache{\n-\t\tdefaultExpiration: de,\n-\t\titems:             m,\n-\t}\n-\treturn c\n-}\n-\n-func newCacheWithJanitor(de time.Duration, ci time.Duration, m map[string]Item) *Cache {\n-\tc := newCache(de, m)\n-\t// This trick ensures that the janitor goroutine (which--granted it\n-\t// was enabled--is running DeleteExpired on c forever) does not keep\n-\t// the returned C object from being garbage collected. When it is\n-\t// garbage collected, the finalizer stops the janitor goroutine, after\n-\t// which c can be collected.\n-\tC := &Cache{c}\n-\tif ci > 0 {\n-\t\trunJanitor(c, ci)\n-\t\truntime.SetFinalizer(C, stopJanitor)\n-\t}\n-\treturn C\n-}\n-\n-// Return a new cache with a given default expiration duration and cleanup\n-// interval. If the expiration duration is less than one (or NoExpiration),\n-// the items in the cache never expire (by default), and must be deleted\n-// manually. If the cleanup interval is less than one, expired items are not\n-// deleted from the cache before calling c.DeleteExpired().\n-func New(defaultExpiration, cleanupInterval time.Duration) *Cache {\n-\titems := make(map[string]Item)\n-\treturn newCacheWithJanitor(defaultExpiration, cleanupInterval, items)\n-}\n-\n-// Return a new cache with a given default expiration duration and cleanup\n-// interval. If the expiration duration is less than one (or NoExpiration),\n-// the items in the cache never expire (by default), and must be deleted\n-// manually. If the cleanup interval is less than one, expired items are not\n-// deleted from the cache before calling c.DeleteExpired().\n-//\n-// NewFrom() also accepts an items map which will serve as the underlying map\n-// for the cache. This is useful for starting from a deserialized cache\n-// (serialized using e.g. gob.Encode() on c.Items()), or passing in e.g.\n-// make(map[string]Item, 500) to improve startup performance when the cache\n-// is expected to reach a certain minimum size.\n-//\n-// Only the cache's methods synchronize access to this map, so it is not\n-// recommended to keep any references to the map around after creating a cache.\n-// If need be, the map can be accessed at a later point using c.Items() (subject\n-// to the same caveat.)\n-//\n-// Note regarding serialization: When using e.g. gob, make sure to\n-// gob.Register() the individual types stored in the cache before encoding a\n-// map retrieved with c.Items(), and to register those same types before\n-// decoding a blob containing an items map.\n-func NewFrom(defaultExpiration, cleanupInterval time.Duration, items map[string]Item) *Cache {\n-\treturn newCacheWithJanitor(defaultExpiration, cleanupInterval, items)\n-}"
    },
    {
      "sha": "bcc0538bcc7abc11bf0a0d35755bae1f76f83a23",
      "filename": "backend/vendor/github.com/patrickmn/go-cache/sharded.go",
      "status": "removed",
      "additions": 0,
      "deletions": 192,
      "changes": 192,
      "blob_url": "https://github.com/umputun/remark42/blob/0d67f7e53d4c07b9d299d2dd2d8d33b75fc83a23/backend/vendor/github.com/patrickmn/go-cache/sharded.go",
      "raw_url": "https://github.com/umputun/remark42/raw/0d67f7e53d4c07b9d299d2dd2d8d33b75fc83a23/backend/vendor/github.com/patrickmn/go-cache/sharded.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/github.com/patrickmn/go-cache/sharded.go?ref=0d67f7e53d4c07b9d299d2dd2d8d33b75fc83a23",
      "patch": "@@ -1,192 +0,0 @@\n-package cache\n-\n-import (\n-\t\"crypto/rand\"\n-\t\"math\"\n-\t\"math/big\"\n-\tinsecurerand \"math/rand\"\n-\t\"os\"\n-\t\"runtime\"\n-\t\"time\"\n-)\n-\n-// This is an experimental and unexported (for now) attempt at making a cache\n-// with better algorithmic complexity than the standard one, namely by\n-// preventing write locks of the entire cache when an item is added. As of the\n-// time of writing, the overhead of selecting buckets results in cache\n-// operations being about twice as slow as for the standard cache with small\n-// total cache sizes, and faster for larger ones.\n-//\n-// See cache_test.go for a few benchmarks.\n-\n-type unexportedShardedCache struct {\n-\t*shardedCache\n-}\n-\n-type shardedCache struct {\n-\tseed    uint32\n-\tm       uint32\n-\tcs      []*cache\n-\tjanitor *shardedJanitor\n-}\n-\n-// djb2 with better shuffling. 5x faster than FNV with the hash.Hash overhead.\n-func djb33(seed uint32, k string) uint32 {\n-\tvar (\n-\t\tl = uint32(len(k))\n-\t\td = 5381 + seed + l\n-\t\ti = uint32(0)\n-\t)\n-\t// Why is all this 5x faster than a for loop?\n-\tif l >= 4 {\n-\t\tfor i < l-4 {\n-\t\t\td = (d * 33) ^ uint32(k[i])\n-\t\t\td = (d * 33) ^ uint32(k[i+1])\n-\t\t\td = (d * 33) ^ uint32(k[i+2])\n-\t\t\td = (d * 33) ^ uint32(k[i+3])\n-\t\t\ti += 4\n-\t\t}\n-\t}\n-\tswitch l - i {\n-\tcase 1:\n-\tcase 2:\n-\t\td = (d * 33) ^ uint32(k[i])\n-\tcase 3:\n-\t\td = (d * 33) ^ uint32(k[i])\n-\t\td = (d * 33) ^ uint32(k[i+1])\n-\tcase 4:\n-\t\td = (d * 33) ^ uint32(k[i])\n-\t\td = (d * 33) ^ uint32(k[i+1])\n-\t\td = (d * 33) ^ uint32(k[i+2])\n-\t}\n-\treturn d ^ (d >> 16)\n-}\n-\n-func (sc *shardedCache) bucket(k string) *cache {\n-\treturn sc.cs[djb33(sc.seed, k)%sc.m]\n-}\n-\n-func (sc *shardedCache) Set(k string, x interface{}, d time.Duration) {\n-\tsc.bucket(k).Set(k, x, d)\n-}\n-\n-func (sc *shardedCache) Add(k string, x interface{}, d time.Duration) error {\n-\treturn sc.bucket(k).Add(k, x, d)\n-}\n-\n-func (sc *shardedCache) Replace(k string, x interface{}, d time.Duration) error {\n-\treturn sc.bucket(k).Replace(k, x, d)\n-}\n-\n-func (sc *shardedCache) Get(k string) (interface{}, bool) {\n-\treturn sc.bucket(k).Get(k)\n-}\n-\n-func (sc *shardedCache) Increment(k string, n int64) error {\n-\treturn sc.bucket(k).Increment(k, n)\n-}\n-\n-func (sc *shardedCache) IncrementFloat(k string, n float64) error {\n-\treturn sc.bucket(k).IncrementFloat(k, n)\n-}\n-\n-func (sc *shardedCache) Decrement(k string, n int64) error {\n-\treturn sc.bucket(k).Decrement(k, n)\n-}\n-\n-func (sc *shardedCache) Delete(k string) {\n-\tsc.bucket(k).Delete(k)\n-}\n-\n-func (sc *shardedCache) DeleteExpired() {\n-\tfor _, v := range sc.cs {\n-\t\tv.DeleteExpired()\n-\t}\n-}\n-\n-// Returns the items in the cache. This may include items that have expired,\n-// but have not yet been cleaned up. If this is significant, the Expiration\n-// fields of the items should be checked. Note that explicit synchronization\n-// is needed to use a cache and its corresponding Items() return values at\n-// the same time, as the maps are shared.\n-func (sc *shardedCache) Items() []map[string]Item {\n-\tres := make([]map[string]Item, len(sc.cs))\n-\tfor i, v := range sc.cs {\n-\t\tres[i] = v.Items()\n-\t}\n-\treturn res\n-}\n-\n-func (sc *shardedCache) Flush() {\n-\tfor _, v := range sc.cs {\n-\t\tv.Flush()\n-\t}\n-}\n-\n-type shardedJanitor struct {\n-\tInterval time.Duration\n-\tstop     chan bool\n-}\n-\n-func (j *shardedJanitor) Run(sc *shardedCache) {\n-\tj.stop = make(chan bool)\n-\ttick := time.Tick(j.Interval)\n-\tfor {\n-\t\tselect {\n-\t\tcase <-tick:\n-\t\t\tsc.DeleteExpired()\n-\t\tcase <-j.stop:\n-\t\t\treturn\n-\t\t}\n-\t}\n-}\n-\n-func stopShardedJanitor(sc *unexportedShardedCache) {\n-\tsc.janitor.stop <- true\n-}\n-\n-func runShardedJanitor(sc *shardedCache, ci time.Duration) {\n-\tj := &shardedJanitor{\n-\t\tInterval: ci,\n-\t}\n-\tsc.janitor = j\n-\tgo j.Run(sc)\n-}\n-\n-func newShardedCache(n int, de time.Duration) *shardedCache {\n-\tmax := big.NewInt(0).SetUint64(uint64(math.MaxUint32))\n-\trnd, err := rand.Int(rand.Reader, max)\n-\tvar seed uint32\n-\tif err != nil {\n-\t\tos.Stderr.Write([]byte(\"WARNING: go-cache's newShardedCache failed to read from the system CSPRNG (/dev/urandom or equivalent.) Your system's security may be compromised. Continuing with an insecure seed.\\n\"))\n-\t\tseed = insecurerand.Uint32()\n-\t} else {\n-\t\tseed = uint32(rnd.Uint64())\n-\t}\n-\tsc := &shardedCache{\n-\t\tseed: seed,\n-\t\tm:    uint32(n),\n-\t\tcs:   make([]*cache, n),\n-\t}\n-\tfor i := 0; i < n; i++ {\n-\t\tc := &cache{\n-\t\t\tdefaultExpiration: de,\n-\t\t\titems:             map[string]Item{},\n-\t\t}\n-\t\tsc.cs[i] = c\n-\t}\n-\treturn sc\n-}\n-\n-func unexportedNewSharded(defaultExpiration, cleanupInterval time.Duration, shards int) *unexportedShardedCache {\n-\tif defaultExpiration == 0 {\n-\t\tdefaultExpiration = -1\n-\t}\n-\tsc := newShardedCache(shards, defaultExpiration)\n-\tSC := &unexportedShardedCache{sc}\n-\tif cleanupInterval > 0 {\n-\t\trunShardedJanitor(sc, cleanupInterval)\n-\t\truntime.SetFinalizer(SC, stopShardedJanitor)\n-\t}\n-\treturn SC\n-}"
    },
    {
      "sha": "010d4ccd58225c4a949eaa72742f7af6eaca6e6e",
      "filename": "backend/vendor/github.com/stretchr/objx/.codeclimate.yml",
      "status": "added",
      "additions": 0,
      "deletions": 0,
      "changes": 0,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/stretchr/objx/.codeclimate.yml",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/stretchr/objx/.codeclimate.yml",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/github.com/stretchr/objx/.codeclimate.yml?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "ea58090bd21e1b3182e644a2b27fb181a12cb6f6",
      "filename": "backend/vendor/github.com/stretchr/objx/.gitignore",
      "status": "modified",
      "additions": 11,
      "deletions": 4,
      "changes": 15,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/stretchr/objx/.gitignore",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/stretchr/objx/.gitignore",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/github.com/stretchr/objx/.gitignore?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "a63efa59d1d5bf42bb86859deb384d7bbd9408ea",
      "filename": "backend/vendor/github.com/stretchr/objx/.travis.yml",
      "status": "modified",
      "additions": 13,
      "deletions": 1,
      "changes": 14,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/stretchr/objx/.travis.yml",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/stretchr/objx/.travis.yml",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/github.com/stretchr/objx/.travis.yml?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "eebe342a9602606d3b719f13a26b9a36685cf3c4",
      "filename": "backend/vendor/github.com/stretchr/objx/Gopkg.lock",
      "status": "modified",
      "additions": 5,
      "deletions": 2,
      "changes": 7,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/stretchr/objx/Gopkg.lock",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/stretchr/objx/Gopkg.lock",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/github.com/stretchr/objx/Gopkg.lock?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "d70f1570b95a71d65a5cbf38ff88497d036c016f",
      "filename": "backend/vendor/github.com/stretchr/objx/Gopkg.toml",
      "status": "modified",
      "additions": 5,
      "deletions": 0,
      "changes": 5,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/stretchr/objx/Gopkg.toml",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/stretchr/objx/Gopkg.toml",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/github.com/stretchr/objx/Gopkg.toml?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "be5750c94c82145577b4f090ac8845a58d1a6fe2",
      "filename": "backend/vendor/github.com/stretchr/objx/README.md",
      "status": "modified",
      "additions": 2,
      "deletions": 0,
      "changes": 2,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/stretchr/objx/README.md",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/stretchr/objx/README.md",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/github.com/stretchr/objx/README.md?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "f8035641f2d172f8a0f0f30c94d905fca15c31c6",
      "filename": "backend/vendor/github.com/stretchr/objx/Taskfile.yml",
      "status": "modified",
      "additions": 7,
      "deletions": 1,
      "changes": 8,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/stretchr/objx/Taskfile.yml",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/stretchr/objx/Taskfile.yml",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/github.com/stretchr/objx/Taskfile.yml?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "204356a22858a39c51848957af90a3855b402ec7",
      "filename": "backend/vendor/github.com/stretchr/objx/accessors.go",
      "status": "modified",
      "additions": 5,
      "deletions": 28,
      "changes": 33,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/stretchr/objx/accessors.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/stretchr/objx/accessors.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/github.com/stretchr/objx/accessors.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "406bc89263515f975b8d6ad2f738d0d71dda1389",
      "filename": "backend/vendor/github.com/stretchr/objx/map.go",
      "status": "modified",
      "additions": 8,
      "deletions": 11,
      "changes": 19,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/stretchr/objx/map.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/stretchr/objx/map.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/github.com/stretchr/objx/map.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "c3400a3f709a29145475a918a8912b550069e3b2",
      "filename": "backend/vendor/github.com/stretchr/objx/mutations.go",
      "status": "modified",
      "additions": 15,
      "deletions": 12,
      "changes": 27,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/stretchr/objx/mutations.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/stretchr/objx/mutations.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/github.com/stretchr/objx/mutations.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "692be8e2a9feb35955e1324b6004adb235838410",
      "filename": "backend/vendor/github.com/stretchr/objx/security.go",
      "status": "modified",
      "additions": 3,
      "deletions": 8,
      "changes": 11,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/stretchr/objx/security.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/stretchr/objx/security.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/github.com/stretchr/objx/security.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "e4b4a14335d741c33843c698bb47bd15d3cedd84",
      "filename": "backend/vendor/github.com/stretchr/objx/value.go",
      "status": "modified",
      "additions": 0,
      "deletions": 3,
      "changes": 3,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/stretchr/objx/value.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/stretchr/objx/value.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/github.com/stretchr/objx/value.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "f391327ea9d01b629badfc255632931377793c53",
      "filename": "backend/vendor/github.com/xdg/stringprep/.travis.yml",
      "status": "modified",
      "additions": 4,
      "deletions": 3,
      "changes": 7,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/xdg/stringprep/.travis.yml",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/github.com/xdg/stringprep/.travis.yml",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/github.com/xdg/stringprep/.travis.yml?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "ae1a87faa7b40f0e47c5bbcd503d8ffe18b6d89e",
      "filename": "backend/vendor/go.mongodb.org/mongo-driver/bson/bson.go",
      "status": "modified",
      "additions": 6,
      "deletions": 16,
      "changes": 22,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/bson/bson.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/bson/bson.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.mongodb.org/mongo-driver/bson/bson.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "bbe7792848f02ba01a0a9591e35ad245f8bcbe1b",
      "filename": "backend/vendor/go.mongodb.org/mongo-driver/bson/bson_1_8.go",
      "status": "modified",
      "additions": 9,
      "deletions": 19,
      "changes": 28,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/bson/bson_1_8.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/bson/bson_1_8.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.mongodb.org/mongo-driver/bson/bson_1_8.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "9eed911ac735e7009b05f05593dff6fa990028f6",
      "filename": "backend/vendor/go.mongodb.org/mongo-driver/bson/bsoncodec/byte_slice_codec.go",
      "status": "added",
      "additions": 87,
      "deletions": 0,
      "changes": 87,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/bson/bsoncodec/byte_slice_codec.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/bson/bsoncodec/byte_slice_codec.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.mongodb.org/mongo-driver/bson/bsoncodec/byte_slice_codec.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "52d2365a34513a60363b2e8b4485ea40fa902b3e",
      "filename": "backend/vendor/go.mongodb.org/mongo-driver/bson/bsoncodec/default_value_decoders.go",
      "status": "modified",
      "additions": 377,
      "deletions": 142,
      "changes": 519,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/bson/bsoncodec/default_value_decoders.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/bson/bsoncodec/default_value_decoders.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.mongodb.org/mongo-driver/bson/bsoncodec/default_value_decoders.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "08078b304d27088717684050c9097e0a953e55e7",
      "filename": "backend/vendor/go.mongodb.org/mongo-driver/bson/bsoncodec/default_value_encoders.go",
      "status": "modified",
      "additions": 170,
      "deletions": 47,
      "changes": 217,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/bson/bsoncodec/default_value_encoders.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/bson/bsoncodec/default_value_encoders.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.mongodb.org/mongo-driver/bson/bsoncodec/default_value_encoders.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "c1e20f9489e6e775cef8b20bd9b81c85e530c154",
      "filename": "backend/vendor/go.mongodb.org/mongo-driver/bson/bsoncodec/doc.go",
      "status": "modified",
      "additions": 45,
      "deletions": 22,
      "changes": 67,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/bson/bsoncodec/doc.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/bson/bsoncodec/doc.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.mongodb.org/mongo-driver/bson/bsoncodec/doc.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "c215ec38498c2eddf17678c319c3891d7bbc57f3",
      "filename": "backend/vendor/go.mongodb.org/mongo-driver/bson/bsoncodec/empty_interface_codec.go",
      "status": "added",
      "additions": 125,
      "deletions": 0,
      "changes": 125,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/bson/bsoncodec/empty_interface_codec.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/bson/bsoncodec/empty_interface_codec.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.mongodb.org/mongo-driver/bson/bsoncodec/empty_interface_codec.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "85ae9c6a19e97b83161e9cfd6e5b0a2dfa18d9b2",
      "filename": "backend/vendor/go.mongodb.org/mongo-driver/bson/bsoncodec/map_codec.go",
      "status": "added",
      "additions": 206,
      "deletions": 0,
      "changes": 206,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/bson/bsoncodec/map_codec.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/bson/bsoncodec/map_codec.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.mongodb.org/mongo-driver/bson/bsoncodec/map_codec.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "10f1aed950e252a301b4cd1c28f5e576337b6ce0",
      "filename": "backend/vendor/go.mongodb.org/mongo-driver/bson/bsoncodec/registry.go",
      "status": "modified",
      "additions": 100,
      "deletions": 28,
      "changes": 128,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/bson/bsoncodec/registry.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/bson/bsoncodec/registry.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.mongodb.org/mongo-driver/bson/bsoncodec/registry.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "f0282eb23b8d0bcff86bca13e5734556df8678f0",
      "filename": "backend/vendor/go.mongodb.org/mongo-driver/bson/bsoncodec/slice_codec.go",
      "status": "added",
      "additions": 196,
      "deletions": 0,
      "changes": 196,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/bson/bsoncodec/slice_codec.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/bson/bsoncodec/slice_codec.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.mongodb.org/mongo-driver/bson/bsoncodec/slice_codec.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "c672cf5a686918e8572cd9043f9f2f72bbd1f5b7",
      "filename": "backend/vendor/go.mongodb.org/mongo-driver/bson/bsoncodec/string_codec.go",
      "status": "added",
      "additions": 94,
      "deletions": 0,
      "changes": 94,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/bson/bsoncodec/string_codec.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/bson/bsoncodec/string_codec.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.mongodb.org/mongo-driver/bson/bsoncodec/string_codec.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "777cdfb69584f389712422b6c655ac181c08fec1",
      "filename": "backend/vendor/go.mongodb.org/mongo-driver/bson/bsoncodec/struct_codec.go",
      "status": "modified",
      "additions": 196,
      "deletions": 27,
      "changes": 223,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/bson/bsoncodec/struct_codec.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/bson/bsoncodec/struct_codec.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.mongodb.org/mongo-driver/bson/bsoncodec/struct_codec.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "6f1b724d19b01d735e95b40cafb5ecab1baa4415",
      "filename": "backend/vendor/go.mongodb.org/mongo-driver/bson/bsoncodec/time_codec.go",
      "status": "added",
      "additions": 101,
      "deletions": 0,
      "changes": 101,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/bson/bsoncodec/time_codec.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/bson/bsoncodec/time_codec.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.mongodb.org/mongo-driver/bson/bsoncodec/time_codec.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "bbb6bb9cea1c6a69c21d6e1fa6e0524d79fd2fc2",
      "filename": "backend/vendor/go.mongodb.org/mongo-driver/bson/bsoncodec/types.go",
      "status": "modified",
      "additions": 1,
      "deletions": 0,
      "changes": 1,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/bson/bsoncodec/types.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/bson/bsoncodec/types.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.mongodb.org/mongo-driver/bson/bsoncodec/types.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "e0df05837486bc868cf30292c909dcacb73e8b39",
      "filename": "backend/vendor/go.mongodb.org/mongo-driver/bson/bsoncodec/uint_codec.go",
      "status": "added",
      "additions": 150,
      "deletions": 0,
      "changes": 150,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/bson/bsoncodec/uint_codec.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/bson/bsoncodec/uint_codec.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.mongodb.org/mongo-driver/bson/bsoncodec/uint_codec.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "b1256a4dcaffad8bdde3bacea9277ef446218c3a",
      "filename": "backend/vendor/go.mongodb.org/mongo-driver/bson/bsonoptions/byte_slice_codec_options.go",
      "status": "added",
      "additions": 38,
      "deletions": 0,
      "changes": 38,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/bson/bsonoptions/byte_slice_codec_options.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/bson/bsonoptions/byte_slice_codec_options.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.mongodb.org/mongo-driver/bson/bsonoptions/byte_slice_codec_options.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "6caaa000e6304f59235e3339a339b1a4e7dee2dd",
      "filename": "backend/vendor/go.mongodb.org/mongo-driver/bson/bsonoptions/empty_interface_codec_options.go",
      "status": "added",
      "additions": 38,
      "deletions": 0,
      "changes": 38,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/bson/bsonoptions/empty_interface_codec_options.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/bson/bsonoptions/empty_interface_codec_options.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.mongodb.org/mongo-driver/bson/bsonoptions/empty_interface_codec_options.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "1ac3e20088a314c53c44ab2fcf9266a0115d7710",
      "filename": "backend/vendor/go.mongodb.org/mongo-driver/bson/bsonoptions/map_codec_options.go",
      "status": "added",
      "additions": 48,
      "deletions": 0,
      "changes": 48,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/bson/bsonoptions/map_codec_options.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/bson/bsonoptions/map_codec_options.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.mongodb.org/mongo-driver/bson/bsonoptions/map_codec_options.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "ef965e4b411d16d429d27574b8e9d6d8a45a874c",
      "filename": "backend/vendor/go.mongodb.org/mongo-driver/bson/bsonoptions/slice_codec_options.go",
      "status": "added",
      "additions": 38,
      "deletions": 0,
      "changes": 38,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/bson/bsonoptions/slice_codec_options.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/bson/bsonoptions/slice_codec_options.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.mongodb.org/mongo-driver/bson/bsonoptions/slice_codec_options.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "65964f4207dd8cf9599b6fb60e1c7190337b4d50",
      "filename": "backend/vendor/go.mongodb.org/mongo-driver/bson/bsonoptions/string_codec_options.go",
      "status": "added",
      "additions": 41,
      "deletions": 0,
      "changes": 41,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/bson/bsonoptions/string_codec_options.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/bson/bsonoptions/string_codec_options.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.mongodb.org/mongo-driver/bson/bsonoptions/string_codec_options.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "ad32c7c38242961df2c64464814eb8eb8796bdf5",
      "filename": "backend/vendor/go.mongodb.org/mongo-driver/bson/bsonoptions/struct_codec_options.go",
      "status": "added",
      "additions": 70,
      "deletions": 0,
      "changes": 70,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/bson/bsonoptions/struct_codec_options.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/bson/bsonoptions/struct_codec_options.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.mongodb.org/mongo-driver/bson/bsonoptions/struct_codec_options.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "13496d121793505b7f921326f6c22f557d6516c4",
      "filename": "backend/vendor/go.mongodb.org/mongo-driver/bson/bsonoptions/time_codec_options.go",
      "status": "added",
      "additions": 38,
      "deletions": 0,
      "changes": 38,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/bson/bsonoptions/time_codec_options.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/bson/bsonoptions/time_codec_options.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.mongodb.org/mongo-driver/bson/bsonoptions/time_codec_options.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "e08b7f192eac3e2bd7ea31171c16a8433024114e",
      "filename": "backend/vendor/go.mongodb.org/mongo-driver/bson/bsonoptions/uint_codec_options.go",
      "status": "added",
      "additions": 38,
      "deletions": 0,
      "changes": 38,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/bson/bsonoptions/uint_codec_options.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/bson/bsonoptions/uint_codec_options.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.mongodb.org/mongo-driver/bson/bsonoptions/uint_codec_options.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "3ff17c197dff6f4871fc260f109f0a86913aa3d8",
      "filename": "backend/vendor/go.mongodb.org/mongo-driver/bson/bsonrw/extjson_parser.go",
      "status": "modified",
      "additions": 10,
      "deletions": 3,
      "changes": 13,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/bson/bsonrw/extjson_parser.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/bson/bsonrw/extjson_parser.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.mongodb.org/mongo-driver/bson/bsonrw/extjson_parser.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "7e9612c07434e0c5c95244735c3e38f5878451dc",
      "filename": "backend/vendor/go.mongodb.org/mongo-driver/bson/bsonrw/extjson_wrappers.go",
      "status": "modified",
      "additions": 25,
      "deletions": 8,
      "changes": 33,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/bson/bsonrw/extjson_wrappers.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/bson/bsonrw/extjson_wrappers.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.mongodb.org/mongo-driver/bson/bsonrw/extjson_wrappers.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "605e41a138ca9cbb22c80404e7dfba47967b53aa",
      "filename": "backend/vendor/go.mongodb.org/mongo-driver/bson/bsonrw/extjson_writer.go",
      "status": "modified",
      "additions": 4,
      "deletions": 1,
      "changes": 5,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/bson/bsonrw/extjson_writer.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/bson/bsonrw/extjson_writer.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.mongodb.org/mongo-driver/bson/bsonrw/extjson_writer.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "212f348348980893e05519e6625b618a71ba5b4f",
      "filename": "backend/vendor/go.mongodb.org/mongo-driver/bson/bsonrw/json_scanner.go",
      "status": "modified",
      "additions": 1,
      "deletions": 1,
      "changes": 2,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/bson/bsonrw/json_scanner.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/bson/bsonrw/json_scanner.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.mongodb.org/mongo-driver/bson/bsonrw/json_scanner.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "55378093a23e524cbc5e7187ddd783d236efaee7",
      "filename": "backend/vendor/go.mongodb.org/mongo-driver/bson/bsonrw/value_reader.go",
      "status": "modified",
      "additions": 5,
      "deletions": 10,
      "changes": 15,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/bson/bsonrw/value_reader.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/bson/bsonrw/value_reader.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.mongodb.org/mongo-driver/bson/bsonrw/value_reader.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "37171983663dfe6d9ee7e93970432ddda9e279d7",
      "filename": "backend/vendor/go.mongodb.org/mongo-driver/bson/bsonrw/value_writer.go",
      "status": "modified",
      "additions": 26,
      "deletions": 11,
      "changes": 37,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/bson/bsonrw/value_writer.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/bson/bsonrw/value_writer.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.mongodb.org/mongo-driver/bson/bsonrw/value_writer.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "7644df129098403db764ef5d39f4f98d4852b5c5",
      "filename": "backend/vendor/go.mongodb.org/mongo-driver/bson/bsonrw/writer.go",
      "status": "modified",
      "additions": 6,
      "deletions": 0,
      "changes": 6,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/bson/bsonrw/writer.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/bson/bsonrw/writer.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.mongodb.org/mongo-driver/bson/bsonrw/writer.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "63a59ca08f34785b6d48faf21990640d51ed505c",
      "filename": "backend/vendor/go.mongodb.org/mongo-driver/bson/bsontype/bsontype.go",
      "status": "modified",
      "additions": 8,
      "deletions": 0,
      "changes": 8,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/bson/bsontype/bsontype.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/bson/bsontype/bsontype.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.mongodb.org/mongo-driver/bson/bsontype/bsontype.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "7f6b7694f9c6400c592715b1b0975f96c7115ec5",
      "filename": "backend/vendor/go.mongodb.org/mongo-driver/bson/decoder.go",
      "status": "modified",
      "additions": 12,
      "deletions": 6,
      "changes": 18,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/bson/decoder.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/bson/decoder.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.mongodb.org/mongo-driver/bson/decoder.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "2943f14ecace1ddf568f82fb6de430e328a52262",
      "filename": "backend/vendor/go.mongodb.org/mongo-driver/bson/doc.go",
      "status": "modified",
      "additions": 94,
      "deletions": 16,
      "changes": 110,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/bson/doc.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/bson/doc.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.mongodb.org/mongo-driver/bson/doc.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "381822af53eb7c58cc770214041e404254333185",
      "filename": "backend/vendor/go.mongodb.org/mongo-driver/bson/marshal.go",
      "status": "modified",
      "additions": 78,
      "deletions": 11,
      "changes": 89,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/bson/marshal.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/bson/marshal.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.mongodb.org/mongo-driver/bson/marshal.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "bd0c5d161a5595ed06a68058f9016faa3432250c",
      "filename": "backend/vendor/go.mongodb.org/mongo-driver/bson/primitive/decimal.go",
      "status": "modified",
      "additions": 224,
      "deletions": 155,
      "changes": 379,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/bson/primitive/decimal.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/bson/primitive/decimal.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.mongodb.org/mongo-driver/bson/primitive/decimal.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "41d1cf28864c9b624fa81a070a732321791bb062",
      "filename": "backend/vendor/go.mongodb.org/mongo-driver/bson/primitive/objectid.go",
      "status": "modified",
      "additions": 1,
      "deletions": 1,
      "changes": 2,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/bson/primitive/objectid.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/bson/primitive/objectid.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.mongodb.org/mongo-driver/bson/primitive/objectid.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "f47f8df3c84ecd2512144f5b49677af05aec1c94",
      "filename": "backend/vendor/go.mongodb.org/mongo-driver/bson/primitive/primitive.go",
      "status": "modified",
      "additions": 35,
      "deletions": 25,
      "changes": 60,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/bson/primitive/primitive.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/bson/primitive/primitive.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.mongodb.org/mongo-driver/bson/primitive/primitive.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "f397fa2d58a5e1370a452c81414e09fdcb1c326b",
      "filename": "backend/vendor/go.mongodb.org/mongo-driver/bson/primitive_codecs.go",
      "status": "modified",
      "additions": 4,
      "deletions": 4,
      "changes": 8,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/bson/primitive_codecs.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/bson/primitive_codecs.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.mongodb.org/mongo-driver/bson/primitive_codecs.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "9b3825983789b8c5c0ec8cad9a9048d7f27368dc",
      "filename": "backend/vendor/go.mongodb.org/mongo-driver/mongo/bulk_write.go",
      "status": "modified",
      "additions": 4,
      "deletions": 4,
      "changes": 8,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/mongo/bulk_write.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/mongo/bulk_write.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.mongodb.org/mongo-driver/mongo/bulk_write.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "d23408b2c7337045962f95bf2f4fcb2d8dbb34f1",
      "filename": "backend/vendor/go.mongodb.org/mongo-driver/mongo/bulk_write_models.go",
      "status": "modified",
      "additions": 56,
      "deletions": 26,
      "changes": 82,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/mongo/bulk_write_models.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/mongo/bulk_write_models.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.mongodb.org/mongo-driver/mongo/bulk_write_models.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "f17045da5c0286170796f260d70d0397ef3fbadf",
      "filename": "backend/vendor/go.mongodb.org/mongo-driver/mongo/change_stream.go",
      "status": "modified",
      "additions": 71,
      "deletions": 39,
      "changes": 110,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/mongo/change_stream.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/mongo/change_stream.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.mongodb.org/mongo-driver/mongo/change_stream.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "255774d04a56944366affe256e83c8a5592c33bd",
      "filename": "backend/vendor/go.mongodb.org/mongo-driver/mongo/client.go",
      "status": "modified",
      "additions": 287,
      "deletions": 77,
      "changes": 364,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/mongo/client.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/mongo/client.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.mongodb.org/mongo-driver/mongo/client.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "4ec0b1770b22b31e7878739603a1ae7e08e81b86",
      "filename": "backend/vendor/go.mongodb.org/mongo-driver/mongo/client_encryption.go",
      "status": "added",
      "additions": 135,
      "deletions": 0,
      "changes": 135,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/mongo/client_encryption.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/mongo/client_encryption.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.mongodb.org/mongo-driver/mongo/client_encryption.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "d050e3b3a95094004bfc58166894b724caae67be",
      "filename": "backend/vendor/go.mongodb.org/mongo-driver/mongo/collection.go",
      "status": "modified",
      "additions": 273,
      "deletions": 82,
      "changes": 355,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/mongo/collection.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/mongo/collection.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.mongodb.org/mongo-driver/mongo/collection.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "e29523e48fa54edf3176fe2d02a128d27cd6912c",
      "filename": "backend/vendor/go.mongodb.org/mongo-driver/mongo/crypt_retrievers.go",
      "status": "added",
      "additions": 59,
      "deletions": 0,
      "changes": 59,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/mongo/crypt_retrievers.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/mongo/crypt_retrievers.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.mongodb.org/mongo-driver/mongo/crypt_retrievers.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "0085cef5f1052a87d93f67af2d9aa8c5a773fe8e",
      "filename": "backend/vendor/go.mongodb.org/mongo-driver/mongo/cursor.go",
      "status": "modified",
      "additions": 55,
      "deletions": 38,
      "changes": 93,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/mongo/cursor.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/mongo/cursor.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.mongodb.org/mongo-driver/mongo/cursor.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "bbfa05f2fe9ec57c7e79b78518e9c89a20ef60e5",
      "filename": "backend/vendor/go.mongodb.org/mongo-driver/mongo/database.go",
      "status": "modified",
      "additions": 94,
      "deletions": 30,
      "changes": 124,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/mongo/database.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/mongo/database.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.mongodb.org/mongo-driver/mongo/database.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "b84927f75812e8943c9cb4d161086a143cf3d615",
      "filename": "backend/vendor/go.mongodb.org/mongo-driver/mongo/doc.go",
      "status": "modified",
      "additions": 46,
      "deletions": 4,
      "changes": 50,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/mongo/doc.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/mongo/doc.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.mongodb.org/mongo-driver/mongo/doc.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "7f58c838e6d737d662810de598609d8cb41232a7",
      "filename": "backend/vendor/go.mongodb.org/mongo-driver/mongo/errors.go",
      "status": "modified",
      "additions": 72,
      "deletions": 26,
      "changes": 98,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/mongo/errors.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/mongo/errors.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.mongodb.org/mongo-driver/mongo/errors.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "51794c6d8b1681e28c4ac3c828e52bcf6229b459",
      "filename": "backend/vendor/go.mongodb.org/mongo-driver/mongo/gridfs/bucket.go",
      "status": "modified",
      "additions": 24,
      "deletions": 0,
      "changes": 24,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/mongo/gridfs/bucket.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/mongo/gridfs/bucket.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.mongodb.org/mongo-driver/mongo/gridfs/bucket.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "b8264c45b7a6b0a349ba0ec620ed3912b381e8ad",
      "filename": "backend/vendor/go.mongodb.org/mongo-driver/mongo/gridfs/doc.go",
      "status": "added",
      "additions": 35,
      "deletions": 0,
      "changes": 35,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/mongo/gridfs/doc.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/mongo/gridfs/doc.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.mongodb.org/mongo-driver/mongo/gridfs/doc.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "93ea79aa7ffa4d4a07477ad2323e938bc0973781",
      "filename": "backend/vendor/go.mongodb.org/mongo-driver/mongo/gridfs/upload_stream.go",
      "status": "modified",
      "additions": 4,
      "deletions": 2,
      "changes": 6,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/mongo/gridfs/upload_stream.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/mongo/gridfs/upload_stream.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.mongodb.org/mongo-driver/mongo/gridfs/upload_stream.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "a4c7c6fb5cc1dbb5ea8ed84c5be011d0d0a57584",
      "filename": "backend/vendor/go.mongodb.org/mongo-driver/mongo/index_options_builder.go",
      "status": "modified",
      "additions": 21,
      "deletions": 21,
      "changes": 42,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/mongo/index_options_builder.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/mongo/index_options_builder.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.mongodb.org/mongo-driver/mongo/index_options_builder.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "e7d9f5803bb547765240de17ad65aecce4d195e7",
      "filename": "backend/vendor/go.mongodb.org/mongo-driver/mongo/index_view.go",
      "status": "modified",
      "additions": 71,
      "deletions": 31,
      "changes": 102,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/mongo/index_view.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/mongo/index_view.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.mongodb.org/mongo-driver/mongo/index_view.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "c88674cc1fbba259b234ed1937ee3c0fa79cd69c",
      "filename": "backend/vendor/go.mongodb.org/mongo-driver/mongo/mongocryptd.go",
      "status": "added",
      "additions": 150,
      "deletions": 0,
      "changes": 150,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/mongo/mongocryptd.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/mongo/mongocryptd.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.mongodb.org/mongo-driver/mongo/mongocryptd.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "cbefe62453c4cba49fb013168f98f4f54994aba2",
      "filename": "backend/vendor/go.mongodb.org/mongo-driver/mongo/options/aggregateoptions.go",
      "status": "modified",
      "additions": 46,
      "deletions": 28,
      "changes": 74,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/mongo/options/aggregateoptions.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/mongo/options/aggregateoptions.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.mongodb.org/mongo-driver/mongo/options/aggregateoptions.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "3cbb60cbbebaf0a3dc99a3d637c51248566bb3c7",
      "filename": "backend/vendor/go.mongodb.org/mongo-driver/mongo/options/autoencryptionoptions.go",
      "status": "added",
      "additions": 109,
      "deletions": 0,
      "changes": 109,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/mongo/options/autoencryptionoptions.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/mongo/options/autoencryptionoptions.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.mongodb.org/mongo-driver/mongo/options/autoencryptionoptions.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "57f98f83d17612a05f39115642ead0d8f5b6f504",
      "filename": "backend/vendor/go.mongodb.org/mongo-driver/mongo/options/bulkwriteoptions.go",
      "status": "modified",
      "additions": 15,
      "deletions": 10,
      "changes": 25,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/mongo/options/bulkwriteoptions.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/mongo/options/bulkwriteoptions.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.mongodb.org/mongo-driver/mongo/options/bulkwriteoptions.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "fe19f45ebb83bcef3388c75f6437ee4ef88a6be5",
      "filename": "backend/vendor/go.mongodb.org/mongo-driver/mongo/options/changestreamoptions.go",
      "status": "modified",
      "additions": 45,
      "deletions": 23,
      "changes": 68,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/mongo/options/changestreamoptions.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/mongo/options/changestreamoptions.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.mongodb.org/mongo-driver/mongo/options/changestreamoptions.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "ea770413a9363751e3e7c5d70f83612f7423327f",
      "filename": "backend/vendor/go.mongodb.org/mongo-driver/mongo/options/clientencryptionoptions.go",
      "status": "added",
      "additions": 49,
      "deletions": 0,
      "changes": 49,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/mongo/options/clientencryptionoptions.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/mongo/options/clientencryptionoptions.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.mongodb.org/mongo-driver/mongo/options/clientencryptionoptions.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "f7cd3fbf54a87712b604f05df2943ca644ff8fbb",
      "filename": "backend/vendor/go.mongodb.org/mongo-driver/mongo/options/clientoptions.go",
      "status": "modified",
      "additions": 329,
      "deletions": 82,
      "changes": 411,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/mongo/options/clientoptions.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/mongo/options/clientoptions.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.mongodb.org/mongo-driver/mongo/options/clientoptions.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "5c8111471bddccb056f9f3faf9865f0ab0c1e567",
      "filename": "backend/vendor/go.mongodb.org/mongo-driver/mongo/options/collectionoptions.go",
      "status": "modified",
      "additions": 23,
      "deletions": 12,
      "changes": 35,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/mongo/options/collectionoptions.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/mongo/options/collectionoptions.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.mongodb.org/mongo-driver/mongo/options/collectionoptions.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "1d1cd8288a0690ad89a7dc5faf99ae76fdd87d9e",
      "filename": "backend/vendor/go.mongodb.org/mongo-driver/mongo/options/countoptions.go",
      "status": "modified",
      "additions": 27,
      "deletions": 14,
      "changes": 41,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/mongo/options/countoptions.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/mongo/options/countoptions.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.mongodb.org/mongo-driver/mongo/options/countoptions.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "2165bf85c774d3d5739878ebc7d4cacbd33fbf57",
      "filename": "backend/vendor/go.mongodb.org/mongo-driver/mongo/options/datakeyoptions.go",
      "status": "added",
      "additions": 55,
      "deletions": 0,
      "changes": 55,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/mongo/options/datakeyoptions.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/mongo/options/datakeyoptions.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.mongodb.org/mongo-driver/mongo/options/datakeyoptions.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "900da51c1df9d5f27030b61eb3011cf15f2f553d",
      "filename": "backend/vendor/go.mongodb.org/mongo-driver/mongo/options/dboptions.go",
      "status": "modified",
      "additions": 22,
      "deletions": 11,
      "changes": 33,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/mongo/options/dboptions.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/mongo/options/dboptions.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.mongodb.org/mongo-driver/mongo/options/dboptions.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "b96ff2ae577d6ed6efb32538c2d7b85ee038e15c",
      "filename": "backend/vendor/go.mongodb.org/mongo-driver/mongo/options/deleteoptions.go",
      "status": "modified",
      "additions": 8,
      "deletions": 6,
      "changes": 14,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/mongo/options/deleteoptions.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/mongo/options/deleteoptions.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.mongodb.org/mongo-driver/mongo/options/deleteoptions.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "40c19c463d08b7d9a8971438b06479413d32d121",
      "filename": "backend/vendor/go.mongodb.org/mongo-driver/mongo/options/distinctoptions.go",
      "status": "modified",
      "additions": 14,
      "deletions": 8,
      "changes": 22,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/mongo/options/distinctoptions.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/mongo/options/distinctoptions.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.mongodb.org/mongo-driver/mongo/options/distinctoptions.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "8a7d797b297453dc2e39ae04eb5a8af9a709d25f",
      "filename": "backend/vendor/go.mongodb.org/mongo-driver/mongo/options/encryptoptions.go",
      "status": "added",
      "additions": 64,
      "deletions": 0,
      "changes": 64,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/mongo/options/encryptoptions.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/mongo/options/encryptoptions.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.mongodb.org/mongo-driver/mongo/options/encryptoptions.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "2b997f4617a8971066f86bd02daf9424b43e2f9e",
      "filename": "backend/vendor/go.mongodb.org/mongo-driver/mongo/options/estimatedcountoptions.go",
      "status": "modified",
      "additions": 8,
      "deletions": 6,
      "changes": 14,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/mongo/options/estimatedcountoptions.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/mongo/options/estimatedcountoptions.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.mongodb.org/mongo-driver/mongo/options/estimatedcountoptions.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "234b7ccec3741f7aa223ed70956adedaad671114",
      "filename": "backend/vendor/go.mongodb.org/mongo-driver/mongo/options/findoptions.go",
      "status": "modified",
      "additions": 297,
      "deletions": 138,
      "changes": 435,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/mongo/options/findoptions.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/mongo/options/findoptions.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.mongodb.org/mongo-driver/mongo/options/findoptions.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "017c952248dd7dd995f02658ff0911ad553fac80",
      "filename": "backend/vendor/go.mongodb.org/mongo-driver/mongo/options/gridfsoptions.go",
      "status": "modified",
      "additions": 86,
      "deletions": 54,
      "changes": 140,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/mongo/options/gridfsoptions.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/mongo/options/gridfsoptions.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.mongodb.org/mongo-driver/mongo/options/gridfsoptions.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "123eb75f3d789474dae35b4da1fdbdf247f35e6f",
      "filename": "backend/vendor/go.mongodb.org/mongo-driver/mongo/options/indexoptions.go",
      "status": "modified",
      "additions": 128,
      "deletions": 67,
      "changes": 195,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/mongo/options/indexoptions.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/mongo/options/indexoptions.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.mongodb.org/mongo-driver/mongo/options/indexoptions.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "368a63e37d8aaec2f36f8666369ddc541248374f",
      "filename": "backend/vendor/go.mongodb.org/mongo-driver/mongo/options/insertoptions.go",
      "status": "modified",
      "additions": 24,
      "deletions": 15,
      "changes": 39,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/mongo/options/insertoptions.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/mongo/options/insertoptions.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.mongodb.org/mongo-driver/mongo/options/insertoptions.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "aa603c1d8920e96b2f72361fd9f2aaa9aec4f814",
      "filename": "backend/vendor/go.mongodb.org/mongo-driver/mongo/options/listcollectionsoptions.go",
      "status": "modified",
      "additions": 7,
      "deletions": 6,
      "changes": 13,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/mongo/options/listcollectionsoptions.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/mongo/options/listcollectionsoptions.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.mongodb.org/mongo-driver/mongo/options/listcollectionsoptions.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "9dc3305f97bdd1599a6320009e39da4ccbaf7d5d",
      "filename": "backend/vendor/go.mongodb.org/mongo-driver/mongo/options/listdatabasesoptions.go",
      "status": "modified",
      "additions": 8,
      "deletions": 6,
      "changes": 14,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/mongo/options/listdatabasesoptions.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/mongo/options/listdatabasesoptions.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.mongodb.org/mongo-driver/mongo/options/listdatabasesoptions.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "eac6771efe5d49c1ba094d1c93f941064265e02a",
      "filename": "backend/vendor/go.mongodb.org/mongo-driver/mongo/options/replaceoptions.go",
      "status": "modified",
      "additions": 21,
      "deletions": 11,
      "changes": 32,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/mongo/options/replaceoptions.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/mongo/options/replaceoptions.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.mongodb.org/mongo-driver/mongo/options/replaceoptions.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "ce2ec728d40bcce0ab7fae95e7f4f14986939f2e",
      "filename": "backend/vendor/go.mongodb.org/mongo-driver/mongo/options/runcmdoptions.go",
      "status": "modified",
      "additions": 7,
      "deletions": 5,
      "changes": 12,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/mongo/options/runcmdoptions.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/mongo/options/runcmdoptions.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.mongodb.org/mongo-driver/mongo/options/runcmdoptions.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "0ea393faf91b9cce6bc7ef0d4c317426abdb14d4",
      "filename": "backend/vendor/go.mongodb.org/mongo-driver/mongo/options/sessionoptions.go",
      "status": "modified",
      "additions": 28,
      "deletions": 13,
      "changes": 41,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/mongo/options/sessionoptions.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/mongo/options/sessionoptions.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.mongodb.org/mongo-driver/mongo/options/sessionoptions.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "a42ddfbb8bbd346c6821fc1be2405f6c95c4dbe4",
      "filename": "backend/vendor/go.mongodb.org/mongo-driver/mongo/options/transactionoptions.go",
      "status": "modified",
      "additions": 27,
      "deletions": 12,
      "changes": 39,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/mongo/options/transactionoptions.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/mongo/options/transactionoptions.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.mongodb.org/mongo-driver/mongo/options/transactionoptions.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "1734a1586735654236955c6e777bc4badf308981",
      "filename": "backend/vendor/go.mongodb.org/mongo-driver/mongo/options/updateoptions.go",
      "status": "modified",
      "additions": 26,
      "deletions": 14,
      "changes": 40,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/mongo/options/updateoptions.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/mongo/options/updateoptions.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.mongodb.org/mongo-driver/mongo/options/updateoptions.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "ce235ba8f25451e9cbbdf219bf5ac535eee83648",
      "filename": "backend/vendor/go.mongodb.org/mongo-driver/mongo/readconcern/readconcern.go",
      "status": "modified",
      "additions": 5,
      "deletions": 0,
      "changes": 5,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/mongo/readconcern/readconcern.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/mongo/readconcern/readconcern.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.mongodb.org/mongo-driver/mongo/readconcern/readconcern.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "8275f623ed26126ab20a71c8026ab5f0ccbb3410",
      "filename": "backend/vendor/go.mongodb.org/mongo-driver/mongo/results.go",
      "status": "modified",
      "additions": 35,
      "deletions": 31,
      "changes": 66,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/mongo/results.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/mongo/results.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.mongodb.org/mongo-driver/mongo/results.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "8136122e8916095a90de1e2d0b0e9b163413072a",
      "filename": "backend/vendor/go.mongodb.org/mongo-driver/mongo/session.go",
      "status": "modified",
      "additions": 83,
      "deletions": 33,
      "changes": 116,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/mongo/session.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/mongo/session.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.mongodb.org/mongo-driver/mongo/session.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "c693dfae25c04443cfc60b025329695bed95a653",
      "filename": "backend/vendor/go.mongodb.org/mongo-driver/mongo/single_result.go",
      "status": "modified",
      "additions": 17,
      "deletions": 15,
      "changes": 32,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/mongo/single_result.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/mongo/single_result.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.mongodb.org/mongo-driver/mongo/single_result.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "0cb521e0379e11f8dd7c6b43ee976c36b088fc15",
      "filename": "backend/vendor/go.mongodb.org/mongo-driver/version/version.go",
      "status": "modified",
      "additions": 1,
      "deletions": 1,
      "changes": 2,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/version/version.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/version/version.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.mongodb.org/mongo-driver/version/version.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "2a20d2e075c088c3e1d3625a00a97d4740367419",
      "filename": "backend/vendor/go.mongodb.org/mongo-driver/x/bsonx/bsoncore/bsoncore.go",
      "status": "modified",
      "additions": 10,
      "deletions": 2,
      "changes": 12,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/x/bsonx/bsoncore/bsoncore.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/x/bsonx/bsoncore/bsoncore.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.mongodb.org/mongo-driver/x/bsonx/bsoncore/bsoncore.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "7328a867147718a833af6f85903c2b7012e00399",
      "filename": "backend/vendor/go.mongodb.org/mongo-driver/x/bsonx/primitive_codecs.go",
      "status": "modified",
      "additions": 8,
      "deletions": 8,
      "changes": 16,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/x/bsonx/primitive_codecs.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/x/bsonx/primitive_codecs.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.mongodb.org/mongo-driver/x/bsonx/primitive_codecs.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "51a869bf7e1e55f05533a56bcd1806d129415080",
      "filename": "backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/auth/auth.go",
      "status": "modified",
      "additions": 46,
      "deletions": 29,
      "changes": 75,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/auth/auth.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/auth/auth.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/auth/auth.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "37168d427d80a482eaf0da60fa2531a8bbdd9066",
      "filename": "backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/batch_cursor.go",
      "status": "modified",
      "additions": 4,
      "deletions": 0,
      "changes": 4,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/batch_cursor.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/batch_cursor.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/batch_cursor.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "f376b6d35d65c4c05cede00aa0753c57d2c4f50e",
      "filename": "backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/batches.go",
      "status": "modified",
      "additions": 11,
      "deletions": 6,
      "changes": 17,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/batches.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/batches.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/batches.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "39d490a9874d48a3080e442d8487f98545806de0",
      "filename": "backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/compression.go",
      "status": "added",
      "additions": 105,
      "deletions": 0,
      "changes": 105,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/compression.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/compression.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/compression.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "f137e3719a4844de8feb7b888a6bededf659fc7a",
      "filename": "backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/connstring/connstring.go",
      "status": "modified",
      "additions": 66,
      "deletions": 0,
      "changes": 66,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/connstring/connstring.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/connstring/connstring.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/connstring/connstring.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "e8c8c5460d51ce4464f5d555181b007103e929c1",
      "filename": "backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/crypt.go",
      "status": "added",
      "additions": 333,
      "deletions": 0,
      "changes": 333,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/crypt.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/crypt.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/crypt.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "076bfe16da7b1aaba3574ae0baecf4cdb4469f69",
      "filename": "backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/description/server_selector.go",
      "status": "modified",
      "additions": 7,
      "deletions": 0,
      "changes": 7,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/description/server_selector.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/description/server_selector.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/description/server_selector.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "b6f4e3ea31d44f09e431ec9939b431641bc8d18f",
      "filename": "backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/description/topology.go",
      "status": "modified",
      "additions": 30,
      "deletions": 73,
      "changes": 103,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/description/topology.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/description/topology.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/description/topology.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "e89f6dabf7add73d290eb24a69dab39f9d80b840",
      "filename": "backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/dns/dns.go",
      "status": "modified",
      "additions": 3,
      "deletions": 1,
      "changes": 4,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/dns/dns.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/dns/dns.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/dns/dns.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "41ae11dab9267a06a2068accdbb6ebffaaa54a27",
      "filename": "backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/driver.go",
      "status": "modified",
      "additions": 26,
      "deletions": 10,
      "changes": 36,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/driver.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/driver.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/driver.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "905f2ee82c30eb0c77f6314e4ec05022110a514a",
      "filename": "backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/errors.go",
      "status": "modified",
      "additions": 4,
      "deletions": 0,
      "changes": 4,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/errors.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/errors.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/errors.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "3794ee5c95c155b1bb0bcc54f793133d69d53a29",
      "filename": "backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/mongocrypt/binary.go",
      "status": "added",
      "additions": 55,
      "deletions": 0,
      "changes": 55,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/mongocrypt/binary.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/mongocrypt/binary.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/mongocrypt/binary.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "8c235c5bfa2de167ca5a78a0efc84705fe7434cc",
      "filename": "backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/mongocrypt/errors.go",
      "status": "added",
      "additions": 43,
      "deletions": 0,
      "changes": 43,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/mongocrypt/errors.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/mongocrypt/errors.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/mongocrypt/errors.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "816099abc95464d44377a58df225edd3ed7e8398",
      "filename": "backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/mongocrypt/errors_not_enabled.go",
      "status": "added",
      "additions": 20,
      "deletions": 0,
      "changes": 20,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/mongocrypt/errors_not_enabled.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/mongocrypt/errors_not_enabled.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/mongocrypt/errors_not_enabled.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "4fc1daf152051c43c1b3cca3d119bc15e40a30ef",
      "filename": "backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/mongocrypt/mongocrypt.go",
      "status": "added",
      "additions": 292,
      "deletions": 0,
      "changes": 292,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/mongocrypt/mongocrypt.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/mongocrypt/mongocrypt.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/mongocrypt/mongocrypt.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "dc0dd51455b648200feefe17801cca57a2864330",
      "filename": "backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/mongocrypt/mongocrypt_context.go",
      "status": "added",
      "additions": 103,
      "deletions": 0,
      "changes": 103,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/mongocrypt/mongocrypt_context.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/mongocrypt/mongocrypt_context.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/mongocrypt/mongocrypt_context.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "b0ff1689203aba211fb7a1281a87fc4ed86d9bd2",
      "filename": "backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/mongocrypt/mongocrypt_context_not_enabled.go",
      "status": "added",
      "additions": 56,
      "deletions": 0,
      "changes": 56,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/mongocrypt/mongocrypt_context_not_enabled.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/mongocrypt/mongocrypt_context_not_enabled.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/mongocrypt/mongocrypt_context_not_enabled.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "77ce8308b5de20d8ae53e34cffff4a2556ab0acc",
      "filename": "backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/mongocrypt/mongocrypt_kms_context.go",
      "status": "added",
      "additions": 69,
      "deletions": 0,
      "changes": 69,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/mongocrypt/mongocrypt_kms_context.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/mongocrypt/mongocrypt_kms_context.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/mongocrypt/mongocrypt_kms_context.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "8026a1902c72bc63aad2a754a5fc8d50d04cdaf2",
      "filename": "backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/mongocrypt/mongocrypt_kms_context_not_enabled.go",
      "status": "added",
      "additions": 38,
      "deletions": 0,
      "changes": 38,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/mongocrypt/mongocrypt_kms_context_not_enabled.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/mongocrypt/mongocrypt_kms_context_not_enabled.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/mongocrypt/mongocrypt_kms_context_not_enabled.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "d91cc674618c9a1baecc3d1c8ddd4c657a6fedd3",
      "filename": "backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/mongocrypt/mongocrypt_not_enabled.go",
      "status": "added",
      "additions": 54,
      "deletions": 0,
      "changes": 54,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/mongocrypt/mongocrypt_not_enabled.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/mongocrypt/mongocrypt_not_enabled.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/mongocrypt/mongocrypt_not_enabled.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "0f7f43b931ba8ed695a6c6188254384855cbc0e3",
      "filename": "backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/mongocrypt/options/mongocrypt_context_options.go",
      "status": "added",
      "additions": 65,
      "deletions": 0,
      "changes": 65,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/mongocrypt/options/mongocrypt_context_options.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/mongocrypt/options/mongocrypt_context_options.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/mongocrypt/options/mongocrypt_context_options.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "09cae44ce80fe616457e85b21b87cce5b38bcd94",
      "filename": "backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/mongocrypt/options/mongocrypt_options.go",
      "status": "added",
      "additions": 41,
      "deletions": 0,
      "changes": 41,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/mongocrypt/options/mongocrypt_options.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/mongocrypt/options/mongocrypt_options.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/mongocrypt/options/mongocrypt_options.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "63daa4c59a4fa17ece0f8417008b53eaed587df5",
      "filename": "backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/mongocrypt/options/provider_options.go",
      "status": "added",
      "additions": 46,
      "deletions": 0,
      "changes": 46,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/mongocrypt/options/provider_options.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/mongocrypt/options/provider_options.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/mongocrypt/options/provider_options.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "c745088bdbd9c2ee1ebb9d6dc0baef18847a243c",
      "filename": "backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/mongocrypt/state.go",
      "status": "added",
      "additions": 43,
      "deletions": 0,
      "changes": 43,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/mongocrypt/state.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/mongocrypt/state.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/mongocrypt/state.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "82dd10dfc11348ee5258e8388b954b66d7424bab",
      "filename": "backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/operation.go",
      "status": "modified",
      "additions": 97,
      "deletions": 37,
      "changes": 134,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/operation.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/operation.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/operation.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "c5c50dab67ef43a26c66a5fca0b487413f432549",
      "filename": "backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/operation/abort_transaction.go",
      "status": "modified",
      "additions": 12,
      "deletions": 0,
      "changes": 12,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/operation/abort_transaction.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/operation/abort_transaction.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/operation/abort_transaction.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "6d6705bf07217c885bdbc3edc0bce93a3f38c39f",
      "filename": "backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/operation/aggregate.go",
      "status": "modified",
      "additions": 12,
      "deletions": 0,
      "changes": 12,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/operation/aggregate.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/operation/aggregate.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/operation/aggregate.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "c5d855f9f99e56495420725b02770d0edfb2cd55",
      "filename": "backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/operation/command.go",
      "status": "modified",
      "additions": 12,
      "deletions": 0,
      "changes": 12,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/operation/command.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/operation/command.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/operation/command.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "0e376a0adcd074d244194620c5d1d4314c383b18",
      "filename": "backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/operation/commit_transaction.go",
      "status": "modified",
      "additions": 12,
      "deletions": 0,
      "changes": 12,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/operation/commit_transaction.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/operation/commit_transaction.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/operation/commit_transaction.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "de09ef151bd947b3a3c4d6511d0a789c1f1cf896",
      "filename": "backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/operation/count.go",
      "status": "modified",
      "additions": 12,
      "deletions": 0,
      "changes": 12,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/operation/count.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/operation/count.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/operation/count.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "e4f6add8aeb64c62b85971a2c5468a32eea47d90",
      "filename": "backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/operation/createIndexes.go",
      "status": "modified",
      "additions": 35,
      "deletions": 10,
      "changes": 45,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/operation/createIndexes.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/operation/createIndexes.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/operation/createIndexes.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "dcb259a4ccfe01ee3c5b23826f5cd2f5846e404d",
      "filename": "backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/operation/createIndexes.toml",
      "status": "modified",
      "additions": 3,
      "deletions": 0,
      "changes": 3,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/operation/createIndexes.toml",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/operation/createIndexes.toml",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/operation/createIndexes.toml?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "d53e7d1ffe47cc76df1b663ff6c351f26446f6ac",
      "filename": "backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/operation/delete.go",
      "status": "modified",
      "additions": 12,
      "deletions": 0,
      "changes": 12,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/operation/delete.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/operation/delete.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/operation/delete.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "768b52234574b4a68f4d3d02304b1b6ac8295e99",
      "filename": "backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/operation/distinct.go",
      "status": "modified",
      "additions": 12,
      "deletions": 0,
      "changes": 12,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/operation/distinct.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/operation/distinct.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/operation/distinct.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "11491757ab01b0290b13aef595b505c55327b05e",
      "filename": "backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/operation/drop_collection.go",
      "status": "modified",
      "additions": 12,
      "deletions": 0,
      "changes": 12,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/operation/drop_collection.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/operation/drop_collection.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/operation/drop_collection.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "5b895dfcaa2ba44fc3b8c7bab6cbc96d4ea0e5ae",
      "filename": "backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/operation/drop_database.go",
      "status": "modified",
      "additions": 12,
      "deletions": 0,
      "changes": 12,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/operation/drop_database.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/operation/drop_database.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/operation/drop_database.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "fe222eb7d1fe708998ab8cd3d7732280728210af",
      "filename": "backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/operation/drop_indexes.go",
      "status": "modified",
      "additions": 12,
      "deletions": 0,
      "changes": 12,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/operation/drop_indexes.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/operation/drop_indexes.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/operation/drop_indexes.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "596e7a238ce1564e17f4ebce5b58c8b3bb8b4be0",
      "filename": "backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/operation/end_sessions.go",
      "status": "modified",
      "additions": 12,
      "deletions": 0,
      "changes": 12,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/operation/end_sessions.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/operation/end_sessions.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/operation/end_sessions.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "e2ebf987b385f7ef7e4544e512043c6ad0bcf1a6",
      "filename": "backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/operation/find.go",
      "status": "modified",
      "additions": 12,
      "deletions": 0,
      "changes": 12,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/operation/find.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/operation/find.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/operation/find.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "b1c63ba3ad28348aedf30700d045cdd96603bc44",
      "filename": "backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/operation/find_and_modify.go",
      "status": "modified",
      "additions": 12,
      "deletions": 0,
      "changes": 12,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/operation/find_and_modify.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/operation/find_and_modify.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/operation/find_and_modify.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "378a1394e4d0f3f98526bbcd8075830caa4a6878",
      "filename": "backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/operation/insert.go",
      "status": "modified",
      "additions": 12,
      "deletions": 0,
      "changes": 12,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/operation/insert.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/operation/insert.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/operation/insert.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "e894fcd8db1d207aff4e8ef012412e7a57c2c8e7",
      "filename": "backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/operation/ismaster.go",
      "status": "modified",
      "additions": 11,
      "deletions": 2,
      "changes": 13,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/operation/ismaster.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/operation/ismaster.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/operation/ismaster.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "e96e1d8fc30daca3d190aa1fbc51328370087da3",
      "filename": "backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/operation/listDatabases.go",
      "status": "modified",
      "additions": 12,
      "deletions": 0,
      "changes": 12,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/operation/listDatabases.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/operation/listDatabases.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/operation/listDatabases.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "793d3cfc075314fe47164bb8fc4ec8784913d607",
      "filename": "backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/operation/list_collections.go",
      "status": "modified",
      "additions": 12,
      "deletions": 0,
      "changes": 12,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/operation/list_collections.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/operation/list_collections.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/operation/list_collections.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "98870585811a1e28ba70e3ddc5b612cd098a12b6",
      "filename": "backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/operation/list_indexes.go",
      "status": "modified",
      "additions": 12,
      "deletions": 0,
      "changes": 12,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/operation/list_indexes.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/operation/list_indexes.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/operation/list_indexes.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "e0ce6010ec98331c067b7b516712e4ddd6153102",
      "filename": "backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/operation/update.go",
      "status": "modified",
      "additions": 12,
      "deletions": 0,
      "changes": 12,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/operation/update.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/operation/update.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/operation/update.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "7e62340a8d7c650bd1af13d8281849124e4eb3f3",
      "filename": "backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/operation_legacy.go",
      "status": "modified",
      "additions": 8,
      "deletions": 6,
      "changes": 14,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/operation_legacy.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/operation_legacy.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/operation_legacy.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "a0d0d3bff66ad7cf30ae6541cd3b9cfa55cedcad",
      "filename": "backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/session/client_session.go",
      "status": "modified",
      "additions": 3,
      "deletions": 2,
      "changes": 5,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/session/client_session.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/session/client_session.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/session/client_session.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "3d1c6bee46834a96f7bba2eae2cf8554fcdb3c0e",
      "filename": "backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/topology/connection.go",
      "status": "modified",
      "additions": 125,
      "deletions": 101,
      "changes": 226,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/topology/connection.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/topology/connection.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/topology/connection.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "39efbd8b9f34bf4355ad61a60f69e57c2ac94995",
      "filename": "backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/topology/connection_options.go",
      "status": "modified",
      "additions": 11,
      "deletions": 6,
      "changes": 17,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/topology/connection_options.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/topology/connection_options.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/topology/connection_options.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "838d561019ac213d83e96de1d931387c8c226ec1",
      "filename": "backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/topology/pool.go",
      "status": "modified",
      "additions": 29,
      "deletions": 6,
      "changes": 35,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/topology/pool.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/topology/pool.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/topology/pool.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "e3436f3e1823dbf07a26c9337c8b8f9992ac755d",
      "filename": "backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/topology/server.go",
      "status": "modified",
      "additions": 64,
      "deletions": 21,
      "changes": 85,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/topology/server.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/topology/server.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/topology/server.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "7a4790005a76194d504d4099174a64adb635fcdd",
      "filename": "backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/topology/server_options.go",
      "status": "modified",
      "additions": 9,
      "deletions": 1,
      "changes": 10,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/topology/server_options.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/topology/server_options.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/topology/server_options.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "79124a986a97b3ec20a37e4b3f0bd8f9e2bb9adf",
      "filename": "backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/topology/topology.go",
      "status": "modified",
      "additions": 129,
      "deletions": 77,
      "changes": 206,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/topology/topology.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/topology/topology.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/topology/topology.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "e54e9eb8ae3d95fa719d3860583fc5ced65b0643",
      "filename": "backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/topology/topology_options.go",
      "status": "modified",
      "additions": 18,
      "deletions": 3,
      "changes": 21,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/topology/topology_options.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/topology/topology_options.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/topology/topology_options.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "877a9be48c5e878c71f7ba36a155253c31563bb8",
      "filename": "backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/wiremessage/wiremessage.go",
      "status": "modified",
      "additions": 8,
      "deletions": 2,
      "changes": 10,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/wiremessage/wiremessage.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/wiremessage/wiremessage.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/wiremessage/wiremessage.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "8cfd6063e7299bf1e4e1786678254557b4d79af2",
      "filename": "backend/vendor/golang.org/x/oauth2/README.md",
      "status": "modified",
      "additions": 7,
      "deletions": 6,
      "changes": 13,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/golang.org/x/oauth2/README.md",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/golang.org/x/oauth2/README.md",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/golang.org/x/oauth2/README.md?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "90657915fbcf0ea51e8d97858432d793060de436",
      "filename": "backend/vendor/golang.org/x/oauth2/transport.go",
      "status": "modified",
      "additions": 12,
      "deletions": 67,
      "changes": 79,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/golang.org/x/oauth2/transport.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/golang.org/x/oauth2/transport.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/golang.org/x/oauth2/transport.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "ab433ccfbb442b37623957515251beeb7abaf80d",
      "filename": "backend/vendor/golang.org/x/sys/unix/README.md",
      "status": "modified",
      "additions": 11,
      "deletions": 0,
      "changes": 11,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/golang.org/x/sys/unix/README.md",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/golang.org/x/sys/unix/README.md",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/golang.org/x/sys/unix/README.md?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "761db66efece2fcc791b5e39a4172a0c51a652bf",
      "filename": "backend/vendor/golang.org/x/sys/unix/errors_freebsd_386.go",
      "status": "modified",
      "additions": 6,
      "deletions": 0,
      "changes": 6,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/golang.org/x/sys/unix/errors_freebsd_386.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/golang.org/x/sys/unix/errors_freebsd_386.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/golang.org/x/sys/unix/errors_freebsd_386.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "070f44b651048ab08531581c1abeaa0f79b965ba",
      "filename": "backend/vendor/golang.org/x/sys/unix/errors_freebsd_amd64.go",
      "status": "modified",
      "additions": 6,
      "deletions": 0,
      "changes": 6,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/golang.org/x/sys/unix/errors_freebsd_amd64.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/golang.org/x/sys/unix/errors_freebsd_amd64.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/golang.org/x/sys/unix/errors_freebsd_amd64.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "946dcf3fc7ecab145f43f366836a1f037013552b",
      "filename": "backend/vendor/golang.org/x/sys/unix/errors_freebsd_arm64.go",
      "status": "added",
      "additions": 17,
      "deletions": 0,
      "changes": 17,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/golang.org/x/sys/unix/errors_freebsd_arm64.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/golang.org/x/sys/unix/errors_freebsd_arm64.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/golang.org/x/sys/unix/errors_freebsd_arm64.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "ece31e9dcdcbfaa64e615580e6aa5ad24fac6585",
      "filename": "backend/vendor/golang.org/x/sys/unix/mkall.sh",
      "status": "modified",
      "additions": 12,
      "deletions": 1,
      "changes": 13,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/golang.org/x/sys/unix/mkall.sh",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/golang.org/x/sys/unix/mkall.sh",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/golang.org/x/sys/unix/mkall.sh?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "bc076cf6291f82f928802a937151c429a3f87580",
      "filename": "backend/vendor/golang.org/x/sys/unix/mkerrors.sh",
      "status": "modified",
      "additions": 8,
      "deletions": 1,
      "changes": 9,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/golang.org/x/sys/unix/mkerrors.sh",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/golang.org/x/sys/unix/mkerrors.sh",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/golang.org/x/sys/unix/mkerrors.sh?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "6932e7c2c1a520cd773a5829aae2d96b025709b0",
      "filename": "backend/vendor/golang.org/x/sys/unix/syscall_freebsd.go",
      "status": "modified",
      "additions": 0,
      "deletions": 4,
      "changes": 4,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/golang.org/x/sys/unix/syscall_freebsd.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/golang.org/x/sys/unix/syscall_freebsd.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/golang.org/x/sys/unix/syscall_freebsd.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "72a506ddcb5d7cae1c764d02b3b5acf519a05762",
      "filename": "backend/vendor/golang.org/x/sys/unix/syscall_freebsd_386.go",
      "status": "modified",
      "additions": 4,
      "deletions": 0,
      "changes": 4,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/golang.org/x/sys/unix/syscall_freebsd_386.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/golang.org/x/sys/unix/syscall_freebsd_386.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/golang.org/x/sys/unix/syscall_freebsd_386.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "d5e376acaeac269372cdbf98d936f4006c973167",
      "filename": "backend/vendor/golang.org/x/sys/unix/syscall_freebsd_amd64.go",
      "status": "modified",
      "additions": 4,
      "deletions": 0,
      "changes": 4,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/golang.org/x/sys/unix/syscall_freebsd_amd64.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/golang.org/x/sys/unix/syscall_freebsd_amd64.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/golang.org/x/sys/unix/syscall_freebsd_amd64.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "99e62dcd828a245f31d26864fa996c6eba44b13a",
      "filename": "backend/vendor/golang.org/x/sys/unix/syscall_illumos.go",
      "status": "added",
      "additions": 57,
      "deletions": 0,
      "changes": 57,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/golang.org/x/sys/unix/syscall_illumos.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/golang.org/x/sys/unix/syscall_illumos.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/golang.org/x/sys/unix/syscall_illumos.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "bbe1abbcee12373a7fdf9d9d3ab3574c80d93346",
      "filename": "backend/vendor/golang.org/x/sys/unix/syscall_linux.go",
      "status": "modified",
      "additions": 84,
      "deletions": 17,
      "changes": 101,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/golang.org/x/sys/unix/syscall_linux.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/golang.org/x/sys/unix/syscall_linux.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/golang.org/x/sys/unix/syscall_linux.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "af77e6e25eb74eba6e199d032090d2369c22b153",
      "filename": "backend/vendor/golang.org/x/sys/unix/syscall_linux_mips64x.go",
      "status": "modified",
      "additions": 4,
      "deletions": 0,
      "changes": 4,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/golang.org/x/sys/unix/syscall_linux_mips64x.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/golang.org/x/sys/unix/syscall_linux_mips64x.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/golang.org/x/sys/unix/syscall_linux_mips64x.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "8f710d01400d1894026031eb2568d5b5ace0a267",
      "filename": "backend/vendor/golang.org/x/sys/unix/syscall_unix.go",
      "status": "modified",
      "additions": 1,
      "deletions": 1,
      "changes": 2,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/golang.org/x/sys/unix/syscall_unix.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/golang.org/x/sys/unix/syscall_unix.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/golang.org/x/sys/unix/syscall_unix.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "84824587346983c50e7e0c91d23300dfbd6b1638",
      "filename": "backend/vendor/golang.org/x/sys/unix/zerrors_freebsd_386.go",
      "status": "modified",
      "additions": 148,
      "deletions": 12,
      "changes": 160,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/golang.org/x/sys/unix/zerrors_freebsd_386.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/golang.org/x/sys/unix/zerrors_freebsd_386.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/golang.org/x/sys/unix/zerrors_freebsd_386.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "4acd101c3eeba9b89336641333a78d43c52438a2",
      "filename": "backend/vendor/golang.org/x/sys/unix/zerrors_freebsd_amd64.go",
      "status": "modified",
      "additions": 146,
      "deletions": 12,
      "changes": 158,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/golang.org/x/sys/unix/zerrors_freebsd_amd64.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/golang.org/x/sys/unix/zerrors_freebsd_amd64.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/golang.org/x/sys/unix/zerrors_freebsd_amd64.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "e4719873b9ef75a3583414cf00f7a182e8b59a23",
      "filename": "backend/vendor/golang.org/x/sys/unix/zerrors_freebsd_arm.go",
      "status": "modified",
      "additions": 16,
      "deletions": 0,
      "changes": 16,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/golang.org/x/sys/unix/zerrors_freebsd_arm.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/golang.org/x/sys/unix/zerrors_freebsd_arm.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/golang.org/x/sys/unix/zerrors_freebsd_arm.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "5e49769d96a87fd58a7fa4258b19178e5998e8d6",
      "filename": "backend/vendor/golang.org/x/sys/unix/zerrors_freebsd_arm64.go",
      "status": "modified",
      "additions": 147,
      "deletions": 12,
      "changes": 159,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/golang.org/x/sys/unix/zerrors_freebsd_arm64.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/golang.org/x/sys/unix/zerrors_freebsd_arm64.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/golang.org/x/sys/unix/zerrors_freebsd_arm64.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "84c599c5249fb2114501655d2ccd09a25e7e3367",
      "filename": "backend/vendor/golang.org/x/sys/unix/zerrors_linux.go",
      "status": "added",
      "additions": 2454,
      "deletions": 0,
      "changes": 2454,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/golang.org/x/sys/unix/zerrors_linux.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/golang.org/x/sys/unix/zerrors_linux.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/golang.org/x/sys/unix/zerrors_linux.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "0876cf92ff3f2de5f31eca5175cd4bb75ca690b4",
      "filename": "backend/vendor/golang.org/x/sys/unix/zerrors_linux_386.go",
      "status": "modified",
      "additions": 462,
      "deletions": 2882,
      "changes": 3344,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/golang.org/x/sys/unix/zerrors_linux_386.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/golang.org/x/sys/unix/zerrors_linux_386.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/golang.org/x/sys/unix/zerrors_linux_386.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "d5be2e83770769c98e7c4ef2901d184fd2b0c4eb",
      "filename": "backend/vendor/golang.org/x/sys/unix/zerrors_linux_amd64.go",
      "status": "modified",
      "additions": 462,
      "deletions": 2882,
      "changes": 3344,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/golang.org/x/sys/unix/zerrors_linux_amd64.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/golang.org/x/sys/unix/zerrors_linux_amd64.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/golang.org/x/sys/unix/zerrors_linux_amd64.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "fbeef8325232a1ea6974a838215c41c9a81c31d0",
      "filename": "backend/vendor/golang.org/x/sys/unix/zerrors_linux_arm.go",
      "status": "modified",
      "additions": 468,
      "deletions": 2888,
      "changes": 3356,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/golang.org/x/sys/unix/zerrors_linux_arm.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/golang.org/x/sys/unix/zerrors_linux_arm.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/golang.org/x/sys/unix/zerrors_linux_arm.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    },
    {
      "sha": "06daa50ebdcf970fcead444c10f92edbe601d90e",
      "filename": "backend/vendor/golang.org/x/sys/unix/zerrors_linux_arm64.go",
      "status": "modified",
      "additions": 455,
      "deletions": 2875,
      "changes": 3330,
      "blob_url": "https://github.com/umputun/remark42/blob/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/golang.org/x/sys/unix/zerrors_linux_arm64.go",
      "raw_url": "https://github.com/umputun/remark42/raw/890d7154e777d0673defcd11db3ef792473d68c9/backend/vendor/golang.org/x/sys/unix/zerrors_linux_arm64.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/golang.org/x/sys/unix/zerrors_linux_arm64.go?ref=890d7154e777d0673defcd11db3ef792473d68c9"
    }
  ]
}
