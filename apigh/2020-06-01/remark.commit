{
  "sha": "80f4862c9c2905475f3484f20ca41f7bdf103de9",
  "node_id": "MDY6Q29tbWl0MTE0ODI5NTAzOjgwZjQ4NjJjOWMyOTA1NDc1ZjM0ODRmMjBjYTQxZjdiZGYxMDNkZTk=",
  "commit": {
    "author": {
      "name": "Dmitry Verkhoturov",
      "email": "paskal.07@gmail.com",
      "date": "2020-05-24T23:55:22Z"
    },
    "committer": {
      "name": "Dmitry Verkhoturov",
      "email": "paskal.07@gmail.com",
      "date": "2020-06-01T07:58:48Z"
    },
    "message": "fix leaking goroutines and add tests for them",
    "tree": {
      "sha": "a9ae967f220503eaa9deda56d7f558149442cdf3",
      "url": "https://api.github.com/repos/umputun/remark42/git/trees/a9ae967f220503eaa9deda56d7f558149442cdf3"
    },
    "url": "https://api.github.com/repos/umputun/remark42/git/commits/80f4862c9c2905475f3484f20ca41f7bdf103de9",
    "comment_count": 0,
    "verification": {
      "verified": false,
      "reason": "unsigned",
      "signature": null,
      "payload": null
    }
  },
  "url": "https://api.github.com/repos/umputun/remark42/commits/80f4862c9c2905475f3484f20ca41f7bdf103de9",
  "html_url": "https://github.com/umputun/remark42/commit/80f4862c9c2905475f3484f20ca41f7bdf103de9",
  "comments_url": "https://api.github.com/repos/umputun/remark42/commits/80f4862c9c2905475f3484f20ca41f7bdf103de9/comments",
  "author": {
    "login": "paskal",
    "id": 712534,
    "node_id": "MDQ6VXNlcjcxMjUzNA==",
    "avatar_url": "https://avatars1.githubusercontent.com/u/712534?v=4",
    "gravatar_id": "",
    "url": "https://api.github.com/users/paskal",
    "html_url": "https://github.com/paskal",
    "followers_url": "https://api.github.com/users/paskal/followers",
    "following_url": "https://api.github.com/users/paskal/following{/other_user}",
    "gists_url": "https://api.github.com/users/paskal/gists{/gist_id}",
    "starred_url": "https://api.github.com/users/paskal/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/paskal/subscriptions",
    "organizations_url": "https://api.github.com/users/paskal/orgs",
    "repos_url": "https://api.github.com/users/paskal/repos",
    "events_url": "https://api.github.com/users/paskal/events{/privacy}",
    "received_events_url": "https://api.github.com/users/paskal/received_events",
    "type": "User",
    "site_admin": false
  },
  "committer": {
    "login": "paskal",
    "id": 712534,
    "node_id": "MDQ6VXNlcjcxMjUzNA==",
    "avatar_url": "https://avatars1.githubusercontent.com/u/712534?v=4",
    "gravatar_id": "",
    "url": "https://api.github.com/users/paskal",
    "html_url": "https://github.com/paskal",
    "followers_url": "https://api.github.com/users/paskal/followers",
    "following_url": "https://api.github.com/users/paskal/following{/other_user}",
    "gists_url": "https://api.github.com/users/paskal/gists{/gist_id}",
    "starred_url": "https://api.github.com/users/paskal/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/paskal/subscriptions",
    "organizations_url": "https://api.github.com/users/paskal/orgs",
    "repos_url": "https://api.github.com/users/paskal/repos",
    "events_url": "https://api.github.com/users/paskal/events{/privacy}",
    "received_events_url": "https://api.github.com/users/paskal/received_events",
    "type": "User",
    "site_admin": false
  },
  "parents": [
    {
      "sha": "55dd5bd25bbe0eaf820f09e7a361d1171477bc93",
      "url": "https://api.github.com/repos/umputun/remark42/commits/55dd5bd25bbe0eaf820f09e7a361d1171477bc93",
      "html_url": "https://github.com/umputun/remark42/commit/55dd5bd25bbe0eaf820f09e7a361d1171477bc93"
    }
  ],
  "stats": {
    "total": 9509,
    "additions": 9474,
    "deletions": 35
  },
  "files": [
    {
      "sha": "073f0fbbedb9241c81a0509f41798bc44d7b7a09",
      "filename": "backend/_example/memory_store/go.sum",
      "status": "modified",
      "additions": 6,
      "deletions": 0,
      "changes": 6,
      "blob_url": "https://github.com/umputun/remark42/blob/80f4862c9c2905475f3484f20ca41f7bdf103de9/backend/_example/memory_store/go.sum",
      "raw_url": "https://github.com/umputun/remark42/raw/80f4862c9c2905475f3484f20ca41f7bdf103de9/backend/_example/memory_store/go.sum",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/_example/memory_store/go.sum?ref=80f4862c9c2905475f3484f20ca41f7bdf103de9",
      "patch": "@@ -196,20 +196,23 @@ github.com/yuin/gopher-lua v0.0.0-20191220021717-ab39c6098bdb/go.mod h1:gqRgreBU\n go.etcd.io/bbolt v1.3.4 h1:hi1bXHMVrlQh6WwxAy+qZCV/SYIlqo+Ushwdpa4tAKg=\n go.etcd.io/bbolt v1.3.4/go.mod h1:G5EMThwa9y8QZGBClrRx5EY+Yw9kAhnjy3bSjsnlVTQ=\n go.mongodb.org/mongo-driver v1.3.2/go.mod h1:MSWZXKOynuguX+JSvwP8i+58jYCXxbia8HS3gZBapIE=\n+go.uber.org/goleak v1.0.0/go.mod h1:8a7PlsEVH3e/a/GLqe5IIrQx6GzcnRmZEufDUTk4A7A=\n golang.org/x/crypto v0.0.0-20180904163835-0709b304e793/go.mod h1:6SG95UA2DQfeDnfUPMdvaQW0Q7yPrPDi9nlGo2tz2b4=\n golang.org/x/crypto v0.0.0-20190308221718-c2843e01d9a2/go.mod h1:djNgcEr1/C05ACkg1iLfiJU5Ep61QUkGW8qpdssI0+w=\n golang.org/x/crypto v0.0.0-20190422162423-af44ce270edf/go.mod h1:WFFai1msRO1wXaEeE5yQxYXgSfI8pQAWXbQop6sCtWE=\n golang.org/x/crypto v0.0.0-20190530122614-20be4c3c3ed5/go.mod h1:yigFU9vqHzYiE8UmvKecakEJjdnWj3jj499lnFckfCI=\n golang.org/x/crypto v0.0.0-20200406173513-056763e48d71/go.mod h1:LzIPMQfyMNhhGPhUkYOs5KpL4U8rLKemX1yGLhDgUto=\n golang.org/x/image v0.0.0-20200119044424-58c23975cae1 h1:5h3ngYt7+vXCDZCup/HkCQgW5XwmSvR/nA2JmJ0RErg=\n golang.org/x/image v0.0.0-20200119044424-58c23975cae1/go.mod h1:FeLwcggjj3mMvU+oOTbSwawSJRM1uh48EjtB4UJZlP0=\n+golang.org/x/lint v0.0.0-20190930215403-16217165b5de/go.mod h1:6SW0HCj/g11FgYtHlgUYUwCkIfeOF89ocIRzGO/8vkc=\n golang.org/x/net v0.0.0-20180218175443-cbe0f9307d01/go.mod h1:mL1N/T3taQHkDXs73rZJwtUhF3w3ftmwwsq0BUmARs4=\n golang.org/x/net v0.0.0-20180724234803-3673e40ba225/go.mod h1:mL1N/T3taQHkDXs73rZJwtUhF3w3ftmwwsq0BUmARs4=\n golang.org/x/net v0.0.0-20180906233101-161cd47e91fd/go.mod h1:mL1N/T3taQHkDXs73rZJwtUhF3w3ftmwwsq0BUmARs4=\n golang.org/x/net v0.0.0-20181220203305-927f97764cc3/go.mod h1:mL1N/T3taQHkDXs73rZJwtUhF3w3ftmwwsq0BUmARs4=\n golang.org/x/net v0.0.0-20190108225652-1e06a53dbb7e/go.mod h1:mL1N/T3taQHkDXs73rZJwtUhF3w3ftmwwsq0BUmARs4=\n golang.org/x/net v0.0.0-20190311183353-d8887717615a/go.mod h1:t9HGtf8HONx5eT2rtn7q6eTqICYqUVnKs3thJo3Qplg=\n golang.org/x/net v0.0.0-20190404232315-eb5bcb51f2a3/go.mod h1:t9HGtf8HONx5eT2rtn7q6eTqICYqUVnKs3thJo3Qplg=\n+golang.org/x/net v0.0.0-20190620200207-3b0461eec859/go.mod h1:z5CRVTTTmAJ677TzLLGU+0bjPO0LkuOLi4/5GtJWs/s=\n golang.org/x/net v0.0.0-20190827160401-ba9fcec4b297/go.mod h1:z5CRVTTTmAJ677TzLLGU+0bjPO0LkuOLi4/5GtJWs/s=\n golang.org/x/net v0.0.0-20190923162816-aa69164e4478/go.mod h1:z5CRVTTTmAJ677TzLLGU+0bjPO0LkuOLi4/5GtJWs/s=\n golang.org/x/net v0.0.0-20200202094626-16171245cfb2/go.mod h1:z5CRVTTTmAJ677TzLLGU+0bjPO0LkuOLi4/5GtJWs/s=\n@@ -245,11 +248,14 @@ golang.org/x/time v0.0.0-20200416051211-89c76fbcd5d1 h1:NusfzzA6yGQ+ua51ck7E3omN\n golang.org/x/time v0.0.0-20200416051211-89c76fbcd5d1/go.mod h1:tRJNPiyCQ0inRvYxbN9jk5I+vvW/OXSQhTDSoE431IQ=\n golang.org/x/tools v0.0.0-20180917221912-90fa682c2a6e/go.mod h1:n7NCudcB/nEzxVGmLbDWY5pfWTLqBcC2KZ6jyYvM4mQ=\n golang.org/x/tools v0.0.0-20181030221726-6c7e314b6563/go.mod h1:n7NCudcB/nEzxVGmLbDWY5pfWTLqBcC2KZ6jyYvM4mQ=\n+golang.org/x/tools v0.0.0-20190311212946-11955173bddd/go.mod h1:LCzVGOaR6xXOjkQ3onu1FJEFr0SW1gC7cKk1uF8kGRs=\n golang.org/x/tools v0.0.0-20190328211700-ab21143f2384/go.mod h1:LCzVGOaR6xXOjkQ3onu1FJEFr0SW1gC7cKk1uF8kGRs=\n golang.org/x/tools v0.0.0-20190329151228-23e29df326fe/go.mod h1:LCzVGOaR6xXOjkQ3onu1FJEFr0SW1gC7cKk1uF8kGRs=\n golang.org/x/tools v0.0.0-20190416151739-9c9e1878f421/go.mod h1:LCzVGOaR6xXOjkQ3onu1FJEFr0SW1gC7cKk1uF8kGRs=\n golang.org/x/tools v0.0.0-20190420181800-aa740d480789/go.mod h1:LCzVGOaR6xXOjkQ3onu1FJEFr0SW1gC7cKk1uF8kGRs=\n golang.org/x/tools v0.0.0-20190531172133-b3315ee88b7d/go.mod h1:/rFqwRUd4F7ZHNgwSSTFct+R/Kf4OFW1sUzUTQQTgfc=\n+golang.org/x/tools v0.0.0-20191108193012-7d206e10da11/go.mod h1:b+2E5dAYhXwXZwtnZ6UAqBI28+e2cm9otk0dWdXHAEo=\n+golang.org/x/xerrors v0.0.0-20190717185122-a985d3407aa7/go.mod h1:I/5z698sn9Ka8TeJc9MKroUUfqBBauWjQqLJ2OPfmY0=\n google.golang.org/appengine v1.4.0/go.mod h1:xpcJRLb0r/rnEns0DIKYYv+WjYCduHsrkT7/EB5XEv4=\n gopkg.in/check.v1 v0.0.0-20161208181325-20d25e280405/go.mod h1:Co6ibVJAznAaIkqp8huTwlJQCZ016jof/cbN4VW5Yz0=\n gopkg.in/check.v1 v1.0.0-20180628173108-788fd7840127/go.mod h1:Co6ibVJAznAaIkqp8huTwlJQCZ016jof/cbN4VW5Yz0="
    },
    {
      "sha": "29ee77c0e158038177c3d03a30dbf394ba36c62f",
      "filename": "backend/app/cmd/server.go",
      "status": "modified",
      "additions": 25,
      "deletions": 14,
      "changes": 39,
      "blob_url": "https://github.com/umputun/remark42/blob/80f4862c9c2905475f3484f20ca41f7bdf103de9/backend/app/cmd/server.go",
      "raw_url": "https://github.com/umputun/remark42/raw/80f4862c9c2905475f3484f20ca41f7bdf103de9/backend/app/cmd/server.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/app/cmd/server.go?ref=80f4862c9c2905475f3484f20ca41f7bdf103de9",
      "patch": "@@ -233,6 +233,7 @@ type RPCGroup struct {\n type LoadingCache interface {\n \tGet(key cache.Key, fn func() ([]byte, error)) (data []byte, err error) // load from cache if found or put to cache and return\n \tFlush(req cache.FlusherRequest)                                        // evict matched records\n+\tClose() error\n }\n \n // serverApp holds all active objects\n@@ -248,6 +249,8 @@ type serverApp struct {\n \timageService  *image.Service\n \tauthenticator *auth.Service\n \tterminated    chan struct{}\n+\n+\tauthRefreshCache *authRefreshCache // stored only to close it properly on shutdown\n }\n \n // Execute is the entry point for \"server\" command, called by flag parser\n@@ -366,7 +369,8 @@ func (s *ServerCommand) newServerApp() (*serverApp, error) {\n \tif err != nil {\n \t\treturn nil, errors.Wrap(err, \"failed to make avatar store\")\n \t}\n-\tauthenticator, err := s.makeAuthenticator(dataService, avatarStore, adminStore)\n+\tauthRefreshCache := newAuthRefreshCache()\n+\tauthenticator, err := s.makeAuthenticator(dataService, avatarStore, adminStore, authRefreshCache)\n \tif err != nil {\n \t\treturn nil, errors.Wrap(err, \"failed to make authenticator\")\n \t}\n@@ -462,16 +466,17 @@ func (s *ServerCommand) newServerApp() (*serverApp, error) {\n \n \treturn &serverApp{\n \t\tServerCommand: s,\n-\t\trestSrv:       srv,\n-\t\tmigratorSrv:   migr,\n-\t\texporter:      exporter,\n-\t\tdevAuth:       devAuth,\n-\t\tdataService:   dataService,\n-\t\tavatarStore:   avatarStore,\n-\t\tnotifyService: notifyService,\n-\t\timageService:  imageService,\n-\t\tauthenticator: authenticator,\n-\t\tterminated:    make(chan struct{}),\n+\t\trestSrv:          srv,\n+\t\tmigratorSrv:      migr,\n+\t\texporter:         exporter,\n+\t\tdevAuth:          devAuth,\n+\t\tdataService:      dataService,\n+\t\tavatarStore:      avatarStore,\n+\t\tnotifyService:    notifyService,\n+\t\timageService:     imageService,\n+\t\tauthenticator:    authenticator,\n+\t\tterminated:       make(chan struct{}),\n+\t\tauthRefreshCache: authRefreshCache,\n \t}, nil\n }\n \n@@ -490,7 +495,7 @@ func (a *serverApp) run(ctx context.Context) error {\n \n \ta.activateBackup(ctx) // runs in goroutine for each site\n \tif a.Auth.Dev {\n-\t\tgo a.devAuth.Run(context.Background()) // dev oauth2 server on :8084\n+\t\tgo a.devAuth.Run(ctx) // dev oauth2 server on :8084\n \t}\n \n \t// staging images resubmit after restart of the app\n@@ -512,6 +517,12 @@ func (a *serverApp) run(ctx context.Context) error {\n \tif e := a.avatarStore.Close(); e != nil {\n \t\tlog.Printf(\"[WARN] failed to close avatar store, %s\", e)\n \t}\n+\tif e := a.restSrv.Cache.Close(); e != nil {\n+\t\tlog.Printf(\"[WARN] failed to close rest server cache, %s\", e)\n+\t}\n+\tif e := a.authRefreshCache.Close(); e != nil {\n+\t\tlog.Printf(\"[WARN] failed to close auth authRefreshCache, %s\", e)\n+\t}\n \ta.notifyService.Close()\n \t// call potentially infinite loop with cancellation after a minute as a safeguard\n \tminuteCtx, cancel := context.WithTimeout(context.Background(), time.Minute)\n@@ -859,7 +870,7 @@ func (s *ServerCommand) makeSSLConfig() (config api.SSLConfig, err error) {\n \treturn config, err\n }\n \n-func (s *ServerCommand) makeAuthenticator(ds *service.DataStore, avas avatar.Store, admns admin.Store) (*auth.Service, error) {\n+func (s *ServerCommand) makeAuthenticator(ds *service.DataStore, avas avatar.Store, admns admin.Store, authRefreshCache *authRefreshCache) (*auth.Service, error) {\n \tauthenticator := auth.NewService(auth.Opts{\n \t\tURL:            strings.TrimSuffix(s.RemarkURL, \"/\"),\n \t\tIssuer:         \"remark42\",\n@@ -912,7 +923,7 @@ func (s *ServerCommand) makeAuthenticator(ds *service.DataStore, avas avatar.Sto\n \t\tAvatarResizeLimit: s.Avatar.RszLmt,\n \t\tAvatarRoutePath:   \"/api/v1/avatar\",\n \t\tLogger:            log.Default(),\n-\t\tRefreshCache:      newAuthRefreshCache(),\n+\t\tRefreshCache:      authRefreshCache,\n \t\tUseGravatar:       true,\n \t})\n "
    },
    {
      "sha": "47be4e9686dec83c017008b58a1c836463d18125",
      "filename": "backend/app/cmd/server_test.go",
      "status": "modified",
      "additions": 13,
      "deletions": 6,
      "changes": 19,
      "blob_url": "https://github.com/umputun/remark42/blob/80f4862c9c2905475f3484f20ca41f7bdf103de9/backend/app/cmd/server_test.go",
      "raw_url": "https://github.com/umputun/remark42/raw/80f4862c9c2905475f3484f20ca41f7bdf103de9/backend/app/cmd/server_test.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/app/cmd/server_test.go?ref=80f4862c9c2905475f3484f20ca41f7bdf103de9",
      "patch": "@@ -18,6 +18,7 @@ import (\n \t\"github.com/dgrijalva/jwt-go\"\n \t\"github.com/go-pkgz/auth/token\"\n \t\"github.com/umputun/go-flags\"\n+\t\"go.uber.org/goleak\"\n \n \t\"github.com/stretchr/testify/assert\"\n \t\"github.com/stretchr/testify/require\"\n@@ -80,10 +81,10 @@ func TestServerApp_DevMode(t *testing.T) {\n \t// send ping\n \tresp, err := http.Get(fmt.Sprintf(\"http://localhost:%d/api/v1/ping\", port))\n \trequire.NoError(t, err)\n-\tdefer resp.Body.Close()\n \tassert.Equal(t, 200, resp.StatusCode)\n \tbody, err := ioutil.ReadAll(resp.Body)\n \tassert.NoError(t, err)\n+\tassert.NoError(t, resp.Body.Close())\n \tassert.Equal(t, \"pong\", string(body))\n \n \tcancel()\n@@ -495,7 +496,7 @@ func TestServerAuthHooks(t *testing.T) {\n \treq.Header.Set(\"X-JWT\", tk)\n \tresp, err := client.Do(req)\n \trequire.NoError(t, err)\n-\tdefer resp.Body.Close()\n+\trequire.NoError(t, resp.Body.Close())\n \tassert.Equal(t, http.StatusCreated, resp.StatusCode, \"non-blocked user able to post\")\n \n \t// add comment with no-aud claim\n@@ -506,14 +507,14 @@ func TestServerAuthHooks(t *testing.T) {\n \tt.Logf(\"no-aud claims: %s\", tkNoAud)\n \treq, err = http.NewRequest(\"POST\", fmt.Sprintf(\"http://localhost:%d/api/v1/comment\", port),\n \t\tstrings.NewReader(`{\"text\": \"test 123\", \"locator\":{\"url\": \"https://radio-t.com/p/2018/12/29/podcast-631/\",\n-\"site\": \"remark\"}}`))\n+\t\"site\": \"remark\"}}`))\n \trequire.NoError(t, err)\n \treq.Header.Set(\"X-JWT\", tkNoAud)\n \tresp, err = client.Do(req)\n \trequire.NoError(t, err)\n-\tdefer resp.Body.Close()\n \tbody, err := ioutil.ReadAll(resp.Body)\n \trequire.NoError(t, err)\n+\trequire.NoError(t, resp.Body.Close())\n \tassert.Equal(t, http.StatusUnauthorized, resp.StatusCode, \"user without aud claim rejected, \\n\"+tkNoAud+\"\\n\"+string(body))\n \n \t// block user dev as admin\n@@ -523,10 +524,10 @@ func TestServerAuthHooks(t *testing.T) {\n \treq.SetBasicAuth(\"admin\", \"password\")\n \tresp, err = client.Do(req)\n \trequire.NoError(t, err)\n-\tdefer resp.Body.Close()\n \tassert.Equal(t, http.StatusOK, resp.StatusCode, \"user dev blocked\")\n \tb, err := ioutil.ReadAll(resp.Body)\n \trequire.NoError(t, err)\n+\trequire.NoError(t, resp.Body.Close())\n \tt.Log(string(b))\n \n \t// try add a comment with blocked user\n@@ -536,14 +537,15 @@ func TestServerAuthHooks(t *testing.T) {\n \treq.Header.Set(\"X-JWT\", tk)\n \tresp, err = client.Do(req)\n \trequire.NoError(t, err)\n-\tdefer resp.Body.Close()\n \tbody, err = ioutil.ReadAll(resp.Body)\n \trequire.NoError(t, err)\n+\trequire.NoError(t, resp.Body.Close())\n \tassert.True(t, resp.StatusCode == http.StatusForbidden || resp.StatusCode == http.StatusUnauthorized,\n \t\t\"blocked user can't post, \\n\"+tk+\"\\n\"+string(body))\n \n \tcancel()\n \tapp.Wait()\n+\tclient.CloseIdleConnections()\n }\n \n func TestServer_loadEmailTemplate(t *testing.T) {\n@@ -635,3 +637,8 @@ func prepServerApp(t *testing.T, fn func(o ServerCommand) ServerCommand) (*serve\n \trand.Seed(time.Now().UnixNano())\n \treturn app, ctx, cancel\n }\n+\n+func TestMain(m *testing.M) {\n+\t// ignore is added only for GitHub Actions, can't reproduce locally\n+\tgoleak.VerifyTestMain(m, goleak.IgnoreTopFunction(\"net/http.(*Server).Shutdown\"))\n+}"
    },
    {
      "sha": "4246b50c1cff406d9677c44c5a55bc8dabd72dfc",
      "filename": "backend/app/main_test.go",
      "status": "modified",
      "additions": 5,
      "deletions": 0,
      "changes": 5,
      "blob_url": "https://github.com/umputun/remark42/blob/80f4862c9c2905475f3484f20ca41f7bdf103de9/backend/app/main_test.go",
      "raw_url": "https://github.com/umputun/remark42/raw/80f4862c9c2905475f3484f20ca41f7bdf103de9/backend/app/main_test.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/app/main_test.go?ref=80f4862c9c2905475f3484f20ca41f7bdf103de9",
      "patch": "@@ -15,6 +15,7 @@ import (\n \n \t\"github.com/stretchr/testify/assert\"\n \t\"github.com/stretchr/testify/require\"\n+\t\"go.uber.org/goleak\"\n )\n \n func Test_Main(t *testing.T) {\n@@ -86,3 +87,7 @@ func waitForHTTPServerStart(port int) {\n \t\t}\n \t}\n }\n+\n+func TestMain(m *testing.M) {\n+\tgoleak.VerifyTestMain(m, goleak.IgnoreTopFunction(\"github.com/umputun/remark42/backend/app.init.0.func1\"))\n+}"
    },
    {
      "sha": "a6ec08c4b7190e802f1dd14cd51c995e05d0e2f3",
      "filename": "backend/app/rest/api/admin_test.go",
      "status": "modified",
      "additions": 25,
      "deletions": 3,
      "changes": 28,
      "blob_url": "https://github.com/umputun/remark42/blob/80f4862c9c2905475f3484f20ca41f7bdf103de9/backend/app/rest/api/admin_test.go",
      "raw_url": "https://github.com/umputun/remark42/raw/80f4862c9c2905475f3484f20ca41f7bdf103de9/backend/app/rest/api/admin_test.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/app/rest/api/admin_test.go?ref=80f4862c9c2905475f3484f20ca41f7bdf103de9",
      "patch": "@@ -17,7 +17,6 @@ import (\n \t\"github.com/go-pkgz/auth/token\"\n \tcache \"github.com/go-pkgz/lcw\"\n \tR \"github.com/go-pkgz/rest\"\n-\n \t\"github.com/stretchr/testify/assert\"\n \t\"github.com/stretchr/testify/require\"\n \n@@ -48,9 +47,9 @@ func TestAdmin_Delete(t *testing.T) {\n \t// check multi count\n \tresp, err := post(t, ts.URL+\"/api/v1/counts?site=remark42\", `[\"https://radio-t.com/blah\",\"https://radio-t.com/blah2\"]`)\n \trequire.NoError(t, err)\n-\tdefer resp.Body.Close()\n \tassert.Equal(t, http.StatusOK, resp.StatusCode)\n \tbb, err := ioutil.ReadAll(resp.Body)\n+\tassert.NoError(t, resp.Body.Close())\n \tassert.NoError(t, err)\n \tj := []store.PostInfo{}\n \terr = json.Unmarshal(bb, &j)\n@@ -62,10 +61,10 @@ func TestAdmin_Delete(t *testing.T) {\n \treq, err := http.NewRequest(http.MethodDelete,\n \t\tfmt.Sprintf(\"%s/api/v1/admin/comment/%s?site=remark42&url=https://radio-t.com/blah\", ts.URL, id1), nil)\n \trequire.NoError(t, err)\n-\tdefer resp.Body.Close()\n \trequireAdminOnly(t, req)\n \tresp, err = sendReq(t, req, adminUmputunToken)\n \tassert.NoError(t, err)\n+\tassert.NoError(t, resp.Body.Close())\n \tassert.Equal(t, 200, resp.StatusCode)\n \n \tbody, code := getWithDevAuth(t, fmt.Sprintf(\"%s/api/v1/id/%s?site=remark42&url=https://radio-t.com/blah\", ts.URL, id1))\n@@ -99,6 +98,7 @@ func TestAdmin_Delete(t *testing.T) {\n \tassert.NoError(t, err)\n \tassert.Equal(t, http.StatusOK, resp.StatusCode)\n \tbb, err = ioutil.ReadAll(resp.Body)\n+\tassert.NoError(t, resp.Body.Close())\n \tassert.NoError(t, err)\n \tj = []store.PostInfo{}\n \terr = json.Unmarshal(bb, &j)\n@@ -141,6 +141,7 @@ func TestAdmin_Title(t *testing.T) {\n \trequireAdminOnly(t, req)\n \tresp, err := sendReq(t, req, adminUmputunToken)\n \trequire.NoError(t, err)\n+\trequire.NoError(t, resp.Body.Close())\n \tassert.Equal(t, 200, resp.StatusCode)\n \n \tbody, code := get(t, fmt.Sprintf(\"%s/api/v1/id/%s?site=remark42&url=%s/post1\", ts.URL, id1, tss.URL))\n@@ -175,6 +176,7 @@ func TestAdmin_DeleteUser(t *testing.T) {\n \trequireAdminOnly(t, req)\n \tresp, err := sendReq(t, req, adminUmputunToken)\n \tassert.NoError(t, err)\n+\trequire.NoError(t, resp.Body.Close())\n \tassert.Equal(t, 200, resp.StatusCode)\n \n \t// all 3 comments here, but for id2 they deleted\n@@ -224,6 +226,7 @@ func TestAdmin_Pin(t *testing.T) {\n \t\treq.SetBasicAuth(\"admin\", \"password\")\n \t\tresp, err := client.Do(req)\n \t\tassert.NoError(t, err)\n+\t\tassert.NoError(t, resp.Body.Close())\n \t\treturn resp.StatusCode\n \t}\n \n@@ -380,6 +383,7 @@ func TestAdmin_BlockedList(t *testing.T) {\n \tassert.NoError(t, err)\n \tres, err := sendReq(t, req, adminUmputunToken)\n \trequire.NoError(t, err)\n+\trequire.NoError(t, res.Body.Close())\n \tassert.Equal(t, 200, res.StatusCode)\n \n \t// block user2\n@@ -388,6 +392,7 @@ func TestAdmin_BlockedList(t *testing.T) {\n \tassert.NoError(t, err)\n \tres, err = sendReq(t, req, adminUmputunToken)\n \trequire.NoError(t, err)\n+\trequire.NoError(t, res.Body.Close())\n \tassert.Equal(t, 200, res.StatusCode)\n \n \treq, err = http.NewRequest(\"GET\", ts.URL+\"/api/v1/admin/blocked?site=remark42\", nil)\n@@ -398,6 +403,7 @@ func TestAdmin_BlockedList(t *testing.T) {\n \tusers := []store.BlockedUser{}\n \terr = json.NewDecoder(res.Body).Decode(&users)\n \tassert.NoError(t, err)\n+\trequire.NoError(t, res.Body.Close())\n \trequire.Equal(t, 2, len(users), \"two users blocked\")\n \tassert.Equal(t, \"user1\", users[0].ID)\n \tassert.Equal(t, \"user1 name\", users[0].Name)\n@@ -414,6 +420,7 @@ func TestAdmin_BlockedList(t *testing.T) {\n \tusers = []store.BlockedUser{}\n \terr = json.NewDecoder(res.Body).Decode(&users)\n \tassert.NoError(t, err)\n+\trequire.NoError(t, res.Body.Close())\n \tassert.Equal(t, 1, len(users), \"one user left blocked\")\n }\n \n@@ -441,9 +448,11 @@ func TestAdmin_ReadOnly(t *testing.T) {\n \tassert.NoError(t, err)\n \tresp, err := sendReq(t, req, \"\") // non-admin user\n \trequire.NoError(t, err)\n+\trequire.NoError(t, resp.Body.Close())\n \tassert.Equal(t, 401, resp.StatusCode)\n \tresp, err = sendReq(t, req, adminUmputunToken)\n \trequire.NoError(t, err)\n+\trequire.NoError(t, resp.Body.Close())\n \tassert.Equal(t, 200, resp.StatusCode)\n \tinfo, err = srv.DataService.Info(store.Locator{SiteID: \"remark42\", URL: \"https://radio-t.com/blah\"}, 0)\n \tassert.NoError(t, err)\n@@ -458,6 +467,7 @@ func TestAdmin_ReadOnly(t *testing.T) {\n \trequire.NoError(t, err)\n \tresp, err = sendReq(t, req, adminUmputunToken)\n \trequire.NoError(t, err)\n+\trequire.NoError(t, resp.Body.Close())\n \tassert.Equal(t, http.StatusForbidden, resp.StatusCode)\n \n \t// reset post's read-only\n@@ -466,6 +476,7 @@ func TestAdmin_ReadOnly(t *testing.T) {\n \tassert.NoError(t, err)\n \tresp, err = sendReq(t, req, adminUmputunToken)\n \trequire.NoError(t, err)\n+\trequire.NoError(t, resp.Body.Close())\n \tassert.Equal(t, 200, resp.StatusCode)\n \tinfo, err = srv.DataService.Info(store.Locator{SiteID: \"remark42\", URL: \"https://radio-t.com/blah\"}, 0)\n \tassert.NoError(t, err)\n@@ -480,6 +491,7 @@ func TestAdmin_ReadOnly(t *testing.T) {\n \trequire.NoError(t, err)\n \tresp, err = sendReq(t, req, adminUmputunToken)\n \trequire.NoError(t, err)\n+\trequire.NoError(t, resp.Body.Close())\n \tassert.Equal(t, http.StatusCreated, resp.StatusCode)\n }\n \n@@ -494,6 +506,7 @@ func TestAdmin_ReadOnlyNoComments(t *testing.T) {\n \trequireAdminOnly(t, req)\n \tresp, err := sendReq(t, req, adminUmputunToken)\n \trequire.NoError(t, err)\n+\trequire.NoError(t, resp.Body.Close())\n \tassert.Equal(t, 200, resp.StatusCode)\n \t_, err = srv.DataService.Info(store.Locator{SiteID: \"remark42\", URL: \"https://radio-t.com/blah\"}, 0)\n \tassert.Error(t, err)\n@@ -529,6 +542,7 @@ func TestAdmin_ReadOnlyWithAge(t *testing.T) {\n \trequireAdminOnly(t, req)\n \tresp, err := sendReq(t, req, adminUmputunToken)\n \trequire.NoError(t, err)\n+\trequire.NoError(t, resp.Body.Close())\n \tassert.Equal(t, 200, resp.StatusCode)\n \tinfo, err = srv.DataService.Info(store.Locator{SiteID: \"remark42\", URL: \"https://radio-t.com/blah\"}, 0)\n \tassert.NoError(t, err)\n@@ -540,6 +554,7 @@ func TestAdmin_ReadOnlyWithAge(t *testing.T) {\n \tassert.NoError(t, err)\n \tresp, err = sendReq(t, req, adminUmputunToken)\n \trequire.NoError(t, err)\n+\trequire.NoError(t, resp.Body.Close())\n \tassert.Equal(t, 403, resp.StatusCode)\n \tinfo, err = srv.DataService.Info(store.Locator{SiteID: \"remark42\", URL: \"https://radio-t.com/blah\"}, 0)\n \tassert.NoError(t, err)\n@@ -569,6 +584,7 @@ func TestAdmin_Verify(t *testing.T) {\n \trequireAdminOnly(t, req)\n \tresp, err := sendReq(t, req, adminUmputunToken)\n \trequire.NoError(t, err)\n+\trequire.NoError(t, resp.Body.Close())\n \tassert.Equal(t, 200, resp.StatusCode)\n \tverified = srv.DataService.IsVerified(\"remark42\", \"user1\")\n \tassert.True(t, verified)\n@@ -587,6 +603,7 @@ func TestAdmin_Verify(t *testing.T) {\n \tassert.NoError(t, err)\n \tresp, err = sendReq(t, req, adminUmputunToken)\n \trequire.NoError(t, err)\n+\trequire.NoError(t, resp.Body.Close())\n \tassert.Equal(t, 200, resp.StatusCode)\n \tverified = srv.DataService.IsVerified(\"remark42\", \"user1\")\n \tassert.False(t, verified)\n@@ -707,6 +724,7 @@ func TestAdmin_DeleteMeRequest(t *testing.T) {\n \treq.SetBasicAuth(\"admin\", \"password\")\n \tresp, err := client.Do(req)\n \trequire.NoError(t, err)\n+\trequire.NoError(t, resp.Body.Close())\n \tassert.Equal(t, 200, resp.StatusCode)\n \n \t_, err = srv.DataService.User(\"remark42\", \"user1\", 0, 0, store.User{})\n@@ -739,6 +757,7 @@ func TestAdmin_DeleteMeRequestFailed(t *testing.T) {\n \treq.SetBasicAuth(\"admin\", \"password\")\n \tresp, err := client.Do(req)\n \tassert.NoError(t, err)\n+\tassert.NoError(t, resp.Body.Close())\n \tassert.Equal(t, 400, resp.StatusCode)\n \n \t// try with bad auth\n@@ -766,6 +785,7 @@ func TestAdmin_DeleteMeRequestFailed(t *testing.T) {\n \treq.SetBasicAuth(\"admin\", \"bad-password\")\n \tresp, err = client.Do(req)\n \tassert.NoError(t, err)\n+\tassert.NoError(t, resp.Body.Close())\n \tassert.Equal(t, 403, resp.StatusCode)\n \n \t// try bad user\n@@ -778,6 +798,7 @@ func TestAdmin_DeleteMeRequestFailed(t *testing.T) {\n \treq.SetBasicAuth(\"admin\", \"password\")\n \tresp, err = client.Do(req)\n \tassert.NoError(t, err)\n+\tassert.NoError(t, resp.Body.Close())\n \tassert.Equal(t, 400, resp.StatusCode, resp.Status)\n \n \t// try without deleteme flag\n@@ -793,6 +814,7 @@ func TestAdmin_DeleteMeRequestFailed(t *testing.T) {\n \tassert.Equal(t, 403, resp.StatusCode)\n \tb, err := ioutil.ReadAll(resp.Body)\n \tassert.NoError(t, err)\n+\tassert.NoError(t, resp.Body.Close())\n \tassert.True(t, strings.Contains(string(b), \"can't use provided token\"))\n }\n "
    },
    {
      "sha": "5fdb3f8d6a114a092395d3a36e0094a436efcbec",
      "filename": "backend/app/rest/api/migrator_test.go",
      "status": "modified",
      "additions": 11,
      "deletions": 0,
      "changes": 11,
      "blob_url": "https://github.com/umputun/remark42/blob/80f4862c9c2905475f3484f20ca41f7bdf103de9/backend/app/rest/api/migrator_test.go",
      "raw_url": "https://github.com/umputun/remark42/raw/80f4862c9c2905475f3484f20ca41f7bdf103de9/backend/app/rest/api/migrator_test.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/app/rest/api/migrator_test.go?ref=80f4862c9c2905475f3484f20ca41f7bdf103de9",
      "patch": "@@ -124,6 +124,7 @@ func TestMigrator_ImportRejected(t *testing.T) {\n \tassert.NoError(t, err)\n \tresp, err := client.Do(req)\n \tassert.NoError(t, err)\n+\tassert.NoError(t, resp.Body.Close())\n \tassert.Equal(t, http.StatusUnauthorized, resp.StatusCode)\n }\n \n@@ -147,6 +148,7 @@ func TestMigrator_ImportDouble(t *testing.T) {\n \tassert.NoError(t, err)\n \tresp, err := client.Do(req)\n \tassert.NoError(t, err)\n+\tassert.NoError(t, resp.Body.Close())\n \tassert.Equal(t, http.StatusAccepted, resp.StatusCode)\n \n \tclient = &http.Client{Timeout: 5 * time.Second}\n@@ -156,6 +158,7 @@ func TestMigrator_ImportDouble(t *testing.T) {\n \tassert.NoError(t, err)\n \tresp, err = client.Do(req)\n \tassert.NoError(t, err)\n+\tassert.NoError(t, resp.Body.Close())\n \tassert.Equal(t, http.StatusConflict, resp.StatusCode)\n \twaitForMigrationCompletion(t, ts)\n }\n@@ -181,6 +184,7 @@ func TestMigrator_ImportWaitExpired(t *testing.T) {\n \trequire.NoError(t, err)\n \tresp, err := client.Do(req)\n \tassert.NoError(t, err)\n+\tassert.NoError(t, resp.Body.Close())\n \tassert.Equal(t, http.StatusAccepted, resp.StatusCode)\n \n \tclient = &http.Client{Timeout: 5 * time.Second}\n@@ -190,6 +194,7 @@ func TestMigrator_ImportWaitExpired(t *testing.T) {\n \tassert.NoError(t, err)\n \tresp, err = client.Do(req)\n \tassert.NoError(t, err)\n+\tassert.NoError(t, resp.Body.Close())\n \tassert.Equal(t, http.StatusGatewayTimeout, resp.StatusCode)\n \n \twaitForMigrationCompletion(t, ts)\n@@ -215,6 +220,7 @@ func TestMigrator_Export(t *testing.T) {\n \treq.SetBasicAuth(\"admin\", \"password\")\n \tresp, err := client.Do(req)\n \trequire.NoError(t, err)\n+\trequire.NoError(t, resp.Body.Close())\n \trequire.Equal(t, http.StatusAccepted, resp.StatusCode)\n \twaitForMigrationCompletion(t, ts)\n \n@@ -231,6 +237,7 @@ func TestMigrator_Export(t *testing.T) {\n \tassert.NoError(t, err)\n \tungzBody, err := ioutil.ReadAll(ungzReader)\n \tassert.NoError(t, err)\n+\trequire.NoError(t, resp.Body.Close())\n \tassert.Equal(t, 3, strings.Count(string(ungzBody), \"\\n\"))\n \tassert.Equal(t, 2, strings.Count(string(ungzBody), \"\\\"text\\\"\"))\n \tt.Logf(\"%s\", string(ungzBody))\n@@ -246,6 +253,7 @@ func TestMigrator_Export(t *testing.T) {\n \n \tbody, err := ioutil.ReadAll(resp.Body)\n \tassert.NoError(t, err)\n+\trequire.NoError(t, resp.Body.Close())\n \tassert.Equal(t, 3, strings.Count(string(body), \"\\n\"))\n \tassert.Equal(t, 2, strings.Count(string(body), \"\\\"text\\\"\"))\n \tt.Logf(\"%s\", string(body))\n@@ -254,6 +262,7 @@ func TestMigrator_Export(t *testing.T) {\n \trequire.NoError(t, err)\n \tresp, err = client.Do(req)\n \trequire.NoError(t, err)\n+\trequire.NoError(t, resp.Body.Close())\n \trequire.Equal(t, http.StatusUnauthorized, resp.StatusCode)\n }\n \n@@ -305,6 +314,7 @@ func TestMigrator_Remap(t *testing.T) {\n \trules := \"https://remark42.com/* https://www.remark42.com/*\"\n \tresp, err := post(t, ts.URL+\"/api/v1/admin/remap?site=remark42\", rules) // auth as admin\n \trequire.NoError(t, err)\n+\trequire.NoError(t, resp.Body.Close())\n \trequire.Equal(t, http.StatusAccepted, resp.StatusCode)\n \twaitForMigrationCompletion(t, ts)\n \n@@ -352,6 +362,7 @@ func TestMigrator_RemapReject(t *testing.T) {\n \trequire.NoError(t, err)\n \tresp, err := client.Do(req)\n \trequire.NoError(t, err)\n+\trequire.NoError(t, resp.Body.Close())\n \trequire.Equal(t, http.StatusUnauthorized, resp.StatusCode)\n }\n "
    },
    {
      "sha": "5d6fdfb60d9327fa11af29df6199d80565455e2c",
      "filename": "backend/app/rest/api/rest.go",
      "status": "modified",
      "additions": 1,
      "deletions": 0,
      "changes": 1,
      "blob_url": "https://github.com/umputun/remark42/blob/80f4862c9c2905475f3484f20ca41f7bdf103de9/backend/app/rest/api/rest.go",
      "raw_url": "https://github.com/umputun/remark42/raw/80f4862c9c2905475f3484f20ca41f7bdf103de9/backend/app/rest/api/rest.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/app/rest/api/rest.go?ref=80f4862c9c2905475f3484f20ca41f7bdf103de9",
      "patch": "@@ -78,6 +78,7 @@ type Rest struct {\n type LoadingCache interface {\n \tGet(key lcw.Key, fn func() ([]byte, error)) (data []byte, err error) // load from cache if found or put to cache and return\n \tFlush(req lcw.FlusherRequest)                                        // evict matched records\n+\tClose() error\n }\n \n const hardBodyLimit = 1024 * 64 // limit size of body"
    },
    {
      "sha": "76ac28d6be8d98d15c32ea3ad7f155dd2490050c",
      "filename": "backend/app/rest/api/rest_private_test.go",
      "status": "modified",
      "additions": 30,
      "deletions": 0,
      "changes": 30,
      "blob_url": "https://github.com/umputun/remark42/blob/80f4862c9c2905475f3484f20ca41f7bdf103de9/backend/app/rest/api/rest_private_test.go",
      "raw_url": "https://github.com/umputun/remark42/raw/80f4862c9c2905475f3484f20ca41f7bdf103de9/backend/app/rest/api/rest_private_test.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/app/rest/api/rest_private_test.go?ref=80f4862c9c2905475f3484f20ca41f7bdf103de9",
      "patch": "@@ -3,6 +3,7 @@ package api\n import (\n \t\"bytes\"\n \t\"compress/gzip\"\n+\t\"context\"\n \t\"encoding/base64\"\n \t\"encoding/json\"\n \t\"fmt\"\n@@ -71,6 +72,7 @@ func TestRest_CreateOldPost(t *testing.T) {\n \tresp, err := post(t, ts.URL+\"/api/v1/comment\",\n \t\t`{\"text\": \"test 123\", \"locator\":{\"site\": \"remark42\",\"url\": \"https://radio-t.com/blah1\"}}`)\n \tassert.NoError(t, err)\n+\tassert.NoError(t, resp.Body.Close())\n \tassert.Equal(t, http.StatusCreated, resp.StatusCode)\n \n \tassert.NoError(t, srv.DataService.DeleteAll(\"remark42\"))\n@@ -83,6 +85,7 @@ func TestRest_CreateOldPost(t *testing.T) {\n \tresp, err = post(t, ts.URL+\"/api/v1/comment\",\n \t\t`{\"text\": \"test 123\", \"locator\":{\"site\": \"remark42\",\"url\": \"https://radio-t.com/blah1\"}}`)\n \tassert.NoError(t, err)\n+\tassert.NoError(t, resp.Body.Close())\n \tassert.Equal(t, http.StatusForbidden, resp.StatusCode)\n }\n \n@@ -144,6 +147,7 @@ func TestRest_CreateRejected(t *testing.T) {\n \t// try to create without auth\n \tresp, err := http.Post(ts.URL+\"/api/v1/comment\", \"\", strings.NewReader(body))\n \trequire.NoError(t, err)\n+\trequire.NoError(t, resp.Body.Close())\n \tassert.Equal(t, 401, resp.StatusCode)\n \n \t// try with wrong aud\n@@ -153,6 +157,7 @@ func TestRest_CreateRejected(t *testing.T) {\n \treq.Header.Add(\"X-JWT\", devTokenBadAud)\n \tresp, err = client.Do(req)\n \trequire.NoError(t, err)\n+\trequire.NoError(t, resp.Body.Close())\n \trequire.Equal(t, http.StatusForbidden, resp.StatusCode, \"reject wrong aud\")\n }\n \n@@ -312,6 +317,7 @@ func TestRest_UpdateNotOwner(t *testing.T) {\n \tassert.NoError(t, err)\n \tbody, err := ioutil.ReadAll(b.Body)\n \tassert.NoError(t, err)\n+\tassert.NoError(t, b.Body.Close())\n \tassert.Equal(t, 403, b.StatusCode, string(body), \"update from non-owner\")\n \tassert.Equal(t, `{\"code\":3,\"details\":\"can not edit comments for other users\",\"error\":\"rejected\"}`+\"\\n\", string(body))\n \n@@ -322,6 +328,7 @@ func TestRest_UpdateNotOwner(t *testing.T) {\n \treq.Header.Add(\"X-JWT\", devToken)\n \tb, err = client.Do(req)\n \tassert.NoError(t, err)\n+\tassert.NoError(t, b.Body.Close())\n \tassert.Equal(t, 400, b.StatusCode, string(body), \"update is not json\")\n }\n \n@@ -340,6 +347,7 @@ func TestRest_UpdateWrongAud(t *testing.T) {\n \treq.Header.Add(\"X-JWT\", devTokenBadAud)\n \tb, err := client.Do(req)\n \tassert.NoError(t, err)\n+\tassert.NoError(t, b.Body.Close())\n \tassert.Equal(t, http.StatusForbidden, b.StatusCode, \"reject update with wrong aut in jwt\")\n }\n \n@@ -388,6 +396,7 @@ func TestRest_Vote(t *testing.T) {\n \t\treq.Header.Add(\"X-JWT\", devToken)\n \t\tresp, err := client.Do(req)\n \t\tassert.NoError(t, err)\n+\t\tassert.NoError(t, resp.Body.Close())\n \t\treturn resp.StatusCode\n \t}\n \n@@ -472,6 +481,7 @@ func TestRest_AnonVote(t *testing.T) {\n \t\treq.Header.Add(\"X-JWT\", anonToken)\n \t\tresp, err := client.Do(req)\n \t\tassert.NoError(t, err)\n+\t\tassert.NoError(t, resp.Body.Close())\n \t\treturn resp.StatusCode\n \t}\n \n@@ -586,6 +596,7 @@ func TestRest_EmailNotification(t *testing.T) {\n \n \tmockDestination := &notify.MockDest{}\n \tsrv.privRest.notifyService = notify.NewService(srv.DataService, 1, mockDestination)\n+\tdefer srv.privRest.notifyService.Close()\n \n \tclient := http.Client{}\n \n@@ -601,6 +612,7 @@ func TestRest_EmailNotification(t *testing.T) {\n \tassert.NoError(t, err)\n \tbody, err := ioutil.ReadAll(resp.Body)\n \trequire.NoError(t, err)\n+\trequire.NoError(t, resp.Body.Close())\n \trequire.Equal(t, http.StatusCreated, resp.StatusCode, string(body))\n \tparentComment := store.Comment{}\n \trequire.NoError(t, render.DecodeJSON(strings.NewReader(string(body)), &parentComment))\n@@ -623,6 +635,7 @@ func TestRest_EmailNotification(t *testing.T) {\n \tassert.NoError(t, err)\n \tbody, err = ioutil.ReadAll(resp.Body)\n \trequire.NoError(t, err)\n+\trequire.NoError(t, resp.Body.Close())\n \trequire.Equal(t, http.StatusCreated, resp.StatusCode, string(body))\n \t// wait for mock notification Submit to kick off\n \ttime.Sleep(time.Millisecond * 30)\n@@ -638,6 +651,7 @@ func TestRest_EmailNotification(t *testing.T) {\n \trequire.NoError(t, err)\n \tbody, err = ioutil.ReadAll(resp.Body)\n \trequire.NoError(t, err)\n+\trequire.NoError(t, resp.Body.Close())\n \trequire.Equal(t, http.StatusOK, resp.StatusCode, string(body))\n \t// wait for mock notification Submit to kick off\n \ttime.Sleep(time.Millisecond * 30)\n@@ -653,6 +667,7 @@ func TestRest_EmailNotification(t *testing.T) {\n \trequire.NoError(t, err)\n \tbody, err = ioutil.ReadAll(resp.Body)\n \trequire.NoError(t, err)\n+\trequire.NoError(t, resp.Body.Close())\n \trequire.Equal(t, http.StatusOK, resp.StatusCode, string(body))\n \n \t// get user information to verify the subscription\n@@ -663,6 +678,7 @@ func TestRest_EmailNotification(t *testing.T) {\n \trequire.NoError(t, err)\n \tbody, err = ioutil.ReadAll(resp.Body)\n \trequire.NoError(t, err)\n+\trequire.NoError(t, resp.Body.Close())\n \trequire.Equal(t, http.StatusOK, resp.StatusCode, string(body))\n \tvar user store.User\n \terr = json.Unmarshal(body, &user)\n@@ -683,6 +699,7 @@ func TestRest_EmailNotification(t *testing.T) {\n \tassert.NoError(t, err)\n \tbody, err = ioutil.ReadAll(resp.Body)\n \trequire.NoError(t, err)\n+\trequire.NoError(t, resp.Body.Close())\n \trequire.Equal(t, http.StatusCreated, resp.StatusCode, string(body))\n \t// wait for mock notification Submit to kick off\n \ttime.Sleep(time.Millisecond * 30)\n@@ -697,6 +714,7 @@ func TestRest_EmailNotification(t *testing.T) {\n \trequire.NoError(t, err)\n \tbody, err = ioutil.ReadAll(resp.Body)\n \trequire.NoError(t, err)\n+\trequire.NoError(t, resp.Body.Close())\n \tassert.Equal(t, http.StatusOK, resp.StatusCode, string(body))\n \n \t// create child comment from another user, no email notification expected except for admin\n@@ -711,6 +729,7 @@ func TestRest_EmailNotification(t *testing.T) {\n \tassert.NoError(t, err)\n \tbody, err = ioutil.ReadAll(resp.Body)\n \trequire.NoError(t, err)\n+\trequire.NoError(t, resp.Body.Close())\n \trequire.Equal(t, http.StatusCreated, resp.StatusCode, string(body))\n \t// wait for mock notification Submit to kick off\n \ttime.Sleep(time.Millisecond * 30)\n@@ -748,6 +767,7 @@ func TestRest_UserAllData(t *testing.T) {\n \n \tungzReader, err := gzip.NewReader(resp.Body)\n \tassert.NoError(t, err)\n+\trequire.NoError(t, resp.Body.Close())\n \tungzBody, err := ioutil.ReadAll(ungzReader)\n \tassert.NoError(t, err)\n \tstrUungzBody := string(ungzBody)\n@@ -770,6 +790,7 @@ func TestRest_UserAllData(t *testing.T) {\n \trequire.NoError(t, err)\n \tresp, err = client.Do(req)\n \trequire.NoError(t, err)\n+\trequire.NoError(t, resp.Body.Close())\n \trequire.Equal(t, 401, resp.StatusCode)\n }\n \n@@ -818,6 +839,7 @@ func TestRest_DeleteMe(t *testing.T) {\n \tassert.NoError(t, err)\n \tassert.Equal(t, 200, resp.StatusCode)\n \tbody, err := ioutil.ReadAll(resp.Body)\n+\tassert.NoError(t, resp.Body.Close())\n \tassert.NoError(t, err)\n \n \tm := map[string]string{}\n@@ -837,6 +859,7 @@ func TestRest_DeleteMe(t *testing.T) {\n \tresp, err = client.Do(req)\n \tassert.NoError(t, err)\n \tassert.Equal(t, 401, resp.StatusCode)\n+\tassert.NoError(t, resp.Body.Close())\n }\n \n func TestRest_SavePictureCtrl(t *testing.T) {\n@@ -864,6 +887,7 @@ func TestRest_SavePictureCtrl(t *testing.T) {\n \t\tassert.Equal(t, 200, resp.StatusCode)\n \t\tbody, err := ioutil.ReadAll(resp.Body)\n \t\trequire.NoError(t, err)\n+\t\trequire.NoError(t, resp.Body.Close())\n \n \t\tm := map[string]string{}\n \t\terr = json.Unmarshal(body, &m)\n@@ -878,29 +902,34 @@ func TestRest_SavePictureCtrl(t *testing.T) {\n \tassert.Equal(t, 200, resp.StatusCode)\n \tbody, err := ioutil.ReadAll(resp.Body)\n \trequire.NoError(t, err)\n+\trequire.NoError(t, resp.Body.Close())\n \tassert.Equal(t, 1462, len(body))\n \tassert.Equal(t, \"image/png\", resp.Header.Get(\"Content-Type\"))\n \n \tid = savePic(\"picture.gif\")\n \tresp, err = http.Get(fmt.Sprintf(\"%s/api/v1/picture/%s\", ts.URL, id))\n \trequire.NoError(t, err)\n+\trequire.NoError(t, resp.Body.Close())\n \tassert.Equal(t, 200, resp.StatusCode)\n \tassert.Equal(t, \"image/png\", resp.Header.Get(\"Content-Type\"))\n \n \tid = savePic(\"picture.jpg\")\n \tresp, err = http.Get(fmt.Sprintf(\"%s/api/v1/picture/%s\", ts.URL, id))\n \trequire.NoError(t, err)\n+\trequire.NoError(t, resp.Body.Close())\n \tassert.Equal(t, 200, resp.StatusCode)\n \tassert.Equal(t, \"image/png\", resp.Header.Get(\"Content-Type\"))\n \n \tid = savePic(\"picture.blah\")\n \tresp, err = http.Get(fmt.Sprintf(\"%s/api/v1/picture/%s\", ts.URL, id))\n \trequire.NoError(t, err)\n+\trequire.NoError(t, resp.Body.Close())\n \tassert.Equal(t, 200, resp.StatusCode)\n \tassert.Equal(t, \"image/png\", resp.Header.Get(\"Content-Type\"))\n \n \tresp, err = http.Get(fmt.Sprintf(\"%s/api/v1/picture/blah/pic.blah\", ts.URL))\n \trequire.NoError(t, err)\n+\trequire.NoError(t, resp.Body.Close())\n \tassert.Equal(t, 400, resp.StatusCode)\n }\n \n@@ -919,6 +948,7 @@ func TestRest_CreateWithPictures(t *testing.T) {\n \t\tEditDuration: 100 * time.Millisecond,\n \t\tMaxSize:      2000,\n \t})\n+\tdefer imageService.Close(context.Background())\n \n \tsvc.privRest.imageService = imageService\n \tsvc.ImageService = imageService"
    },
    {
      "sha": "456bb56cb13a49cfe40192b722791f7bec365ee6",
      "filename": "backend/app/rest/api/rest_public_test.go",
      "status": "modified",
      "additions": 11,
      "deletions": 3,
      "changes": 14,
      "blob_url": "https://github.com/umputun/remark42/blob/80f4862c9c2905475f3484f20ca41f7bdf103de9/backend/app/rest/api/rest_public_test.go",
      "raw_url": "https://github.com/umputun/remark42/raw/80f4862c9c2905475f3484f20ca41f7bdf103de9/backend/app/rest/api/rest_public_test.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/app/rest/api/rest_public_test.go?ref=80f4862c9c2905475f3484f20ca41f7bdf103de9",
      "patch": "@@ -39,10 +39,12 @@ func TestRest_Preview(t *testing.T) {\n \tassert.Equal(t, http.StatusOK, resp.StatusCode)\n \tb, err := ioutil.ReadAll(resp.Body)\n \tassert.NoError(t, err)\n+\tassert.NoError(t, resp.Body.Close())\n \tassert.Equal(t, \"<p>test 123</p>\\n\", string(b))\n \n \tresp, err = post(t, ts.URL+\"/api/v1/preview\", \"bad\")\n \tassert.NoError(t, err)\n+\tassert.NoError(t, resp.Body.Close())\n \tassert.Equal(t, 400, resp.StatusCode)\n }\n \n@@ -211,8 +213,9 @@ func TestRest_FindReadOnly(t *testing.T) {\n \t\tfmt.Sprintf(\"%s/api/v1/admin/readonly?site=remark42&url=https://radio-t.com/blah1&ro=1\", ts.URL), nil)\n \tassert.NoError(t, err)\n \treq.SetBasicAuth(\"admin\", \"password\")\n-\t_, err = client.Do(req)\n+\tresp, err := client.Do(req)\n \trequire.NoError(t, err)\n+\trequire.NoError(t, resp.Body.Close())\n \n \ttree := service.Tree{}\n \tres, code := get(t, ts.URL+\"/api/v1/find?site=remark42&url=https://radio-t.com/blah1&format=tree\")\n@@ -445,6 +448,7 @@ func TestRest_Counts(t *testing.T) {\n \n \tbody, err := ioutil.ReadAll(resp.Body)\n \tassert.NoError(t, err)\n+\tassert.NoError(t, resp.Body.Close())\n \n \tj := []store.PostInfo{}\n \terr = json.Unmarshal(body, &j)\n@@ -455,6 +459,7 @@ func TestRest_Counts(t *testing.T) {\n \tresp, err = post(t, ts.URL+\"/api/v1/counts?site=radio-XXX\", `{}`)\n \trequire.NoError(t, err)\n \tassert.Equal(t, 400, resp.StatusCode)\n+\tassert.NoError(t, resp.Body.Close())\n }\n \n func TestRest_List(t *testing.T) {\n@@ -738,6 +743,7 @@ func TestRest_LastCommentsStream(t *testing.T) {\n \tcacheBackend, err := cache.NewExpirableCache()\n \trequire.NoError(t, err)\n \tmemCache := cache.NewScache(cacheBackend)\n+\tdefer memCache.Close()\n \tsrv.privRest.cache = memCache\n \tsrv.pubRest.cache = memCache\n \n@@ -791,6 +797,7 @@ func TestRest_LastCommentsStreamTimeout(t *testing.T) {\n \n func TestRest_LastCommentsStreamCancel(t *testing.T) {\n \tts, srv, teardown := startupT(t)\n+\tdefer teardown()\n \tsrv.pubRest.readOnlyAge = 10000000 // make sure we don't hit read-only\n \tsrv.pubRest.streamer.Refresh = 10 * time.Millisecond\n \tsrv.pubRest.streamer.TimeOut = 500 * time.Millisecond\n@@ -800,12 +807,12 @@ func TestRest_LastCommentsStreamCancel(t *testing.T) {\n \tcacheBackend, err := cache.NewExpirableCache()\n \trequire.NoError(t, err)\n \tmemCache := cache.NewScache(cacheBackend)\n+\tdefer memCache.Close()\n \tsrv.privRest.cache = memCache\n \tsrv.pubRest.cache = memCache\n \n \tpostComment(t, ts.URL)\n \n-\tdefer teardown()\n \tdone := make(chan struct{})\n \tgo func() {\n \t\tdefer close(done)\n@@ -864,6 +871,7 @@ func TestRest_LastCommentsStreamTooMany(t *testing.T) {\n \n func TestRest_LastCommentsStreamSince(t *testing.T) {\n \tts, srv, teardown := startupT(t)\n+\tdefer teardown()\n \tsrv.pubRest.readOnlyAge = 10000000 // make sure we don't hit read-only\n \tsrv.pubRest.streamer.Refresh = 10 * time.Millisecond\n \tsrv.pubRest.streamer.TimeOut = 500 * time.Millisecond\n@@ -873,12 +881,12 @@ func TestRest_LastCommentsStreamSince(t *testing.T) {\n \tcacheBackend, err := cache.NewExpirableCache()\n \trequire.NoError(t, err)\n \tmemCache := cache.NewScache(cacheBackend)\n+\tdefer memCache.Close()\n \tsrv.privRest.cache = memCache\n \tsrv.pubRest.cache = memCache\n \n \tpostComment(t, ts.URL)\n \n-\tdefer teardown()\n \tdone := make(chan struct{})\n \tgo func() {\n \t\tdefer close(done)"
    },
    {
      "sha": "4d01b2898075ed78497958a7bb41252fe0a6edfe",
      "filename": "backend/app/rest/api/rest_test.go",
      "status": "modified",
      "additions": 14,
      "deletions": 3,
      "changes": 17,
      "blob_url": "https://github.com/umputun/remark42/blob/80f4862c9c2905475f3484f20ca41f7bdf103de9/backend/app/rest/api/rest_test.go",
      "raw_url": "https://github.com/umputun/remark42/raw/80f4862c9c2905475f3484f20ca41f7bdf103de9/backend/app/rest/api/rest_test.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/app/rest/api/rest_test.go?ref=80f4862c9c2905475f3484f20ca41f7bdf103de9",
      "patch": "@@ -25,6 +25,7 @@ import (\n \t\"github.com/stretchr/testify/assert\"\n \t\"github.com/stretchr/testify/require\"\n \tbolt \"go.etcd.io/bbolt\"\n+\t\"go.uber.org/goleak\"\n \n \t\"github.com/umputun/remark42/backend/app/migrator\"\n \t\"github.com/umputun/remark42/backend/app/notify\"\n@@ -210,14 +211,17 @@ func TestRest_rejectAnonUser(t *testing.T) {\n \n \tresp, err := http.Get(ts.URL)\n \trequire.NoError(t, err)\n+\trequire.NoError(t, resp.Body.Close())\n \tassert.Equal(t, http.StatusUnauthorized, resp.StatusCode, \"use not logged in\")\n \n \tresp, err = http.Get(ts.URL + \"?fake_id=anonymous_user123&fake_name=test\")\n \trequire.NoError(t, err)\n+\trequire.NoError(t, resp.Body.Close())\n \tassert.Equal(t, http.StatusForbidden, resp.StatusCode, \"anon rejected\")\n \n \tresp, err = http.Get(ts.URL + \"?fake_id=real_user123&fake_name=test\")\n \trequire.NoError(t, err)\n+\trequire.NoError(t, resp.Body.Close())\n \tassert.Equal(t, http.StatusOK, resp.StatusCode, \"real user\")\n }\n \n@@ -431,9 +435,9 @@ func fakeAuth(next http.Handler) http.Handler {\n func get(t *testing.T, url string) (response string, statusCode int) {\n \tr, err := http.Get(url)\n \trequire.NoError(t, err)\n-\tdefer r.Body.Close()\n \tbody, err := ioutil.ReadAll(r.Body)\n \trequire.NoError(t, err)\n+\trequire.NoError(t, r.Body.Close())\n \treturn string(body), r.StatusCode\n }\n \n@@ -452,9 +456,9 @@ func getWithDevAuth(t *testing.T, url string) (body string, code int) {\n \treq.Header.Add(\"X-JWT\", devToken)\n \tr, err := client.Do(req)\n \trequire.NoError(t, err)\n-\tdefer r.Body.Close()\n \tb, err := ioutil.ReadAll(r.Body)\n \tassert.NoError(t, err)\n+\trequire.NoError(t, r.Body.Close())\n \treturn string(b), r.StatusCode\n }\n \n@@ -465,9 +469,9 @@ func getWithAdminAuth(t *testing.T, url string) (response string, statusCode int\n \treq.SetBasicAuth(\"admin\", \"password\")\n \tr, err := client.Do(req)\n \trequire.NoError(t, err)\n-\tdefer r.Body.Close()\n \tbody, err := ioutil.ReadAll(r.Body)\n \tassert.NoError(t, err)\n+\trequire.NoError(t, r.Body.Close())\n \treturn string(body), r.StatusCode\n }\n func post(t *testing.T, url, body string) (*http.Response, error) {\n@@ -490,6 +494,7 @@ func addComment(t *testing.T, c store.Comment, ts *httptest.Server) string {\n \trequire.NoError(t, err)\n \trequire.Equal(t, http.StatusCreated, resp.StatusCode)\n \tb, err = ioutil.ReadAll(resp.Body)\n+\trequire.NoError(t, resp.Body.Close())\n \trequire.NoError(t, err)\n \n \tcrResp := R.JSON{}\n@@ -502,10 +507,12 @@ func addComment(t *testing.T, c store.Comment, ts *httptest.Server) string {\n func requireAdminOnly(t *testing.T, req *http.Request) {\n \tresp, err := sendReq(t, req, \"\") // no-auth user\n \trequire.NoError(t, err)\n+\trequire.NoError(t, resp.Body.Close())\n \tassert.Equal(t, 401, resp.StatusCode)\n \n \tresp, err = sendReq(t, req, devToken) // non-admin user\n \trequire.NoError(t, err)\n+\trequire.NoError(t, resp.Body.Close())\n \tassert.Equal(t, 403, resp.StatusCode)\n }\n \n@@ -531,3 +538,7 @@ func waitForHTTPSServerStart(port int) {\n \t\t}\n \t}\n }\n+\n+func TestMain(m *testing.M) {\n+\tgoleak.VerifyTestMain(m)\n+}"
    },
    {
      "sha": "dc20ebfe109be71713fde0f1153c215584dbaaac",
      "filename": "backend/app/rest/api/stream_test.go",
      "status": "modified",
      "additions": 3,
      "deletions": 1,
      "changes": 4,
      "blob_url": "https://github.com/umputun/remark42/blob/80f4862c9c2905475f3484f20ca41f7bdf103de9/backend/app/rest/api/stream_test.go",
      "raw_url": "https://github.com/umputun/remark42/raw/80f4862c9c2905475f3484f20ca41f7bdf103de9/backend/app/rest/api/stream_test.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/app/rest/api/stream_test.go?ref=80f4862c9c2905475f3484f20ca41f7bdf103de9",
      "patch": "@@ -29,7 +29,9 @@ func TestStream_Timeout(t *testing.T) {\n \t}\n \n \tbuf := bytes.Buffer{}\n-\terr := s.Activate(context.Background(), eventFn, &buf)\n+\tctx, cancel := context.WithCancel(context.Background())\n+\tdefer cancel()\n+\terr := s.Activate(ctx, eventFn, &buf)\n \tassert.NoError(t, err)\n \tassert.Equal(t, \"event: test\\ndata: some data 1\\n\\nevent: test\\ndata: some data 3\\n\\nevent: test\\ndata: some data 5\\n\\nevent: test\\ndata: some data 7\\n\\nevent: test\\ndata: some data 9\\n\\n\", buf.String())\n }"
    },
    {
      "sha": "9976121e5e70318f4e05de183a69526c32cc0394",
      "filename": "backend/app/store/image/bolt_store_test.go",
      "status": "modified",
      "additions": 1,
      "deletions": 2,
      "changes": 3,
      "blob_url": "https://github.com/umputun/remark42/blob/80f4862c9c2905475f3484f20ca41f7bdf103de9/backend/app/store/image/bolt_store_test.go",
      "raw_url": "https://github.com/umputun/remark42/raw/80f4862c9c2905475f3484f20ca41f7bdf103de9/backend/app/store/image/bolt_store_test.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/app/store/image/bolt_store_test.go?ref=80f4862c9c2905475f3484f20ca41f7bdf103de9",
      "patch": "@@ -8,10 +8,9 @@ import (\n \t\"testing\"\n \t\"time\"\n \n-\tbolt \"go.etcd.io/bbolt\"\n-\n \t\"github.com/stretchr/testify/assert\"\n \t\"github.com/stretchr/testify/require\"\n+\tbolt \"go.etcd.io/bbolt\"\n )\n \n func TestBoltStore_SaveCommit(t *testing.T) {"
    },
    {
      "sha": "e35156be17294344836bdcd97839d53a2d31f5ce",
      "filename": "backend/app/store/service/service.go",
      "status": "modified",
      "additions": 9,
      "deletions": 1,
      "changes": 10,
      "blob_url": "https://github.com/umputun/remark42/blob/80f4862c9c2905475f3484f20ca41f7bdf103de9/backend/app/store/service/service.go",
      "raw_url": "https://github.com/umputun/remark42/raw/80f4862c9c2905475f3484f20ca41f7bdf103de9/backend/app/store/service/service.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/app/store/service/service.go?ref=80f4862c9c2905475f3484f20ca41f7bdf103de9",
      "patch": "@@ -846,7 +846,15 @@ func (s *DataStore) Last(siteID string, limit int, since time.Time, user store.U\n \n // Close store service\n func (s *DataStore) Close() error {\n-\treturn s.Engine.Close()\n+\terrs := new(multierror.Error)\n+\tif s.repliesCache.LoadingCache != nil {\n+\t\terrs = multierror.Append(errs, s.repliesCache.LoadingCache.Close())\n+\t}\n+\tif s.TitleExtractor != nil {\n+\t\terrs = multierror.Append(errs, s.TitleExtractor.Close())\n+\t}\n+\terrs = multierror.Append(errs, s.Engine.Close())\n+\treturn errs.ErrorOrNil()\n }\n \n func (s *DataStore) upsAndDowns(c store.Comment) (ups, downs int) {"
    },
    {
      "sha": "be4b0372fe14ca16550a794d556c5747cd008115",
      "filename": "backend/app/store/service/service_test.go",
      "status": "modified",
      "additions": 9,
      "deletions": 2,
      "changes": 11,
      "blob_url": "https://github.com/umputun/remark42/blob/80f4862c9c2905475f3484f20ca41f7bdf103de9/backend/app/store/service/service_test.go",
      "raw_url": "https://github.com/umputun/remark42/raw/80f4862c9c2905475f3484f20ca41f7bdf103de9/backend/app/store/service/service_test.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/app/store/service/service_test.go?ref=80f4862c9c2905475f3484f20ca41f7bdf103de9",
      "patch": "@@ -104,6 +104,7 @@ func TestService_CreateFromPartialWithTitle(t *testing.T) {\n \tdefer teardown()\n \tb := DataStore{Engine: eng, AdminStore: ks,\n \t\tTitleExtractor: NewTitleExtractor(http.Client{Timeout: 5 * time.Second})}\n+\tdefer b.Close()\n \n \tpostPath := \"/post/42\"\n \tpostTitle := \"Post Title 42\"\n@@ -168,6 +169,7 @@ func TestService_SetTitle(t *testing.T) {\n \tdefer teardown()\n \tb := DataStore{Engine: eng, AdminStore: ks,\n \t\tTitleExtractor: NewTitleExtractor(http.Client{Timeout: 5 * time.Second})}\n+\tdefer b.Close()\n \tcomment := store.Comment{\n \t\tText:      \"text\",\n \t\tTimestamp: time.Date(2018, 3, 25, 16, 34, 33, 0, time.UTC),\n@@ -192,8 +194,9 @@ func TestService_SetTitle(t *testing.T) {\n \trequire.NoError(t, err)\n \tassert.Equal(t, \"post1 blah 123\", c.PostTitle)\n \n-\tb = DataStore{Engine: eng, AdminStore: ks}\n-\t_, err = b.SetTitle(store.Locator{URL: tss.URL + \"/post1\", SiteID: \"radio-t\"}, id)\n+\tbErr := DataStore{Engine: eng, AdminStore: ks}\n+\tdefer bErr.Close()\n+\t_, err = bErr.SetTitle(store.Locator{URL: tss.URL + \"/post1\", SiteID: \"radio-t\"}, id)\n \trequire.EqualError(t, err, \"no title extractor\")\n }\n \n@@ -609,6 +612,7 @@ func TestService_EditComment(t *testing.T) {\n \teng, teardown := prepStoreEngine(t)\n \tdefer teardown()\n \tb := DataStore{Engine: eng, AdminStore: admin.NewStaticKeyStore(\"secret 123\")}\n+\tdefer b.Close()\n \n \tres, err := b.Last(\"radio-t\", 0, time.Time{}, store.User{})\n \tt.Logf(\"%+v\", res[0])\n@@ -638,6 +642,7 @@ func TestService_DeleteComment(t *testing.T) {\n \teng, teardown := prepStoreEngine(t)\n \tdefer teardown()\n \tb := DataStore{Engine: eng, AdminStore: admin.NewStaticKeyStore(\"secret 123\")}\n+\tdefer b.Close()\n \n \tres, err := b.Last(\"radio-t\", 0, time.Time{}, store.User{})\n \tt.Logf(\"%+v\", res[0])\n@@ -679,6 +684,7 @@ func TestService_EditCommentReplyFailed(t *testing.T) {\n \teng, teardown := prepStoreEngine(t)\n \tdefer teardown()\n \tb := DataStore{Engine: eng, AdminStore: admin.NewStaticKeyStore(\"secret 123\")}\n+\tdefer b.Close()\n \n \tres, err := b.Last(\"radio-t\", 0, time.Time{}, store.User{})\n \tt.Logf(\"%+v\", res[1])\n@@ -892,6 +898,7 @@ func TestService_HasReplies(t *testing.T) {\n \tdefer teardown()\n \tb := DataStore{Engine: eng, EditDuration: 100 * time.Millisecond,\n \t\tAdminStore: admin.NewStaticStore(\"secret 123\", []string{\"radio-t\"}, []string{\"user2\"}, \"user@email.com\")}\n+\tdefer b.Close()\n \n \tcomment := store.Comment{\n \t\tID:        \"id-1\","
    },
    {
      "sha": "0a22a28b4cb0e73d50d21ea2f9c20bec52910490",
      "filename": "backend/app/store/service/title.go",
      "status": "modified",
      "additions": 5,
      "deletions": 0,
      "changes": 5,
      "blob_url": "https://github.com/umputun/remark42/blob/80f4862c9c2905475f3484f20ca41f7bdf103de9/backend/app/store/service/title.go",
      "raw_url": "https://github.com/umputun/remark42/raw/80f4862c9c2905475f3484f20ca41f7bdf103de9/backend/app/store/service/title.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/app/store/service/title.go?ref=80f4862c9c2905475f3484f20ca41f7bdf103de9",
      "patch": "@@ -70,6 +70,11 @@ func (t *TitleExtractor) Get(url string) (string, error) {\n \treturn b.(string), nil\n }\n \n+// Close title extractor\n+func (t *TitleExtractor) Close() error {\n+\treturn t.cache.Close()\n+}\n+\n // get title from body reader, traverse recursively\n func (t *TitleExtractor) getTitle(r io.Reader) (string, bool) {\n \tdoc, err := html.Parse(r)"
    },
    {
      "sha": "959501f8e181142761f3a028c227322598bb7c1d",
      "filename": "backend/app/store/service/title_test.go",
      "status": "modified",
      "additions": 4,
      "deletions": 0,
      "changes": 4,
      "blob_url": "https://github.com/umputun/remark42/blob/80f4862c9c2905475f3484f20ca41f7bdf103de9/backend/app/store/service/title_test.go",
      "raw_url": "https://github.com/umputun/remark42/raw/80f4862c9c2905475f3484f20ca41f7bdf103de9/backend/app/store/service/title_test.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/app/store/service/title_test.go?ref=80f4862c9c2905475f3484f20ca41f7bdf103de9",
      "patch": "@@ -30,6 +30,7 @@ func TestTitle_GetTitle(t *testing.T) {\n \t}\n \n \tex := NewTitleExtractor(http.Client{Timeout: 5 * time.Second})\n+\tdefer ex.Close()\n \tfor i, tt := range tbl {\n \t\ttt := tt\n \t\tt.Run(fmt.Sprintf(\"check-%d\", i), func(t *testing.T) {\n@@ -42,6 +43,7 @@ func TestTitle_GetTitle(t *testing.T) {\n \n func TestTitle_Get(t *testing.T) {\n \tex := NewTitleExtractor(http.Client{Timeout: 5 * time.Second})\n+\tdefer ex.Close()\n \tvar hits int32\n \tts := httptest.NewServer(http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {\n \t\tif r.URL.String() == \"/good\" {\n@@ -75,6 +77,7 @@ func TestTitle_GetConcurrent(t *testing.T) {\n \t\tbody += \"something something blah blah\\n\"\n \t}\n \tex := NewTitleExtractor(http.Client{Timeout: 5 * time.Second})\n+\tdefer ex.Close()\n \tvar hits int32\n \tts := httptest.NewServer(http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {\n \t\tif strings.HasPrefix(r.URL.String(), \"/good\") {\n@@ -103,6 +106,7 @@ func TestTitle_GetConcurrent(t *testing.T) {\n \n func TestTitle_GetFailed(t *testing.T) {\n \tex := NewTitleExtractor(http.Client{Timeout: 5 * time.Second})\n+\tdefer ex.Close()\n \tvar hits int32\n \tts := httptest.NewServer(http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {\n \t\tatomic.AddInt32(&hits, 1)"
    },
    {
      "sha": "71238b38b8f9f7f9a012dd7dbe6b5718cc690d2b",
      "filename": "backend/go.mod",
      "status": "modified",
      "additions": 1,
      "deletions": 0,
      "changes": 1,
      "blob_url": "https://github.com/umputun/remark42/blob/80f4862c9c2905475f3484f20ca41f7bdf103de9/backend/go.mod",
      "raw_url": "https://github.com/umputun/remark42/raw/80f4862c9c2905475f3484f20ca41f7bdf103de9/backend/go.mod",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/go.mod?ref=80f4862c9c2905475f3484f20ca41f7bdf103de9",
      "patch": "@@ -31,6 +31,7 @@ require (\n \tgithub.com/stretchr/testify v1.5.1\n \tgithub.com/umputun/go-flags v1.5.1\n \tgo.etcd.io/bbolt v1.3.4\n+\tgo.uber.org/goleak v1.0.0\n \tgolang.org/x/crypto v0.0.0-20200406173513-056763e48d71\n \tgolang.org/x/image v0.0.0-20200119044424-58c23975cae1\n \tgolang.org/x/net v0.0.0-20200520182314-0ba52f642ac2"
    },
    {
      "sha": "7b7955ea17f1d7a70be946eb3a27d85a98e8e7e3",
      "filename": "backend/go.sum",
      "status": "modified",
      "additions": 9,
      "deletions": 0,
      "changes": 9,
      "blob_url": "https://github.com/umputun/remark42/blob/80f4862c9c2905475f3484f20ca41f7bdf103de9/backend/go.sum",
      "raw_url": "https://github.com/umputun/remark42/raw/80f4862c9c2905475f3484f20ca41f7bdf103de9/backend/go.sum",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/go.sum?ref=80f4862c9c2905475f3484f20ca41f7bdf103de9",
      "patch": "@@ -258,6 +258,8 @@ go.etcd.io/bbolt v1.3.4 h1:hi1bXHMVrlQh6WwxAy+qZCV/SYIlqo+Ushwdpa4tAKg=\n go.etcd.io/bbolt v1.3.4/go.mod h1:G5EMThwa9y8QZGBClrRx5EY+Yw9kAhnjy3bSjsnlVTQ=\n go.mongodb.org/mongo-driver v1.3.2 h1:IYppNjEV/C+/3VPbhHVxQ4t04eVW0cLp0/pNdW++6Ug=\n go.mongodb.org/mongo-driver v1.3.2/go.mod h1:MSWZXKOynuguX+JSvwP8i+58jYCXxbia8HS3gZBapIE=\n+go.uber.org/goleak v1.0.0 h1:qsup4IcBdlmsnGfqyLl4Ntn3C2XCCuKAE7DwHpScyUo=\n+go.uber.org/goleak v1.0.0/go.mod h1:8a7PlsEVH3e/a/GLqe5IIrQx6GzcnRmZEufDUTk4A7A=\n golang.org/x/crypto v0.0.0-20180904163835-0709b304e793/go.mod h1:6SG95UA2DQfeDnfUPMdvaQW0Q7yPrPDi9nlGo2tz2b4=\n golang.org/x/crypto v0.0.0-20190308221718-c2843e01d9a2/go.mod h1:djNgcEr1/C05ACkg1iLfiJU5Ep61QUkGW8qpdssI0+w=\n golang.org/x/crypto v0.0.0-20190422162423-af44ce270edf/go.mod h1:WFFai1msRO1wXaEeE5yQxYXgSfI8pQAWXbQop6sCtWE=\n@@ -266,13 +268,16 @@ golang.org/x/crypto v0.0.0-20200406173513-056763e48d71 h1:DOmugCavvUtnUD114C1Wh+\n golang.org/x/crypto v0.0.0-20200406173513-056763e48d71/go.mod h1:LzIPMQfyMNhhGPhUkYOs5KpL4U8rLKemX1yGLhDgUto=\n golang.org/x/image v0.0.0-20200119044424-58c23975cae1 h1:5h3ngYt7+vXCDZCup/HkCQgW5XwmSvR/nA2JmJ0RErg=\n golang.org/x/image v0.0.0-20200119044424-58c23975cae1/go.mod h1:FeLwcggjj3mMvU+oOTbSwawSJRM1uh48EjtB4UJZlP0=\n+golang.org/x/lint v0.0.0-20190930215403-16217165b5de h1:5hukYrvBGR8/eNkX5mdUezrA6JiaEZDtJb9Ei+1LlBs=\n+golang.org/x/lint v0.0.0-20190930215403-16217165b5de/go.mod h1:6SW0HCj/g11FgYtHlgUYUwCkIfeOF89ocIRzGO/8vkc=\n golang.org/x/net v0.0.0-20180218175443-cbe0f9307d01/go.mod h1:mL1N/T3taQHkDXs73rZJwtUhF3w3ftmwwsq0BUmARs4=\n golang.org/x/net v0.0.0-20180724234803-3673e40ba225/go.mod h1:mL1N/T3taQHkDXs73rZJwtUhF3w3ftmwwsq0BUmARs4=\n golang.org/x/net v0.0.0-20180906233101-161cd47e91fd/go.mod h1:mL1N/T3taQHkDXs73rZJwtUhF3w3ftmwwsq0BUmARs4=\n golang.org/x/net v0.0.0-20181220203305-927f97764cc3/go.mod h1:mL1N/T3taQHkDXs73rZJwtUhF3w3ftmwwsq0BUmARs4=\n golang.org/x/net v0.0.0-20190108225652-1e06a53dbb7e/go.mod h1:mL1N/T3taQHkDXs73rZJwtUhF3w3ftmwwsq0BUmARs4=\n golang.org/x/net v0.0.0-20190311183353-d8887717615a/go.mod h1:t9HGtf8HONx5eT2rtn7q6eTqICYqUVnKs3thJo3Qplg=\n golang.org/x/net v0.0.0-20190404232315-eb5bcb51f2a3/go.mod h1:t9HGtf8HONx5eT2rtn7q6eTqICYqUVnKs3thJo3Qplg=\n+golang.org/x/net v0.0.0-20190620200207-3b0461eec859/go.mod h1:z5CRVTTTmAJ677TzLLGU+0bjPO0LkuOLi4/5GtJWs/s=\n golang.org/x/net v0.0.0-20190827160401-ba9fcec4b297/go.mod h1:z5CRVTTTmAJ677TzLLGU+0bjPO0LkuOLi4/5GtJWs/s=\n golang.org/x/net v0.0.0-20190923162816-aa69164e4478/go.mod h1:z5CRVTTTmAJ677TzLLGU+0bjPO0LkuOLi4/5GtJWs/s=\n golang.org/x/net v0.0.0-20200202094626-16171245cfb2/go.mod h1:z5CRVTTTmAJ677TzLLGU+0bjPO0LkuOLi4/5GtJWs/s=\n@@ -310,11 +315,15 @@ golang.org/x/time v0.0.0-20200416051211-89c76fbcd5d1 h1:NusfzzA6yGQ+ua51ck7E3omN\n golang.org/x/time v0.0.0-20200416051211-89c76fbcd5d1/go.mod h1:tRJNPiyCQ0inRvYxbN9jk5I+vvW/OXSQhTDSoE431IQ=\n golang.org/x/tools v0.0.0-20180917221912-90fa682c2a6e/go.mod h1:n7NCudcB/nEzxVGmLbDWY5pfWTLqBcC2KZ6jyYvM4mQ=\n golang.org/x/tools v0.0.0-20181030221726-6c7e314b6563/go.mod h1:n7NCudcB/nEzxVGmLbDWY5pfWTLqBcC2KZ6jyYvM4mQ=\n+golang.org/x/tools v0.0.0-20190311212946-11955173bddd/go.mod h1:LCzVGOaR6xXOjkQ3onu1FJEFr0SW1gC7cKk1uF8kGRs=\n golang.org/x/tools v0.0.0-20190328211700-ab21143f2384/go.mod h1:LCzVGOaR6xXOjkQ3onu1FJEFr0SW1gC7cKk1uF8kGRs=\n golang.org/x/tools v0.0.0-20190329151228-23e29df326fe/go.mod h1:LCzVGOaR6xXOjkQ3onu1FJEFr0SW1gC7cKk1uF8kGRs=\n golang.org/x/tools v0.0.0-20190416151739-9c9e1878f421/go.mod h1:LCzVGOaR6xXOjkQ3onu1FJEFr0SW1gC7cKk1uF8kGRs=\n golang.org/x/tools v0.0.0-20190420181800-aa740d480789/go.mod h1:LCzVGOaR6xXOjkQ3onu1FJEFr0SW1gC7cKk1uF8kGRs=\n golang.org/x/tools v0.0.0-20190531172133-b3315ee88b7d/go.mod h1:/rFqwRUd4F7ZHNgwSSTFct+R/Kf4OFW1sUzUTQQTgfc=\n+golang.org/x/tools v0.0.0-20191108193012-7d206e10da11 h1:Yq9t9jnGoR+dBuitxdo9l6Q7xh/zOyNnYUtDKaQ3x0E=\n+golang.org/x/tools v0.0.0-20191108193012-7d206e10da11/go.mod h1:b+2E5dAYhXwXZwtnZ6UAqBI28+e2cm9otk0dWdXHAEo=\n+golang.org/x/xerrors v0.0.0-20190717185122-a985d3407aa7/go.mod h1:I/5z698sn9Ka8TeJc9MKroUUfqBBauWjQqLJ2OPfmY0=\n google.golang.org/appengine v1.4.0 h1:/wp5JvzpHIxhs/dumFmF7BXTf3Z+dd4uXta4kVyO508=\n google.golang.org/appengine v1.4.0/go.mod h1:xpcJRLb0r/rnEns0DIKYYv+WjYCduHsrkT7/EB5XEv4=\n gopkg.in/check.v1 v0.0.0-20161208181325-20d25e280405/go.mod h1:Co6ibVJAznAaIkqp8huTwlJQCZ016jof/cbN4VW5Yz0="
    },
    {
      "sha": "0fff519a4ab7b25c429206c0c589e8a651e4c74c",
      "filename": "backend/vendor/go.uber.org/goleak/.gitignore",
      "status": "added",
      "additions": 5,
      "deletions": 0,
      "changes": 5,
      "blob_url": "https://github.com/umputun/remark42/blob/80f4862c9c2905475f3484f20ca41f7bdf103de9/backend/vendor/go.uber.org/goleak/.gitignore",
      "raw_url": "https://github.com/umputun/remark42/raw/80f4862c9c2905475f3484f20ca41f7bdf103de9/backend/vendor/go.uber.org/goleak/.gitignore",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.uber.org/goleak/.gitignore?ref=80f4862c9c2905475f3484f20ca41f7bdf103de9",
      "patch": "@@ -0,0 +1,5 @@\n+vendor/\n+/bin\n+/lint.log\n+/cover.out\n+/cover.html"
    },
    {
      "sha": "b215cef17cdbad49002e68b96a92887d2c1a6b75",
      "filename": "backend/vendor/go.uber.org/goleak/.travis.yml",
      "status": "added",
      "additions": 24,
      "deletions": 0,
      "changes": 24,
      "blob_url": "https://github.com/umputun/remark42/blob/80f4862c9c2905475f3484f20ca41f7bdf103de9/backend/vendor/go.uber.org/goleak/.travis.yml",
      "raw_url": "https://github.com/umputun/remark42/raw/80f4862c9c2905475f3484f20ca41f7bdf103de9/backend/vendor/go.uber.org/goleak/.travis.yml",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.uber.org/goleak/.travis.yml?ref=80f4862c9c2905475f3484f20ca41f7bdf103de9",
      "patch": "@@ -0,0 +1,24 @@\n+sudo: false\n+language: go\n+go_import_path: go.uber.org/goleak\n+\n+env:\n+  global:\n+    - GO111MODULE=on\n+\n+matrix:\n+  include:\n+  - go: 1.12.x\n+  - go: 1.13.x\n+    env: LINT=1\n+\n+install:\n+  - make install\n+\n+script:\n+  - test -z \"$LINT\" || make lint\n+  - make test\n+\n+after_success:\n+  - make cover\n+  - bash <(curl -s https://codecov.io/bash)"
    },
    {
      "sha": "2dfb98f619becc50cbd330cd0f41cff0a3c74da5",
      "filename": "backend/vendor/go.uber.org/goleak/CHANGELOG.md",
      "status": "added",
      "additions": 17,
      "deletions": 0,
      "changes": 17,
      "blob_url": "https://github.com/umputun/remark42/blob/80f4862c9c2905475f3484f20ca41f7bdf103de9/backend/vendor/go.uber.org/goleak/CHANGELOG.md",
      "raw_url": "https://github.com/umputun/remark42/raw/80f4862c9c2905475f3484f20ca41f7bdf103de9/backend/vendor/go.uber.org/goleak/CHANGELOG.md",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.uber.org/goleak/CHANGELOG.md?ref=80f4862c9c2905475f3484f20ca41f7bdf103de9",
      "patch": "@@ -0,0 +1,17 @@\n+# Changelog\n+All notable changes to this project will be documented in this file.\n+\n+The format is based on [Keep a Changelog](http://keepachangelog.com/en/1.0.0/)\n+and this project adheres to [Semantic Versioning](http://semver.org/spec/v2.0.0.html).\n+\n+## [1.0.0]\n+### Changed\n+- Migrate to Go modules.\n+\n+### Fixed\n+- Ignore trace related goroutines that cause false positives with -trace.\n+\n+## 0.10.0\n+- Initial release.\n+\n+[1.0.0]: https://github.com/uber-go/goleak/compare/v0.10.0...v1.0.0"
    },
    {
      "sha": "6c9bde216e21179676a9025308e1610a1a93c58b",
      "filename": "backend/vendor/go.uber.org/goleak/LICENSE",
      "status": "added",
      "additions": 21,
      "deletions": 0,
      "changes": 21,
      "blob_url": "https://github.com/umputun/remark42/blob/80f4862c9c2905475f3484f20ca41f7bdf103de9/backend/vendor/go.uber.org/goleak/LICENSE",
      "raw_url": "https://github.com/umputun/remark42/raw/80f4862c9c2905475f3484f20ca41f7bdf103de9/backend/vendor/go.uber.org/goleak/LICENSE",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.uber.org/goleak/LICENSE?ref=80f4862c9c2905475f3484f20ca41f7bdf103de9",
      "patch": "@@ -0,0 +1,21 @@\n+The MIT License (MIT)\n+\n+Copyright (c) 2018 Uber Technologies, Inc.\n+\n+Permission is hereby granted, free of charge, to any person obtaining a copy\n+of this software and associated documentation files (the \"Software\"), to deal\n+in the Software without restriction, including without limitation the rights\n+to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n+copies of the Software, and to permit persons to whom the Software is\n+furnished to do so, subject to the following conditions:\n+\n+The above copyright notice and this permission notice shall be included in\n+all copies or substantial portions of the Software.\n+\n+THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n+IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n+FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n+AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n+LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n+OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n+THE SOFTWARE."
    },
    {
      "sha": "53763fa8d112aab012dd980f14a811fc270f2f47",
      "filename": "backend/vendor/go.uber.org/goleak/Makefile",
      "status": "added",
      "additions": 41,
      "deletions": 0,
      "changes": 41,
      "blob_url": "https://github.com/umputun/remark42/blob/80f4862c9c2905475f3484f20ca41f7bdf103de9/backend/vendor/go.uber.org/goleak/Makefile",
      "raw_url": "https://github.com/umputun/remark42/raw/80f4862c9c2905475f3484f20ca41f7bdf103de9/backend/vendor/go.uber.org/goleak/Makefile",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.uber.org/goleak/Makefile?ref=80f4862c9c2905475f3484f20ca41f7bdf103de9",
      "patch": "@@ -0,0 +1,41 @@\n+export GOBIN ?= $(shell pwd)/bin\n+\n+GOLINT = $(GOBIN)/golint\n+\n+GO_FILES := $(shell \\\n+\tfind . '(' -path '*/.*' -o -path './vendor' ')' -prune \\\n+\t-o -name '*.go' -print | cut -b3-)\n+\n+.PHONY: build\n+build:\n+\tgo build ./...\n+\n+.PHONY: install\n+install:\n+\tgo mod download\n+\n+.PHONY: test\n+test:\n+\tgo test -v -race ./...\n+\tgo test -v -trace=/dev/null .\n+\n+.PHONY: cover\n+cover:\n+\tgo test -race -coverprofile=cover.out -coverpkg=./... ./...\n+\tgo tool cover -html=cover.out -o cover.html\n+\n+$(GOLINT):\n+\tgo install golang.org/x/lint/golint\n+\n+.PHONY: lint\n+lint: $(GOLINT)\n+\t@rm -rf lint.log\n+\t@echo \"Checking formatting...\"\n+\t@gofmt -d -s $(GO_FILES) 2>&1 | tee lint.log\n+\t@echo \"Checking vet...\"\n+\t@go vet ./... 2>&1 | tee -a lint.log\n+\t@echo \"Checking lint...\"\n+\t@$(GOLINT) ./... 2>&1 | tee -a lint.log\n+\t@echo \"Checking for unresolved FIXMEs...\"\n+\t@git grep -i fixme | grep -v -e '^vendor/' -e '^Makefile' | tee -a lint.log\n+\t@[ ! -s lint.log ]"
    },
    {
      "sha": "8702de9fd1f09ccddaa916eb49550c4f8923941e",
      "filename": "backend/vendor/go.uber.org/goleak/README.md",
      "status": "added",
      "additions": 70,
      "deletions": 0,
      "changes": 70,
      "blob_url": "https://github.com/umputun/remark42/blob/80f4862c9c2905475f3484f20ca41f7bdf103de9/backend/vendor/go.uber.org/goleak/README.md",
      "raw_url": "https://github.com/umputun/remark42/raw/80f4862c9c2905475f3484f20ca41f7bdf103de9/backend/vendor/go.uber.org/goleak/README.md",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.uber.org/goleak/README.md?ref=80f4862c9c2905475f3484f20ca41f7bdf103de9",
      "patch": "@@ -0,0 +1,70 @@\n+# goleak [![GoDoc][doc-img]][doc] [![Build Status][ci-img]][ci] [![Coverage Status][cov-img]][cov]\n+\n+Goroutine leak detector to help avoid Goroutine leaks.\n+\n+## Development Status: Alpha\n+\n+goleak is still in development, and APIs are still in flux.\n+\n+## Installation\n+\n+You can use `go get` to get the latest version:\n+\n+`go get -u go.uber.org/goleak`\n+\n+`goleak` also supports semver releases. It is compatible with Go 1.5+.\n+\n+## Quick Start\n+\n+To verify that there are no unexpected goroutines running at the end of a test:\n+\n+```go\n+func TestA(t *testing.T) {\n+\tdefer goleak.VerifyNone(t)\n+\n+\t// test logic here.\n+}\n+```\n+\n+Instead of checking for leaks at the end of every test, `goleak` can also be run\n+at the end of every test package by creating a `TestMain` function for your\n+package:\n+\n+```go\n+func TestMain(m *testing.M) {\n+\tgoleak.VerifyTestMain(m)\n+}\n+```\n+\n+## Determine Source of Package Leaks\n+\n+When verifying leaks using `TestMain`, the leak test is only run once after all tests\n+have been run. This is typically enough to ensure there's no goroutines leaked from\n+tests, but when there are leaks, it's hard to determine which test is causing them.\n+\n+You can use the following bash script to determine the source of the failing test:\n+\n+```sh\n+# Create a test binary which will be used to run each test individually\n+$ go test -c -o tests\n+\n+# Run each test individually, printing \".\" for successful tests, or the test name\n+# for failing tests.\n+$ for test in $(go test -list . | grep \"^Test\"); do ./tests -test.run \"^$test\\$\" &>/dev/null && echo -n \".\" || echo \"\\n$test failed\"; done\n+```\n+\n+This will only print names of failing tests which can be investigated individually. E.g.,\n+\n+```\n+.....\n+TestLeakyTest failed\n+.......\n+```\n+\n+\n+[doc-img]: https://godoc.org/go.uber.org/goleak?status.svg\n+[doc]: https://godoc.org/go.uber.org/goleak\n+[ci-img]: https://travis-ci.com/uber-go/goleak.svg?branch=master\n+[ci]: https://travis-ci.com/uber-go/goleak\n+[cov-img]: https://codecov.io/gh/uber-go/goleak/branch/master/graph/badge.svg\n+[cov]: https://codecov.io/gh/uber-go/goleak"
    },
    {
      "sha": "3832f8dbc5f4519328713ea5465204bca0d3c11e",
      "filename": "backend/vendor/go.uber.org/goleak/doc.go",
      "status": "added",
      "additions": 22,
      "deletions": 0,
      "changes": 22,
      "blob_url": "https://github.com/umputun/remark42/blob/80f4862c9c2905475f3484f20ca41f7bdf103de9/backend/vendor/go.uber.org/goleak/doc.go",
      "raw_url": "https://github.com/umputun/remark42/raw/80f4862c9c2905475f3484f20ca41f7bdf103de9/backend/vendor/go.uber.org/goleak/doc.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.uber.org/goleak/doc.go?ref=80f4862c9c2905475f3484f20ca41f7bdf103de9",
      "patch": "@@ -0,0 +1,22 @@\n+// Copyright (c) 2018 Uber Technologies, Inc.\n+\n+// Permission is hereby granted, free of charge, to any person obtaining a copy\n+// of this software and associated documentation files (the \"Software\"), to deal\n+// in the Software without restriction, including without limitation the rights\n+// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n+// copies of the Software, and to permit persons to whom the Software is\n+// furnished to do so, subject to the following conditions:\n+//\n+// The above copyright notice and this permission notice shall be included in\n+// all copies or substantial portions of the Software.\n+//\n+// THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n+// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n+// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n+// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n+// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n+// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n+// THE SOFTWARE.\n+\n+// Package goleak is a Goroutine leak detector.\n+package goleak // import \"go.uber.org/goleak\""
    },
    {
      "sha": "c6e7a00a06d6cf3a02aa2c6949e89c2e317ced5e",
      "filename": "backend/vendor/go.uber.org/goleak/glide.yaml",
      "status": "added",
      "additions": 8,
      "deletions": 0,
      "changes": 8,
      "blob_url": "https://github.com/umputun/remark42/blob/80f4862c9c2905475f3484f20ca41f7bdf103de9/backend/vendor/go.uber.org/goleak/glide.yaml",
      "raw_url": "https://github.com/umputun/remark42/raw/80f4862c9c2905475f3484f20ca41f7bdf103de9/backend/vendor/go.uber.org/goleak/glide.yaml",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.uber.org/goleak/glide.yaml?ref=80f4862c9c2905475f3484f20ca41f7bdf103de9",
      "patch": "@@ -0,0 +1,8 @@\n+package: go.uber.org/goleak\n+import: []\n+testImport:\n+- package: github.com/stretchr/testify\n+  version: ^1.1.4\n+  subpackages:\n+  - assert\n+  - require"
    },
    {
      "sha": "742547abd739b892e1d80a00b9c28c44e9a689e4",
      "filename": "backend/vendor/go.uber.org/goleak/go.mod",
      "status": "added",
      "additions": 11,
      "deletions": 0,
      "changes": 11,
      "blob_url": "https://github.com/umputun/remark42/blob/80f4862c9c2905475f3484f20ca41f7bdf103de9/backend/vendor/go.uber.org/goleak/go.mod",
      "raw_url": "https://github.com/umputun/remark42/raw/80f4862c9c2905475f3484f20ca41f7bdf103de9/backend/vendor/go.uber.org/goleak/go.mod",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.uber.org/goleak/go.mod?ref=80f4862c9c2905475f3484f20ca41f7bdf103de9",
      "patch": "@@ -0,0 +1,11 @@\n+module go.uber.org/goleak\n+\n+go 1.13\n+\n+require (\n+\tgithub.com/kr/pretty v0.1.0 // indirect\n+\tgithub.com/stretchr/testify v1.4.0\n+\tgolang.org/x/lint v0.0.0-20190930215403-16217165b5de\n+\tgolang.org/x/tools v0.0.0-20191108193012-7d206e10da11 // indirect\n+\tgopkg.in/check.v1 v1.0.0-20180628173108-788fd7840127 // indirect\n+)"
    },
    {
      "sha": "09b27d7eebfb193ec63f3f3a4d2f6f6afc537284",
      "filename": "backend/vendor/go.uber.org/goleak/go.sum",
      "status": "added",
      "additions": 30,
      "deletions": 0,
      "changes": 30,
      "blob_url": "https://github.com/umputun/remark42/blob/80f4862c9c2905475f3484f20ca41f7bdf103de9/backend/vendor/go.uber.org/goleak/go.sum",
      "raw_url": "https://github.com/umputun/remark42/raw/80f4862c9c2905475f3484f20ca41f7bdf103de9/backend/vendor/go.uber.org/goleak/go.sum",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.uber.org/goleak/go.sum?ref=80f4862c9c2905475f3484f20ca41f7bdf103de9",
      "patch": "@@ -0,0 +1,30 @@\n+github.com/davecgh/go-spew v1.1.0 h1:ZDRjVQ15GmhC3fiQ8ni8+OwkZQO4DARzQgrnXU1Liz8=\n+github.com/davecgh/go-spew v1.1.0/go.mod h1:J7Y8YcW2NihsgmVo/mv3lAwl/skON4iLHjSsI+c5H38=\n+github.com/kr/pretty v0.1.0 h1:L/CwN0zerZDmRFUapSPitk6f+Q3+0za1rQkzVuMiMFI=\n+github.com/kr/pretty v0.1.0/go.mod h1:dAy3ld7l9f0ibDNOQOHHMYYIIbhfbHSm3C4ZsoJORNo=\n+github.com/kr/pty v1.1.1/go.mod h1:pFQYn66WHrOpPYNljwOMqo10TkYh1fy3cYio2l3bCsQ=\n+github.com/kr/text v0.1.0 h1:45sCR5RtlFHMR4UwH9sdQ5TC8v0qDQCHnXt+kaKSTVE=\n+github.com/kr/text v0.1.0/go.mod h1:4Jbv+DJW3UT/LiOwJeYQe1efqtUx/iVham/4vfdArNI=\n+github.com/pmezard/go-difflib v1.0.0 h1:4DBwDE0NGyQoBHbLQYPwSUPoCMWR5BEzIk/f1lZbAQM=\n+github.com/pmezard/go-difflib v1.0.0/go.mod h1:iKH77koFhYxTK1pcRnkKkqfTogsbg7gZNVY4sRDYZ/4=\n+github.com/stretchr/objx v0.1.0/go.mod h1:HFkY916IF+rwdDfMAkV7OtwuqBVzrE8GR6GFx+wExME=\n+github.com/stretchr/testify v1.4.0 h1:2E4SXV/wtOkTonXsotYi4li6zVWxYlZuYNCXe9XRJyk=\n+github.com/stretchr/testify v1.4.0/go.mod h1:j7eGeouHqKxXV5pUuKE4zz7dFj8WfuZ+81PSLYec5m4=\n+golang.org/x/crypto v0.0.0-20190308221718-c2843e01d9a2/go.mod h1:djNgcEr1/C05ACkg1iLfiJU5Ep61QUkGW8qpdssI0+w=\n+golang.org/x/lint v0.0.0-20190930215403-16217165b5de h1:5hukYrvBGR8/eNkX5mdUezrA6JiaEZDtJb9Ei+1LlBs=\n+golang.org/x/lint v0.0.0-20190930215403-16217165b5de/go.mod h1:6SW0HCj/g11FgYtHlgUYUwCkIfeOF89ocIRzGO/8vkc=\n+golang.org/x/net v0.0.0-20190311183353-d8887717615a/go.mod h1:t9HGtf8HONx5eT2rtn7q6eTqICYqUVnKs3thJo3Qplg=\n+golang.org/x/net v0.0.0-20190620200207-3b0461eec859/go.mod h1:z5CRVTTTmAJ677TzLLGU+0bjPO0LkuOLi4/5GtJWs/s=\n+golang.org/x/sync v0.0.0-20190423024810-112230192c58/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=\n+golang.org/x/sys v0.0.0-20190215142949-d0b11bdaac8a/go.mod h1:STP8DvDyc/dI5b8T5hshtkjS+E42TnysNCUPdjciGhY=\n+golang.org/x/text v0.3.0/go.mod h1:NqM8EUOU14njkJ3fqMW+pc6Ldnwhi/IjpwHt7yyuwOQ=\n+golang.org/x/tools v0.0.0-20190311212946-11955173bddd/go.mod h1:LCzVGOaR6xXOjkQ3onu1FJEFr0SW1gC7cKk1uF8kGRs=\n+golang.org/x/tools v0.0.0-20191108193012-7d206e10da11 h1:Yq9t9jnGoR+dBuitxdo9l6Q7xh/zOyNnYUtDKaQ3x0E=\n+golang.org/x/tools v0.0.0-20191108193012-7d206e10da11/go.mod h1:b+2E5dAYhXwXZwtnZ6UAqBI28+e2cm9otk0dWdXHAEo=\n+golang.org/x/xerrors v0.0.0-20190717185122-a985d3407aa7/go.mod h1:I/5z698sn9Ka8TeJc9MKroUUfqBBauWjQqLJ2OPfmY0=\n+gopkg.in/check.v1 v0.0.0-20161208181325-20d25e280405 h1:yhCVgyC4o1eVCa2tZl7eS0r+SDo693bJlVdllGtEeKM=\n+gopkg.in/check.v1 v0.0.0-20161208181325-20d25e280405/go.mod h1:Co6ibVJAznAaIkqp8huTwlJQCZ016jof/cbN4VW5Yz0=\n+gopkg.in/check.v1 v1.0.0-20180628173108-788fd7840127 h1:qIbj1fsPNlZgppZ+VLlY7N33q108Sa+fhmuc+sWQYwY=\n+gopkg.in/check.v1 v1.0.0-20180628173108-788fd7840127/go.mod h1:Co6ibVJAznAaIkqp8huTwlJQCZ016jof/cbN4VW5Yz0=\n+gopkg.in/yaml.v2 v2.2.2 h1:ZCJp+EgiOT7lHqUV2J862kp8Qj64Jo6az82+3Td9dZw=\n+gopkg.in/yaml.v2 v2.2.2/go.mod h1:hI93XBmqTisBFMUTm0b8Fm+jr3Dg1NNxqwp+5A1VGuI="
    },
    {
      "sha": "94f82e4c0d50912095215d39cfb094297db67996",
      "filename": "backend/vendor/go.uber.org/goleak/internal/stack/stacks.go",
      "status": "added",
      "additions": 155,
      "deletions": 0,
      "changes": 155,
      "blob_url": "https://github.com/umputun/remark42/blob/80f4862c9c2905475f3484f20ca41f7bdf103de9/backend/vendor/go.uber.org/goleak/internal/stack/stacks.go",
      "raw_url": "https://github.com/umputun/remark42/raw/80f4862c9c2905475f3484f20ca41f7bdf103de9/backend/vendor/go.uber.org/goleak/internal/stack/stacks.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.uber.org/goleak/internal/stack/stacks.go?ref=80f4862c9c2905475f3484f20ca41f7bdf103de9",
      "patch": "@@ -0,0 +1,155 @@\n+// Copyright (c) 2017 Uber Technologies, Inc.\n+//\n+// Permission is hereby granted, free of charge, to any person obtaining a copy\n+// of this software and associated documentation files (the \"Software\"), to deal\n+// in the Software without restriction, including without limitation the rights\n+// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n+// copies of the Software, and to permit persons to whom the Software is\n+// furnished to do so, subject to the following conditions:\n+//\n+// The above copyright notice and this permission notice shall be included in\n+// all copies or substantial portions of the Software.\n+//\n+// THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n+// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n+// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n+// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n+// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n+// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n+// THE SOFTWARE.\n+\n+package stack\n+\n+import (\n+\t\"bufio\"\n+\t\"bytes\"\n+\t\"fmt\"\n+\t\"io\"\n+\t\"runtime\"\n+\t\"strconv\"\n+\t\"strings\"\n+)\n+\n+const _defaultBufferSize = 64 * 1024 // 64 KiB\n+\n+// Stack represents a single Goroutine's stack.\n+type Stack struct {\n+\tid            int\n+\tstate         string\n+\tfirstFunction string\n+\tfullStack     *bytes.Buffer\n+}\n+\n+// ID returns the goroutine ID.\n+func (s Stack) ID() int {\n+\treturn s.id\n+}\n+\n+// State returns the Goroutine's state.\n+func (s Stack) State() string {\n+\treturn s.state\n+}\n+\n+// Full returns the full stack trace for this goroutine.\n+func (s Stack) Full() string {\n+\treturn s.fullStack.String()\n+}\n+\n+// FirstFunction returns the name of the first function on the stack.\n+func (s Stack) FirstFunction() string {\n+\treturn s.firstFunction\n+}\n+\n+func (s Stack) String() string {\n+\treturn fmt.Sprintf(\n+\t\t\"Goroutine %v in state %v, with %v on top of the stack:\\n%s\",\n+\t\ts.id, s.state, s.firstFunction, s.Full())\n+}\n+\n+func getStacks(all bool) []Stack {\n+\tvar stacks []Stack\n+\n+\tvar curStack *Stack\n+\tstackReader := bufio.NewReader(bytes.NewReader(getStackBuffer(all)))\n+\tfor {\n+\t\tline, err := stackReader.ReadString('\\n')\n+\t\tif err == io.EOF {\n+\t\t\tbreak\n+\t\t}\n+\t\tif err != nil {\n+\t\t\t// We're reading using bytes.NewReader which should never fail.\n+\t\t\tpanic(\"bufio.NewReader failed on a fixed string\")\n+\t\t}\n+\n+\t\t// If we see the goroutine header, start a new stack.\n+\t\tisFirstLine := false\n+\t\tif strings.HasPrefix(line, \"goroutine \") {\n+\t\t\t// flush any previous stack\n+\t\t\tif curStack != nil {\n+\t\t\t\tstacks = append(stacks, *curStack)\n+\t\t\t}\n+\t\t\tid, goState := parseGoStackHeader(line)\n+\t\t\tcurStack = &Stack{\n+\t\t\t\tid:        id,\n+\t\t\t\tstate:     goState,\n+\t\t\t\tfullStack: &bytes.Buffer{},\n+\t\t\t}\n+\t\t\tisFirstLine = true\n+\t\t}\n+\t\tcurStack.fullStack.WriteString(line)\n+\t\tif !isFirstLine && curStack.firstFunction == \"\" {\n+\t\t\tcurStack.firstFunction = parseFirstFunc(line)\n+\t\t}\n+\t}\n+\n+\tif curStack != nil {\n+\t\tstacks = append(stacks, *curStack)\n+\t}\n+\treturn stacks\n+}\n+\n+// All returns the stacks for all running goroutines.\n+func All() []Stack {\n+\treturn getStacks(true)\n+}\n+\n+// Current returns the stack for the current goroutine.\n+func Current() Stack {\n+\treturn getStacks(false)[0]\n+}\n+\n+func getStackBuffer(all bool) []byte {\n+\tfor i := _defaultBufferSize; ; i *= 2 {\n+\t\tbuf := make([]byte, i)\n+\t\tif n := runtime.Stack(buf, all); n < i {\n+\t\t\treturn buf[:n]\n+\t\t}\n+\t}\n+}\n+\n+func parseFirstFunc(line string) string {\n+\tline = strings.TrimSpace(line)\n+\tif idx := strings.LastIndex(line, \"(\"); idx > 0 {\n+\t\treturn line[:idx]\n+\t}\n+\tpanic(fmt.Sprintf(\"function calls missing parents: %q\", line))\n+}\n+\n+// parseGoStackHeader parses a stack header that looks like:\n+// goroutine 643 [runnable]:\\n\n+// And returns the goroutine ID, and the state.\n+func parseGoStackHeader(line string) (goroutineID int, state string) {\n+\tline = strings.TrimSuffix(line, \":\\n\")\n+\tparts := strings.SplitN(line, \" \", 3)\n+\tif len(parts) != 3 {\n+\t\tpanic(fmt.Sprintf(\"unexpected stack header format: %q\", line))\n+\t}\n+\n+\tid, err := strconv.Atoi(parts[1])\n+\tif err != nil {\n+\t\tpanic(fmt.Sprintf(\"failed to parse goroutine ID: %v in line %q\", parts[1], line))\n+\t}\n+\n+\tstate = strings.TrimSuffix(strings.TrimPrefix(parts[2], \"[\"), \"]\")\n+\treturn id, state\n+}"
    },
    {
      "sha": "468dbaf9517e22aed3176f846203d3f6dad7163f",
      "filename": "backend/vendor/go.uber.org/goleak/leaks.go",
      "status": "added",
      "additions": 80,
      "deletions": 0,
      "changes": 80,
      "blob_url": "https://github.com/umputun/remark42/blob/80f4862c9c2905475f3484f20ca41f7bdf103de9/backend/vendor/go.uber.org/goleak/leaks.go",
      "raw_url": "https://github.com/umputun/remark42/raw/80f4862c9c2905475f3484f20ca41f7bdf103de9/backend/vendor/go.uber.org/goleak/leaks.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.uber.org/goleak/leaks.go?ref=80f4862c9c2905475f3484f20ca41f7bdf103de9",
      "patch": "@@ -0,0 +1,80 @@\n+// Copyright (c) 2017 Uber Technologies, Inc.\n+\n+// Permission is hereby granted, free of charge, to any person obtaining a copy\n+// of this software and associated documentation files (the \"Software\"), to deal\n+// in the Software without restriction, including without limitation the rights\n+// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n+// copies of the Software, and to permit persons to whom the Software is\n+// furnished to do so, subject to the following conditions:\n+//\n+// The above copyright notice and this permission notice shall be included in\n+// all copies or substantial portions of the Software.\n+//\n+// THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n+// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n+// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n+// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n+// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n+// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n+// THE SOFTWARE.\n+\n+package goleak\n+\n+import (\n+\t\"fmt\"\n+\n+\t\"go.uber.org/goleak/internal/stack\"\n+)\n+\n+// TestingT is the minimal subset of testing.TB that we use.\n+type TestingT interface {\n+\tError(...interface{})\n+}\n+\n+// filterStacks will filter any stacks excluded by the given opts.\n+// filterStacks modifies the passed in stacks slice.\n+func filterStacks(stacks []stack.Stack, skipID int, opts *opts) []stack.Stack {\n+\tfiltered := stacks[:0]\n+\tfor _, stack := range stacks {\n+\t\t// Always skip the running goroutine.\n+\t\tif stack.ID() == skipID {\n+\t\t\tcontinue\n+\t\t}\n+\t\t// Run any default or user-specified filters.\n+\t\tif opts.filter(stack) {\n+\t\t\tcontinue\n+\t\t}\n+\t\tfiltered = append(filtered, stack)\n+\t}\n+\treturn filtered\n+}\n+\n+// Find looks for extra goroutines, and returns a descriptive error if\n+// any are found.\n+func Find(options ...Option) error {\n+\tcur := stack.Current().ID()\n+\n+\topts := buildOpts(options...)\n+\tvar stacks []stack.Stack\n+\tretry := true\n+\tfor i := 0; retry; i++ {\n+\t\tstacks = filterStacks(stack.All(), cur, opts)\n+\n+\t\tif len(stacks) == 0 {\n+\t\t\treturn nil\n+\t\t}\n+\t\tretry = opts.retry(i)\n+\t}\n+\n+\treturn fmt.Errorf(\"found unexpected goroutines:\\n%s\", stacks)\n+}\n+\n+// VerifyNone marks the given TestingT as failed if any extra goroutines are\n+// found by Find. This is a helper method to make it easier to integrate in\n+// tests by doing:\n+// \tdefer VerifyNone(t)\n+func VerifyNone(t TestingT, options ...Option) {\n+\tif err := Find(options...); err != nil {\n+\t\tt.Error(err)\n+\t}\n+}"
    },
    {
      "sha": "e011ba1b85d9fbfeffcdc9e20e65e5baa1be2d53",
      "filename": "backend/vendor/go.uber.org/goleak/options.go",
      "status": "added",
      "additions": 152,
      "deletions": 0,
      "changes": 152,
      "blob_url": "https://github.com/umputun/remark42/blob/80f4862c9c2905475f3484f20ca41f7bdf103de9/backend/vendor/go.uber.org/goleak/options.go",
      "raw_url": "https://github.com/umputun/remark42/raw/80f4862c9c2905475f3484f20ca41f7bdf103de9/backend/vendor/go.uber.org/goleak/options.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.uber.org/goleak/options.go?ref=80f4862c9c2905475f3484f20ca41f7bdf103de9",
      "patch": "@@ -0,0 +1,152 @@\n+// Copyright (c) 2017 Uber Technologies, Inc.\n+//\n+// Permission is hereby granted, free of charge, to any person obtaining a copy\n+// of this software and associated documentation files (the \"Software\"), to deal\n+// in the Software without restriction, including without limitation the rights\n+// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n+// copies of the Software, and to permit persons to whom the Software is\n+// furnished to do so, subject to the following conditions:\n+//\n+// The above copyright notice and this permission notice shall be included in\n+// all copies or substantial portions of the Software.\n+//\n+// THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n+// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n+// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n+// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n+// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n+// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n+// THE SOFTWARE.\n+\n+package goleak\n+\n+import (\n+\t\"strings\"\n+\t\"time\"\n+\n+\t\"go.uber.org/goleak/internal/stack\"\n+)\n+\n+// Option lets users specify custom verifications.\n+type Option interface {\n+\tapply(*opts)\n+}\n+\n+// We retry up to 20 times if we can't find the goroutine that\n+// we are looking for. In between each attempt, we will sleep for\n+// a short while to let any running goroutines complete.\n+const _defaultRetries = 20\n+\n+type opts struct {\n+\tfilters    []func(stack.Stack) bool\n+\tmaxRetries int\n+\tmaxSleep   time.Duration\n+}\n+\n+// optionFunc lets us easily write options without a custom type.\n+type optionFunc func(*opts)\n+\n+func (f optionFunc) apply(opts *opts) { f(opts) }\n+\n+// IgnoreTopFunction ignores any goroutines where the specified function\n+// is at the top of the stack. The function name should be fully qualified,\n+// e.g., go.uber.org/goleak.IgnoreTopFunction\n+func IgnoreTopFunction(f string) Option {\n+\treturn addFilter(func(s stack.Stack) bool {\n+\t\treturn s.FirstFunction() == f\n+\t})\n+}\n+\n+func maxSleep(d time.Duration) Option {\n+\treturn optionFunc(func(opts *opts) {\n+\t\topts.maxSleep = d\n+\t})\n+}\n+\n+func addFilter(f func(stack.Stack) bool) Option {\n+\treturn optionFunc(func(opts *opts) {\n+\t\topts.filters = append(opts.filters, f)\n+\t})\n+}\n+\n+func buildOpts(options ...Option) *opts {\n+\topts := &opts{\n+\t\tmaxRetries: _defaultRetries,\n+\t\tmaxSleep:   100 * time.Millisecond,\n+\t}\n+\topts.filters = append(opts.filters,\n+\t\tisTestStack,\n+\t\tisSyscallStack,\n+\t\tisStdLibStack,\n+\t\tisTraceStack,\n+\t)\n+\tfor _, option := range options {\n+\t\toption.apply(opts)\n+\t}\n+\treturn opts\n+}\n+\n+func (vo *opts) filter(s stack.Stack) bool {\n+\tfor _, filter := range vo.filters {\n+\t\tif filter(s) {\n+\t\t\treturn true\n+\t\t}\n+\t}\n+\treturn false\n+}\n+\n+func (vo *opts) retry(i int) bool {\n+\tif i >= vo.maxRetries {\n+\t\treturn false\n+\t}\n+\n+\td := time.Duration(int(time.Microsecond) << uint(i))\n+\tif d > vo.maxSleep {\n+\t\td = vo.maxSleep\n+\t}\n+\ttime.Sleep(d)\n+\treturn true\n+}\n+\n+// isTestStack is a default filter installed to automatically skip goroutines\n+// that the testing package runs while the user's tests are running.\n+func isTestStack(s stack.Stack) bool {\n+\t// Until go1.7, the main goroutine ran RunTests, which started\n+\t// the test in a separate goroutine and waited for that test goroutine\n+\t// to end by waiting on a channel.\n+\t// Since go1.7, a separate goroutine is started to wait for signals.\n+\t// T.Parallel is for parallel tests, which are blocked until all serial\n+\t// tests have run with T.Parallel at the top of the stack.\n+\tswitch s.FirstFunction() {\n+\tcase \"testing.RunTests\", \"testing.(*T).Run\", \"testing.(*T).Parallel\":\n+\t\t// In pre1.7 and post-1.7, background goroutines started by the testing\n+\t\t// package are blocked waiting on a channel.\n+\t\treturn strings.HasPrefix(s.State(), \"chan receive\")\n+\t}\n+\treturn false\n+}\n+\n+func isSyscallStack(s stack.Stack) bool {\n+\t// Typically runs in the background when code uses CGo:\n+\t// https://github.com/golang/go/issues/16714\n+\treturn s.FirstFunction() == \"runtime.goexit\" && strings.HasPrefix(s.State(), \"syscall\")\n+}\n+\n+func isStdLibStack(s stack.Stack) bool {\n+\t// Importing os/signal starts a background goroutine.\n+\t// The name of the function at the top has changed between versions.\n+\tif f := s.FirstFunction(); f == \"os/signal.signal_recv\" || f == \"os/signal.loop\" {\n+\t\treturn true\n+\t}\n+\n+\t// Using signal.Notify will start a runtime goroutine.\n+\treturn strings.Contains(s.Full(), \"runtime.ensureSigM\")\n+}\n+\n+func isTraceStack(s stack.Stack) bool {\n+\tif f := s.FirstFunction(); f != \"runtime.goparkunlock\" {\n+\t\treturn false\n+\t}\n+\n+\treturn strings.Contains(s.Full(), \"runtime.ReadTrace\")\n+}"
    },
    {
      "sha": "316f6e1badd71bdfd5a80cecae0a79af48f6adcf",
      "filename": "backend/vendor/go.uber.org/goleak/testmain.go",
      "status": "added",
      "additions": 63,
      "deletions": 0,
      "changes": 63,
      "blob_url": "https://github.com/umputun/remark42/blob/80f4862c9c2905475f3484f20ca41f7bdf103de9/backend/vendor/go.uber.org/goleak/testmain.go",
      "raw_url": "https://github.com/umputun/remark42/raw/80f4862c9c2905475f3484f20ca41f7bdf103de9/backend/vendor/go.uber.org/goleak/testmain.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.uber.org/goleak/testmain.go?ref=80f4862c9c2905475f3484f20ca41f7bdf103de9",
      "patch": "@@ -0,0 +1,63 @@\n+// Copyright (c) 2017 Uber Technologies, Inc.\n+//\n+// Permission is hereby granted, free of charge, to any person obtaining a copy\n+// of this software and associated documentation files (the \"Software\"), to deal\n+// in the Software without restriction, including without limitation the rights\n+// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n+// copies of the Software, and to permit persons to whom the Software is\n+// furnished to do so, subject to the following conditions:\n+//\n+// The above copyright notice and this permission notice shall be included in\n+// all copies or substantial portions of the Software.\n+//\n+// THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n+// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n+// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n+// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n+// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n+// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n+// THE SOFTWARE.\n+\n+package goleak\n+\n+import (\n+\t\"fmt\"\n+\t\"io\"\n+\t\"os\"\n+)\n+\n+// Variables for stubbing in unit tests.\n+var (\n+\t_osExit             = os.Exit\n+\t_osStderr io.Writer = os.Stderr\n+)\n+\n+// TestingM is the minimal subset of testing.M that we use.\n+type TestingM interface {\n+\tRun() int\n+}\n+\n+// VerifyTestMain can be used in a TestMain function for package tests to\n+// verify that there were no goroutine leaks.\n+// To use it, your TestMain function should look like:\n+//\n+//  func TestMain(m *testing.M) {\n+//    goleak.VerifyTestMain(m)\n+//  }\n+//\n+// See https://golang.org/pkg/testing/#hdr-Main for more details.\n+//\n+// This will run all tests as per normal, and if they were successful, look\n+// for any goroutine leaks and fail the tests if any leaks were found.\n+func VerifyTestMain(m TestingM, options ...Option) {\n+\texitCode := m.Run()\n+\n+\tif exitCode == 0 {\n+\t\tif err := Find(options...); err != nil {\n+\t\t\tfmt.Fprintf(_osStderr, \"goleak: Errors on successful test run: %v\\n\", err)\n+\t\t\texitCode = 1\n+\t\t}\n+\t}\n+\n+\t_osExit(exitCode)\n+}"
    },
    {
      "sha": "6a87612cc03e7d886a0731d1d05e3a792d63bb7c",
      "filename": "backend/vendor/go.uber.org/goleak/tools.go",
      "status": "added",
      "additions": 28,
      "deletions": 0,
      "changes": 28,
      "blob_url": "https://github.com/umputun/remark42/blob/80f4862c9c2905475f3484f20ca41f7bdf103de9/backend/vendor/go.uber.org/goleak/tools.go",
      "raw_url": "https://github.com/umputun/remark42/raw/80f4862c9c2905475f3484f20ca41f7bdf103de9/backend/vendor/go.uber.org/goleak/tools.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/go.uber.org/goleak/tools.go?ref=80f4862c9c2905475f3484f20ca41f7bdf103de9",
      "patch": "@@ -0,0 +1,28 @@\n+// Copyright (c) 2019 Uber Technologies, Inc.\n+//\n+// Permission is hereby granted, free of charge, to any person obtaining a copy\n+// of this software and associated documentation files (the \"Software\"), to deal\n+// in the Software without restriction, including without limitation the rights\n+// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n+// copies of the Software, and to permit persons to whom the Software is\n+// furnished to do so, subject to the following conditions:\n+//\n+// The above copyright notice and this permission notice shall be included in\n+// all copies or substantial portions of the Software.\n+//\n+// THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n+// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n+// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n+// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n+// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n+// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n+// THE SOFTWARE.\n+\n+// +build tools\n+\n+package goleak\n+\n+import (\n+\t// Tools we use during development.\n+\t_ \"golang.org/x/lint/golint\"\n+)"
    },
    {
      "sha": "50553ebd004a3da11af03520d704a97d494b28f5",
      "filename": "backend/vendor/golang.org/x/lint/.travis.yml",
      "status": "added",
      "additions": 19,
      "deletions": 0,
      "changes": 19,
      "blob_url": "https://github.com/umputun/remark42/blob/80f4862c9c2905475f3484f20ca41f7bdf103de9/backend/vendor/golang.org/x/lint/.travis.yml",
      "raw_url": "https://github.com/umputun/remark42/raw/80f4862c9c2905475f3484f20ca41f7bdf103de9/backend/vendor/golang.org/x/lint/.travis.yml",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/golang.org/x/lint/.travis.yml?ref=80f4862c9c2905475f3484f20ca41f7bdf103de9",
      "patch": "@@ -0,0 +1,19 @@\n+sudo: false\n+language: go\n+go:\n+  - 1.10.x\n+  - 1.11.x\n+  - master\n+\n+go_import_path: golang.org/x/lint\n+\n+install:\n+  - go get -t -v ./...\n+\n+script:\n+  - go test -v -race ./...\n+\n+matrix:\n+  allow_failures:\n+    - go: master\n+  fast_finish: true"
    },
    {
      "sha": "1fadda62d2fc8b451a866b645d2286615d621693",
      "filename": "backend/vendor/golang.org/x/lint/CONTRIBUTING.md",
      "status": "added",
      "additions": 15,
      "deletions": 0,
      "changes": 15,
      "blob_url": "https://github.com/umputun/remark42/blob/80f4862c9c2905475f3484f20ca41f7bdf103de9/backend/vendor/golang.org/x/lint/CONTRIBUTING.md",
      "raw_url": "https://github.com/umputun/remark42/raw/80f4862c9c2905475f3484f20ca41f7bdf103de9/backend/vendor/golang.org/x/lint/CONTRIBUTING.md",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/golang.org/x/lint/CONTRIBUTING.md?ref=80f4862c9c2905475f3484f20ca41f7bdf103de9",
      "patch": "@@ -0,0 +1,15 @@\n+# Contributing to Golint\n+\n+## Before filing an issue:\n+\n+### Are you having trouble building golint?\n+\n+Check you have the latest version of its dependencies. Run\n+```\n+go get -u golang.org/x/lint/golint\n+```\n+If you still have problems, consider searching for existing issues before filing a new issue.\n+\n+## Before sending a pull request:\n+\n+Have you understood the purpose of golint? Make sure to carefully read `README`."
    },
    {
      "sha": "65d761bc9f28c7de26b4f39c495d5ebd365b114d",
      "filename": "backend/vendor/golang.org/x/lint/LICENSE",
      "status": "added",
      "additions": 27,
      "deletions": 0,
      "changes": 27,
      "blob_url": "https://github.com/umputun/remark42/blob/80f4862c9c2905475f3484f20ca41f7bdf103de9/backend/vendor/golang.org/x/lint/LICENSE",
      "raw_url": "https://github.com/umputun/remark42/raw/80f4862c9c2905475f3484f20ca41f7bdf103de9/backend/vendor/golang.org/x/lint/LICENSE",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/golang.org/x/lint/LICENSE?ref=80f4862c9c2905475f3484f20ca41f7bdf103de9",
      "patch": "@@ -0,0 +1,27 @@\n+Copyright (c) 2013 The Go Authors. All rights reserved.\n+\n+Redistribution and use in source and binary forms, with or without\n+modification, are permitted provided that the following conditions are\n+met:\n+\n+   * Redistributions of source code must retain the above copyright\n+notice, this list of conditions and the following disclaimer.\n+   * Redistributions in binary form must reproduce the above\n+copyright notice, this list of conditions and the following disclaimer\n+in the documentation and/or other materials provided with the\n+distribution.\n+   * Neither the name of Google Inc. nor the names of its\n+contributors may be used to endorse or promote products derived from\n+this software without specific prior written permission.\n+\n+THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n+\"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\n+LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\n+A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\n+OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\n+SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\n+LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\n+DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\n+THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n+(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n+OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE."
    },
    {
      "sha": "4968b13aef7e853c7b12e0c7803d7ef8ec97843d",
      "filename": "backend/vendor/golang.org/x/lint/README.md",
      "status": "added",
      "additions": 88,
      "deletions": 0,
      "changes": 88,
      "blob_url": "https://github.com/umputun/remark42/blob/80f4862c9c2905475f3484f20ca41f7bdf103de9/backend/vendor/golang.org/x/lint/README.md",
      "raw_url": "https://github.com/umputun/remark42/raw/80f4862c9c2905475f3484f20ca41f7bdf103de9/backend/vendor/golang.org/x/lint/README.md",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/golang.org/x/lint/README.md?ref=80f4862c9c2905475f3484f20ca41f7bdf103de9",
      "patch": "@@ -0,0 +1,88 @@\n+Golint is a linter for Go source code.\n+\n+[![Build Status](https://travis-ci.org/golang/lint.svg?branch=master)](https://travis-ci.org/golang/lint)\n+\n+## Installation\n+\n+Golint requires a\n+[supported release of Go](https://golang.org/doc/devel/release.html#policy).\n+\n+    go get -u golang.org/x/lint/golint\n+\n+To find out where `golint` was installed you can run `go list -f {{.Target}} golang.org/x/lint/golint`. For `golint` to be used globally add that directory to the `$PATH` environment setting.\n+\n+## Usage\n+\n+Invoke `golint` with one or more filenames, directories, or packages named\n+by its import path. Golint uses the same\n+[import path syntax](https://golang.org/cmd/go/#hdr-Import_path_syntax) as\n+the `go` command and therefore\n+also supports relative import paths like `./...`. Additionally the `...`\n+wildcard can be used as suffix on relative and absolute file paths to recurse\n+into them.\n+\n+The output of this tool is a list of suggestions in Vim quickfix format,\n+which is accepted by lots of different editors.\n+\n+## Purpose\n+\n+Golint differs from gofmt. Gofmt reformats Go source code, whereas\n+golint prints out style mistakes.\n+\n+Golint differs from govet. Govet is concerned with correctness, whereas\n+golint is concerned with coding style. Golint is in use at Google, and it\n+seeks to match the accepted style of the open source Go project.\n+\n+The suggestions made by golint are exactly that: suggestions.\n+Golint is not perfect, and has both false positives and false negatives.\n+Do not treat its output as a gold standard. We will not be adding pragmas\n+or other knobs to suppress specific warnings, so do not expect or require\n+code to be completely \"lint-free\".\n+In short, this tool is not, and will never be, trustworthy enough for its\n+suggestions to be enforced automatically, for example as part of a build process.\n+Golint makes suggestions for many of the mechanically checkable items listed in\n+[Effective Go](https://golang.org/doc/effective_go.html) and the\n+[CodeReviewComments wiki page](https://golang.org/wiki/CodeReviewComments).\n+\n+## Scope\n+\n+Golint is meant to carry out the stylistic conventions put forth in\n+[Effective Go](https://golang.org/doc/effective_go.html) and\n+[CodeReviewComments](https://golang.org/wiki/CodeReviewComments).\n+Changes that are not aligned with those documents will not be considered.\n+\n+## Contributions\n+\n+Contributions to this project are welcome provided they are [in scope](#scope),\n+though please send mail before starting work on anything major.\n+Contributors retain their copyright, so we need you to fill out\n+[a short form](https://developers.google.com/open-source/cla/individual)\n+before we can accept your contribution.\n+\n+## Vim\n+\n+Add this to your ~/.vimrc:\n+\n+    set rtp+=$GOPATH/src/golang.org/x/lint/misc/vim\n+\n+If you have multiple entries in your GOPATH, replace `$GOPATH` with the right value.\n+\n+Running `:Lint` will run golint on the current file and populate the quickfix list.\n+\n+Optionally, add this to your `~/.vimrc` to automatically run `golint` on `:w`\n+\n+    autocmd BufWritePost,FileWritePost *.go execute 'Lint' | cwindow\n+\n+\n+## Emacs\n+\n+Add this to your `.emacs` file:\n+\n+    (add-to-list 'load-path (concat (getenv \"GOPATH\")  \"/src/golang.org/x/lint/misc/emacs/\"))\n+    (require 'golint)\n+\n+If you have multiple entries in your GOPATH, replace `$GOPATH` with the right value.\n+\n+Running M-x golint will run golint on the current file.\n+\n+For more usage, see [Compilation-Mode](http://www.gnu.org/software/emacs/manual/html_node/emacs/Compilation-Mode.html)."
    },
    {
      "sha": "d5ba4dbfd6cf086c70cc4d1dee7c5f396df5194e",
      "filename": "backend/vendor/golang.org/x/lint/go.mod",
      "status": "added",
      "additions": 3,
      "deletions": 0,
      "changes": 3,
      "blob_url": "https://github.com/umputun/remark42/blob/80f4862c9c2905475f3484f20ca41f7bdf103de9/backend/vendor/golang.org/x/lint/go.mod",
      "raw_url": "https://github.com/umputun/remark42/raw/80f4862c9c2905475f3484f20ca41f7bdf103de9/backend/vendor/golang.org/x/lint/go.mod",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/golang.org/x/lint/go.mod?ref=80f4862c9c2905475f3484f20ca41f7bdf103de9",
      "patch": "@@ -0,0 +1,3 @@\n+module golang.org/x/lint\n+\n+require golang.org/x/tools v0.0.0-20190311212946-11955173bddd"
    },
    {
      "sha": "7d0e2e61884befa50ade7d6d4c0b3d3d5c53e4bc",
      "filename": "backend/vendor/golang.org/x/lint/go.sum",
      "status": "added",
      "additions": 6,
      "deletions": 0,
      "changes": 6,
      "blob_url": "https://github.com/umputun/remark42/blob/80f4862c9c2905475f3484f20ca41f7bdf103de9/backend/vendor/golang.org/x/lint/go.sum",
      "raw_url": "https://github.com/umputun/remark42/raw/80f4862c9c2905475f3484f20ca41f7bdf103de9/backend/vendor/golang.org/x/lint/go.sum",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/golang.org/x/lint/go.sum?ref=80f4862c9c2905475f3484f20ca41f7bdf103de9",
      "patch": "@@ -0,0 +1,6 @@\n+golang.org/x/crypto v0.0.0-20190308221718-c2843e01d9a2/go.mod h1:djNgcEr1/C05ACkg1iLfiJU5Ep61QUkGW8qpdssI0+w=\n+golang.org/x/net v0.0.0-20190311183353-d8887717615a/go.mod h1:t9HGtf8HONx5eT2rtn7q6eTqICYqUVnKs3thJo3Qplg=\n+golang.org/x/sys v0.0.0-20190215142949-d0b11bdaac8a/go.mod h1:STP8DvDyc/dI5b8T5hshtkjS+E42TnysNCUPdjciGhY=\n+golang.org/x/text v0.3.0/go.mod h1:NqM8EUOU14njkJ3fqMW+pc6Ldnwhi/IjpwHt7yyuwOQ=\n+golang.org/x/tools v0.0.0-20190311212946-11955173bddd h1:/e+gpKk9r3dJobndpTytxS2gOy6m5uvpg+ISQoEcusQ=\n+golang.org/x/tools v0.0.0-20190311212946-11955173bddd/go.mod h1:LCzVGOaR6xXOjkQ3onu1FJEFr0SW1gC7cKk1uF8kGRs="
    },
    {
      "sha": "ac024b6d26f188d5880ddff524dbcb3940d4d6e7",
      "filename": "backend/vendor/golang.org/x/lint/golint/golint.go",
      "status": "added",
      "additions": 159,
      "deletions": 0,
      "changes": 159,
      "blob_url": "https://github.com/umputun/remark42/blob/80f4862c9c2905475f3484f20ca41f7bdf103de9/backend/vendor/golang.org/x/lint/golint/golint.go",
      "raw_url": "https://github.com/umputun/remark42/raw/80f4862c9c2905475f3484f20ca41f7bdf103de9/backend/vendor/golang.org/x/lint/golint/golint.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/golang.org/x/lint/golint/golint.go?ref=80f4862c9c2905475f3484f20ca41f7bdf103de9",
      "patch": "@@ -0,0 +1,159 @@\n+// Copyright (c) 2013 The Go Authors. All rights reserved.\n+//\n+// Use of this source code is governed by a BSD-style\n+// license that can be found in the LICENSE file or at\n+// https://developers.google.com/open-source/licenses/bsd.\n+\n+// golint lints the Go source files named on its command line.\n+package main\n+\n+import (\n+\t\"flag\"\n+\t\"fmt\"\n+\t\"go/build\"\n+\t\"io/ioutil\"\n+\t\"os\"\n+\t\"path/filepath\"\n+\t\"strings\"\n+\n+\t\"golang.org/x/lint\"\n+)\n+\n+var (\n+\tminConfidence = flag.Float64(\"min_confidence\", 0.8, \"minimum confidence of a problem to print it\")\n+\tsetExitStatus = flag.Bool(\"set_exit_status\", false, \"set exit status to 1 if any issues are found\")\n+\tsuggestions   int\n+)\n+\n+func usage() {\n+\tfmt.Fprintf(os.Stderr, \"Usage of %s:\\n\", os.Args[0])\n+\tfmt.Fprintf(os.Stderr, \"\\tgolint [flags] # runs on package in current directory\\n\")\n+\tfmt.Fprintf(os.Stderr, \"\\tgolint [flags] [packages]\\n\")\n+\tfmt.Fprintf(os.Stderr, \"\\tgolint [flags] [directories] # where a '/...' suffix includes all sub-directories\\n\")\n+\tfmt.Fprintf(os.Stderr, \"\\tgolint [flags] [files] # all must belong to a single package\\n\")\n+\tfmt.Fprintf(os.Stderr, \"Flags:\\n\")\n+\tflag.PrintDefaults()\n+}\n+\n+func main() {\n+\tflag.Usage = usage\n+\tflag.Parse()\n+\n+\tif flag.NArg() == 0 {\n+\t\tlintDir(\".\")\n+\t} else {\n+\t\t// dirsRun, filesRun, and pkgsRun indicate whether golint is applied to\n+\t\t// directory, file or package targets. The distinction affects which\n+\t\t// checks are run. It is no valid to mix target types.\n+\t\tvar dirsRun, filesRun, pkgsRun int\n+\t\tvar args []string\n+\t\tfor _, arg := range flag.Args() {\n+\t\t\tif strings.HasSuffix(arg, \"/...\") && isDir(arg[:len(arg)-len(\"/...\")]) {\n+\t\t\t\tdirsRun = 1\n+\t\t\t\tfor _, dirname := range allPackagesInFS(arg) {\n+\t\t\t\t\targs = append(args, dirname)\n+\t\t\t\t}\n+\t\t\t} else if isDir(arg) {\n+\t\t\t\tdirsRun = 1\n+\t\t\t\targs = append(args, arg)\n+\t\t\t} else if exists(arg) {\n+\t\t\t\tfilesRun = 1\n+\t\t\t\targs = append(args, arg)\n+\t\t\t} else {\n+\t\t\t\tpkgsRun = 1\n+\t\t\t\targs = append(args, arg)\n+\t\t\t}\n+\t\t}\n+\n+\t\tif dirsRun+filesRun+pkgsRun != 1 {\n+\t\t\tusage()\n+\t\t\tos.Exit(2)\n+\t\t}\n+\t\tswitch {\n+\t\tcase dirsRun == 1:\n+\t\t\tfor _, dir := range args {\n+\t\t\t\tlintDir(dir)\n+\t\t\t}\n+\t\tcase filesRun == 1:\n+\t\t\tlintFiles(args...)\n+\t\tcase pkgsRun == 1:\n+\t\t\tfor _, pkg := range importPaths(args) {\n+\t\t\t\tlintPackage(pkg)\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tif *setExitStatus && suggestions > 0 {\n+\t\tfmt.Fprintf(os.Stderr, \"Found %d lint suggestions; failing.\\n\", suggestions)\n+\t\tos.Exit(1)\n+\t}\n+}\n+\n+func isDir(filename string) bool {\n+\tfi, err := os.Stat(filename)\n+\treturn err == nil && fi.IsDir()\n+}\n+\n+func exists(filename string) bool {\n+\t_, err := os.Stat(filename)\n+\treturn err == nil\n+}\n+\n+func lintFiles(filenames ...string) {\n+\tfiles := make(map[string][]byte)\n+\tfor _, filename := range filenames {\n+\t\tsrc, err := ioutil.ReadFile(filename)\n+\t\tif err != nil {\n+\t\t\tfmt.Fprintln(os.Stderr, err)\n+\t\t\tcontinue\n+\t\t}\n+\t\tfiles[filename] = src\n+\t}\n+\n+\tl := new(lint.Linter)\n+\tps, err := l.LintFiles(files)\n+\tif err != nil {\n+\t\tfmt.Fprintf(os.Stderr, \"%v\\n\", err)\n+\t\treturn\n+\t}\n+\tfor _, p := range ps {\n+\t\tif p.Confidence >= *minConfidence {\n+\t\t\tfmt.Printf(\"%v: %s\\n\", p.Position, p.Text)\n+\t\t\tsuggestions++\n+\t\t}\n+\t}\n+}\n+\n+func lintDir(dirname string) {\n+\tpkg, err := build.ImportDir(dirname, 0)\n+\tlintImportedPackage(pkg, err)\n+}\n+\n+func lintPackage(pkgname string) {\n+\tpkg, err := build.Import(pkgname, \".\", 0)\n+\tlintImportedPackage(pkg, err)\n+}\n+\n+func lintImportedPackage(pkg *build.Package, err error) {\n+\tif err != nil {\n+\t\tif _, nogo := err.(*build.NoGoError); nogo {\n+\t\t\t// Don't complain if the failure is due to no Go source files.\n+\t\t\treturn\n+\t\t}\n+\t\tfmt.Fprintln(os.Stderr, err)\n+\t\treturn\n+\t}\n+\n+\tvar files []string\n+\tfiles = append(files, pkg.GoFiles...)\n+\tfiles = append(files, pkg.CgoFiles...)\n+\tfiles = append(files, pkg.TestGoFiles...)\n+\tif pkg.Dir != \".\" {\n+\t\tfor i, f := range files {\n+\t\t\tfiles[i] = filepath.Join(pkg.Dir, f)\n+\t\t}\n+\t}\n+\t// TODO(dsymonds): Do foo_test too (pkg.XTestGoFiles)\n+\n+\tlintFiles(files...)\n+}"
    },
    {
      "sha": "2ba9dea779273cfaafc5d506eb564e1ac8ac6025",
      "filename": "backend/vendor/golang.org/x/lint/golint/import.go",
      "status": "added",
      "additions": 309,
      "deletions": 0,
      "changes": 309,
      "blob_url": "https://github.com/umputun/remark42/blob/80f4862c9c2905475f3484f20ca41f7bdf103de9/backend/vendor/golang.org/x/lint/golint/import.go",
      "raw_url": "https://github.com/umputun/remark42/raw/80f4862c9c2905475f3484f20ca41f7bdf103de9/backend/vendor/golang.org/x/lint/golint/import.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/golang.org/x/lint/golint/import.go?ref=80f4862c9c2905475f3484f20ca41f7bdf103de9",
      "patch": "@@ -0,0 +1,309 @@\n+package main\n+\n+/*\n+\n+This file holds a direct copy of the import path matching code of\n+https://github.com/golang/go/blob/master/src/cmd/go/main.go. It can be\n+replaced when https://golang.org/issue/8768 is resolved.\n+\n+It has been updated to follow upstream changes in a few ways.\n+\n+*/\n+\n+import (\n+\t\"fmt\"\n+\t\"go/build\"\n+\t\"log\"\n+\t\"os\"\n+\t\"path\"\n+\t\"path/filepath\"\n+\t\"regexp\"\n+\t\"runtime\"\n+\t\"strings\"\n+)\n+\n+var (\n+\tbuildContext = build.Default\n+\tgoroot       = filepath.Clean(runtime.GOROOT())\n+\tgorootSrc    = filepath.Join(goroot, \"src\")\n+)\n+\n+// importPathsNoDotExpansion returns the import paths to use for the given\n+// command line, but it does no ... expansion.\n+func importPathsNoDotExpansion(args []string) []string {\n+\tif len(args) == 0 {\n+\t\treturn []string{\".\"}\n+\t}\n+\tvar out []string\n+\tfor _, a := range args {\n+\t\t// Arguments are supposed to be import paths, but\n+\t\t// as a courtesy to Windows developers, rewrite \\ to /\n+\t\t// in command-line arguments.  Handles .\\... and so on.\n+\t\tif filepath.Separator == '\\\\' {\n+\t\t\ta = strings.Replace(a, `\\`, `/`, -1)\n+\t\t}\n+\n+\t\t// Put argument in canonical form, but preserve leading ./.\n+\t\tif strings.HasPrefix(a, \"./\") {\n+\t\t\ta = \"./\" + path.Clean(a)\n+\t\t\tif a == \"./.\" {\n+\t\t\t\ta = \".\"\n+\t\t\t}\n+\t\t} else {\n+\t\t\ta = path.Clean(a)\n+\t\t}\n+\t\tif a == \"all\" || a == \"std\" {\n+\t\t\tout = append(out, allPackages(a)...)\n+\t\t\tcontinue\n+\t\t}\n+\t\tout = append(out, a)\n+\t}\n+\treturn out\n+}\n+\n+// importPaths returns the import paths to use for the given command line.\n+func importPaths(args []string) []string {\n+\targs = importPathsNoDotExpansion(args)\n+\tvar out []string\n+\tfor _, a := range args {\n+\t\tif strings.Contains(a, \"...\") {\n+\t\t\tif build.IsLocalImport(a) {\n+\t\t\t\tout = append(out, allPackagesInFS(a)...)\n+\t\t\t} else {\n+\t\t\t\tout = append(out, allPackages(a)...)\n+\t\t\t}\n+\t\t\tcontinue\n+\t\t}\n+\t\tout = append(out, a)\n+\t}\n+\treturn out\n+}\n+\n+// matchPattern(pattern)(name) reports whether\n+// name matches pattern.  Pattern is a limited glob\n+// pattern in which '...' means 'any string' and there\n+// is no other special syntax.\n+func matchPattern(pattern string) func(name string) bool {\n+\tre := regexp.QuoteMeta(pattern)\n+\tre = strings.Replace(re, `\\.\\.\\.`, `.*`, -1)\n+\t// Special case: foo/... matches foo too.\n+\tif strings.HasSuffix(re, `/.*`) {\n+\t\tre = re[:len(re)-len(`/.*`)] + `(/.*)?`\n+\t}\n+\treg := regexp.MustCompile(`^` + re + `$`)\n+\treturn func(name string) bool {\n+\t\treturn reg.MatchString(name)\n+\t}\n+}\n+\n+// hasPathPrefix reports whether the path s begins with the\n+// elements in prefix.\n+func hasPathPrefix(s, prefix string) bool {\n+\tswitch {\n+\tdefault:\n+\t\treturn false\n+\tcase len(s) == len(prefix):\n+\t\treturn s == prefix\n+\tcase len(s) > len(prefix):\n+\t\tif prefix != \"\" && prefix[len(prefix)-1] == '/' {\n+\t\t\treturn strings.HasPrefix(s, prefix)\n+\t\t}\n+\t\treturn s[len(prefix)] == '/' && s[:len(prefix)] == prefix\n+\t}\n+}\n+\n+// treeCanMatchPattern(pattern)(name) reports whether\n+// name or children of name can possibly match pattern.\n+// Pattern is the same limited glob accepted by matchPattern.\n+func treeCanMatchPattern(pattern string) func(name string) bool {\n+\twildCard := false\n+\tif i := strings.Index(pattern, \"...\"); i >= 0 {\n+\t\twildCard = true\n+\t\tpattern = pattern[:i]\n+\t}\n+\treturn func(name string) bool {\n+\t\treturn len(name) <= len(pattern) && hasPathPrefix(pattern, name) ||\n+\t\t\twildCard && strings.HasPrefix(name, pattern)\n+\t}\n+}\n+\n+// allPackages returns all the packages that can be found\n+// under the $GOPATH directories and $GOROOT matching pattern.\n+// The pattern is either \"all\" (all packages), \"std\" (standard packages)\n+// or a path including \"...\".\n+func allPackages(pattern string) []string {\n+\tpkgs := matchPackages(pattern)\n+\tif len(pkgs) == 0 {\n+\t\tfmt.Fprintf(os.Stderr, \"warning: %q matched no packages\\n\", pattern)\n+\t}\n+\treturn pkgs\n+}\n+\n+func matchPackages(pattern string) []string {\n+\tmatch := func(string) bool { return true }\n+\ttreeCanMatch := func(string) bool { return true }\n+\tif pattern != \"all\" && pattern != \"std\" {\n+\t\tmatch = matchPattern(pattern)\n+\t\ttreeCanMatch = treeCanMatchPattern(pattern)\n+\t}\n+\n+\thave := map[string]bool{\n+\t\t\"builtin\": true, // ignore pseudo-package that exists only for documentation\n+\t}\n+\tif !buildContext.CgoEnabled {\n+\t\thave[\"runtime/cgo\"] = true // ignore during walk\n+\t}\n+\tvar pkgs []string\n+\n+\t// Commands\n+\tcmd := filepath.Join(goroot, \"src/cmd\") + string(filepath.Separator)\n+\tfilepath.Walk(cmd, func(path string, fi os.FileInfo, err error) error {\n+\t\tif err != nil || !fi.IsDir() || path == cmd {\n+\t\t\treturn nil\n+\t\t}\n+\t\tname := path[len(cmd):]\n+\t\tif !treeCanMatch(name) {\n+\t\t\treturn filepath.SkipDir\n+\t\t}\n+\t\t// Commands are all in cmd/, not in subdirectories.\n+\t\tif strings.Contains(name, string(filepath.Separator)) {\n+\t\t\treturn filepath.SkipDir\n+\t\t}\n+\n+\t\t// We use, e.g., cmd/gofmt as the pseudo import path for gofmt.\n+\t\tname = \"cmd/\" + name\n+\t\tif have[name] {\n+\t\t\treturn nil\n+\t\t}\n+\t\thave[name] = true\n+\t\tif !match(name) {\n+\t\t\treturn nil\n+\t\t}\n+\t\t_, err = buildContext.ImportDir(path, 0)\n+\t\tif err != nil {\n+\t\t\tif _, noGo := err.(*build.NoGoError); !noGo {\n+\t\t\t\tlog.Print(err)\n+\t\t\t}\n+\t\t\treturn nil\n+\t\t}\n+\t\tpkgs = append(pkgs, name)\n+\t\treturn nil\n+\t})\n+\n+\tfor _, src := range buildContext.SrcDirs() {\n+\t\tif (pattern == \"std\" || pattern == \"cmd\") && src != gorootSrc {\n+\t\t\tcontinue\n+\t\t}\n+\t\tsrc = filepath.Clean(src) + string(filepath.Separator)\n+\t\troot := src\n+\t\tif pattern == \"cmd\" {\n+\t\t\troot += \"cmd\" + string(filepath.Separator)\n+\t\t}\n+\t\tfilepath.Walk(root, func(path string, fi os.FileInfo, err error) error {\n+\t\t\tif err != nil || !fi.IsDir() || path == src {\n+\t\t\t\treturn nil\n+\t\t\t}\n+\n+\t\t\t// Avoid .foo, _foo, and testdata directory trees.\n+\t\t\t_, elem := filepath.Split(path)\n+\t\t\tif strings.HasPrefix(elem, \".\") || strings.HasPrefix(elem, \"_\") || elem == \"testdata\" {\n+\t\t\t\treturn filepath.SkipDir\n+\t\t\t}\n+\n+\t\t\tname := filepath.ToSlash(path[len(src):])\n+\t\t\tif pattern == \"std\" && (strings.Contains(name, \".\") || name == \"cmd\") {\n+\t\t\t\t// The name \"std\" is only the standard library.\n+\t\t\t\t// If the name is cmd, it's the root of the command tree.\n+\t\t\t\treturn filepath.SkipDir\n+\t\t\t}\n+\t\t\tif !treeCanMatch(name) {\n+\t\t\t\treturn filepath.SkipDir\n+\t\t\t}\n+\t\t\tif have[name] {\n+\t\t\t\treturn nil\n+\t\t\t}\n+\t\t\thave[name] = true\n+\t\t\tif !match(name) {\n+\t\t\t\treturn nil\n+\t\t\t}\n+\t\t\t_, err = buildContext.ImportDir(path, 0)\n+\t\t\tif err != nil {\n+\t\t\t\tif _, noGo := err.(*build.NoGoError); noGo {\n+\t\t\t\t\treturn nil\n+\t\t\t\t}\n+\t\t\t}\n+\t\t\tpkgs = append(pkgs, name)\n+\t\t\treturn nil\n+\t\t})\n+\t}\n+\treturn pkgs\n+}\n+\n+// allPackagesInFS is like allPackages but is passed a pattern\n+// beginning ./ or ../, meaning it should scan the tree rooted\n+// at the given directory.  There are ... in the pattern too.\n+func allPackagesInFS(pattern string) []string {\n+\tpkgs := matchPackagesInFS(pattern)\n+\tif len(pkgs) == 0 {\n+\t\tfmt.Fprintf(os.Stderr, \"warning: %q matched no packages\\n\", pattern)\n+\t}\n+\treturn pkgs\n+}\n+\n+func matchPackagesInFS(pattern string) []string {\n+\t// Find directory to begin the scan.\n+\t// Could be smarter but this one optimization\n+\t// is enough for now, since ... is usually at the\n+\t// end of a path.\n+\ti := strings.Index(pattern, \"...\")\n+\tdir, _ := path.Split(pattern[:i])\n+\n+\t// pattern begins with ./ or ../.\n+\t// path.Clean will discard the ./ but not the ../.\n+\t// We need to preserve the ./ for pattern matching\n+\t// and in the returned import paths.\n+\tprefix := \"\"\n+\tif strings.HasPrefix(pattern, \"./\") {\n+\t\tprefix = \"./\"\n+\t}\n+\tmatch := matchPattern(pattern)\n+\n+\tvar pkgs []string\n+\tfilepath.Walk(dir, func(path string, fi os.FileInfo, err error) error {\n+\t\tif err != nil || !fi.IsDir() {\n+\t\t\treturn nil\n+\t\t}\n+\t\tif path == dir {\n+\t\t\t// filepath.Walk starts at dir and recurses. For the recursive case,\n+\t\t\t// the path is the result of filepath.Join, which calls filepath.Clean.\n+\t\t\t// The initial case is not Cleaned, though, so we do this explicitly.\n+\t\t\t//\n+\t\t\t// This converts a path like \"./io/\" to \"io\". Without this step, running\n+\t\t\t// \"cd $GOROOT/src/pkg; go list ./io/...\" would incorrectly skip the io\n+\t\t\t// package, because prepending the prefix \"./\" to the unclean path would\n+\t\t\t// result in \"././io\", and match(\"././io\") returns false.\n+\t\t\tpath = filepath.Clean(path)\n+\t\t}\n+\n+\t\t// Avoid .foo, _foo, and testdata directory trees, but do not avoid \".\" or \"..\".\n+\t\t_, elem := filepath.Split(path)\n+\t\tdot := strings.HasPrefix(elem, \".\") && elem != \".\" && elem != \"..\"\n+\t\tif dot || strings.HasPrefix(elem, \"_\") || elem == \"testdata\" {\n+\t\t\treturn filepath.SkipDir\n+\t\t}\n+\n+\t\tname := prefix + filepath.ToSlash(path)\n+\t\tif !match(name) {\n+\t\t\treturn nil\n+\t\t}\n+\t\tif _, err = build.ImportDir(path, 0); err != nil {\n+\t\t\tif _, noGo := err.(*build.NoGoError); !noGo {\n+\t\t\t\tlog.Print(err)\n+\t\t\t}\n+\t\t\treturn nil\n+\t\t}\n+\t\tpkgs = append(pkgs, name)\n+\t\treturn nil\n+\t})\n+\treturn pkgs\n+}"
    },
    {
      "sha": "d5b32f7346494262099012b582235cd1082c1977",
      "filename": "backend/vendor/golang.org/x/lint/golint/importcomment.go",
      "status": "added",
      "additions": 13,
      "deletions": 0,
      "changes": 13,
      "blob_url": "https://github.com/umputun/remark42/blob/80f4862c9c2905475f3484f20ca41f7bdf103de9/backend/vendor/golang.org/x/lint/golint/importcomment.go",
      "raw_url": "https://github.com/umputun/remark42/raw/80f4862c9c2905475f3484f20ca41f7bdf103de9/backend/vendor/golang.org/x/lint/golint/importcomment.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/golang.org/x/lint/golint/importcomment.go?ref=80f4862c9c2905475f3484f20ca41f7bdf103de9",
      "patch": "@@ -0,0 +1,13 @@\n+// Copyright (c) 2018 The Go Authors. All rights reserved.\n+//\n+// Use of this source code is governed by a BSD-style\n+// license that can be found in the LICENSE file or at\n+// https://developers.google.com/open-source/licenses/bsd.\n+\n+// +build go1.12\n+\n+// Require use of the correct import path only for Go 1.12+ users, so\n+// any breakages coincide with people updating their CI configs or\n+// whatnot.\n+\n+package main // import \"golang.org/x/lint/golint\""
    },
    {
      "sha": "532a75ad247809d394eb41795df960f18117a10f",
      "filename": "backend/vendor/golang.org/x/lint/lint.go",
      "status": "added",
      "additions": 1614,
      "deletions": 0,
      "changes": 1614,
      "blob_url": "https://github.com/umputun/remark42/blob/80f4862c9c2905475f3484f20ca41f7bdf103de9/backend/vendor/golang.org/x/lint/lint.go",
      "raw_url": "https://github.com/umputun/remark42/raw/80f4862c9c2905475f3484f20ca41f7bdf103de9/backend/vendor/golang.org/x/lint/lint.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/golang.org/x/lint/lint.go?ref=80f4862c9c2905475f3484f20ca41f7bdf103de9",
      "patch": "@@ -0,0 +1,1614 @@\n+// Copyright (c) 2013 The Go Authors. All rights reserved.\n+//\n+// Use of this source code is governed by a BSD-style\n+// license that can be found in the LICENSE file or at\n+// https://developers.google.com/open-source/licenses/bsd.\n+\n+// Package lint contains a linter for Go source code.\n+package lint // import \"golang.org/x/lint\"\n+\n+import (\n+\t\"bufio\"\n+\t\"bytes\"\n+\t\"fmt\"\n+\t\"go/ast\"\n+\t\"go/parser\"\n+\t\"go/printer\"\n+\t\"go/token\"\n+\t\"go/types\"\n+\t\"regexp\"\n+\t\"sort\"\n+\t\"strconv\"\n+\t\"strings\"\n+\t\"unicode\"\n+\t\"unicode/utf8\"\n+\n+\t\"golang.org/x/tools/go/ast/astutil\"\n+\t\"golang.org/x/tools/go/gcexportdata\"\n+)\n+\n+const styleGuideBase = \"https://golang.org/wiki/CodeReviewComments\"\n+\n+// A Linter lints Go source code.\n+type Linter struct {\n+}\n+\n+// Problem represents a problem in some source code.\n+type Problem struct {\n+\tPosition   token.Position // position in source file\n+\tText       string         // the prose that describes the problem\n+\tLink       string         // (optional) the link to the style guide for the problem\n+\tConfidence float64        // a value in (0,1] estimating the confidence in this problem's correctness\n+\tLineText   string         // the source line\n+\tCategory   string         // a short name for the general category of the problem\n+\n+\t// If the problem has a suggested fix (the minority case),\n+\t// ReplacementLine is a full replacement for the relevant line of the source file.\n+\tReplacementLine string\n+}\n+\n+func (p *Problem) String() string {\n+\tif p.Link != \"\" {\n+\t\treturn p.Text + \"\\n\\n\" + p.Link\n+\t}\n+\treturn p.Text\n+}\n+\n+type byPosition []Problem\n+\n+func (p byPosition) Len() int      { return len(p) }\n+func (p byPosition) Swap(i, j int) { p[i], p[j] = p[j], p[i] }\n+\n+func (p byPosition) Less(i, j int) bool {\n+\tpi, pj := p[i].Position, p[j].Position\n+\n+\tif pi.Filename != pj.Filename {\n+\t\treturn pi.Filename < pj.Filename\n+\t}\n+\tif pi.Line != pj.Line {\n+\t\treturn pi.Line < pj.Line\n+\t}\n+\tif pi.Column != pj.Column {\n+\t\treturn pi.Column < pj.Column\n+\t}\n+\n+\treturn p[i].Text < p[j].Text\n+}\n+\n+// Lint lints src.\n+func (l *Linter) Lint(filename string, src []byte) ([]Problem, error) {\n+\treturn l.LintFiles(map[string][]byte{filename: src})\n+}\n+\n+// LintFiles lints a set of files of a single package.\n+// The argument is a map of filename to source.\n+func (l *Linter) LintFiles(files map[string][]byte) ([]Problem, error) {\n+\tpkg := &pkg{\n+\t\tfset:  token.NewFileSet(),\n+\t\tfiles: make(map[string]*file),\n+\t}\n+\tvar pkgName string\n+\tfor filename, src := range files {\n+\t\tif isGenerated(src) {\n+\t\t\tcontinue // See issue #239\n+\t\t}\n+\t\tf, err := parser.ParseFile(pkg.fset, filename, src, parser.ParseComments)\n+\t\tif err != nil {\n+\t\t\treturn nil, err\n+\t\t}\n+\t\tif pkgName == \"\" {\n+\t\t\tpkgName = f.Name.Name\n+\t\t} else if f.Name.Name != pkgName {\n+\t\t\treturn nil, fmt.Errorf(\"%s is in package %s, not %s\", filename, f.Name.Name, pkgName)\n+\t\t}\n+\t\tpkg.files[filename] = &file{\n+\t\t\tpkg:      pkg,\n+\t\t\tf:        f,\n+\t\t\tfset:     pkg.fset,\n+\t\t\tsrc:      src,\n+\t\t\tfilename: filename,\n+\t\t}\n+\t}\n+\tif len(pkg.files) == 0 {\n+\t\treturn nil, nil\n+\t}\n+\treturn pkg.lint(), nil\n+}\n+\n+var (\n+\tgenHdr = []byte(\"// Code generated \")\n+\tgenFtr = []byte(\" DO NOT EDIT.\")\n+)\n+\n+// isGenerated reports whether the source file is generated code\n+// according the rules from https://golang.org/s/generatedcode.\n+func isGenerated(src []byte) bool {\n+\tsc := bufio.NewScanner(bytes.NewReader(src))\n+\tfor sc.Scan() {\n+\t\tb := sc.Bytes()\n+\t\tif bytes.HasPrefix(b, genHdr) && bytes.HasSuffix(b, genFtr) && len(b) >= len(genHdr)+len(genFtr) {\n+\t\t\treturn true\n+\t\t}\n+\t}\n+\treturn false\n+}\n+\n+// pkg represents a package being linted.\n+type pkg struct {\n+\tfset  *token.FileSet\n+\tfiles map[string]*file\n+\n+\ttypesPkg  *types.Package\n+\ttypesInfo *types.Info\n+\n+\t// sortable is the set of types in the package that implement sort.Interface.\n+\tsortable map[string]bool\n+\t// main is whether this is a \"main\" package.\n+\tmain bool\n+\n+\tproblems []Problem\n+}\n+\n+func (p *pkg) lint() []Problem {\n+\tif err := p.typeCheck(); err != nil {\n+\t\t/* TODO(dsymonds): Consider reporting these errors when golint operates on entire packages.\n+\t\tif e, ok := err.(types.Error); ok {\n+\t\t\tpos := p.fset.Position(e.Pos)\n+\t\t\tconf := 1.0\n+\t\t\tif strings.Contains(e.Msg, \"can't find import: \") {\n+\t\t\t\t// Golint is probably being run in a context that doesn't support\n+\t\t\t\t// typechecking (e.g. package files aren't found), so don't warn about it.\n+\t\t\t\tconf = 0\n+\t\t\t}\n+\t\t\tif conf > 0 {\n+\t\t\t\tp.errorfAt(pos, conf, category(\"typechecking\"), e.Msg)\n+\t\t\t}\n+\n+\t\t\t// TODO(dsymonds): Abort if !e.Soft?\n+\t\t}\n+\t\t*/\n+\t}\n+\n+\tp.scanSortable()\n+\tp.main = p.isMain()\n+\n+\tfor _, f := range p.files {\n+\t\tf.lint()\n+\t}\n+\n+\tsort.Sort(byPosition(p.problems))\n+\n+\treturn p.problems\n+}\n+\n+// file represents a file being linted.\n+type file struct {\n+\tpkg      *pkg\n+\tf        *ast.File\n+\tfset     *token.FileSet\n+\tsrc      []byte\n+\tfilename string\n+}\n+\n+func (f *file) isTest() bool { return strings.HasSuffix(f.filename, \"_test.go\") }\n+\n+func (f *file) lint() {\n+\tf.lintPackageComment()\n+\tf.lintImports()\n+\tf.lintBlankImports()\n+\tf.lintExported()\n+\tf.lintNames()\n+\tf.lintElses()\n+\tf.lintRanges()\n+\tf.lintErrorf()\n+\tf.lintErrors()\n+\tf.lintErrorStrings()\n+\tf.lintReceiverNames()\n+\tf.lintIncDec()\n+\tf.lintErrorReturn()\n+\tf.lintUnexportedReturn()\n+\tf.lintTimeNames()\n+\tf.lintContextKeyTypes()\n+\tf.lintContextArgs()\n+}\n+\n+type link string\n+type category string\n+\n+// The variadic arguments may start with link and category types,\n+// and must end with a format string and any arguments.\n+// It returns the new Problem.\n+func (f *file) errorf(n ast.Node, confidence float64, args ...interface{}) *Problem {\n+\tpos := f.fset.Position(n.Pos())\n+\tif pos.Filename == \"\" {\n+\t\tpos.Filename = f.filename\n+\t}\n+\treturn f.pkg.errorfAt(pos, confidence, args...)\n+}\n+\n+func (p *pkg) errorfAt(pos token.Position, confidence float64, args ...interface{}) *Problem {\n+\tproblem := Problem{\n+\t\tPosition:   pos,\n+\t\tConfidence: confidence,\n+\t}\n+\tif pos.Filename != \"\" {\n+\t\t// The file might not exist in our mapping if a //line directive was encountered.\n+\t\tif f, ok := p.files[pos.Filename]; ok {\n+\t\t\tproblem.LineText = srcLine(f.src, pos)\n+\t\t}\n+\t}\n+\n+argLoop:\n+\tfor len(args) > 1 { // always leave at least the format string in args\n+\t\tswitch v := args[0].(type) {\n+\t\tcase link:\n+\t\t\tproblem.Link = string(v)\n+\t\tcase category:\n+\t\t\tproblem.Category = string(v)\n+\t\tdefault:\n+\t\t\tbreak argLoop\n+\t\t}\n+\t\targs = args[1:]\n+\t}\n+\n+\tproblem.Text = fmt.Sprintf(args[0].(string), args[1:]...)\n+\n+\tp.problems = append(p.problems, problem)\n+\treturn &p.problems[len(p.problems)-1]\n+}\n+\n+var newImporter = func(fset *token.FileSet) types.ImporterFrom {\n+\treturn gcexportdata.NewImporter(fset, make(map[string]*types.Package))\n+}\n+\n+func (p *pkg) typeCheck() error {\n+\tconfig := &types.Config{\n+\t\t// By setting a no-op error reporter, the type checker does as much work as possible.\n+\t\tError:    func(error) {},\n+\t\tImporter: newImporter(p.fset),\n+\t}\n+\tinfo := &types.Info{\n+\t\tTypes:  make(map[ast.Expr]types.TypeAndValue),\n+\t\tDefs:   make(map[*ast.Ident]types.Object),\n+\t\tUses:   make(map[*ast.Ident]types.Object),\n+\t\tScopes: make(map[ast.Node]*types.Scope),\n+\t}\n+\tvar anyFile *file\n+\tvar astFiles []*ast.File\n+\tfor _, f := range p.files {\n+\t\tanyFile = f\n+\t\tastFiles = append(astFiles, f.f)\n+\t}\n+\tpkg, err := config.Check(anyFile.f.Name.Name, p.fset, astFiles, info)\n+\t// Remember the typechecking info, even if config.Check failed,\n+\t// since we will get partial information.\n+\tp.typesPkg = pkg\n+\tp.typesInfo = info\n+\treturn err\n+}\n+\n+func (p *pkg) typeOf(expr ast.Expr) types.Type {\n+\tif p.typesInfo == nil {\n+\t\treturn nil\n+\t}\n+\treturn p.typesInfo.TypeOf(expr)\n+}\n+\n+func (p *pkg) isNamedType(typ types.Type, importPath, name string) bool {\n+\tn, ok := typ.(*types.Named)\n+\tif !ok {\n+\t\treturn false\n+\t}\n+\ttn := n.Obj()\n+\treturn tn != nil && tn.Pkg() != nil && tn.Pkg().Path() == importPath && tn.Name() == name\n+}\n+\n+// scopeOf returns the tightest scope encompassing id.\n+func (p *pkg) scopeOf(id *ast.Ident) *types.Scope {\n+\tvar scope *types.Scope\n+\tif obj := p.typesInfo.ObjectOf(id); obj != nil {\n+\t\tscope = obj.Parent()\n+\t}\n+\tif scope == p.typesPkg.Scope() {\n+\t\t// We were given a top-level identifier.\n+\t\t// Use the file-level scope instead of the package-level scope.\n+\t\tpos := id.Pos()\n+\t\tfor _, f := range p.files {\n+\t\t\tif f.f.Pos() <= pos && pos < f.f.End() {\n+\t\t\t\tscope = p.typesInfo.Scopes[f.f]\n+\t\t\t\tbreak\n+\t\t\t}\n+\t\t}\n+\t}\n+\treturn scope\n+}\n+\n+func (p *pkg) scanSortable() {\n+\tp.sortable = make(map[string]bool)\n+\n+\t// bitfield for which methods exist on each type.\n+\tconst (\n+\t\tLen = 1 << iota\n+\t\tLess\n+\t\tSwap\n+\t)\n+\tnmap := map[string]int{\"Len\": Len, \"Less\": Less, \"Swap\": Swap}\n+\thas := make(map[string]int)\n+\tfor _, f := range p.files {\n+\t\tf.walk(func(n ast.Node) bool {\n+\t\t\tfn, ok := n.(*ast.FuncDecl)\n+\t\t\tif !ok || fn.Recv == nil || len(fn.Recv.List) == 0 {\n+\t\t\t\treturn true\n+\t\t\t}\n+\t\t\t// TODO(dsymonds): We could check the signature to be more precise.\n+\t\t\trecv := receiverType(fn)\n+\t\t\tif i, ok := nmap[fn.Name.Name]; ok {\n+\t\t\t\thas[recv] |= i\n+\t\t\t}\n+\t\t\treturn false\n+\t\t})\n+\t}\n+\tfor typ, ms := range has {\n+\t\tif ms == Len|Less|Swap {\n+\t\t\tp.sortable[typ] = true\n+\t\t}\n+\t}\n+}\n+\n+func (p *pkg) isMain() bool {\n+\tfor _, f := range p.files {\n+\t\tif f.isMain() {\n+\t\t\treturn true\n+\t\t}\n+\t}\n+\treturn false\n+}\n+\n+func (f *file) isMain() bool {\n+\tif f.f.Name.Name == \"main\" {\n+\t\treturn true\n+\t}\n+\treturn false\n+}\n+\n+// lintPackageComment checks package comments. It complains if\n+// there is no package comment, or if it is not of the right form.\n+// This has a notable false positive in that a package comment\n+// could rightfully appear in a different file of the same package,\n+// but that's not easy to fix since this linter is file-oriented.\n+func (f *file) lintPackageComment() {\n+\tif f.isTest() {\n+\t\treturn\n+\t}\n+\n+\tconst ref = styleGuideBase + \"#package-comments\"\n+\tprefix := \"Package \" + f.f.Name.Name + \" \"\n+\n+\t// Look for a detached package comment.\n+\t// First, scan for the last comment that occurs before the \"package\" keyword.\n+\tvar lastCG *ast.CommentGroup\n+\tfor _, cg := range f.f.Comments {\n+\t\tif cg.Pos() > f.f.Package {\n+\t\t\t// Gone past \"package\" keyword.\n+\t\t\tbreak\n+\t\t}\n+\t\tlastCG = cg\n+\t}\n+\tif lastCG != nil && strings.HasPrefix(lastCG.Text(), prefix) {\n+\t\tendPos := f.fset.Position(lastCG.End())\n+\t\tpkgPos := f.fset.Position(f.f.Package)\n+\t\tif endPos.Line+1 < pkgPos.Line {\n+\t\t\t// There isn't a great place to anchor this error;\n+\t\t\t// the start of the blank lines between the doc and the package statement\n+\t\t\t// is at least pointing at the location of the problem.\n+\t\t\tpos := token.Position{\n+\t\t\t\tFilename: endPos.Filename,\n+\t\t\t\t// Offset not set; it is non-trivial, and doesn't appear to be needed.\n+\t\t\t\tLine:   endPos.Line + 1,\n+\t\t\t\tColumn: 1,\n+\t\t\t}\n+\t\t\tf.pkg.errorfAt(pos, 0.9, link(ref), category(\"comments\"), \"package comment is detached; there should be no blank lines between it and the package statement\")\n+\t\t\treturn\n+\t\t}\n+\t}\n+\n+\tif f.f.Doc == nil {\n+\t\tf.errorf(f.f, 0.2, link(ref), category(\"comments\"), \"should have a package comment, unless it's in another file for this package\")\n+\t\treturn\n+\t}\n+\ts := f.f.Doc.Text()\n+\tif ts := strings.TrimLeft(s, \" \\t\"); ts != s {\n+\t\tf.errorf(f.f.Doc, 1, link(ref), category(\"comments\"), \"package comment should not have leading space\")\n+\t\ts = ts\n+\t}\n+\t// Only non-main packages need to keep to this form.\n+\tif !f.pkg.main && !strings.HasPrefix(s, prefix) {\n+\t\tf.errorf(f.f.Doc, 1, link(ref), category(\"comments\"), `package comment should be of the form \"%s...\"`, prefix)\n+\t}\n+}\n+\n+// lintBlankImports complains if a non-main package has blank imports that are\n+// not documented.\n+func (f *file) lintBlankImports() {\n+\t// In package main and in tests, we don't complain about blank imports.\n+\tif f.pkg.main || f.isTest() {\n+\t\treturn\n+\t}\n+\n+\t// The first element of each contiguous group of blank imports should have\n+\t// an explanatory comment of some kind.\n+\tfor i, imp := range f.f.Imports {\n+\t\tpos := f.fset.Position(imp.Pos())\n+\n+\t\tif !isBlank(imp.Name) {\n+\t\t\tcontinue // Ignore non-blank imports.\n+\t\t}\n+\t\tif i > 0 {\n+\t\t\tprev := f.f.Imports[i-1]\n+\t\t\tprevPos := f.fset.Position(prev.Pos())\n+\t\t\tif isBlank(prev.Name) && prevPos.Line+1 == pos.Line {\n+\t\t\t\tcontinue // A subsequent blank in a group.\n+\t\t\t}\n+\t\t}\n+\n+\t\t// This is the first blank import of a group.\n+\t\tif imp.Doc == nil && imp.Comment == nil {\n+\t\t\tref := \"\"\n+\t\t\tf.errorf(imp, 1, link(ref), category(\"imports\"), \"a blank import should be only in a main or test package, or have a comment justifying it\")\n+\t\t}\n+\t}\n+}\n+\n+// lintImports examines import blocks.\n+func (f *file) lintImports() {\n+\tfor i, is := range f.f.Imports {\n+\t\t_ = i\n+\t\tif is.Name != nil && is.Name.Name == \".\" && !f.isTest() {\n+\t\t\tf.errorf(is, 1, link(styleGuideBase+\"#import-dot\"), category(\"imports\"), \"should not use dot imports\")\n+\t\t}\n+\n+\t}\n+}\n+\n+const docCommentsLink = styleGuideBase + \"#doc-comments\"\n+\n+// lintExported examines the exported names.\n+// It complains if any required doc comments are missing,\n+// or if they are not of the right form. The exact rules are in\n+// lintFuncDoc, lintTypeDoc and lintValueSpecDoc; this function\n+// also tracks the GenDecl structure being traversed to permit\n+// doc comments for constants to be on top of the const block.\n+// It also complains if the names stutter when combined with\n+// the package name.\n+func (f *file) lintExported() {\n+\tif f.isTest() {\n+\t\treturn\n+\t}\n+\n+\tvar lastGen *ast.GenDecl // last GenDecl entered.\n+\n+\t// Set of GenDecls that have already had missing comments flagged.\n+\tgenDeclMissingComments := make(map[*ast.GenDecl]bool)\n+\n+\tf.walk(func(node ast.Node) bool {\n+\t\tswitch v := node.(type) {\n+\t\tcase *ast.GenDecl:\n+\t\t\tif v.Tok == token.IMPORT {\n+\t\t\t\treturn false\n+\t\t\t}\n+\t\t\t// token.CONST, token.TYPE or token.VAR\n+\t\t\tlastGen = v\n+\t\t\treturn true\n+\t\tcase *ast.FuncDecl:\n+\t\t\tf.lintFuncDoc(v)\n+\t\t\tif v.Recv == nil {\n+\t\t\t\t// Only check for stutter on functions, not methods.\n+\t\t\t\t// Method names are not used package-qualified.\n+\t\t\t\tf.checkStutter(v.Name, \"func\")\n+\t\t\t}\n+\t\t\t// Don't proceed inside funcs.\n+\t\t\treturn false\n+\t\tcase *ast.TypeSpec:\n+\t\t\t// inside a GenDecl, which usually has the doc\n+\t\t\tdoc := v.Doc\n+\t\t\tif doc == nil {\n+\t\t\t\tdoc = lastGen.Doc\n+\t\t\t}\n+\t\t\tf.lintTypeDoc(v, doc)\n+\t\t\tf.checkStutter(v.Name, \"type\")\n+\t\t\t// Don't proceed inside types.\n+\t\t\treturn false\n+\t\tcase *ast.ValueSpec:\n+\t\t\tf.lintValueSpecDoc(v, lastGen, genDeclMissingComments)\n+\t\t\treturn false\n+\t\t}\n+\t\treturn true\n+\t})\n+}\n+\n+var (\n+\tallCapsRE = regexp.MustCompile(`^[A-Z0-9_]+$`)\n+\tanyCapsRE = regexp.MustCompile(`[A-Z]`)\n+)\n+\n+// knownNameExceptions is a set of names that are known to be exempt from naming checks.\n+// This is usually because they are constrained by having to match names in the\n+// standard library.\n+var knownNameExceptions = map[string]bool{\n+\t\"LastInsertId\": true, // must match database/sql\n+\t\"kWh\":          true,\n+}\n+\n+func isInTopLevel(f *ast.File, ident *ast.Ident) bool {\n+\tpath, _ := astutil.PathEnclosingInterval(f, ident.Pos(), ident.End())\n+\tfor _, f := range path {\n+\t\tswitch f.(type) {\n+\t\tcase *ast.File, *ast.GenDecl, *ast.ValueSpec, *ast.Ident:\n+\t\t\tcontinue\n+\t\t}\n+\t\treturn false\n+\t}\n+\treturn true\n+}\n+\n+// lintNames examines all names in the file.\n+// It complains if any use underscores or incorrect known initialisms.\n+func (f *file) lintNames() {\n+\t// Package names need slightly different handling than other names.\n+\tif strings.Contains(f.f.Name.Name, \"_\") && !strings.HasSuffix(f.f.Name.Name, \"_test\") {\n+\t\tf.errorf(f.f, 1, link(\"http://golang.org/doc/effective_go.html#package-names\"), category(\"naming\"), \"don't use an underscore in package name\")\n+\t}\n+\tif anyCapsRE.MatchString(f.f.Name.Name) {\n+\t\tf.errorf(f.f, 1, link(\"http://golang.org/doc/effective_go.html#package-names\"), category(\"mixed-caps\"), \"don't use MixedCaps in package name; %s should be %s\", f.f.Name.Name, strings.ToLower(f.f.Name.Name))\n+\t}\n+\n+\tcheck := func(id *ast.Ident, thing string) {\n+\t\tif id.Name == \"_\" {\n+\t\t\treturn\n+\t\t}\n+\t\tif knownNameExceptions[id.Name] {\n+\t\t\treturn\n+\t\t}\n+\n+\t\t// Handle two common styles from other languages that don't belong in Go.\n+\t\tif len(id.Name) >= 5 && allCapsRE.MatchString(id.Name) && strings.Contains(id.Name, \"_\") {\n+\t\t\tcapCount := 0\n+\t\t\tfor _, c := range id.Name {\n+\t\t\t\tif 'A' <= c && c <= 'Z' {\n+\t\t\t\t\tcapCount++\n+\t\t\t\t}\n+\t\t\t}\n+\t\t\tif capCount >= 2 {\n+\t\t\t\tf.errorf(id, 0.8, link(styleGuideBase+\"#mixed-caps\"), category(\"naming\"), \"don't use ALL_CAPS in Go names; use CamelCase\")\n+\t\t\t\treturn\n+\t\t\t}\n+\t\t}\n+\t\tif thing == \"const\" || (thing == \"var\" && isInTopLevel(f.f, id)) {\n+\t\t\tif len(id.Name) > 2 && id.Name[0] == 'k' && id.Name[1] >= 'A' && id.Name[1] <= 'Z' {\n+\t\t\t\tshould := string(id.Name[1]+'a'-'A') + id.Name[2:]\n+\t\t\t\tf.errorf(id, 0.8, link(styleGuideBase+\"#mixed-caps\"), category(\"naming\"), \"don't use leading k in Go names; %s %s should be %s\", thing, id.Name, should)\n+\t\t\t}\n+\t\t}\n+\n+\t\tshould := lintName(id.Name)\n+\t\tif id.Name == should {\n+\t\t\treturn\n+\t\t}\n+\n+\t\tif len(id.Name) > 2 && strings.Contains(id.Name[1:], \"_\") {\n+\t\t\tf.errorf(id, 0.9, link(\"http://golang.org/doc/effective_go.html#mixed-caps\"), category(\"naming\"), \"don't use underscores in Go names; %s %s should be %s\", thing, id.Name, should)\n+\t\t\treturn\n+\t\t}\n+\t\tf.errorf(id, 0.8, link(styleGuideBase+\"#initialisms\"), category(\"naming\"), \"%s %s should be %s\", thing, id.Name, should)\n+\t}\n+\tcheckList := func(fl *ast.FieldList, thing string) {\n+\t\tif fl == nil {\n+\t\t\treturn\n+\t\t}\n+\t\tfor _, f := range fl.List {\n+\t\t\tfor _, id := range f.Names {\n+\t\t\t\tcheck(id, thing)\n+\t\t\t}\n+\t\t}\n+\t}\n+\tf.walk(func(node ast.Node) bool {\n+\t\tswitch v := node.(type) {\n+\t\tcase *ast.AssignStmt:\n+\t\t\tif v.Tok == token.ASSIGN {\n+\t\t\t\treturn true\n+\t\t\t}\n+\t\t\tfor _, exp := range v.Lhs {\n+\t\t\t\tif id, ok := exp.(*ast.Ident); ok {\n+\t\t\t\t\tcheck(id, \"var\")\n+\t\t\t\t}\n+\t\t\t}\n+\t\tcase *ast.FuncDecl:\n+\t\t\tif f.isTest() && (strings.HasPrefix(v.Name.Name, \"Example\") || strings.HasPrefix(v.Name.Name, \"Test\") || strings.HasPrefix(v.Name.Name, \"Benchmark\")) {\n+\t\t\t\treturn true\n+\t\t\t}\n+\n+\t\t\tthing := \"func\"\n+\t\t\tif v.Recv != nil {\n+\t\t\t\tthing = \"method\"\n+\t\t\t}\n+\n+\t\t\t// Exclude naming warnings for functions that are exported to C but\n+\t\t\t// not exported in the Go API.\n+\t\t\t// See https://github.com/golang/lint/issues/144.\n+\t\t\tif ast.IsExported(v.Name.Name) || !isCgoExported(v) {\n+\t\t\t\tcheck(v.Name, thing)\n+\t\t\t}\n+\n+\t\t\tcheckList(v.Type.Params, thing+\" parameter\")\n+\t\t\tcheckList(v.Type.Results, thing+\" result\")\n+\t\tcase *ast.GenDecl:\n+\t\t\tif v.Tok == token.IMPORT {\n+\t\t\t\treturn true\n+\t\t\t}\n+\t\t\tvar thing string\n+\t\t\tswitch v.Tok {\n+\t\t\tcase token.CONST:\n+\t\t\t\tthing = \"const\"\n+\t\t\tcase token.TYPE:\n+\t\t\t\tthing = \"type\"\n+\t\t\tcase token.VAR:\n+\t\t\t\tthing = \"var\"\n+\t\t\t}\n+\t\t\tfor _, spec := range v.Specs {\n+\t\t\t\tswitch s := spec.(type) {\n+\t\t\t\tcase *ast.TypeSpec:\n+\t\t\t\t\tcheck(s.Name, thing)\n+\t\t\t\tcase *ast.ValueSpec:\n+\t\t\t\t\tfor _, id := range s.Names {\n+\t\t\t\t\t\tcheck(id, thing)\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t}\n+\t\tcase *ast.InterfaceType:\n+\t\t\t// Do not check interface method names.\n+\t\t\t// They are often constrainted by the method names of concrete types.\n+\t\t\tfor _, x := range v.Methods.List {\n+\t\t\t\tft, ok := x.Type.(*ast.FuncType)\n+\t\t\t\tif !ok { // might be an embedded interface name\n+\t\t\t\t\tcontinue\n+\t\t\t\t}\n+\t\t\t\tcheckList(ft.Params, \"interface method parameter\")\n+\t\t\t\tcheckList(ft.Results, \"interface method result\")\n+\t\t\t}\n+\t\tcase *ast.RangeStmt:\n+\t\t\tif v.Tok == token.ASSIGN {\n+\t\t\t\treturn true\n+\t\t\t}\n+\t\t\tif id, ok := v.Key.(*ast.Ident); ok {\n+\t\t\t\tcheck(id, \"range var\")\n+\t\t\t}\n+\t\t\tif id, ok := v.Value.(*ast.Ident); ok {\n+\t\t\t\tcheck(id, \"range var\")\n+\t\t\t}\n+\t\tcase *ast.StructType:\n+\t\t\tfor _, f := range v.Fields.List {\n+\t\t\t\tfor _, id := range f.Names {\n+\t\t\t\t\tcheck(id, \"struct field\")\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t\treturn true\n+\t})\n+}\n+\n+// lintName returns a different name if it should be different.\n+func lintName(name string) (should string) {\n+\t// Fast path for simple cases: \"_\" and all lowercase.\n+\tif name == \"_\" {\n+\t\treturn name\n+\t}\n+\tallLower := true\n+\tfor _, r := range name {\n+\t\tif !unicode.IsLower(r) {\n+\t\t\tallLower = false\n+\t\t\tbreak\n+\t\t}\n+\t}\n+\tif allLower {\n+\t\treturn name\n+\t}\n+\n+\t// Split camelCase at any lower->upper transition, and split on underscores.\n+\t// Check each word for common initialisms.\n+\trunes := []rune(name)\n+\tw, i := 0, 0 // index of start of word, scan\n+\tfor i+1 <= len(runes) {\n+\t\teow := false // whether we hit the end of a word\n+\t\tif i+1 == len(runes) {\n+\t\t\teow = true\n+\t\t} else if runes[i+1] == '_' {\n+\t\t\t// underscore; shift the remainder forward over any run of underscores\n+\t\t\teow = true\n+\t\t\tn := 1\n+\t\t\tfor i+n+1 < len(runes) && runes[i+n+1] == '_' {\n+\t\t\t\tn++\n+\t\t\t}\n+\n+\t\t\t// Leave at most one underscore if the underscore is between two digits\n+\t\t\tif i+n+1 < len(runes) && unicode.IsDigit(runes[i]) && unicode.IsDigit(runes[i+n+1]) {\n+\t\t\t\tn--\n+\t\t\t}\n+\n+\t\t\tcopy(runes[i+1:], runes[i+n+1:])\n+\t\t\trunes = runes[:len(runes)-n]\n+\t\t} else if unicode.IsLower(runes[i]) && !unicode.IsLower(runes[i+1]) {\n+\t\t\t// lower->non-lower\n+\t\t\teow = true\n+\t\t}\n+\t\ti++\n+\t\tif !eow {\n+\t\t\tcontinue\n+\t\t}\n+\n+\t\t// [w,i) is a word.\n+\t\tword := string(runes[w:i])\n+\t\tif u := strings.ToUpper(word); commonInitialisms[u] {\n+\t\t\t// Keep consistent case, which is lowercase only at the start.\n+\t\t\tif w == 0 && unicode.IsLower(runes[w]) {\n+\t\t\t\tu = strings.ToLower(u)\n+\t\t\t}\n+\t\t\t// All the common initialisms are ASCII,\n+\t\t\t// so we can replace the bytes exactly.\n+\t\t\tcopy(runes[w:], []rune(u))\n+\t\t} else if w > 0 && strings.ToLower(word) == word {\n+\t\t\t// already all lowercase, and not the first word, so uppercase the first character.\n+\t\t\trunes[w] = unicode.ToUpper(runes[w])\n+\t\t}\n+\t\tw = i\n+\t}\n+\treturn string(runes)\n+}\n+\n+// commonInitialisms is a set of common initialisms.\n+// Only add entries that are highly unlikely to be non-initialisms.\n+// For instance, \"ID\" is fine (Freudian code is rare), but \"AND\" is not.\n+var commonInitialisms = map[string]bool{\n+\t\"ACL\":   true,\n+\t\"API\":   true,\n+\t\"ASCII\": true,\n+\t\"CPU\":   true,\n+\t\"CSS\":   true,\n+\t\"DNS\":   true,\n+\t\"EOF\":   true,\n+\t\"GUID\":  true,\n+\t\"HTML\":  true,\n+\t\"HTTP\":  true,\n+\t\"HTTPS\": true,\n+\t\"ID\":    true,\n+\t\"IP\":    true,\n+\t\"JSON\":  true,\n+\t\"LHS\":   true,\n+\t\"QPS\":   true,\n+\t\"RAM\":   true,\n+\t\"RHS\":   true,\n+\t\"RPC\":   true,\n+\t\"SLA\":   true,\n+\t\"SMTP\":  true,\n+\t\"SQL\":   true,\n+\t\"SSH\":   true,\n+\t\"TCP\":   true,\n+\t\"TLS\":   true,\n+\t\"TTL\":   true,\n+\t\"UDP\":   true,\n+\t\"UI\":    true,\n+\t\"UID\":   true,\n+\t\"UUID\":  true,\n+\t\"URI\":   true,\n+\t\"URL\":   true,\n+\t\"UTF8\":  true,\n+\t\"VM\":    true,\n+\t\"XML\":   true,\n+\t\"XMPP\":  true,\n+\t\"XSRF\":  true,\n+\t\"XSS\":   true,\n+}\n+\n+// lintTypeDoc examines the doc comment on a type.\n+// It complains if they are missing from an exported type,\n+// or if they are not of the standard form.\n+func (f *file) lintTypeDoc(t *ast.TypeSpec, doc *ast.CommentGroup) {\n+\tif !ast.IsExported(t.Name.Name) {\n+\t\treturn\n+\t}\n+\tif doc == nil {\n+\t\tf.errorf(t, 1, link(docCommentsLink), category(\"comments\"), \"exported type %v should have comment or be unexported\", t.Name)\n+\t\treturn\n+\t}\n+\n+\ts := doc.Text()\n+\tarticles := [...]string{\"A\", \"An\", \"The\"}\n+\tfor _, a := range articles {\n+\t\tif strings.HasPrefix(s, a+\" \") {\n+\t\t\ts = s[len(a)+1:]\n+\t\t\tbreak\n+\t\t}\n+\t}\n+\tif !strings.HasPrefix(s, t.Name.Name+\" \") {\n+\t\tf.errorf(doc, 1, link(docCommentsLink), category(\"comments\"), `comment on exported type %v should be of the form \"%v ...\" (with optional leading article)`, t.Name, t.Name)\n+\t}\n+}\n+\n+var commonMethods = map[string]bool{\n+\t\"Error\":     true,\n+\t\"Read\":      true,\n+\t\"ServeHTTP\": true,\n+\t\"String\":    true,\n+\t\"Write\":     true,\n+}\n+\n+// lintFuncDoc examines doc comments on functions and methods.\n+// It complains if they are missing, or not of the right form.\n+// It has specific exclusions for well-known methods (see commonMethods above).\n+func (f *file) lintFuncDoc(fn *ast.FuncDecl) {\n+\tif !ast.IsExported(fn.Name.Name) {\n+\t\t// func is unexported\n+\t\treturn\n+\t}\n+\tkind := \"function\"\n+\tname := fn.Name.Name\n+\tif fn.Recv != nil && len(fn.Recv.List) > 0 {\n+\t\t// method\n+\t\tkind = \"method\"\n+\t\trecv := receiverType(fn)\n+\t\tif !ast.IsExported(recv) {\n+\t\t\t// receiver is unexported\n+\t\t\treturn\n+\t\t}\n+\t\tif commonMethods[name] {\n+\t\t\treturn\n+\t\t}\n+\t\tswitch name {\n+\t\tcase \"Len\", \"Less\", \"Swap\":\n+\t\t\tif f.pkg.sortable[recv] {\n+\t\t\t\treturn\n+\t\t\t}\n+\t\t}\n+\t\tname = recv + \".\" + name\n+\t}\n+\tif fn.Doc == nil {\n+\t\tf.errorf(fn, 1, link(docCommentsLink), category(\"comments\"), \"exported %s %s should have comment or be unexported\", kind, name)\n+\t\treturn\n+\t}\n+\ts := fn.Doc.Text()\n+\tprefix := fn.Name.Name + \" \"\n+\tif !strings.HasPrefix(s, prefix) {\n+\t\tf.errorf(fn.Doc, 1, link(docCommentsLink), category(\"comments\"), `comment on exported %s %s should be of the form \"%s...\"`, kind, name, prefix)\n+\t}\n+}\n+\n+// lintValueSpecDoc examines package-global variables and constants.\n+// It complains if they are not individually declared,\n+// or if they are not suitably documented in the right form (unless they are in a block that is commented).\n+func (f *file) lintValueSpecDoc(vs *ast.ValueSpec, gd *ast.GenDecl, genDeclMissingComments map[*ast.GenDecl]bool) {\n+\tkind := \"var\"\n+\tif gd.Tok == token.CONST {\n+\t\tkind = \"const\"\n+\t}\n+\n+\tif len(vs.Names) > 1 {\n+\t\t// Check that none are exported except for the first.\n+\t\tfor _, n := range vs.Names[1:] {\n+\t\t\tif ast.IsExported(n.Name) {\n+\t\t\t\tf.errorf(vs, 1, category(\"comments\"), \"exported %s %s should have its own declaration\", kind, n.Name)\n+\t\t\t\treturn\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\t// Only one name.\n+\tname := vs.Names[0].Name\n+\tif !ast.IsExported(name) {\n+\t\treturn\n+\t}\n+\n+\tif vs.Doc == nil && gd.Doc == nil {\n+\t\tif genDeclMissingComments[gd] {\n+\t\t\treturn\n+\t\t}\n+\t\tblock := \"\"\n+\t\tif kind == \"const\" && gd.Lparen.IsValid() {\n+\t\t\tblock = \" (or a comment on this block)\"\n+\t\t}\n+\t\tf.errorf(vs, 1, link(docCommentsLink), category(\"comments\"), \"exported %s %s should have comment%s or be unexported\", kind, name, block)\n+\t\tgenDeclMissingComments[gd] = true\n+\t\treturn\n+\t}\n+\t// If this GenDecl has parens and a comment, we don't check its comment form.\n+\tif gd.Lparen.IsValid() && gd.Doc != nil {\n+\t\treturn\n+\t}\n+\t// The relevant text to check will be on either vs.Doc or gd.Doc.\n+\t// Use vs.Doc preferentially.\n+\tdoc := vs.Doc\n+\tif doc == nil {\n+\t\tdoc = gd.Doc\n+\t}\n+\tprefix := name + \" \"\n+\tif !strings.HasPrefix(doc.Text(), prefix) {\n+\t\tf.errorf(doc, 1, link(docCommentsLink), category(\"comments\"), `comment on exported %s %s should be of the form \"%s...\"`, kind, name, prefix)\n+\t}\n+}\n+\n+func (f *file) checkStutter(id *ast.Ident, thing string) {\n+\tpkg, name := f.f.Name.Name, id.Name\n+\tif !ast.IsExported(name) {\n+\t\t// unexported name\n+\t\treturn\n+\t}\n+\t// A name stutters if the package name is a strict prefix\n+\t// and the next character of the name starts a new word.\n+\tif len(name) <= len(pkg) {\n+\t\t// name is too short to stutter.\n+\t\t// This permits the name to be the same as the package name.\n+\t\treturn\n+\t}\n+\tif !strings.EqualFold(pkg, name[:len(pkg)]) {\n+\t\treturn\n+\t}\n+\t// We can assume the name is well-formed UTF-8.\n+\t// If the next rune after the package name is uppercase or an underscore\n+\t// the it's starting a new word and thus this name stutters.\n+\trem := name[len(pkg):]\n+\tif next, _ := utf8.DecodeRuneInString(rem); next == '_' || unicode.IsUpper(next) {\n+\t\tf.errorf(id, 0.8, link(styleGuideBase+\"#package-names\"), category(\"naming\"), \"%s name will be used as %s.%s by other packages, and that stutters; consider calling this %s\", thing, pkg, name, rem)\n+\t}\n+}\n+\n+// zeroLiteral is a set of ast.BasicLit values that are zero values.\n+// It is not exhaustive.\n+var zeroLiteral = map[string]bool{\n+\t\"false\": true, // bool\n+\t// runes\n+\t`'\\x00'`: true,\n+\t`'\\000'`: true,\n+\t// strings\n+\t`\"\"`: true,\n+\t\"``\": true,\n+\t// numerics\n+\t\"0\":   true,\n+\t\"0.\":  true,\n+\t\"0.0\": true,\n+\t\"0i\":  true,\n+}\n+\n+// lintElses examines else blocks. It complains about any else block whose if block ends in a return.\n+func (f *file) lintElses() {\n+\t// We don't want to flag if { } else if { } else { } constructions.\n+\t// They will appear as an IfStmt whose Else field is also an IfStmt.\n+\t// Record such a node so we ignore it when we visit it.\n+\tignore := make(map[*ast.IfStmt]bool)\n+\n+\tf.walk(func(node ast.Node) bool {\n+\t\tifStmt, ok := node.(*ast.IfStmt)\n+\t\tif !ok || ifStmt.Else == nil {\n+\t\t\treturn true\n+\t\t}\n+\t\tif elseif, ok := ifStmt.Else.(*ast.IfStmt); ok {\n+\t\t\tignore[elseif] = true\n+\t\t\treturn true\n+\t\t}\n+\t\tif ignore[ifStmt] {\n+\t\t\treturn true\n+\t\t}\n+\t\tif _, ok := ifStmt.Else.(*ast.BlockStmt); !ok {\n+\t\t\t// only care about elses without conditions\n+\t\t\treturn true\n+\t\t}\n+\t\tif len(ifStmt.Body.List) == 0 {\n+\t\t\treturn true\n+\t\t}\n+\t\tshortDecl := false // does the if statement have a \":=\" initialization statement?\n+\t\tif ifStmt.Init != nil {\n+\t\t\tif as, ok := ifStmt.Init.(*ast.AssignStmt); ok && as.Tok == token.DEFINE {\n+\t\t\t\tshortDecl = true\n+\t\t\t}\n+\t\t}\n+\t\tlastStmt := ifStmt.Body.List[len(ifStmt.Body.List)-1]\n+\t\tif _, ok := lastStmt.(*ast.ReturnStmt); ok {\n+\t\t\textra := \"\"\n+\t\t\tif shortDecl {\n+\t\t\t\textra = \" (move short variable declaration to its own line if necessary)\"\n+\t\t\t}\n+\t\t\tf.errorf(ifStmt.Else, 1, link(styleGuideBase+\"#indent-error-flow\"), category(\"indent\"), \"if block ends with a return statement, so drop this else and outdent its block\"+extra)\n+\t\t}\n+\t\treturn true\n+\t})\n+}\n+\n+// lintRanges examines range clauses. It complains about redundant constructions.\n+func (f *file) lintRanges() {\n+\tf.walk(func(node ast.Node) bool {\n+\t\trs, ok := node.(*ast.RangeStmt)\n+\t\tif !ok {\n+\t\t\treturn true\n+\t\t}\n+\n+\t\tif isIdent(rs.Key, \"_\") && (rs.Value == nil || isIdent(rs.Value, \"_\")) {\n+\t\t\tp := f.errorf(rs.Key, 1, category(\"range-loop\"), \"should omit values from range; this loop is equivalent to `for range ...`\")\n+\n+\t\t\tnewRS := *rs // shallow copy\n+\t\t\tnewRS.Value = nil\n+\t\t\tnewRS.Key = nil\n+\t\t\tp.ReplacementLine = f.firstLineOf(&newRS, rs)\n+\n+\t\t\treturn true\n+\t\t}\n+\n+\t\tif isIdent(rs.Value, \"_\") {\n+\t\t\tp := f.errorf(rs.Value, 1, category(\"range-loop\"), \"should omit 2nd value from range; this loop is equivalent to `for %s %s range ...`\", f.render(rs.Key), rs.Tok)\n+\n+\t\t\tnewRS := *rs // shallow copy\n+\t\t\tnewRS.Value = nil\n+\t\t\tp.ReplacementLine = f.firstLineOf(&newRS, rs)\n+\t\t}\n+\n+\t\treturn true\n+\t})\n+}\n+\n+// lintErrorf examines errors.New and testing.Error calls. It complains if its only argument is an fmt.Sprintf invocation.\n+func (f *file) lintErrorf() {\n+\tf.walk(func(node ast.Node) bool {\n+\t\tce, ok := node.(*ast.CallExpr)\n+\t\tif !ok || len(ce.Args) != 1 {\n+\t\t\treturn true\n+\t\t}\n+\t\tisErrorsNew := isPkgDot(ce.Fun, \"errors\", \"New\")\n+\t\tvar isTestingError bool\n+\t\tse, ok := ce.Fun.(*ast.SelectorExpr)\n+\t\tif ok && se.Sel.Name == \"Error\" {\n+\t\t\tif typ := f.pkg.typeOf(se.X); typ != nil {\n+\t\t\t\tisTestingError = typ.String() == \"*testing.T\"\n+\t\t\t}\n+\t\t}\n+\t\tif !isErrorsNew && !isTestingError {\n+\t\t\treturn true\n+\t\t}\n+\t\tif !f.imports(\"errors\") {\n+\t\t\treturn true\n+\t\t}\n+\t\targ := ce.Args[0]\n+\t\tce, ok = arg.(*ast.CallExpr)\n+\t\tif !ok || !isPkgDot(ce.Fun, \"fmt\", \"Sprintf\") {\n+\t\t\treturn true\n+\t\t}\n+\t\terrorfPrefix := \"fmt\"\n+\t\tif isTestingError {\n+\t\t\terrorfPrefix = f.render(se.X)\n+\t\t}\n+\t\tp := f.errorf(node, 1, category(\"errors\"), \"should replace %s(fmt.Sprintf(...)) with %s.Errorf(...)\", f.render(se), errorfPrefix)\n+\n+\t\tm := f.srcLineWithMatch(ce, `^(.*)`+f.render(se)+`\\(fmt\\.Sprintf\\((.*)\\)\\)(.*)$`)\n+\t\tif m != nil {\n+\t\t\tp.ReplacementLine = m[1] + errorfPrefix + \".Errorf(\" + m[2] + \")\" + m[3]\n+\t\t}\n+\n+\t\treturn true\n+\t})\n+}\n+\n+// lintErrors examines global error vars. It complains if they aren't named in the standard way.\n+func (f *file) lintErrors() {\n+\tfor _, decl := range f.f.Decls {\n+\t\tgd, ok := decl.(*ast.GenDecl)\n+\t\tif !ok || gd.Tok != token.VAR {\n+\t\t\tcontinue\n+\t\t}\n+\t\tfor _, spec := range gd.Specs {\n+\t\t\tspec := spec.(*ast.ValueSpec)\n+\t\t\tif len(spec.Names) != 1 || len(spec.Values) != 1 {\n+\t\t\t\tcontinue\n+\t\t\t}\n+\t\t\tce, ok := spec.Values[0].(*ast.CallExpr)\n+\t\t\tif !ok {\n+\t\t\t\tcontinue\n+\t\t\t}\n+\t\t\tif !isPkgDot(ce.Fun, \"errors\", \"New\") && !isPkgDot(ce.Fun, \"fmt\", \"Errorf\") {\n+\t\t\t\tcontinue\n+\t\t\t}\n+\n+\t\t\tid := spec.Names[0]\n+\t\t\tprefix := \"err\"\n+\t\t\tif id.IsExported() {\n+\t\t\t\tprefix = \"Err\"\n+\t\t\t}\n+\t\t\tif !strings.HasPrefix(id.Name, prefix) {\n+\t\t\t\tf.errorf(id, 0.9, category(\"naming\"), \"error var %s should have name of the form %sFoo\", id.Name, prefix)\n+\t\t\t}\n+\t\t}\n+\t}\n+}\n+\n+func lintErrorString(s string) (isClean bool, conf float64) {\n+\tconst basicConfidence = 0.8\n+\tconst capConfidence = basicConfidence - 0.2\n+\tfirst, firstN := utf8.DecodeRuneInString(s)\n+\tlast, _ := utf8.DecodeLastRuneInString(s)\n+\tif last == '.' || last == ':' || last == '!' || last == '\\n' {\n+\t\treturn false, basicConfidence\n+\t}\n+\tif unicode.IsUpper(first) {\n+\t\t// People use proper nouns and exported Go identifiers in error strings,\n+\t\t// so decrease the confidence of warnings for capitalization.\n+\t\tif len(s) <= firstN {\n+\t\t\treturn false, capConfidence\n+\t\t}\n+\t\t// Flag strings starting with something that doesn't look like an initialism.\n+\t\tif second, _ := utf8.DecodeRuneInString(s[firstN:]); !unicode.IsUpper(second) {\n+\t\t\treturn false, capConfidence\n+\t\t}\n+\t}\n+\treturn true, 0\n+}\n+\n+// lintErrorStrings examines error strings.\n+// It complains if they are capitalized or end in punctuation or a newline.\n+func (f *file) lintErrorStrings() {\n+\tf.walk(func(node ast.Node) bool {\n+\t\tce, ok := node.(*ast.CallExpr)\n+\t\tif !ok {\n+\t\t\treturn true\n+\t\t}\n+\t\tif !isPkgDot(ce.Fun, \"errors\", \"New\") && !isPkgDot(ce.Fun, \"fmt\", \"Errorf\") {\n+\t\t\treturn true\n+\t\t}\n+\t\tif len(ce.Args) < 1 {\n+\t\t\treturn true\n+\t\t}\n+\t\tstr, ok := ce.Args[0].(*ast.BasicLit)\n+\t\tif !ok || str.Kind != token.STRING {\n+\t\t\treturn true\n+\t\t}\n+\t\ts, _ := strconv.Unquote(str.Value) // can assume well-formed Go\n+\t\tif s == \"\" {\n+\t\t\treturn true\n+\t\t}\n+\t\tclean, conf := lintErrorString(s)\n+\t\tif clean {\n+\t\t\treturn true\n+\t\t}\n+\n+\t\tf.errorf(str, conf, link(styleGuideBase+\"#error-strings\"), category(\"errors\"),\n+\t\t\t\"error strings should not be capitalized or end with punctuation or a newline\")\n+\t\treturn true\n+\t})\n+}\n+\n+// lintReceiverNames examines receiver names. It complains about inconsistent\n+// names used for the same type and names such as \"this\".\n+func (f *file) lintReceiverNames() {\n+\ttypeReceiver := map[string]string{}\n+\tf.walk(func(n ast.Node) bool {\n+\t\tfn, ok := n.(*ast.FuncDecl)\n+\t\tif !ok || fn.Recv == nil || len(fn.Recv.List) == 0 {\n+\t\t\treturn true\n+\t\t}\n+\t\tnames := fn.Recv.List[0].Names\n+\t\tif len(names) < 1 {\n+\t\t\treturn true\n+\t\t}\n+\t\tname := names[0].Name\n+\t\tconst ref = styleGuideBase + \"#receiver-names\"\n+\t\tif name == \"_\" {\n+\t\t\tf.errorf(n, 1, link(ref), category(\"naming\"), `receiver name should not be an underscore, omit the name if it is unused`)\n+\t\t\treturn true\n+\t\t}\n+\t\tif name == \"this\" || name == \"self\" {\n+\t\t\tf.errorf(n, 1, link(ref), category(\"naming\"), `receiver name should be a reflection of its identity; don't use generic names such as \"this\" or \"self\"`)\n+\t\t\treturn true\n+\t\t}\n+\t\trecv := receiverType(fn)\n+\t\tif prev, ok := typeReceiver[recv]; ok && prev != name {\n+\t\t\tf.errorf(n, 1, link(ref), category(\"naming\"), \"receiver name %s should be consistent with previous receiver name %s for %s\", name, prev, recv)\n+\t\t\treturn true\n+\t\t}\n+\t\ttypeReceiver[recv] = name\n+\t\treturn true\n+\t})\n+}\n+\n+// lintIncDec examines statements that increment or decrement a variable.\n+// It complains if they don't use x++ or x--.\n+func (f *file) lintIncDec() {\n+\tf.walk(func(n ast.Node) bool {\n+\t\tas, ok := n.(*ast.AssignStmt)\n+\t\tif !ok {\n+\t\t\treturn true\n+\t\t}\n+\t\tif len(as.Lhs) != 1 {\n+\t\t\treturn true\n+\t\t}\n+\t\tif !isOne(as.Rhs[0]) {\n+\t\t\treturn true\n+\t\t}\n+\t\tvar suffix string\n+\t\tswitch as.Tok {\n+\t\tcase token.ADD_ASSIGN:\n+\t\t\tsuffix = \"++\"\n+\t\tcase token.SUB_ASSIGN:\n+\t\t\tsuffix = \"--\"\n+\t\tdefault:\n+\t\t\treturn true\n+\t\t}\n+\t\tf.errorf(as, 0.8, category(\"unary-op\"), \"should replace %s with %s%s\", f.render(as), f.render(as.Lhs[0]), suffix)\n+\t\treturn true\n+\t})\n+}\n+\n+// lintErrorReturn examines function declarations that return an error.\n+// It complains if the error isn't the last parameter.\n+func (f *file) lintErrorReturn() {\n+\tf.walk(func(n ast.Node) bool {\n+\t\tfn, ok := n.(*ast.FuncDecl)\n+\t\tif !ok || fn.Type.Results == nil {\n+\t\t\treturn true\n+\t\t}\n+\t\tret := fn.Type.Results.List\n+\t\tif len(ret) <= 1 {\n+\t\t\treturn true\n+\t\t}\n+\t\tif isIdent(ret[len(ret)-1].Type, \"error\") {\n+\t\t\treturn true\n+\t\t}\n+\t\t// An error return parameter should be the last parameter.\n+\t\t// Flag any error parameters found before the last.\n+\t\tfor _, r := range ret[:len(ret)-1] {\n+\t\t\tif isIdent(r.Type, \"error\") {\n+\t\t\t\tf.errorf(fn, 0.9, category(\"arg-order\"), \"error should be the last type when returning multiple items\")\n+\t\t\t\tbreak // only flag one\n+\t\t\t}\n+\t\t}\n+\t\treturn true\n+\t})\n+}\n+\n+// lintUnexportedReturn examines exported function declarations.\n+// It complains if any return an unexported type.\n+func (f *file) lintUnexportedReturn() {\n+\tf.walk(func(n ast.Node) bool {\n+\t\tfn, ok := n.(*ast.FuncDecl)\n+\t\tif !ok {\n+\t\t\treturn true\n+\t\t}\n+\t\tif fn.Type.Results == nil {\n+\t\t\treturn false\n+\t\t}\n+\t\tif !fn.Name.IsExported() {\n+\t\t\treturn false\n+\t\t}\n+\t\tthing := \"func\"\n+\t\tif fn.Recv != nil && len(fn.Recv.List) > 0 {\n+\t\t\tthing = \"method\"\n+\t\t\tif !ast.IsExported(receiverType(fn)) {\n+\t\t\t\t// Don't report exported methods of unexported types,\n+\t\t\t\t// such as private implementations of sort.Interface.\n+\t\t\t\treturn false\n+\t\t\t}\n+\t\t}\n+\t\tfor _, ret := range fn.Type.Results.List {\n+\t\t\ttyp := f.pkg.typeOf(ret.Type)\n+\t\t\tif exportedType(typ) {\n+\t\t\t\tcontinue\n+\t\t\t}\n+\t\t\tf.errorf(ret.Type, 0.8, category(\"unexported-type-in-api\"),\n+\t\t\t\t\"exported %s %s returns unexported type %s, which can be annoying to use\",\n+\t\t\t\tthing, fn.Name.Name, typ)\n+\t\t\tbreak // only flag one\n+\t\t}\n+\t\treturn false\n+\t})\n+}\n+\n+// exportedType reports whether typ is an exported type.\n+// It is imprecise, and will err on the side of returning true,\n+// such as for composite types.\n+func exportedType(typ types.Type) bool {\n+\tswitch T := typ.(type) {\n+\tcase *types.Named:\n+\t\t// Builtin types have no package.\n+\t\treturn T.Obj().Pkg() == nil || T.Obj().Exported()\n+\tcase *types.Map:\n+\t\treturn exportedType(T.Key()) && exportedType(T.Elem())\n+\tcase interface {\n+\t\tElem() types.Type\n+\t}: // array, slice, pointer, chan\n+\t\treturn exportedType(T.Elem())\n+\t}\n+\t// Be conservative about other types, such as struct, interface, etc.\n+\treturn true\n+}\n+\n+// timeSuffixes is a list of name suffixes that imply a time unit.\n+// This is not an exhaustive list.\n+var timeSuffixes = []string{\n+\t\"Sec\", \"Secs\", \"Seconds\",\n+\t\"Msec\", \"Msecs\",\n+\t\"Milli\", \"Millis\", \"Milliseconds\",\n+\t\"Usec\", \"Usecs\", \"Microseconds\",\n+\t\"MS\", \"Ms\",\n+}\n+\n+func (f *file) lintTimeNames() {\n+\tf.walk(func(node ast.Node) bool {\n+\t\tv, ok := node.(*ast.ValueSpec)\n+\t\tif !ok {\n+\t\t\treturn true\n+\t\t}\n+\t\tfor _, name := range v.Names {\n+\t\t\torigTyp := f.pkg.typeOf(name)\n+\t\t\t// Look for time.Duration or *time.Duration;\n+\t\t\t// the latter is common when using flag.Duration.\n+\t\t\ttyp := origTyp\n+\t\t\tif pt, ok := typ.(*types.Pointer); ok {\n+\t\t\t\ttyp = pt.Elem()\n+\t\t\t}\n+\t\t\tif !f.pkg.isNamedType(typ, \"time\", \"Duration\") {\n+\t\t\t\tcontinue\n+\t\t\t}\n+\t\t\tsuffix := \"\"\n+\t\t\tfor _, suf := range timeSuffixes {\n+\t\t\t\tif strings.HasSuffix(name.Name, suf) {\n+\t\t\t\t\tsuffix = suf\n+\t\t\t\t\tbreak\n+\t\t\t\t}\n+\t\t\t}\n+\t\t\tif suffix == \"\" {\n+\t\t\t\tcontinue\n+\t\t\t}\n+\t\t\tf.errorf(v, 0.9, category(\"time\"), \"var %s is of type %v; don't use unit-specific suffix %q\", name.Name, origTyp, suffix)\n+\t\t}\n+\t\treturn true\n+\t})\n+}\n+\n+// lintContextKeyTypes checks for call expressions to context.WithValue with\n+// basic types used for the key argument.\n+// See: https://golang.org/issue/17293\n+func (f *file) lintContextKeyTypes() {\n+\tf.walk(func(node ast.Node) bool {\n+\t\tswitch node := node.(type) {\n+\t\tcase *ast.CallExpr:\n+\t\t\tf.checkContextKeyType(node)\n+\t\t}\n+\n+\t\treturn true\n+\t})\n+}\n+\n+// checkContextKeyType reports an error if the call expression calls\n+// context.WithValue with a key argument of basic type.\n+func (f *file) checkContextKeyType(x *ast.CallExpr) {\n+\tsel, ok := x.Fun.(*ast.SelectorExpr)\n+\tif !ok {\n+\t\treturn\n+\t}\n+\tpkg, ok := sel.X.(*ast.Ident)\n+\tif !ok || pkg.Name != \"context\" {\n+\t\treturn\n+\t}\n+\tif sel.Sel.Name != \"WithValue\" {\n+\t\treturn\n+\t}\n+\n+\t// key is second argument to context.WithValue\n+\tif len(x.Args) != 3 {\n+\t\treturn\n+\t}\n+\tkey := f.pkg.typesInfo.Types[x.Args[1]]\n+\n+\tif ktyp, ok := key.Type.(*types.Basic); ok && ktyp.Kind() != types.Invalid {\n+\t\tf.errorf(x, 1.0, category(\"context\"), fmt.Sprintf(\"should not use basic type %s as key in context.WithValue\", key.Type))\n+\t}\n+}\n+\n+// lintContextArgs examines function declarations that contain an\n+// argument with a type of context.Context\n+// It complains if that argument isn't the first parameter.\n+func (f *file) lintContextArgs() {\n+\tf.walk(func(n ast.Node) bool {\n+\t\tfn, ok := n.(*ast.FuncDecl)\n+\t\tif !ok || len(fn.Type.Params.List) <= 1 {\n+\t\t\treturn true\n+\t\t}\n+\t\t// A context.Context should be the first parameter of a function.\n+\t\t// Flag any that show up after the first.\n+\t\tfor _, arg := range fn.Type.Params.List[1:] {\n+\t\t\tif isPkgDot(arg.Type, \"context\", \"Context\") {\n+\t\t\t\tf.errorf(fn, 0.9, link(\"https://golang.org/pkg/context/\"), category(\"arg-order\"), \"context.Context should be the first parameter of a function\")\n+\t\t\t\tbreak // only flag one\n+\t\t\t}\n+\t\t}\n+\t\treturn true\n+\t})\n+}\n+\n+// containsComments returns whether the interval [start, end) contains any\n+// comments without \"// MATCH \" prefix.\n+func (f *file) containsComments(start, end token.Pos) bool {\n+\tfor _, cgroup := range f.f.Comments {\n+\t\tcomments := cgroup.List\n+\t\tif comments[0].Slash >= end {\n+\t\t\t// All comments starting with this group are after end pos.\n+\t\t\treturn false\n+\t\t}\n+\t\tif comments[len(comments)-1].Slash < start {\n+\t\t\t// Comments group ends before start pos.\n+\t\t\tcontinue\n+\t\t}\n+\t\tfor _, c := range comments {\n+\t\t\tif start <= c.Slash && c.Slash < end && !strings.HasPrefix(c.Text, \"// MATCH \") {\n+\t\t\t\treturn true\n+\t\t\t}\n+\t\t}\n+\t}\n+\treturn false\n+}\n+\n+// receiverType returns the named type of the method receiver, sans \"*\",\n+// or \"invalid-type\" if fn.Recv is ill formed.\n+func receiverType(fn *ast.FuncDecl) string {\n+\tswitch e := fn.Recv.List[0].Type.(type) {\n+\tcase *ast.Ident:\n+\t\treturn e.Name\n+\tcase *ast.StarExpr:\n+\t\tif id, ok := e.X.(*ast.Ident); ok {\n+\t\t\treturn id.Name\n+\t\t}\n+\t}\n+\t// The parser accepts much more than just the legal forms.\n+\treturn \"invalid-type\"\n+}\n+\n+func (f *file) walk(fn func(ast.Node) bool) {\n+\tast.Walk(walker(fn), f.f)\n+}\n+\n+func (f *file) render(x interface{}) string {\n+\tvar buf bytes.Buffer\n+\tif err := printer.Fprint(&buf, f.fset, x); err != nil {\n+\t\tpanic(err)\n+\t}\n+\treturn buf.String()\n+}\n+\n+func (f *file) debugRender(x interface{}) string {\n+\tvar buf bytes.Buffer\n+\tif err := ast.Fprint(&buf, f.fset, x, nil); err != nil {\n+\t\tpanic(err)\n+\t}\n+\treturn buf.String()\n+}\n+\n+// walker adapts a function to satisfy the ast.Visitor interface.\n+// The function return whether the walk should proceed into the node's children.\n+type walker func(ast.Node) bool\n+\n+func (w walker) Visit(node ast.Node) ast.Visitor {\n+\tif w(node) {\n+\t\treturn w\n+\t}\n+\treturn nil\n+}\n+\n+func isIdent(expr ast.Expr, ident string) bool {\n+\tid, ok := expr.(*ast.Ident)\n+\treturn ok && id.Name == ident\n+}\n+\n+// isBlank returns whether id is the blank identifier \"_\".\n+// If id == nil, the answer is false.\n+func isBlank(id *ast.Ident) bool { return id != nil && id.Name == \"_\" }\n+\n+func isPkgDot(expr ast.Expr, pkg, name string) bool {\n+\tsel, ok := expr.(*ast.SelectorExpr)\n+\treturn ok && isIdent(sel.X, pkg) && isIdent(sel.Sel, name)\n+}\n+\n+func isOne(expr ast.Expr) bool {\n+\tlit, ok := expr.(*ast.BasicLit)\n+\treturn ok && lit.Kind == token.INT && lit.Value == \"1\"\n+}\n+\n+func isCgoExported(f *ast.FuncDecl) bool {\n+\tif f.Recv != nil || f.Doc == nil {\n+\t\treturn false\n+\t}\n+\n+\tcgoExport := regexp.MustCompile(fmt.Sprintf(\"(?m)^//export %s$\", regexp.QuoteMeta(f.Name.Name)))\n+\tfor _, c := range f.Doc.List {\n+\t\tif cgoExport.MatchString(c.Text) {\n+\t\t\treturn true\n+\t\t}\n+\t}\n+\treturn false\n+}\n+\n+var basicTypeKinds = map[types.BasicKind]string{\n+\ttypes.UntypedBool:    \"bool\",\n+\ttypes.UntypedInt:     \"int\",\n+\ttypes.UntypedRune:    \"rune\",\n+\ttypes.UntypedFloat:   \"float64\",\n+\ttypes.UntypedComplex: \"complex128\",\n+\ttypes.UntypedString:  \"string\",\n+}\n+\n+// isUntypedConst reports whether expr is an untyped constant,\n+// and indicates what its default type is.\n+// scope may be nil.\n+func (f *file) isUntypedConst(expr ast.Expr) (defType string, ok bool) {\n+\t// Re-evaluate expr outside of its context to see if it's untyped.\n+\t// (An expr evaluated within, for example, an assignment context will get the type of the LHS.)\n+\texprStr := f.render(expr)\n+\ttv, err := types.Eval(f.fset, f.pkg.typesPkg, expr.Pos(), exprStr)\n+\tif err != nil {\n+\t\treturn \"\", false\n+\t}\n+\tif b, ok := tv.Type.(*types.Basic); ok {\n+\t\tif dt, ok := basicTypeKinds[b.Kind()]; ok {\n+\t\t\treturn dt, true\n+\t\t}\n+\t}\n+\n+\treturn \"\", false\n+}\n+\n+// firstLineOf renders the given node and returns its first line.\n+// It will also match the indentation of another node.\n+func (f *file) firstLineOf(node, match ast.Node) string {\n+\tline := f.render(node)\n+\tif i := strings.Index(line, \"\\n\"); i >= 0 {\n+\t\tline = line[:i]\n+\t}\n+\treturn f.indentOf(match) + line\n+}\n+\n+func (f *file) indentOf(node ast.Node) string {\n+\tline := srcLine(f.src, f.fset.Position(node.Pos()))\n+\tfor i, r := range line {\n+\t\tswitch r {\n+\t\tcase ' ', '\\t':\n+\t\tdefault:\n+\t\t\treturn line[:i]\n+\t\t}\n+\t}\n+\treturn line // unusual or empty line\n+}\n+\n+func (f *file) srcLineWithMatch(node ast.Node, pattern string) (m []string) {\n+\tline := srcLine(f.src, f.fset.Position(node.Pos()))\n+\tline = strings.TrimSuffix(line, \"\\n\")\n+\trx := regexp.MustCompile(pattern)\n+\treturn rx.FindStringSubmatch(line)\n+}\n+\n+// imports returns true if the current file imports the specified package path.\n+func (f *file) imports(importPath string) bool {\n+\tall := astutil.Imports(f.fset, f.f)\n+\tfor _, p := range all {\n+\t\tfor _, i := range p {\n+\t\t\tuq, err := strconv.Unquote(i.Path.Value)\n+\t\t\tif err == nil && importPath == uq {\n+\t\t\t\treturn true\n+\t\t\t}\n+\t\t}\n+\t}\n+\treturn false\n+}\n+\n+// srcLine returns the complete line at p, including the terminating newline.\n+func srcLine(src []byte, p token.Position) string {\n+\t// Run to end of line in both directions if not at line start/end.\n+\tlo, hi := p.Offset, p.Offset+1\n+\tfor lo > 0 && src[lo-1] != '\\n' {\n+\t\tlo--\n+\t}\n+\tfor hi < len(src) && src[hi-1] != '\\n' {\n+\t\thi++\n+\t}\n+\treturn string(src[lo:hi])\n+}"
    },
    {
      "sha": "15167cd746c560e5b3d3b233a169aa64d3e9101e",
      "filename": "backend/vendor/golang.org/x/tools/AUTHORS",
      "status": "added",
      "additions": 3,
      "deletions": 0,
      "changes": 3,
      "blob_url": "https://github.com/umputun/remark42/blob/80f4862c9c2905475f3484f20ca41f7bdf103de9/backend/vendor/golang.org/x/tools/AUTHORS",
      "raw_url": "https://github.com/umputun/remark42/raw/80f4862c9c2905475f3484f20ca41f7bdf103de9/backend/vendor/golang.org/x/tools/AUTHORS",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/golang.org/x/tools/AUTHORS?ref=80f4862c9c2905475f3484f20ca41f7bdf103de9",
      "patch": "@@ -0,0 +1,3 @@\n+# This source code refers to The Go Authors for copyright purposes.\n+# The master list of authors is in the main Go distribution,\n+# visible at http://tip.golang.org/AUTHORS."
    },
    {
      "sha": "1c4577e9680611383f46044d17fa343a96997c3c",
      "filename": "backend/vendor/golang.org/x/tools/CONTRIBUTORS",
      "status": "added",
      "additions": 3,
      "deletions": 0,
      "changes": 3,
      "blob_url": "https://github.com/umputun/remark42/blob/80f4862c9c2905475f3484f20ca41f7bdf103de9/backend/vendor/golang.org/x/tools/CONTRIBUTORS",
      "raw_url": "https://github.com/umputun/remark42/raw/80f4862c9c2905475f3484f20ca41f7bdf103de9/backend/vendor/golang.org/x/tools/CONTRIBUTORS",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/golang.org/x/tools/CONTRIBUTORS?ref=80f4862c9c2905475f3484f20ca41f7bdf103de9",
      "patch": "@@ -0,0 +1,3 @@\n+# This source code was written by the Go contributors.\n+# The master list of contributors is in the main Go distribution,\n+# visible at http://tip.golang.org/CONTRIBUTORS."
    },
    {
      "sha": "6a66aea5eafe0ca6a688840c47219556c552488e",
      "filename": "backend/vendor/golang.org/x/tools/LICENSE",
      "status": "added",
      "additions": 27,
      "deletions": 0,
      "changes": 27,
      "blob_url": "https://github.com/umputun/remark42/blob/80f4862c9c2905475f3484f20ca41f7bdf103de9/backend/vendor/golang.org/x/tools/LICENSE",
      "raw_url": "https://github.com/umputun/remark42/raw/80f4862c9c2905475f3484f20ca41f7bdf103de9/backend/vendor/golang.org/x/tools/LICENSE",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/golang.org/x/tools/LICENSE?ref=80f4862c9c2905475f3484f20ca41f7bdf103de9",
      "patch": "@@ -0,0 +1,27 @@\n+Copyright (c) 2009 The Go Authors. All rights reserved.\n+\n+Redistribution and use in source and binary forms, with or without\n+modification, are permitted provided that the following conditions are\n+met:\n+\n+   * Redistributions of source code must retain the above copyright\n+notice, this list of conditions and the following disclaimer.\n+   * Redistributions in binary form must reproduce the above\n+copyright notice, this list of conditions and the following disclaimer\n+in the documentation and/or other materials provided with the\n+distribution.\n+   * Neither the name of Google Inc. nor the names of its\n+contributors may be used to endorse or promote products derived from\n+this software without specific prior written permission.\n+\n+THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n+\"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\n+LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\n+A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\n+OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\n+SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\n+LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\n+DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\n+THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n+(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n+OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE."
    },
    {
      "sha": "733099041f84fa1e58611ab2e11af51c1f26d1d2",
      "filename": "backend/vendor/golang.org/x/tools/PATENTS",
      "status": "added",
      "additions": 22,
      "deletions": 0,
      "changes": 22,
      "blob_url": "https://github.com/umputun/remark42/blob/80f4862c9c2905475f3484f20ca41f7bdf103de9/backend/vendor/golang.org/x/tools/PATENTS",
      "raw_url": "https://github.com/umputun/remark42/raw/80f4862c9c2905475f3484f20ca41f7bdf103de9/backend/vendor/golang.org/x/tools/PATENTS",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/golang.org/x/tools/PATENTS?ref=80f4862c9c2905475f3484f20ca41f7bdf103de9",
      "patch": "@@ -0,0 +1,22 @@\n+Additional IP Rights Grant (Patents)\n+\n+\"This implementation\" means the copyrightable works distributed by\n+Google as part of the Go project.\n+\n+Google hereby grants to You a perpetual, worldwide, non-exclusive,\n+no-charge, royalty-free, irrevocable (except as stated in this section)\n+patent license to make, have made, use, offer to sell, sell, import,\n+transfer and otherwise run, modify and propagate the contents of this\n+implementation of Go, where such license applies only to those patent\n+claims, both currently owned or controlled by Google and acquired in\n+the future, licensable by Google that are necessarily infringed by this\n+implementation of Go.  This grant does not include claims that would be\n+infringed only as a consequence of further modification of this\n+implementation.  If you or your agent or exclusive licensee institute or\n+order or agree to the institution of patent litigation against any\n+entity (including a cross-claim or counterclaim in a lawsuit) alleging\n+that this implementation of Go or any code incorporated within this\n+implementation of Go constitutes direct or contributory patent\n+infringement, or inducement of patent infringement, then any patent\n+rights granted to you under this License for this implementation of Go\n+shall terminate as of the date such litigation is filed."
    },
    {
      "sha": "6b7052b892ca07ba23a1eddd35645d50992dc42c",
      "filename": "backend/vendor/golang.org/x/tools/go/ast/astutil/enclosing.go",
      "status": "added",
      "additions": 627,
      "deletions": 0,
      "changes": 627,
      "blob_url": "https://github.com/umputun/remark42/blob/80f4862c9c2905475f3484f20ca41f7bdf103de9/backend/vendor/golang.org/x/tools/go/ast/astutil/enclosing.go",
      "raw_url": "https://github.com/umputun/remark42/raw/80f4862c9c2905475f3484f20ca41f7bdf103de9/backend/vendor/golang.org/x/tools/go/ast/astutil/enclosing.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/golang.org/x/tools/go/ast/astutil/enclosing.go?ref=80f4862c9c2905475f3484f20ca41f7bdf103de9",
      "patch": "@@ -0,0 +1,627 @@\n+// Copyright 2013 The Go Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style\n+// license that can be found in the LICENSE file.\n+\n+package astutil\n+\n+// This file defines utilities for working with source positions.\n+\n+import (\n+\t\"fmt\"\n+\t\"go/ast\"\n+\t\"go/token\"\n+\t\"sort\"\n+)\n+\n+// PathEnclosingInterval returns the node that encloses the source\n+// interval [start, end), and all its ancestors up to the AST root.\n+//\n+// The definition of \"enclosing\" used by this function considers\n+// additional whitespace abutting a node to be enclosed by it.\n+// In this example:\n+//\n+//              z := x + y // add them\n+//                   <-A->\n+//                  <----B----->\n+//\n+// the ast.BinaryExpr(+) node is considered to enclose interval B\n+// even though its [Pos()..End()) is actually only interval A.\n+// This behaviour makes user interfaces more tolerant of imperfect\n+// input.\n+//\n+// This function treats tokens as nodes, though they are not included\n+// in the result. e.g. PathEnclosingInterval(\"+\") returns the\n+// enclosing ast.BinaryExpr(\"x + y\").\n+//\n+// If start==end, the 1-char interval following start is used instead.\n+//\n+// The 'exact' result is true if the interval contains only path[0]\n+// and perhaps some adjacent whitespace.  It is false if the interval\n+// overlaps multiple children of path[0], or if it contains only\n+// interior whitespace of path[0].\n+// In this example:\n+//\n+//              z := x + y // add them\n+//                <--C-->     <---E-->\n+//                  ^\n+//                  D\n+//\n+// intervals C, D and E are inexact.  C is contained by the\n+// z-assignment statement, because it spans three of its children (:=,\n+// x, +).  So too is the 1-char interval D, because it contains only\n+// interior whitespace of the assignment.  E is considered interior\n+// whitespace of the BlockStmt containing the assignment.\n+//\n+// Precondition: [start, end) both lie within the same file as root.\n+// TODO(adonovan): return (nil, false) in this case and remove precond.\n+// Requires FileSet; see loader.tokenFileContainsPos.\n+//\n+// Postcondition: path is never nil; it always contains at least 'root'.\n+//\n+func PathEnclosingInterval(root *ast.File, start, end token.Pos) (path []ast.Node, exact bool) {\n+\t// fmt.Printf(\"EnclosingInterval %d %d\\n\", start, end) // debugging\n+\n+\t// Precondition: node.[Pos..End) and adjoining whitespace contain [start, end).\n+\tvar visit func(node ast.Node) bool\n+\tvisit = func(node ast.Node) bool {\n+\t\tpath = append(path, node)\n+\n+\t\tnodePos := node.Pos()\n+\t\tnodeEnd := node.End()\n+\n+\t\t// fmt.Printf(\"visit(%T, %d, %d)\\n\", node, nodePos, nodeEnd) // debugging\n+\n+\t\t// Intersect [start, end) with interval of node.\n+\t\tif start < nodePos {\n+\t\t\tstart = nodePos\n+\t\t}\n+\t\tif end > nodeEnd {\n+\t\t\tend = nodeEnd\n+\t\t}\n+\n+\t\t// Find sole child that contains [start, end).\n+\t\tchildren := childrenOf(node)\n+\t\tl := len(children)\n+\t\tfor i, child := range children {\n+\t\t\t// [childPos, childEnd) is unaugmented interval of child.\n+\t\t\tchildPos := child.Pos()\n+\t\t\tchildEnd := child.End()\n+\n+\t\t\t// [augPos, augEnd) is whitespace-augmented interval of child.\n+\t\t\taugPos := childPos\n+\t\t\taugEnd := childEnd\n+\t\t\tif i > 0 {\n+\t\t\t\taugPos = children[i-1].End() // start of preceding whitespace\n+\t\t\t}\n+\t\t\tif i < l-1 {\n+\t\t\t\tnextChildPos := children[i+1].Pos()\n+\t\t\t\t// Does [start, end) lie between child and next child?\n+\t\t\t\tif start >= augEnd && end <= nextChildPos {\n+\t\t\t\t\treturn false // inexact match\n+\t\t\t\t}\n+\t\t\t\taugEnd = nextChildPos // end of following whitespace\n+\t\t\t}\n+\n+\t\t\t// fmt.Printf(\"\\tchild %d: [%d..%d)\\tcontains interval [%d..%d)?\\n\",\n+\t\t\t// \ti, augPos, augEnd, start, end) // debugging\n+\n+\t\t\t// Does augmented child strictly contain [start, end)?\n+\t\t\tif augPos <= start && end <= augEnd {\n+\t\t\t\t_, isToken := child.(tokenNode)\n+\t\t\t\treturn isToken || visit(child)\n+\t\t\t}\n+\n+\t\t\t// Does [start, end) overlap multiple children?\n+\t\t\t// i.e. left-augmented child contains start\n+\t\t\t// but LR-augmented child does not contain end.\n+\t\t\tif start < childEnd && end > augEnd {\n+\t\t\t\tbreak\n+\t\t\t}\n+\t\t}\n+\n+\t\t// No single child contained [start, end),\n+\t\t// so node is the result.  Is it exact?\n+\n+\t\t// (It's tempting to put this condition before the\n+\t\t// child loop, but it gives the wrong result in the\n+\t\t// case where a node (e.g. ExprStmt) and its sole\n+\t\t// child have equal intervals.)\n+\t\tif start == nodePos && end == nodeEnd {\n+\t\t\treturn true // exact match\n+\t\t}\n+\n+\t\treturn false // inexact: overlaps multiple children\n+\t}\n+\n+\tif start > end {\n+\t\tstart, end = end, start\n+\t}\n+\n+\tif start < root.End() && end > root.Pos() {\n+\t\tif start == end {\n+\t\t\tend = start + 1 // empty interval => interval of size 1\n+\t\t}\n+\t\texact = visit(root)\n+\n+\t\t// Reverse the path:\n+\t\tfor i, l := 0, len(path); i < l/2; i++ {\n+\t\t\tpath[i], path[l-1-i] = path[l-1-i], path[i]\n+\t\t}\n+\t} else {\n+\t\t// Selection lies within whitespace preceding the\n+\t\t// first (or following the last) declaration in the file.\n+\t\t// The result nonetheless always includes the ast.File.\n+\t\tpath = append(path, root)\n+\t}\n+\n+\treturn\n+}\n+\n+// tokenNode is a dummy implementation of ast.Node for a single token.\n+// They are used transiently by PathEnclosingInterval but never escape\n+// this package.\n+//\n+type tokenNode struct {\n+\tpos token.Pos\n+\tend token.Pos\n+}\n+\n+func (n tokenNode) Pos() token.Pos {\n+\treturn n.pos\n+}\n+\n+func (n tokenNode) End() token.Pos {\n+\treturn n.end\n+}\n+\n+func tok(pos token.Pos, len int) ast.Node {\n+\treturn tokenNode{pos, pos + token.Pos(len)}\n+}\n+\n+// childrenOf returns the direct non-nil children of ast.Node n.\n+// It may include fake ast.Node implementations for bare tokens.\n+// it is not safe to call (e.g.) ast.Walk on such nodes.\n+//\n+func childrenOf(n ast.Node) []ast.Node {\n+\tvar children []ast.Node\n+\n+\t// First add nodes for all true subtrees.\n+\tast.Inspect(n, func(node ast.Node) bool {\n+\t\tif node == n { // push n\n+\t\t\treturn true // recur\n+\t\t}\n+\t\tif node != nil { // push child\n+\t\t\tchildren = append(children, node)\n+\t\t}\n+\t\treturn false // no recursion\n+\t})\n+\n+\t// Then add fake Nodes for bare tokens.\n+\tswitch n := n.(type) {\n+\tcase *ast.ArrayType:\n+\t\tchildren = append(children,\n+\t\t\ttok(n.Lbrack, len(\"[\")),\n+\t\t\ttok(n.Elt.End(), len(\"]\")))\n+\n+\tcase *ast.AssignStmt:\n+\t\tchildren = append(children,\n+\t\t\ttok(n.TokPos, len(n.Tok.String())))\n+\n+\tcase *ast.BasicLit:\n+\t\tchildren = append(children,\n+\t\t\ttok(n.ValuePos, len(n.Value)))\n+\n+\tcase *ast.BinaryExpr:\n+\t\tchildren = append(children, tok(n.OpPos, len(n.Op.String())))\n+\n+\tcase *ast.BlockStmt:\n+\t\tchildren = append(children,\n+\t\t\ttok(n.Lbrace, len(\"{\")),\n+\t\t\ttok(n.Rbrace, len(\"}\")))\n+\n+\tcase *ast.BranchStmt:\n+\t\tchildren = append(children,\n+\t\t\ttok(n.TokPos, len(n.Tok.String())))\n+\n+\tcase *ast.CallExpr:\n+\t\tchildren = append(children,\n+\t\t\ttok(n.Lparen, len(\"(\")),\n+\t\t\ttok(n.Rparen, len(\")\")))\n+\t\tif n.Ellipsis != 0 {\n+\t\t\tchildren = append(children, tok(n.Ellipsis, len(\"...\")))\n+\t\t}\n+\n+\tcase *ast.CaseClause:\n+\t\tif n.List == nil {\n+\t\t\tchildren = append(children,\n+\t\t\t\ttok(n.Case, len(\"default\")))\n+\t\t} else {\n+\t\t\tchildren = append(children,\n+\t\t\t\ttok(n.Case, len(\"case\")))\n+\t\t}\n+\t\tchildren = append(children, tok(n.Colon, len(\":\")))\n+\n+\tcase *ast.ChanType:\n+\t\tswitch n.Dir {\n+\t\tcase ast.RECV:\n+\t\t\tchildren = append(children, tok(n.Begin, len(\"<-chan\")))\n+\t\tcase ast.SEND:\n+\t\t\tchildren = append(children, tok(n.Begin, len(\"chan<-\")))\n+\t\tcase ast.RECV | ast.SEND:\n+\t\t\tchildren = append(children, tok(n.Begin, len(\"chan\")))\n+\t\t}\n+\n+\tcase *ast.CommClause:\n+\t\tif n.Comm == nil {\n+\t\t\tchildren = append(children,\n+\t\t\t\ttok(n.Case, len(\"default\")))\n+\t\t} else {\n+\t\t\tchildren = append(children,\n+\t\t\t\ttok(n.Case, len(\"case\")))\n+\t\t}\n+\t\tchildren = append(children, tok(n.Colon, len(\":\")))\n+\n+\tcase *ast.Comment:\n+\t\t// nop\n+\n+\tcase *ast.CommentGroup:\n+\t\t// nop\n+\n+\tcase *ast.CompositeLit:\n+\t\tchildren = append(children,\n+\t\t\ttok(n.Lbrace, len(\"{\")),\n+\t\t\ttok(n.Rbrace, len(\"{\")))\n+\n+\tcase *ast.DeclStmt:\n+\t\t// nop\n+\n+\tcase *ast.DeferStmt:\n+\t\tchildren = append(children,\n+\t\t\ttok(n.Defer, len(\"defer\")))\n+\n+\tcase *ast.Ellipsis:\n+\t\tchildren = append(children,\n+\t\t\ttok(n.Ellipsis, len(\"...\")))\n+\n+\tcase *ast.EmptyStmt:\n+\t\t// nop\n+\n+\tcase *ast.ExprStmt:\n+\t\t// nop\n+\n+\tcase *ast.Field:\n+\t\t// TODO(adonovan): Field.{Doc,Comment,Tag}?\n+\n+\tcase *ast.FieldList:\n+\t\tchildren = append(children,\n+\t\t\ttok(n.Opening, len(\"(\")),\n+\t\t\ttok(n.Closing, len(\")\")))\n+\n+\tcase *ast.File:\n+\t\t// TODO test: Doc\n+\t\tchildren = append(children,\n+\t\t\ttok(n.Package, len(\"package\")))\n+\n+\tcase *ast.ForStmt:\n+\t\tchildren = append(children,\n+\t\t\ttok(n.For, len(\"for\")))\n+\n+\tcase *ast.FuncDecl:\n+\t\t// TODO(adonovan): FuncDecl.Comment?\n+\n+\t\t// Uniquely, FuncDecl breaks the invariant that\n+\t\t// preorder traversal yields tokens in lexical order:\n+\t\t// in fact, FuncDecl.Recv precedes FuncDecl.Type.Func.\n+\t\t//\n+\t\t// As a workaround, we inline the case for FuncType\n+\t\t// here and order things correctly.\n+\t\t//\n+\t\tchildren = nil // discard ast.Walk(FuncDecl) info subtrees\n+\t\tchildren = append(children, tok(n.Type.Func, len(\"func\")))\n+\t\tif n.Recv != nil {\n+\t\t\tchildren = append(children, n.Recv)\n+\t\t}\n+\t\tchildren = append(children, n.Name)\n+\t\tif n.Type.Params != nil {\n+\t\t\tchildren = append(children, n.Type.Params)\n+\t\t}\n+\t\tif n.Type.Results != nil {\n+\t\t\tchildren = append(children, n.Type.Results)\n+\t\t}\n+\t\tif n.Body != nil {\n+\t\t\tchildren = append(children, n.Body)\n+\t\t}\n+\n+\tcase *ast.FuncLit:\n+\t\t// nop\n+\n+\tcase *ast.FuncType:\n+\t\tif n.Func != 0 {\n+\t\t\tchildren = append(children,\n+\t\t\t\ttok(n.Func, len(\"func\")))\n+\t\t}\n+\n+\tcase *ast.GenDecl:\n+\t\tchildren = append(children,\n+\t\t\ttok(n.TokPos, len(n.Tok.String())))\n+\t\tif n.Lparen != 0 {\n+\t\t\tchildren = append(children,\n+\t\t\t\ttok(n.Lparen, len(\"(\")),\n+\t\t\t\ttok(n.Rparen, len(\")\")))\n+\t\t}\n+\n+\tcase *ast.GoStmt:\n+\t\tchildren = append(children,\n+\t\t\ttok(n.Go, len(\"go\")))\n+\n+\tcase *ast.Ident:\n+\t\tchildren = append(children,\n+\t\t\ttok(n.NamePos, len(n.Name)))\n+\n+\tcase *ast.IfStmt:\n+\t\tchildren = append(children,\n+\t\t\ttok(n.If, len(\"if\")))\n+\n+\tcase *ast.ImportSpec:\n+\t\t// TODO(adonovan): ImportSpec.{Doc,EndPos}?\n+\n+\tcase *ast.IncDecStmt:\n+\t\tchildren = append(children,\n+\t\t\ttok(n.TokPos, len(n.Tok.String())))\n+\n+\tcase *ast.IndexExpr:\n+\t\tchildren = append(children,\n+\t\t\ttok(n.Lbrack, len(\"{\")),\n+\t\t\ttok(n.Rbrack, len(\"}\")))\n+\n+\tcase *ast.InterfaceType:\n+\t\tchildren = append(children,\n+\t\t\ttok(n.Interface, len(\"interface\")))\n+\n+\tcase *ast.KeyValueExpr:\n+\t\tchildren = append(children,\n+\t\t\ttok(n.Colon, len(\":\")))\n+\n+\tcase *ast.LabeledStmt:\n+\t\tchildren = append(children,\n+\t\t\ttok(n.Colon, len(\":\")))\n+\n+\tcase *ast.MapType:\n+\t\tchildren = append(children,\n+\t\t\ttok(n.Map, len(\"map\")))\n+\n+\tcase *ast.ParenExpr:\n+\t\tchildren = append(children,\n+\t\t\ttok(n.Lparen, len(\"(\")),\n+\t\t\ttok(n.Rparen, len(\")\")))\n+\n+\tcase *ast.RangeStmt:\n+\t\tchildren = append(children,\n+\t\t\ttok(n.For, len(\"for\")),\n+\t\t\ttok(n.TokPos, len(n.Tok.String())))\n+\n+\tcase *ast.ReturnStmt:\n+\t\tchildren = append(children,\n+\t\t\ttok(n.Return, len(\"return\")))\n+\n+\tcase *ast.SelectStmt:\n+\t\tchildren = append(children,\n+\t\t\ttok(n.Select, len(\"select\")))\n+\n+\tcase *ast.SelectorExpr:\n+\t\t// nop\n+\n+\tcase *ast.SendStmt:\n+\t\tchildren = append(children,\n+\t\t\ttok(n.Arrow, len(\"<-\")))\n+\n+\tcase *ast.SliceExpr:\n+\t\tchildren = append(children,\n+\t\t\ttok(n.Lbrack, len(\"[\")),\n+\t\t\ttok(n.Rbrack, len(\"]\")))\n+\n+\tcase *ast.StarExpr:\n+\t\tchildren = append(children, tok(n.Star, len(\"*\")))\n+\n+\tcase *ast.StructType:\n+\t\tchildren = append(children, tok(n.Struct, len(\"struct\")))\n+\n+\tcase *ast.SwitchStmt:\n+\t\tchildren = append(children, tok(n.Switch, len(\"switch\")))\n+\n+\tcase *ast.TypeAssertExpr:\n+\t\tchildren = append(children,\n+\t\t\ttok(n.Lparen-1, len(\".\")),\n+\t\t\ttok(n.Lparen, len(\"(\")),\n+\t\t\ttok(n.Rparen, len(\")\")))\n+\n+\tcase *ast.TypeSpec:\n+\t\t// TODO(adonovan): TypeSpec.{Doc,Comment}?\n+\n+\tcase *ast.TypeSwitchStmt:\n+\t\tchildren = append(children, tok(n.Switch, len(\"switch\")))\n+\n+\tcase *ast.UnaryExpr:\n+\t\tchildren = append(children, tok(n.OpPos, len(n.Op.String())))\n+\n+\tcase *ast.ValueSpec:\n+\t\t// TODO(adonovan): ValueSpec.{Doc,Comment}?\n+\n+\tcase *ast.BadDecl, *ast.BadExpr, *ast.BadStmt:\n+\t\t// nop\n+\t}\n+\n+\t// TODO(adonovan): opt: merge the logic of ast.Inspect() into\n+\t// the switch above so we can make interleaved callbacks for\n+\t// both Nodes and Tokens in the right order and avoid the need\n+\t// to sort.\n+\tsort.Sort(byPos(children))\n+\n+\treturn children\n+}\n+\n+type byPos []ast.Node\n+\n+func (sl byPos) Len() int {\n+\treturn len(sl)\n+}\n+func (sl byPos) Less(i, j int) bool {\n+\treturn sl[i].Pos() < sl[j].Pos()\n+}\n+func (sl byPos) Swap(i, j int) {\n+\tsl[i], sl[j] = sl[j], sl[i]\n+}\n+\n+// NodeDescription returns a description of the concrete type of n suitable\n+// for a user interface.\n+//\n+// TODO(adonovan): in some cases (e.g. Field, FieldList, Ident,\n+// StarExpr) we could be much more specific given the path to the AST\n+// root.  Perhaps we should do that.\n+//\n+func NodeDescription(n ast.Node) string {\n+\tswitch n := n.(type) {\n+\tcase *ast.ArrayType:\n+\t\treturn \"array type\"\n+\tcase *ast.AssignStmt:\n+\t\treturn \"assignment\"\n+\tcase *ast.BadDecl:\n+\t\treturn \"bad declaration\"\n+\tcase *ast.BadExpr:\n+\t\treturn \"bad expression\"\n+\tcase *ast.BadStmt:\n+\t\treturn \"bad statement\"\n+\tcase *ast.BasicLit:\n+\t\treturn \"basic literal\"\n+\tcase *ast.BinaryExpr:\n+\t\treturn fmt.Sprintf(\"binary %s operation\", n.Op)\n+\tcase *ast.BlockStmt:\n+\t\treturn \"block\"\n+\tcase *ast.BranchStmt:\n+\t\tswitch n.Tok {\n+\t\tcase token.BREAK:\n+\t\t\treturn \"break statement\"\n+\t\tcase token.CONTINUE:\n+\t\t\treturn \"continue statement\"\n+\t\tcase token.GOTO:\n+\t\t\treturn \"goto statement\"\n+\t\tcase token.FALLTHROUGH:\n+\t\t\treturn \"fall-through statement\"\n+\t\t}\n+\tcase *ast.CallExpr:\n+\t\tif len(n.Args) == 1 && !n.Ellipsis.IsValid() {\n+\t\t\treturn \"function call (or conversion)\"\n+\t\t}\n+\t\treturn \"function call\"\n+\tcase *ast.CaseClause:\n+\t\treturn \"case clause\"\n+\tcase *ast.ChanType:\n+\t\treturn \"channel type\"\n+\tcase *ast.CommClause:\n+\t\treturn \"communication clause\"\n+\tcase *ast.Comment:\n+\t\treturn \"comment\"\n+\tcase *ast.CommentGroup:\n+\t\treturn \"comment group\"\n+\tcase *ast.CompositeLit:\n+\t\treturn \"composite literal\"\n+\tcase *ast.DeclStmt:\n+\t\treturn NodeDescription(n.Decl) + \" statement\"\n+\tcase *ast.DeferStmt:\n+\t\treturn \"defer statement\"\n+\tcase *ast.Ellipsis:\n+\t\treturn \"ellipsis\"\n+\tcase *ast.EmptyStmt:\n+\t\treturn \"empty statement\"\n+\tcase *ast.ExprStmt:\n+\t\treturn \"expression statement\"\n+\tcase *ast.Field:\n+\t\t// Can be any of these:\n+\t\t// struct {x, y int}  -- struct field(s)\n+\t\t// struct {T}         -- anon struct field\n+\t\t// interface {I}      -- interface embedding\n+\t\t// interface {f()}    -- interface method\n+\t\t// func (A) func(B) C -- receiver, param(s), result(s)\n+\t\treturn \"field/method/parameter\"\n+\tcase *ast.FieldList:\n+\t\treturn \"field/method/parameter list\"\n+\tcase *ast.File:\n+\t\treturn \"source file\"\n+\tcase *ast.ForStmt:\n+\t\treturn \"for loop\"\n+\tcase *ast.FuncDecl:\n+\t\treturn \"function declaration\"\n+\tcase *ast.FuncLit:\n+\t\treturn \"function literal\"\n+\tcase *ast.FuncType:\n+\t\treturn \"function type\"\n+\tcase *ast.GenDecl:\n+\t\tswitch n.Tok {\n+\t\tcase token.IMPORT:\n+\t\t\treturn \"import declaration\"\n+\t\tcase token.CONST:\n+\t\t\treturn \"constant declaration\"\n+\t\tcase token.TYPE:\n+\t\t\treturn \"type declaration\"\n+\t\tcase token.VAR:\n+\t\t\treturn \"variable declaration\"\n+\t\t}\n+\tcase *ast.GoStmt:\n+\t\treturn \"go statement\"\n+\tcase *ast.Ident:\n+\t\treturn \"identifier\"\n+\tcase *ast.IfStmt:\n+\t\treturn \"if statement\"\n+\tcase *ast.ImportSpec:\n+\t\treturn \"import specification\"\n+\tcase *ast.IncDecStmt:\n+\t\tif n.Tok == token.INC {\n+\t\t\treturn \"increment statement\"\n+\t\t}\n+\t\treturn \"decrement statement\"\n+\tcase *ast.IndexExpr:\n+\t\treturn \"index expression\"\n+\tcase *ast.InterfaceType:\n+\t\treturn \"interface type\"\n+\tcase *ast.KeyValueExpr:\n+\t\treturn \"key/value association\"\n+\tcase *ast.LabeledStmt:\n+\t\treturn \"statement label\"\n+\tcase *ast.MapType:\n+\t\treturn \"map type\"\n+\tcase *ast.Package:\n+\t\treturn \"package\"\n+\tcase *ast.ParenExpr:\n+\t\treturn \"parenthesized \" + NodeDescription(n.X)\n+\tcase *ast.RangeStmt:\n+\t\treturn \"range loop\"\n+\tcase *ast.ReturnStmt:\n+\t\treturn \"return statement\"\n+\tcase *ast.SelectStmt:\n+\t\treturn \"select statement\"\n+\tcase *ast.SelectorExpr:\n+\t\treturn \"selector\"\n+\tcase *ast.SendStmt:\n+\t\treturn \"channel send\"\n+\tcase *ast.SliceExpr:\n+\t\treturn \"slice expression\"\n+\tcase *ast.StarExpr:\n+\t\treturn \"*-operation\" // load/store expr or pointer type\n+\tcase *ast.StructType:\n+\t\treturn \"struct type\"\n+\tcase *ast.SwitchStmt:\n+\t\treturn \"switch statement\"\n+\tcase *ast.TypeAssertExpr:\n+\t\treturn \"type assertion\"\n+\tcase *ast.TypeSpec:\n+\t\treturn \"type specification\"\n+\tcase *ast.TypeSwitchStmt:\n+\t\treturn \"type switch\"\n+\tcase *ast.UnaryExpr:\n+\t\treturn fmt.Sprintf(\"unary %s operation\", n.Op)\n+\tcase *ast.ValueSpec:\n+\t\treturn \"value specification\"\n+\n+\t}\n+\tpanic(fmt.Sprintf(\"unexpected node type: %T\", n))\n+}"
    },
    {
      "sha": "3e4b195368b35dd5820b7fdeca5894a6620c7c12",
      "filename": "backend/vendor/golang.org/x/tools/go/ast/astutil/imports.go",
      "status": "added",
      "additions": 481,
      "deletions": 0,
      "changes": 481,
      "blob_url": "https://github.com/umputun/remark42/blob/80f4862c9c2905475f3484f20ca41f7bdf103de9/backend/vendor/golang.org/x/tools/go/ast/astutil/imports.go",
      "raw_url": "https://github.com/umputun/remark42/raw/80f4862c9c2905475f3484f20ca41f7bdf103de9/backend/vendor/golang.org/x/tools/go/ast/astutil/imports.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/golang.org/x/tools/go/ast/astutil/imports.go?ref=80f4862c9c2905475f3484f20ca41f7bdf103de9",
      "patch": "@@ -0,0 +1,481 @@\n+// Copyright 2013 The Go Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style\n+// license that can be found in the LICENSE file.\n+\n+// Package astutil contains common utilities for working with the Go AST.\n+package astutil // import \"golang.org/x/tools/go/ast/astutil\"\n+\n+import (\n+\t\"fmt\"\n+\t\"go/ast\"\n+\t\"go/token\"\n+\t\"strconv\"\n+\t\"strings\"\n+)\n+\n+// AddImport adds the import path to the file f, if absent.\n+func AddImport(fset *token.FileSet, f *ast.File, path string) (added bool) {\n+\treturn AddNamedImport(fset, f, \"\", path)\n+}\n+\n+// AddNamedImport adds the import with the given name and path to the file f, if absent.\n+// If name is not empty, it is used to rename the import.\n+//\n+// For example, calling\n+//\tAddNamedImport(fset, f, \"pathpkg\", \"path\")\n+// adds\n+//\timport pathpkg \"path\"\n+func AddNamedImport(fset *token.FileSet, f *ast.File, name, path string) (added bool) {\n+\tif imports(f, name, path) {\n+\t\treturn false\n+\t}\n+\n+\tnewImport := &ast.ImportSpec{\n+\t\tPath: &ast.BasicLit{\n+\t\t\tKind:  token.STRING,\n+\t\t\tValue: strconv.Quote(path),\n+\t\t},\n+\t}\n+\tif name != \"\" {\n+\t\tnewImport.Name = &ast.Ident{Name: name}\n+\t}\n+\n+\t// Find an import decl to add to.\n+\t// The goal is to find an existing import\n+\t// whose import path has the longest shared\n+\t// prefix with path.\n+\tvar (\n+\t\tbestMatch  = -1         // length of longest shared prefix\n+\t\tlastImport = -1         // index in f.Decls of the file's final import decl\n+\t\timpDecl    *ast.GenDecl // import decl containing the best match\n+\t\timpIndex   = -1         // spec index in impDecl containing the best match\n+\n+\t\tisThirdPartyPath = isThirdParty(path)\n+\t)\n+\tfor i, decl := range f.Decls {\n+\t\tgen, ok := decl.(*ast.GenDecl)\n+\t\tif ok && gen.Tok == token.IMPORT {\n+\t\t\tlastImport = i\n+\t\t\t// Do not add to import \"C\", to avoid disrupting the\n+\t\t\t// association with its doc comment, breaking cgo.\n+\t\t\tif declImports(gen, \"C\") {\n+\t\t\t\tcontinue\n+\t\t\t}\n+\n+\t\t\t// Match an empty import decl if that's all that is available.\n+\t\t\tif len(gen.Specs) == 0 && bestMatch == -1 {\n+\t\t\t\timpDecl = gen\n+\t\t\t}\n+\n+\t\t\t// Compute longest shared prefix with imports in this group and find best\n+\t\t\t// matched import spec.\n+\t\t\t// 1. Always prefer import spec with longest shared prefix.\n+\t\t\t// 2. While match length is 0,\n+\t\t\t// - for stdlib package: prefer first import spec.\n+\t\t\t// - for third party package: prefer first third party import spec.\n+\t\t\t// We cannot use last import spec as best match for third party package\n+\t\t\t// because grouped imports are usually placed last by goimports -local\n+\t\t\t// flag.\n+\t\t\t// See issue #19190.\n+\t\t\tseenAnyThirdParty := false\n+\t\t\tfor j, spec := range gen.Specs {\n+\t\t\t\timpspec := spec.(*ast.ImportSpec)\n+\t\t\t\tp := importPath(impspec)\n+\t\t\t\tn := matchLen(p, path)\n+\t\t\t\tif n > bestMatch || (bestMatch == 0 && !seenAnyThirdParty && isThirdPartyPath) {\n+\t\t\t\t\tbestMatch = n\n+\t\t\t\t\timpDecl = gen\n+\t\t\t\t\timpIndex = j\n+\t\t\t\t}\n+\t\t\t\tseenAnyThirdParty = seenAnyThirdParty || isThirdParty(p)\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\t// If no import decl found, add one after the last import.\n+\tif impDecl == nil {\n+\t\timpDecl = &ast.GenDecl{\n+\t\t\tTok: token.IMPORT,\n+\t\t}\n+\t\tif lastImport >= 0 {\n+\t\t\timpDecl.TokPos = f.Decls[lastImport].End()\n+\t\t} else {\n+\t\t\t// There are no existing imports.\n+\t\t\t// Our new import, preceded by a blank line,  goes after the package declaration\n+\t\t\t// and after the comment, if any, that starts on the same line as the\n+\t\t\t// package declaration.\n+\t\t\timpDecl.TokPos = f.Package\n+\n+\t\t\tfile := fset.File(f.Package)\n+\t\t\tpkgLine := file.Line(f.Package)\n+\t\t\tfor _, c := range f.Comments {\n+\t\t\t\tif file.Line(c.Pos()) > pkgLine {\n+\t\t\t\t\tbreak\n+\t\t\t\t}\n+\t\t\t\t// +2 for a blank line\n+\t\t\t\timpDecl.TokPos = c.End() + 2\n+\t\t\t}\n+\t\t}\n+\t\tf.Decls = append(f.Decls, nil)\n+\t\tcopy(f.Decls[lastImport+2:], f.Decls[lastImport+1:])\n+\t\tf.Decls[lastImport+1] = impDecl\n+\t}\n+\n+\t// Insert new import at insertAt.\n+\tinsertAt := 0\n+\tif impIndex >= 0 {\n+\t\t// insert after the found import\n+\t\tinsertAt = impIndex + 1\n+\t}\n+\timpDecl.Specs = append(impDecl.Specs, nil)\n+\tcopy(impDecl.Specs[insertAt+1:], impDecl.Specs[insertAt:])\n+\timpDecl.Specs[insertAt] = newImport\n+\tpos := impDecl.Pos()\n+\tif insertAt > 0 {\n+\t\t// If there is a comment after an existing import, preserve the comment\n+\t\t// position by adding the new import after the comment.\n+\t\tif spec, ok := impDecl.Specs[insertAt-1].(*ast.ImportSpec); ok && spec.Comment != nil {\n+\t\t\tpos = spec.Comment.End()\n+\t\t} else {\n+\t\t\t// Assign same position as the previous import,\n+\t\t\t// so that the sorter sees it as being in the same block.\n+\t\t\tpos = impDecl.Specs[insertAt-1].Pos()\n+\t\t}\n+\t}\n+\tif newImport.Name != nil {\n+\t\tnewImport.Name.NamePos = pos\n+\t}\n+\tnewImport.Path.ValuePos = pos\n+\tnewImport.EndPos = pos\n+\n+\t// Clean up parens. impDecl contains at least one spec.\n+\tif len(impDecl.Specs) == 1 {\n+\t\t// Remove unneeded parens.\n+\t\timpDecl.Lparen = token.NoPos\n+\t} else if !impDecl.Lparen.IsValid() {\n+\t\t// impDecl needs parens added.\n+\t\timpDecl.Lparen = impDecl.Specs[0].Pos()\n+\t}\n+\n+\tf.Imports = append(f.Imports, newImport)\n+\n+\tif len(f.Decls) <= 1 {\n+\t\treturn true\n+\t}\n+\n+\t// Merge all the import declarations into the first one.\n+\tvar first *ast.GenDecl\n+\tfor i := 0; i < len(f.Decls); i++ {\n+\t\tdecl := f.Decls[i]\n+\t\tgen, ok := decl.(*ast.GenDecl)\n+\t\tif !ok || gen.Tok != token.IMPORT || declImports(gen, \"C\") {\n+\t\t\tcontinue\n+\t\t}\n+\t\tif first == nil {\n+\t\t\tfirst = gen\n+\t\t\tcontinue // Don't touch the first one.\n+\t\t}\n+\t\t// We now know there is more than one package in this import\n+\t\t// declaration. Ensure that it ends up parenthesized.\n+\t\tfirst.Lparen = first.Pos()\n+\t\t// Move the imports of the other import declaration to the first one.\n+\t\tfor _, spec := range gen.Specs {\n+\t\t\tspec.(*ast.ImportSpec).Path.ValuePos = first.Pos()\n+\t\t\tfirst.Specs = append(first.Specs, spec)\n+\t\t}\n+\t\tf.Decls = append(f.Decls[:i], f.Decls[i+1:]...)\n+\t\ti--\n+\t}\n+\n+\treturn true\n+}\n+\n+func isThirdParty(importPath string) bool {\n+\t// Third party package import path usually contains \".\" (\".com\", \".org\", ...)\n+\t// This logic is taken from golang.org/x/tools/imports package.\n+\treturn strings.Contains(importPath, \".\")\n+}\n+\n+// DeleteImport deletes the import path from the file f, if present.\n+// If there are duplicate import declarations, all matching ones are deleted.\n+func DeleteImport(fset *token.FileSet, f *ast.File, path string) (deleted bool) {\n+\treturn DeleteNamedImport(fset, f, \"\", path)\n+}\n+\n+// DeleteNamedImport deletes the import with the given name and path from the file f, if present.\n+// If there are duplicate import declarations, all matching ones are deleted.\n+func DeleteNamedImport(fset *token.FileSet, f *ast.File, name, path string) (deleted bool) {\n+\tvar delspecs []*ast.ImportSpec\n+\tvar delcomments []*ast.CommentGroup\n+\n+\t// Find the import nodes that import path, if any.\n+\tfor i := 0; i < len(f.Decls); i++ {\n+\t\tdecl := f.Decls[i]\n+\t\tgen, ok := decl.(*ast.GenDecl)\n+\t\tif !ok || gen.Tok != token.IMPORT {\n+\t\t\tcontinue\n+\t\t}\n+\t\tfor j := 0; j < len(gen.Specs); j++ {\n+\t\t\tspec := gen.Specs[j]\n+\t\t\timpspec := spec.(*ast.ImportSpec)\n+\t\t\tif importName(impspec) != name || importPath(impspec) != path {\n+\t\t\t\tcontinue\n+\t\t\t}\n+\n+\t\t\t// We found an import spec that imports path.\n+\t\t\t// Delete it.\n+\t\t\tdelspecs = append(delspecs, impspec)\n+\t\t\tdeleted = true\n+\t\t\tcopy(gen.Specs[j:], gen.Specs[j+1:])\n+\t\t\tgen.Specs = gen.Specs[:len(gen.Specs)-1]\n+\n+\t\t\t// If this was the last import spec in this decl,\n+\t\t\t// delete the decl, too.\n+\t\t\tif len(gen.Specs) == 0 {\n+\t\t\t\tcopy(f.Decls[i:], f.Decls[i+1:])\n+\t\t\t\tf.Decls = f.Decls[:len(f.Decls)-1]\n+\t\t\t\ti--\n+\t\t\t\tbreak\n+\t\t\t} else if len(gen.Specs) == 1 {\n+\t\t\t\tif impspec.Doc != nil {\n+\t\t\t\t\tdelcomments = append(delcomments, impspec.Doc)\n+\t\t\t\t}\n+\t\t\t\tif impspec.Comment != nil {\n+\t\t\t\t\tdelcomments = append(delcomments, impspec.Comment)\n+\t\t\t\t}\n+\t\t\t\tfor _, cg := range f.Comments {\n+\t\t\t\t\t// Found comment on the same line as the import spec.\n+\t\t\t\t\tif cg.End() < impspec.Pos() && fset.Position(cg.End()).Line == fset.Position(impspec.Pos()).Line {\n+\t\t\t\t\t\tdelcomments = append(delcomments, cg)\n+\t\t\t\t\t\tbreak\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\n+\t\t\t\tspec := gen.Specs[0].(*ast.ImportSpec)\n+\n+\t\t\t\t// Move the documentation right after the import decl.\n+\t\t\t\tif spec.Doc != nil {\n+\t\t\t\t\tfor fset.Position(gen.TokPos).Line+1 < fset.Position(spec.Doc.Pos()).Line {\n+\t\t\t\t\t\tfset.File(gen.TokPos).MergeLine(fset.Position(gen.TokPos).Line)\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t\tfor _, cg := range f.Comments {\n+\t\t\t\t\tif cg.End() < spec.Pos() && fset.Position(cg.End()).Line == fset.Position(spec.Pos()).Line {\n+\t\t\t\t\t\tfor fset.Position(gen.TokPos).Line+1 < fset.Position(spec.Pos()).Line {\n+\t\t\t\t\t\t\tfset.File(gen.TokPos).MergeLine(fset.Position(gen.TokPos).Line)\n+\t\t\t\t\t\t}\n+\t\t\t\t\t\tbreak\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t}\n+\t\t\tif j > 0 {\n+\t\t\t\tlastImpspec := gen.Specs[j-1].(*ast.ImportSpec)\n+\t\t\t\tlastLine := fset.Position(lastImpspec.Path.ValuePos).Line\n+\t\t\t\tline := fset.Position(impspec.Path.ValuePos).Line\n+\n+\t\t\t\t// We deleted an entry but now there may be\n+\t\t\t\t// a blank line-sized hole where the import was.\n+\t\t\t\tif line-lastLine > 1 {\n+\t\t\t\t\t// There was a blank line immediately preceding the deleted import,\n+\t\t\t\t\t// so there's no need to close the hole.\n+\t\t\t\t\t// Do nothing.\n+\t\t\t\t} else if line != fset.File(gen.Rparen).LineCount() {\n+\t\t\t\t\t// There was no blank line. Close the hole.\n+\t\t\t\t\tfset.File(gen.Rparen).MergeLine(line)\n+\t\t\t\t}\n+\t\t\t}\n+\t\t\tj--\n+\t\t}\n+\t}\n+\n+\t// Delete imports from f.Imports.\n+\tfor i := 0; i < len(f.Imports); i++ {\n+\t\timp := f.Imports[i]\n+\t\tfor j, del := range delspecs {\n+\t\t\tif imp == del {\n+\t\t\t\tcopy(f.Imports[i:], f.Imports[i+1:])\n+\t\t\t\tf.Imports = f.Imports[:len(f.Imports)-1]\n+\t\t\t\tcopy(delspecs[j:], delspecs[j+1:])\n+\t\t\t\tdelspecs = delspecs[:len(delspecs)-1]\n+\t\t\t\ti--\n+\t\t\t\tbreak\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\t// Delete comments from f.Comments.\n+\tfor i := 0; i < len(f.Comments); i++ {\n+\t\tcg := f.Comments[i]\n+\t\tfor j, del := range delcomments {\n+\t\t\tif cg == del {\n+\t\t\t\tcopy(f.Comments[i:], f.Comments[i+1:])\n+\t\t\t\tf.Comments = f.Comments[:len(f.Comments)-1]\n+\t\t\t\tcopy(delcomments[j:], delcomments[j+1:])\n+\t\t\t\tdelcomments = delcomments[:len(delcomments)-1]\n+\t\t\t\ti--\n+\t\t\t\tbreak\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tif len(delspecs) > 0 {\n+\t\tpanic(fmt.Sprintf(\"deleted specs from Decls but not Imports: %v\", delspecs))\n+\t}\n+\n+\treturn\n+}\n+\n+// RewriteImport rewrites any import of path oldPath to path newPath.\n+func RewriteImport(fset *token.FileSet, f *ast.File, oldPath, newPath string) (rewrote bool) {\n+\tfor _, imp := range f.Imports {\n+\t\tif importPath(imp) == oldPath {\n+\t\t\trewrote = true\n+\t\t\t// record old End, because the default is to compute\n+\t\t\t// it using the length of imp.Path.Value.\n+\t\t\timp.EndPos = imp.End()\n+\t\t\timp.Path.Value = strconv.Quote(newPath)\n+\t\t}\n+\t}\n+\treturn\n+}\n+\n+// UsesImport reports whether a given import is used.\n+func UsesImport(f *ast.File, path string) (used bool) {\n+\tspec := importSpec(f, path)\n+\tif spec == nil {\n+\t\treturn\n+\t}\n+\n+\tname := spec.Name.String()\n+\tswitch name {\n+\tcase \"<nil>\":\n+\t\t// If the package name is not explicitly specified,\n+\t\t// make an educated guess. This is not guaranteed to be correct.\n+\t\tlastSlash := strings.LastIndex(path, \"/\")\n+\t\tif lastSlash == -1 {\n+\t\t\tname = path\n+\t\t} else {\n+\t\t\tname = path[lastSlash+1:]\n+\t\t}\n+\tcase \"_\", \".\":\n+\t\t// Not sure if this import is used - err on the side of caution.\n+\t\treturn true\n+\t}\n+\n+\tast.Walk(visitFn(func(n ast.Node) {\n+\t\tsel, ok := n.(*ast.SelectorExpr)\n+\t\tif ok && isTopName(sel.X, name) {\n+\t\t\tused = true\n+\t\t}\n+\t}), f)\n+\n+\treturn\n+}\n+\n+type visitFn func(node ast.Node)\n+\n+func (fn visitFn) Visit(node ast.Node) ast.Visitor {\n+\tfn(node)\n+\treturn fn\n+}\n+\n+// imports reports whether f has an import with the specified name and path.\n+func imports(f *ast.File, name, path string) bool {\n+\tfor _, s := range f.Imports {\n+\t\tif importName(s) == name && importPath(s) == path {\n+\t\t\treturn true\n+\t\t}\n+\t}\n+\treturn false\n+}\n+\n+// importSpec returns the import spec if f imports path,\n+// or nil otherwise.\n+func importSpec(f *ast.File, path string) *ast.ImportSpec {\n+\tfor _, s := range f.Imports {\n+\t\tif importPath(s) == path {\n+\t\t\treturn s\n+\t\t}\n+\t}\n+\treturn nil\n+}\n+\n+// importName returns the name of s,\n+// or \"\" if the import is not named.\n+func importName(s *ast.ImportSpec) string {\n+\tif s.Name == nil {\n+\t\treturn \"\"\n+\t}\n+\treturn s.Name.Name\n+}\n+\n+// importPath returns the unquoted import path of s,\n+// or \"\" if the path is not properly quoted.\n+func importPath(s *ast.ImportSpec) string {\n+\tt, err := strconv.Unquote(s.Path.Value)\n+\tif err != nil {\n+\t\treturn \"\"\n+\t}\n+\treturn t\n+}\n+\n+// declImports reports whether gen contains an import of path.\n+func declImports(gen *ast.GenDecl, path string) bool {\n+\tif gen.Tok != token.IMPORT {\n+\t\treturn false\n+\t}\n+\tfor _, spec := range gen.Specs {\n+\t\timpspec := spec.(*ast.ImportSpec)\n+\t\tif importPath(impspec) == path {\n+\t\t\treturn true\n+\t\t}\n+\t}\n+\treturn false\n+}\n+\n+// matchLen returns the length of the longest path segment prefix shared by x and y.\n+func matchLen(x, y string) int {\n+\tn := 0\n+\tfor i := 0; i < len(x) && i < len(y) && x[i] == y[i]; i++ {\n+\t\tif x[i] == '/' {\n+\t\t\tn++\n+\t\t}\n+\t}\n+\treturn n\n+}\n+\n+// isTopName returns true if n is a top-level unresolved identifier with the given name.\n+func isTopName(n ast.Expr, name string) bool {\n+\tid, ok := n.(*ast.Ident)\n+\treturn ok && id.Name == name && id.Obj == nil\n+}\n+\n+// Imports returns the file imports grouped by paragraph.\n+func Imports(fset *token.FileSet, f *ast.File) [][]*ast.ImportSpec {\n+\tvar groups [][]*ast.ImportSpec\n+\n+\tfor _, decl := range f.Decls {\n+\t\tgenDecl, ok := decl.(*ast.GenDecl)\n+\t\tif !ok || genDecl.Tok != token.IMPORT {\n+\t\t\tbreak\n+\t\t}\n+\n+\t\tgroup := []*ast.ImportSpec{}\n+\n+\t\tvar lastLine int\n+\t\tfor _, spec := range genDecl.Specs {\n+\t\t\timportSpec := spec.(*ast.ImportSpec)\n+\t\t\tpos := importSpec.Path.ValuePos\n+\t\t\tline := fset.Position(pos).Line\n+\t\t\tif lastLine > 0 && pos > 0 && line-lastLine > 1 {\n+\t\t\t\tgroups = append(groups, group)\n+\t\t\t\tgroup = []*ast.ImportSpec{}\n+\t\t\t}\n+\t\t\tgroup = append(group, importSpec)\n+\t\t\tlastLine = line\n+\t\t}\n+\t\tgroups = append(groups, group)\n+\t}\n+\n+\treturn groups\n+}"
    },
    {
      "sha": "cf72ea990bda2c11ae7a87865c352b7fc9029730",
      "filename": "backend/vendor/golang.org/x/tools/go/ast/astutil/rewrite.go",
      "status": "added",
      "additions": 477,
      "deletions": 0,
      "changes": 477,
      "blob_url": "https://github.com/umputun/remark42/blob/80f4862c9c2905475f3484f20ca41f7bdf103de9/backend/vendor/golang.org/x/tools/go/ast/astutil/rewrite.go",
      "raw_url": "https://github.com/umputun/remark42/raw/80f4862c9c2905475f3484f20ca41f7bdf103de9/backend/vendor/golang.org/x/tools/go/ast/astutil/rewrite.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/golang.org/x/tools/go/ast/astutil/rewrite.go?ref=80f4862c9c2905475f3484f20ca41f7bdf103de9",
      "patch": "@@ -0,0 +1,477 @@\n+// Copyright 2017 The Go Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style\n+// license that can be found in the LICENSE file.\n+\n+package astutil\n+\n+import (\n+\t\"fmt\"\n+\t\"go/ast\"\n+\t\"reflect\"\n+\t\"sort\"\n+)\n+\n+// An ApplyFunc is invoked by Apply for each node n, even if n is nil,\n+// before and/or after the node's children, using a Cursor describing\n+// the current node and providing operations on it.\n+//\n+// The return value of ApplyFunc controls the syntax tree traversal.\n+// See Apply for details.\n+type ApplyFunc func(*Cursor) bool\n+\n+// Apply traverses a syntax tree recursively, starting with root,\n+// and calling pre and post for each node as described below.\n+// Apply returns the syntax tree, possibly modified.\n+//\n+// If pre is not nil, it is called for each node before the node's\n+// children are traversed (pre-order). If pre returns false, no\n+// children are traversed, and post is not called for that node.\n+//\n+// If post is not nil, and a prior call of pre didn't return false,\n+// post is called for each node after its children are traversed\n+// (post-order). If post returns false, traversal is terminated and\n+// Apply returns immediately.\n+//\n+// Only fields that refer to AST nodes are considered children;\n+// i.e., token.Pos, Scopes, Objects, and fields of basic types\n+// (strings, etc.) are ignored.\n+//\n+// Children are traversed in the order in which they appear in the\n+// respective node's struct definition. A package's files are\n+// traversed in the filenames' alphabetical order.\n+//\n+func Apply(root ast.Node, pre, post ApplyFunc) (result ast.Node) {\n+\tparent := &struct{ ast.Node }{root}\n+\tdefer func() {\n+\t\tif r := recover(); r != nil && r != abort {\n+\t\t\tpanic(r)\n+\t\t}\n+\t\tresult = parent.Node\n+\t}()\n+\ta := &application{pre: pre, post: post}\n+\ta.apply(parent, \"Node\", nil, root)\n+\treturn\n+}\n+\n+var abort = new(int) // singleton, to signal termination of Apply\n+\n+// A Cursor describes a node encountered during Apply.\n+// Information about the node and its parent is available\n+// from the Node, Parent, Name, and Index methods.\n+//\n+// If p is a variable of type and value of the current parent node\n+// c.Parent(), and f is the field identifier with name c.Name(),\n+// the following invariants hold:\n+//\n+//   p.f            == c.Node()  if c.Index() <  0\n+//   p.f[c.Index()] == c.Node()  if c.Index() >= 0\n+//\n+// The methods Replace, Delete, InsertBefore, and InsertAfter\n+// can be used to change the AST without disrupting Apply.\n+type Cursor struct {\n+\tparent ast.Node\n+\tname   string\n+\titer   *iterator // valid if non-nil\n+\tnode   ast.Node\n+}\n+\n+// Node returns the current Node.\n+func (c *Cursor) Node() ast.Node { return c.node }\n+\n+// Parent returns the parent of the current Node.\n+func (c *Cursor) Parent() ast.Node { return c.parent }\n+\n+// Name returns the name of the parent Node field that contains the current Node.\n+// If the parent is a *ast.Package and the current Node is a *ast.File, Name returns\n+// the filename for the current Node.\n+func (c *Cursor) Name() string { return c.name }\n+\n+// Index reports the index >= 0 of the current Node in the slice of Nodes that\n+// contains it, or a value < 0 if the current Node is not part of a slice.\n+// The index of the current node changes if InsertBefore is called while\n+// processing the current node.\n+func (c *Cursor) Index() int {\n+\tif c.iter != nil {\n+\t\treturn c.iter.index\n+\t}\n+\treturn -1\n+}\n+\n+// field returns the current node's parent field value.\n+func (c *Cursor) field() reflect.Value {\n+\treturn reflect.Indirect(reflect.ValueOf(c.parent)).FieldByName(c.name)\n+}\n+\n+// Replace replaces the current Node with n.\n+// The replacement node is not walked by Apply.\n+func (c *Cursor) Replace(n ast.Node) {\n+\tif _, ok := c.node.(*ast.File); ok {\n+\t\tfile, ok := n.(*ast.File)\n+\t\tif !ok {\n+\t\t\tpanic(\"attempt to replace *ast.File with non-*ast.File\")\n+\t\t}\n+\t\tc.parent.(*ast.Package).Files[c.name] = file\n+\t\treturn\n+\t}\n+\n+\tv := c.field()\n+\tif i := c.Index(); i >= 0 {\n+\t\tv = v.Index(i)\n+\t}\n+\tv.Set(reflect.ValueOf(n))\n+}\n+\n+// Delete deletes the current Node from its containing slice.\n+// If the current Node is not part of a slice, Delete panics.\n+// As a special case, if the current node is a package file,\n+// Delete removes it from the package's Files map.\n+func (c *Cursor) Delete() {\n+\tif _, ok := c.node.(*ast.File); ok {\n+\t\tdelete(c.parent.(*ast.Package).Files, c.name)\n+\t\treturn\n+\t}\n+\n+\ti := c.Index()\n+\tif i < 0 {\n+\t\tpanic(\"Delete node not contained in slice\")\n+\t}\n+\tv := c.field()\n+\tl := v.Len()\n+\treflect.Copy(v.Slice(i, l), v.Slice(i+1, l))\n+\tv.Index(l - 1).Set(reflect.Zero(v.Type().Elem()))\n+\tv.SetLen(l - 1)\n+\tc.iter.step--\n+}\n+\n+// InsertAfter inserts n after the current Node in its containing slice.\n+// If the current Node is not part of a slice, InsertAfter panics.\n+// Apply does not walk n.\n+func (c *Cursor) InsertAfter(n ast.Node) {\n+\ti := c.Index()\n+\tif i < 0 {\n+\t\tpanic(\"InsertAfter node not contained in slice\")\n+\t}\n+\tv := c.field()\n+\tv.Set(reflect.Append(v, reflect.Zero(v.Type().Elem())))\n+\tl := v.Len()\n+\treflect.Copy(v.Slice(i+2, l), v.Slice(i+1, l))\n+\tv.Index(i + 1).Set(reflect.ValueOf(n))\n+\tc.iter.step++\n+}\n+\n+// InsertBefore inserts n before the current Node in its containing slice.\n+// If the current Node is not part of a slice, InsertBefore panics.\n+// Apply will not walk n.\n+func (c *Cursor) InsertBefore(n ast.Node) {\n+\ti := c.Index()\n+\tif i < 0 {\n+\t\tpanic(\"InsertBefore node not contained in slice\")\n+\t}\n+\tv := c.field()\n+\tv.Set(reflect.Append(v, reflect.Zero(v.Type().Elem())))\n+\tl := v.Len()\n+\treflect.Copy(v.Slice(i+1, l), v.Slice(i, l))\n+\tv.Index(i).Set(reflect.ValueOf(n))\n+\tc.iter.index++\n+}\n+\n+// application carries all the shared data so we can pass it around cheaply.\n+type application struct {\n+\tpre, post ApplyFunc\n+\tcursor    Cursor\n+\titer      iterator\n+}\n+\n+func (a *application) apply(parent ast.Node, name string, iter *iterator, n ast.Node) {\n+\t// convert typed nil into untyped nil\n+\tif v := reflect.ValueOf(n); v.Kind() == reflect.Ptr && v.IsNil() {\n+\t\tn = nil\n+\t}\n+\n+\t// avoid heap-allocating a new cursor for each apply call; reuse a.cursor instead\n+\tsaved := a.cursor\n+\ta.cursor.parent = parent\n+\ta.cursor.name = name\n+\ta.cursor.iter = iter\n+\ta.cursor.node = n\n+\n+\tif a.pre != nil && !a.pre(&a.cursor) {\n+\t\ta.cursor = saved\n+\t\treturn\n+\t}\n+\n+\t// walk children\n+\t// (the order of the cases matches the order of the corresponding node types in go/ast)\n+\tswitch n := n.(type) {\n+\tcase nil:\n+\t\t// nothing to do\n+\n+\t// Comments and fields\n+\tcase *ast.Comment:\n+\t\t// nothing to do\n+\n+\tcase *ast.CommentGroup:\n+\t\tif n != nil {\n+\t\t\ta.applyList(n, \"List\")\n+\t\t}\n+\n+\tcase *ast.Field:\n+\t\ta.apply(n, \"Doc\", nil, n.Doc)\n+\t\ta.applyList(n, \"Names\")\n+\t\ta.apply(n, \"Type\", nil, n.Type)\n+\t\ta.apply(n, \"Tag\", nil, n.Tag)\n+\t\ta.apply(n, \"Comment\", nil, n.Comment)\n+\n+\tcase *ast.FieldList:\n+\t\ta.applyList(n, \"List\")\n+\n+\t// Expressions\n+\tcase *ast.BadExpr, *ast.Ident, *ast.BasicLit:\n+\t\t// nothing to do\n+\n+\tcase *ast.Ellipsis:\n+\t\ta.apply(n, \"Elt\", nil, n.Elt)\n+\n+\tcase *ast.FuncLit:\n+\t\ta.apply(n, \"Type\", nil, n.Type)\n+\t\ta.apply(n, \"Body\", nil, n.Body)\n+\n+\tcase *ast.CompositeLit:\n+\t\ta.apply(n, \"Type\", nil, n.Type)\n+\t\ta.applyList(n, \"Elts\")\n+\n+\tcase *ast.ParenExpr:\n+\t\ta.apply(n, \"X\", nil, n.X)\n+\n+\tcase *ast.SelectorExpr:\n+\t\ta.apply(n, \"X\", nil, n.X)\n+\t\ta.apply(n, \"Sel\", nil, n.Sel)\n+\n+\tcase *ast.IndexExpr:\n+\t\ta.apply(n, \"X\", nil, n.X)\n+\t\ta.apply(n, \"Index\", nil, n.Index)\n+\n+\tcase *ast.SliceExpr:\n+\t\ta.apply(n, \"X\", nil, n.X)\n+\t\ta.apply(n, \"Low\", nil, n.Low)\n+\t\ta.apply(n, \"High\", nil, n.High)\n+\t\ta.apply(n, \"Max\", nil, n.Max)\n+\n+\tcase *ast.TypeAssertExpr:\n+\t\ta.apply(n, \"X\", nil, n.X)\n+\t\ta.apply(n, \"Type\", nil, n.Type)\n+\n+\tcase *ast.CallExpr:\n+\t\ta.apply(n, \"Fun\", nil, n.Fun)\n+\t\ta.applyList(n, \"Args\")\n+\n+\tcase *ast.StarExpr:\n+\t\ta.apply(n, \"X\", nil, n.X)\n+\n+\tcase *ast.UnaryExpr:\n+\t\ta.apply(n, \"X\", nil, n.X)\n+\n+\tcase *ast.BinaryExpr:\n+\t\ta.apply(n, \"X\", nil, n.X)\n+\t\ta.apply(n, \"Y\", nil, n.Y)\n+\n+\tcase *ast.KeyValueExpr:\n+\t\ta.apply(n, \"Key\", nil, n.Key)\n+\t\ta.apply(n, \"Value\", nil, n.Value)\n+\n+\t// Types\n+\tcase *ast.ArrayType:\n+\t\ta.apply(n, \"Len\", nil, n.Len)\n+\t\ta.apply(n, \"Elt\", nil, n.Elt)\n+\n+\tcase *ast.StructType:\n+\t\ta.apply(n, \"Fields\", nil, n.Fields)\n+\n+\tcase *ast.FuncType:\n+\t\ta.apply(n, \"Params\", nil, n.Params)\n+\t\ta.apply(n, \"Results\", nil, n.Results)\n+\n+\tcase *ast.InterfaceType:\n+\t\ta.apply(n, \"Methods\", nil, n.Methods)\n+\n+\tcase *ast.MapType:\n+\t\ta.apply(n, \"Key\", nil, n.Key)\n+\t\ta.apply(n, \"Value\", nil, n.Value)\n+\n+\tcase *ast.ChanType:\n+\t\ta.apply(n, \"Value\", nil, n.Value)\n+\n+\t// Statements\n+\tcase *ast.BadStmt:\n+\t\t// nothing to do\n+\n+\tcase *ast.DeclStmt:\n+\t\ta.apply(n, \"Decl\", nil, n.Decl)\n+\n+\tcase *ast.EmptyStmt:\n+\t\t// nothing to do\n+\n+\tcase *ast.LabeledStmt:\n+\t\ta.apply(n, \"Label\", nil, n.Label)\n+\t\ta.apply(n, \"Stmt\", nil, n.Stmt)\n+\n+\tcase *ast.ExprStmt:\n+\t\ta.apply(n, \"X\", nil, n.X)\n+\n+\tcase *ast.SendStmt:\n+\t\ta.apply(n, \"Chan\", nil, n.Chan)\n+\t\ta.apply(n, \"Value\", nil, n.Value)\n+\n+\tcase *ast.IncDecStmt:\n+\t\ta.apply(n, \"X\", nil, n.X)\n+\n+\tcase *ast.AssignStmt:\n+\t\ta.applyList(n, \"Lhs\")\n+\t\ta.applyList(n, \"Rhs\")\n+\n+\tcase *ast.GoStmt:\n+\t\ta.apply(n, \"Call\", nil, n.Call)\n+\n+\tcase *ast.DeferStmt:\n+\t\ta.apply(n, \"Call\", nil, n.Call)\n+\n+\tcase *ast.ReturnStmt:\n+\t\ta.applyList(n, \"Results\")\n+\n+\tcase *ast.BranchStmt:\n+\t\ta.apply(n, \"Label\", nil, n.Label)\n+\n+\tcase *ast.BlockStmt:\n+\t\ta.applyList(n, \"List\")\n+\n+\tcase *ast.IfStmt:\n+\t\ta.apply(n, \"Init\", nil, n.Init)\n+\t\ta.apply(n, \"Cond\", nil, n.Cond)\n+\t\ta.apply(n, \"Body\", nil, n.Body)\n+\t\ta.apply(n, \"Else\", nil, n.Else)\n+\n+\tcase *ast.CaseClause:\n+\t\ta.applyList(n, \"List\")\n+\t\ta.applyList(n, \"Body\")\n+\n+\tcase *ast.SwitchStmt:\n+\t\ta.apply(n, \"Init\", nil, n.Init)\n+\t\ta.apply(n, \"Tag\", nil, n.Tag)\n+\t\ta.apply(n, \"Body\", nil, n.Body)\n+\n+\tcase *ast.TypeSwitchStmt:\n+\t\ta.apply(n, \"Init\", nil, n.Init)\n+\t\ta.apply(n, \"Assign\", nil, n.Assign)\n+\t\ta.apply(n, \"Body\", nil, n.Body)\n+\n+\tcase *ast.CommClause:\n+\t\ta.apply(n, \"Comm\", nil, n.Comm)\n+\t\ta.applyList(n, \"Body\")\n+\n+\tcase *ast.SelectStmt:\n+\t\ta.apply(n, \"Body\", nil, n.Body)\n+\n+\tcase *ast.ForStmt:\n+\t\ta.apply(n, \"Init\", nil, n.Init)\n+\t\ta.apply(n, \"Cond\", nil, n.Cond)\n+\t\ta.apply(n, \"Post\", nil, n.Post)\n+\t\ta.apply(n, \"Body\", nil, n.Body)\n+\n+\tcase *ast.RangeStmt:\n+\t\ta.apply(n, \"Key\", nil, n.Key)\n+\t\ta.apply(n, \"Value\", nil, n.Value)\n+\t\ta.apply(n, \"X\", nil, n.X)\n+\t\ta.apply(n, \"Body\", nil, n.Body)\n+\n+\t// Declarations\n+\tcase *ast.ImportSpec:\n+\t\ta.apply(n, \"Doc\", nil, n.Doc)\n+\t\ta.apply(n, \"Name\", nil, n.Name)\n+\t\ta.apply(n, \"Path\", nil, n.Path)\n+\t\ta.apply(n, \"Comment\", nil, n.Comment)\n+\n+\tcase *ast.ValueSpec:\n+\t\ta.apply(n, \"Doc\", nil, n.Doc)\n+\t\ta.applyList(n, \"Names\")\n+\t\ta.apply(n, \"Type\", nil, n.Type)\n+\t\ta.applyList(n, \"Values\")\n+\t\ta.apply(n, \"Comment\", nil, n.Comment)\n+\n+\tcase *ast.TypeSpec:\n+\t\ta.apply(n, \"Doc\", nil, n.Doc)\n+\t\ta.apply(n, \"Name\", nil, n.Name)\n+\t\ta.apply(n, \"Type\", nil, n.Type)\n+\t\ta.apply(n, \"Comment\", nil, n.Comment)\n+\n+\tcase *ast.BadDecl:\n+\t\t// nothing to do\n+\n+\tcase *ast.GenDecl:\n+\t\ta.apply(n, \"Doc\", nil, n.Doc)\n+\t\ta.applyList(n, \"Specs\")\n+\n+\tcase *ast.FuncDecl:\n+\t\ta.apply(n, \"Doc\", nil, n.Doc)\n+\t\ta.apply(n, \"Recv\", nil, n.Recv)\n+\t\ta.apply(n, \"Name\", nil, n.Name)\n+\t\ta.apply(n, \"Type\", nil, n.Type)\n+\t\ta.apply(n, \"Body\", nil, n.Body)\n+\n+\t// Files and packages\n+\tcase *ast.File:\n+\t\ta.apply(n, \"Doc\", nil, n.Doc)\n+\t\ta.apply(n, \"Name\", nil, n.Name)\n+\t\ta.applyList(n, \"Decls\")\n+\t\t// Don't walk n.Comments; they have either been walked already if\n+\t\t// they are Doc comments, or they can be easily walked explicitly.\n+\n+\tcase *ast.Package:\n+\t\t// collect and sort names for reproducible behavior\n+\t\tvar names []string\n+\t\tfor name := range n.Files {\n+\t\t\tnames = append(names, name)\n+\t\t}\n+\t\tsort.Strings(names)\n+\t\tfor _, name := range names {\n+\t\t\ta.apply(n, name, nil, n.Files[name])\n+\t\t}\n+\n+\tdefault:\n+\t\tpanic(fmt.Sprintf(\"Apply: unexpected node type %T\", n))\n+\t}\n+\n+\tif a.post != nil && !a.post(&a.cursor) {\n+\t\tpanic(abort)\n+\t}\n+\n+\ta.cursor = saved\n+}\n+\n+// An iterator controls iteration over a slice of nodes.\n+type iterator struct {\n+\tindex, step int\n+}\n+\n+func (a *application) applyList(parent ast.Node, name string) {\n+\t// avoid heap-allocating a new iterator for each applyList call; reuse a.iter instead\n+\tsaved := a.iter\n+\ta.iter.index = 0\n+\tfor {\n+\t\t// must reload parent.name each time, since cursor modifications might change it\n+\t\tv := reflect.Indirect(reflect.ValueOf(parent)).FieldByName(name)\n+\t\tif a.iter.index >= v.Len() {\n+\t\t\tbreak\n+\t\t}\n+\n+\t\t// element x may be nil in a bad AST - be cautious\n+\t\tvar x ast.Node\n+\t\tif e := v.Index(a.iter.index); e.IsValid() {\n+\t\t\tx = e.Interface().(ast.Node)\n+\t\t}\n+\n+\t\ta.iter.step = 1\n+\t\ta.apply(parent, name, &a.iter, x)\n+\t\ta.iter.index += a.iter.step\n+\t}\n+\ta.iter = saved\n+}"
    },
    {
      "sha": "7630629824af1e839b5954b72a5d5021191d69ae",
      "filename": "backend/vendor/golang.org/x/tools/go/ast/astutil/util.go",
      "status": "added",
      "additions": 14,
      "deletions": 0,
      "changes": 14,
      "blob_url": "https://github.com/umputun/remark42/blob/80f4862c9c2905475f3484f20ca41f7bdf103de9/backend/vendor/golang.org/x/tools/go/ast/astutil/util.go",
      "raw_url": "https://github.com/umputun/remark42/raw/80f4862c9c2905475f3484f20ca41f7bdf103de9/backend/vendor/golang.org/x/tools/go/ast/astutil/util.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/golang.org/x/tools/go/ast/astutil/util.go?ref=80f4862c9c2905475f3484f20ca41f7bdf103de9",
      "patch": "@@ -0,0 +1,14 @@\n+package astutil\n+\n+import \"go/ast\"\n+\n+// Unparen returns e with any enclosing parentheses stripped.\n+func Unparen(e ast.Expr) ast.Expr {\n+\tfor {\n+\t\tp, ok := e.(*ast.ParenExpr)\n+\t\tif !ok {\n+\t\t\treturn e\n+\t\t}\n+\t\te = p.X\n+\t}\n+}"
    },
    {
      "sha": "f8363d8faae379e98d29cb7f27b9f7818f5c8407",
      "filename": "backend/vendor/golang.org/x/tools/go/gcexportdata/gcexportdata.go",
      "status": "added",
      "additions": 109,
      "deletions": 0,
      "changes": 109,
      "blob_url": "https://github.com/umputun/remark42/blob/80f4862c9c2905475f3484f20ca41f7bdf103de9/backend/vendor/golang.org/x/tools/go/gcexportdata/gcexportdata.go",
      "raw_url": "https://github.com/umputun/remark42/raw/80f4862c9c2905475f3484f20ca41f7bdf103de9/backend/vendor/golang.org/x/tools/go/gcexportdata/gcexportdata.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/golang.org/x/tools/go/gcexportdata/gcexportdata.go?ref=80f4862c9c2905475f3484f20ca41f7bdf103de9",
      "patch": "@@ -0,0 +1,109 @@\n+// Copyright 2016 The Go Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style\n+// license that can be found in the LICENSE file.\n+\n+// Package gcexportdata provides functions for locating, reading, and\n+// writing export data files containing type information produced by the\n+// gc compiler.  This package supports go1.7 export data format and all\n+// later versions.\n+//\n+// Although it might seem convenient for this package to live alongside\n+// go/types in the standard library, this would cause version skew\n+// problems for developer tools that use it, since they must be able to\n+// consume the outputs of the gc compiler both before and after a Go\n+// update such as from Go 1.7 to Go 1.8.  Because this package lives in\n+// golang.org/x/tools, sites can update their version of this repo some\n+// time before the Go 1.8 release and rebuild and redeploy their\n+// developer tools, which will then be able to consume both Go 1.7 and\n+// Go 1.8 export data files, so they will work before and after the\n+// Go update. (See discussion at https://golang.org/issue/15651.)\n+//\n+package gcexportdata // import \"golang.org/x/tools/go/gcexportdata\"\n+\n+import (\n+\t\"bufio\"\n+\t\"bytes\"\n+\t\"fmt\"\n+\t\"go/token\"\n+\t\"go/types\"\n+\t\"io\"\n+\t\"io/ioutil\"\n+\n+\t\"golang.org/x/tools/go/internal/gcimporter\"\n+)\n+\n+// Find returns the name of an object (.o) or archive (.a) file\n+// containing type information for the specified import path,\n+// using the workspace layout conventions of go/build.\n+// If no file was found, an empty filename is returned.\n+//\n+// A relative srcDir is interpreted relative to the current working directory.\n+//\n+// Find also returns the package's resolved (canonical) import path,\n+// reflecting the effects of srcDir and vendoring on importPath.\n+func Find(importPath, srcDir string) (filename, path string) {\n+\treturn gcimporter.FindPkg(importPath, srcDir)\n+}\n+\n+// NewReader returns a reader for the export data section of an object\n+// (.o) or archive (.a) file read from r.  The new reader may provide\n+// additional trailing data beyond the end of the export data.\n+func NewReader(r io.Reader) (io.Reader, error) {\n+\tbuf := bufio.NewReader(r)\n+\t_, err := gcimporter.FindExportData(buf)\n+\t// If we ever switch to a zip-like archive format with the ToC\n+\t// at the end, we can return the correct portion of export data,\n+\t// but for now we must return the entire rest of the file.\n+\treturn buf, err\n+}\n+\n+// Read reads export data from in, decodes it, and returns type\n+// information for the package.\n+// The package name is specified by path.\n+// File position information is added to fset.\n+//\n+// Read may inspect and add to the imports map to ensure that references\n+// within the export data to other packages are consistent.  The caller\n+// must ensure that imports[path] does not exist, or exists but is\n+// incomplete (see types.Package.Complete), and Read inserts the\n+// resulting package into this map entry.\n+//\n+// On return, the state of the reader is undefined.\n+func Read(in io.Reader, fset *token.FileSet, imports map[string]*types.Package, path string) (*types.Package, error) {\n+\tdata, err := ioutil.ReadAll(in)\n+\tif err != nil {\n+\t\treturn nil, fmt.Errorf(\"reading export data for %q: %v\", path, err)\n+\t}\n+\n+\tif bytes.HasPrefix(data, []byte(\"!<arch>\")) {\n+\t\treturn nil, fmt.Errorf(\"can't read export data for %q directly from an archive file (call gcexportdata.NewReader first to extract export data)\", path)\n+\t}\n+\n+\t// The App Engine Go runtime v1.6 uses the old export data format.\n+\t// TODO(adonovan): delete once v1.7 has been around for a while.\n+\tif bytes.HasPrefix(data, []byte(\"package \")) {\n+\t\treturn gcimporter.ImportData(imports, path, path, bytes.NewReader(data))\n+\t}\n+\n+\t// The indexed export format starts with an 'i'; the older\n+\t// binary export format starts with a 'c', 'd', or 'v'\n+\t// (from \"version\"). Select appropriate importer.\n+\tif len(data) > 0 && data[0] == 'i' {\n+\t\t_, pkg, err := gcimporter.IImportData(fset, imports, data[1:], path)\n+\t\treturn pkg, err\n+\t}\n+\n+\t_, pkg, err := gcimporter.BImportData(fset, imports, data, path)\n+\treturn pkg, err\n+}\n+\n+// Write writes encoded type information for the specified package to out.\n+// The FileSet provides file position information for named objects.\n+func Write(out io.Writer, fset *token.FileSet, pkg *types.Package) error {\n+\tb, err := gcimporter.IExportData(fset, pkg)\n+\tif err != nil {\n+\t\treturn err\n+\t}\n+\t_, err = out.Write(b)\n+\treturn err\n+}"
    },
    {
      "sha": "efe221e7e1423e370d968eb512be02bb7ea6601e",
      "filename": "backend/vendor/golang.org/x/tools/go/gcexportdata/importer.go",
      "status": "added",
      "additions": 73,
      "deletions": 0,
      "changes": 73,
      "blob_url": "https://github.com/umputun/remark42/blob/80f4862c9c2905475f3484f20ca41f7bdf103de9/backend/vendor/golang.org/x/tools/go/gcexportdata/importer.go",
      "raw_url": "https://github.com/umputun/remark42/raw/80f4862c9c2905475f3484f20ca41f7bdf103de9/backend/vendor/golang.org/x/tools/go/gcexportdata/importer.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/golang.org/x/tools/go/gcexportdata/importer.go?ref=80f4862c9c2905475f3484f20ca41f7bdf103de9",
      "patch": "@@ -0,0 +1,73 @@\n+// Copyright 2016 The Go Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style\n+// license that can be found in the LICENSE file.\n+\n+package gcexportdata\n+\n+import (\n+\t\"fmt\"\n+\t\"go/token\"\n+\t\"go/types\"\n+\t\"os\"\n+)\n+\n+// NewImporter returns a new instance of the types.Importer interface\n+// that reads type information from export data files written by gc.\n+// The Importer also satisfies types.ImporterFrom.\n+//\n+// Export data files are located using \"go build\" workspace conventions\n+// and the build.Default context.\n+//\n+// Use this importer instead of go/importer.For(\"gc\", ...) to avoid the\n+// version-skew problems described in the documentation of this package,\n+// or to control the FileSet or access the imports map populated during\n+// package loading.\n+//\n+func NewImporter(fset *token.FileSet, imports map[string]*types.Package) types.ImporterFrom {\n+\treturn importer{fset, imports}\n+}\n+\n+type importer struct {\n+\tfset    *token.FileSet\n+\timports map[string]*types.Package\n+}\n+\n+func (imp importer) Import(importPath string) (*types.Package, error) {\n+\treturn imp.ImportFrom(importPath, \"\", 0)\n+}\n+\n+func (imp importer) ImportFrom(importPath, srcDir string, mode types.ImportMode) (_ *types.Package, err error) {\n+\tfilename, path := Find(importPath, srcDir)\n+\tif filename == \"\" {\n+\t\tif importPath == \"unsafe\" {\n+\t\t\t// Even for unsafe, call Find first in case\n+\t\t\t// the package was vendored.\n+\t\t\treturn types.Unsafe, nil\n+\t\t}\n+\t\treturn nil, fmt.Errorf(\"can't find import: %s\", importPath)\n+\t}\n+\n+\tif pkg, ok := imp.imports[path]; ok && pkg.Complete() {\n+\t\treturn pkg, nil // cache hit\n+\t}\n+\n+\t// open file\n+\tf, err := os.Open(filename)\n+\tif err != nil {\n+\t\treturn nil, err\n+\t}\n+\tdefer func() {\n+\t\tf.Close()\n+\t\tif err != nil {\n+\t\t\t// add file name to error\n+\t\t\terr = fmt.Errorf(\"reading export data: %s: %v\", filename, err)\n+\t\t}\n+\t}()\n+\n+\tr, err := NewReader(f)\n+\tif err != nil {\n+\t\treturn nil, err\n+\t}\n+\n+\treturn Read(r, imp.fset, imp.imports, path)\n+}"
    },
    {
      "sha": "a807d0aaa281325c537ceb8f6f6f81383225bfd2",
      "filename": "backend/vendor/golang.org/x/tools/go/internal/gcimporter/bexport.go",
      "status": "added",
      "additions": 852,
      "deletions": 0,
      "changes": 852,
      "blob_url": "https://github.com/umputun/remark42/blob/80f4862c9c2905475f3484f20ca41f7bdf103de9/backend/vendor/golang.org/x/tools/go/internal/gcimporter/bexport.go",
      "raw_url": "https://github.com/umputun/remark42/raw/80f4862c9c2905475f3484f20ca41f7bdf103de9/backend/vendor/golang.org/x/tools/go/internal/gcimporter/bexport.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/golang.org/x/tools/go/internal/gcimporter/bexport.go?ref=80f4862c9c2905475f3484f20ca41f7bdf103de9",
      "patch": "@@ -0,0 +1,852 @@\n+// Copyright 2016 The Go Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style\n+// license that can be found in the LICENSE file.\n+\n+// Binary package export.\n+// This file was derived from $GOROOT/src/cmd/compile/internal/gc/bexport.go;\n+// see that file for specification of the format.\n+\n+package gcimporter\n+\n+import (\n+\t\"bytes\"\n+\t\"encoding/binary\"\n+\t\"fmt\"\n+\t\"go/ast\"\n+\t\"go/constant\"\n+\t\"go/token\"\n+\t\"go/types\"\n+\t\"math\"\n+\t\"math/big\"\n+\t\"sort\"\n+\t\"strings\"\n+)\n+\n+// If debugFormat is set, each integer and string value is preceded by a marker\n+// and position information in the encoding. This mechanism permits an importer\n+// to recognize immediately when it is out of sync. The importer recognizes this\n+// mode automatically (i.e., it can import export data produced with debugging\n+// support even if debugFormat is not set at the time of import). This mode will\n+// lead to massively larger export data (by a factor of 2 to 3) and should only\n+// be enabled during development and debugging.\n+//\n+// NOTE: This flag is the first flag to enable if importing dies because of\n+// (suspected) format errors, and whenever a change is made to the format.\n+const debugFormat = false // default: false\n+\n+// If trace is set, debugging output is printed to std out.\n+const trace = false // default: false\n+\n+// Current export format version. Increase with each format change.\n+// Note: The latest binary (non-indexed) export format is at version 6.\n+//       This exporter is still at level 4, but it doesn't matter since\n+//       the binary importer can handle older versions just fine.\n+// 6: package height (CL 105038) -- NOT IMPLEMENTED HERE\n+// 5: improved position encoding efficiency (issue 20080, CL 41619) -- NOT IMPLEMEMTED HERE\n+// 4: type name objects support type aliases, uses aliasTag\n+// 3: Go1.8 encoding (same as version 2, aliasTag defined but never used)\n+// 2: removed unused bool in ODCL export (compiler only)\n+// 1: header format change (more regular), export package for _ struct fields\n+// 0: Go1.7 encoding\n+const exportVersion = 4\n+\n+// trackAllTypes enables cycle tracking for all types, not just named\n+// types. The existing compiler invariants assume that unnamed types\n+// that are not completely set up are not used, or else there are spurious\n+// errors.\n+// If disabled, only named types are tracked, possibly leading to slightly\n+// less efficient encoding in rare cases. It also prevents the export of\n+// some corner-case type declarations (but those are not handled correctly\n+// with with the textual export format either).\n+// TODO(gri) enable and remove once issues caused by it are fixed\n+const trackAllTypes = false\n+\n+type exporter struct {\n+\tfset *token.FileSet\n+\tout  bytes.Buffer\n+\n+\t// object -> index maps, indexed in order of serialization\n+\tstrIndex map[string]int\n+\tpkgIndex map[*types.Package]int\n+\ttypIndex map[types.Type]int\n+\n+\t// position encoding\n+\tposInfoFormat bool\n+\tprevFile      string\n+\tprevLine      int\n+\n+\t// debugging support\n+\twritten int // bytes written\n+\tindent  int // for trace\n+}\n+\n+// internalError represents an error generated inside this package.\n+type internalError string\n+\n+func (e internalError) Error() string { return \"gcimporter: \" + string(e) }\n+\n+func internalErrorf(format string, args ...interface{}) error {\n+\treturn internalError(fmt.Sprintf(format, args...))\n+}\n+\n+// BExportData returns binary export data for pkg.\n+// If no file set is provided, position info will be missing.\n+func BExportData(fset *token.FileSet, pkg *types.Package) (b []byte, err error) {\n+\tdefer func() {\n+\t\tif e := recover(); e != nil {\n+\t\t\tif ierr, ok := e.(internalError); ok {\n+\t\t\t\terr = ierr\n+\t\t\t\treturn\n+\t\t\t}\n+\t\t\t// Not an internal error; panic again.\n+\t\t\tpanic(e)\n+\t\t}\n+\t}()\n+\n+\tp := exporter{\n+\t\tfset:          fset,\n+\t\tstrIndex:      map[string]int{\"\": 0}, // empty string is mapped to 0\n+\t\tpkgIndex:      make(map[*types.Package]int),\n+\t\ttypIndex:      make(map[types.Type]int),\n+\t\tposInfoFormat: true, // TODO(gri) might become a flag, eventually\n+\t}\n+\n+\t// write version info\n+\t// The version string must start with \"version %d\" where %d is the version\n+\t// number. Additional debugging information may follow after a blank; that\n+\t// text is ignored by the importer.\n+\tp.rawStringln(fmt.Sprintf(\"version %d\", exportVersion))\n+\tvar debug string\n+\tif debugFormat {\n+\t\tdebug = \"debug\"\n+\t}\n+\tp.rawStringln(debug) // cannot use p.bool since it's affected by debugFormat; also want to see this clearly\n+\tp.bool(trackAllTypes)\n+\tp.bool(p.posInfoFormat)\n+\n+\t// --- generic export data ---\n+\n+\t// populate type map with predeclared \"known\" types\n+\tfor index, typ := range predeclared() {\n+\t\tp.typIndex[typ] = index\n+\t}\n+\tif len(p.typIndex) != len(predeclared()) {\n+\t\treturn nil, internalError(\"duplicate entries in type map?\")\n+\t}\n+\n+\t// write package data\n+\tp.pkg(pkg, true)\n+\tif trace {\n+\t\tp.tracef(\"\\n\")\n+\t}\n+\n+\t// write objects\n+\tobjcount := 0\n+\tscope := pkg.Scope()\n+\tfor _, name := range scope.Names() {\n+\t\tif !ast.IsExported(name) {\n+\t\t\tcontinue\n+\t\t}\n+\t\tif trace {\n+\t\t\tp.tracef(\"\\n\")\n+\t\t}\n+\t\tp.obj(scope.Lookup(name))\n+\t\tobjcount++\n+\t}\n+\n+\t// indicate end of list\n+\tif trace {\n+\t\tp.tracef(\"\\n\")\n+\t}\n+\tp.tag(endTag)\n+\n+\t// for self-verification only (redundant)\n+\tp.int(objcount)\n+\n+\tif trace {\n+\t\tp.tracef(\"\\n\")\n+\t}\n+\n+\t// --- end of export data ---\n+\n+\treturn p.out.Bytes(), nil\n+}\n+\n+func (p *exporter) pkg(pkg *types.Package, emptypath bool) {\n+\tif pkg == nil {\n+\t\tpanic(internalError(\"unexpected nil pkg\"))\n+\t}\n+\n+\t// if we saw the package before, write its index (>= 0)\n+\tif i, ok := p.pkgIndex[pkg]; ok {\n+\t\tp.index('P', i)\n+\t\treturn\n+\t}\n+\n+\t// otherwise, remember the package, write the package tag (< 0) and package data\n+\tif trace {\n+\t\tp.tracef(\"P%d = { \", len(p.pkgIndex))\n+\t\tdefer p.tracef(\"} \")\n+\t}\n+\tp.pkgIndex[pkg] = len(p.pkgIndex)\n+\n+\tp.tag(packageTag)\n+\tp.string(pkg.Name())\n+\tif emptypath {\n+\t\tp.string(\"\")\n+\t} else {\n+\t\tp.string(pkg.Path())\n+\t}\n+}\n+\n+func (p *exporter) obj(obj types.Object) {\n+\tswitch obj := obj.(type) {\n+\tcase *types.Const:\n+\t\tp.tag(constTag)\n+\t\tp.pos(obj)\n+\t\tp.qualifiedName(obj)\n+\t\tp.typ(obj.Type())\n+\t\tp.value(obj.Val())\n+\n+\tcase *types.TypeName:\n+\t\tif obj.IsAlias() {\n+\t\t\tp.tag(aliasTag)\n+\t\t\tp.pos(obj)\n+\t\t\tp.qualifiedName(obj)\n+\t\t} else {\n+\t\t\tp.tag(typeTag)\n+\t\t}\n+\t\tp.typ(obj.Type())\n+\n+\tcase *types.Var:\n+\t\tp.tag(varTag)\n+\t\tp.pos(obj)\n+\t\tp.qualifiedName(obj)\n+\t\tp.typ(obj.Type())\n+\n+\tcase *types.Func:\n+\t\tp.tag(funcTag)\n+\t\tp.pos(obj)\n+\t\tp.qualifiedName(obj)\n+\t\tsig := obj.Type().(*types.Signature)\n+\t\tp.paramList(sig.Params(), sig.Variadic())\n+\t\tp.paramList(sig.Results(), false)\n+\n+\tdefault:\n+\t\tpanic(internalErrorf(\"unexpected object %v (%T)\", obj, obj))\n+\t}\n+}\n+\n+func (p *exporter) pos(obj types.Object) {\n+\tif !p.posInfoFormat {\n+\t\treturn\n+\t}\n+\n+\tfile, line := p.fileLine(obj)\n+\tif file == p.prevFile {\n+\t\t// common case: write line delta\n+\t\t// delta == 0 means different file or no line change\n+\t\tdelta := line - p.prevLine\n+\t\tp.int(delta)\n+\t\tif delta == 0 {\n+\t\t\tp.int(-1) // -1 means no file change\n+\t\t}\n+\t} else {\n+\t\t// different file\n+\t\tp.int(0)\n+\t\t// Encode filename as length of common prefix with previous\n+\t\t// filename, followed by (possibly empty) suffix. Filenames\n+\t\t// frequently share path prefixes, so this can save a lot\n+\t\t// of space and make export data size less dependent on file\n+\t\t// path length. The suffix is unlikely to be empty because\n+\t\t// file names tend to end in \".go\".\n+\t\tn := commonPrefixLen(p.prevFile, file)\n+\t\tp.int(n)           // n >= 0\n+\t\tp.string(file[n:]) // write suffix only\n+\t\tp.prevFile = file\n+\t\tp.int(line)\n+\t}\n+\tp.prevLine = line\n+}\n+\n+func (p *exporter) fileLine(obj types.Object) (file string, line int) {\n+\tif p.fset != nil {\n+\t\tpos := p.fset.Position(obj.Pos())\n+\t\tfile = pos.Filename\n+\t\tline = pos.Line\n+\t}\n+\treturn\n+}\n+\n+func commonPrefixLen(a, b string) int {\n+\tif len(a) > len(b) {\n+\t\ta, b = b, a\n+\t}\n+\t// len(a) <= len(b)\n+\ti := 0\n+\tfor i < len(a) && a[i] == b[i] {\n+\t\ti++\n+\t}\n+\treturn i\n+}\n+\n+func (p *exporter) qualifiedName(obj types.Object) {\n+\tp.string(obj.Name())\n+\tp.pkg(obj.Pkg(), false)\n+}\n+\n+func (p *exporter) typ(t types.Type) {\n+\tif t == nil {\n+\t\tpanic(internalError(\"nil type\"))\n+\t}\n+\n+\t// Possible optimization: Anonymous pointer types *T where\n+\t// T is a named type are common. We could canonicalize all\n+\t// such types *T to a single type PT = *T. This would lead\n+\t// to at most one *T entry in typIndex, and all future *T's\n+\t// would be encoded as the respective index directly. Would\n+\t// save 1 byte (pointerTag) per *T and reduce the typIndex\n+\t// size (at the cost of a canonicalization map). We can do\n+\t// this later, without encoding format change.\n+\n+\t// if we saw the type before, write its index (>= 0)\n+\tif i, ok := p.typIndex[t]; ok {\n+\t\tp.index('T', i)\n+\t\treturn\n+\t}\n+\n+\t// otherwise, remember the type, write the type tag (< 0) and type data\n+\tif trackAllTypes {\n+\t\tif trace {\n+\t\t\tp.tracef(\"T%d = {>\\n\", len(p.typIndex))\n+\t\t\tdefer p.tracef(\"<\\n} \")\n+\t\t}\n+\t\tp.typIndex[t] = len(p.typIndex)\n+\t}\n+\n+\tswitch t := t.(type) {\n+\tcase *types.Named:\n+\t\tif !trackAllTypes {\n+\t\t\t// if we don't track all types, track named types now\n+\t\t\tp.typIndex[t] = len(p.typIndex)\n+\t\t}\n+\n+\t\tp.tag(namedTag)\n+\t\tp.pos(t.Obj())\n+\t\tp.qualifiedName(t.Obj())\n+\t\tp.typ(t.Underlying())\n+\t\tif !types.IsInterface(t) {\n+\t\t\tp.assocMethods(t)\n+\t\t}\n+\n+\tcase *types.Array:\n+\t\tp.tag(arrayTag)\n+\t\tp.int64(t.Len())\n+\t\tp.typ(t.Elem())\n+\n+\tcase *types.Slice:\n+\t\tp.tag(sliceTag)\n+\t\tp.typ(t.Elem())\n+\n+\tcase *dddSlice:\n+\t\tp.tag(dddTag)\n+\t\tp.typ(t.elem)\n+\n+\tcase *types.Struct:\n+\t\tp.tag(structTag)\n+\t\tp.fieldList(t)\n+\n+\tcase *types.Pointer:\n+\t\tp.tag(pointerTag)\n+\t\tp.typ(t.Elem())\n+\n+\tcase *types.Signature:\n+\t\tp.tag(signatureTag)\n+\t\tp.paramList(t.Params(), t.Variadic())\n+\t\tp.paramList(t.Results(), false)\n+\n+\tcase *types.Interface:\n+\t\tp.tag(interfaceTag)\n+\t\tp.iface(t)\n+\n+\tcase *types.Map:\n+\t\tp.tag(mapTag)\n+\t\tp.typ(t.Key())\n+\t\tp.typ(t.Elem())\n+\n+\tcase *types.Chan:\n+\t\tp.tag(chanTag)\n+\t\tp.int(int(3 - t.Dir())) // hack\n+\t\tp.typ(t.Elem())\n+\n+\tdefault:\n+\t\tpanic(internalErrorf(\"unexpected type %T: %s\", t, t))\n+\t}\n+}\n+\n+func (p *exporter) assocMethods(named *types.Named) {\n+\t// Sort methods (for determinism).\n+\tvar methods []*types.Func\n+\tfor i := 0; i < named.NumMethods(); i++ {\n+\t\tmethods = append(methods, named.Method(i))\n+\t}\n+\tsort.Sort(methodsByName(methods))\n+\n+\tp.int(len(methods))\n+\n+\tif trace && methods != nil {\n+\t\tp.tracef(\"associated methods {>\\n\")\n+\t}\n+\n+\tfor i, m := range methods {\n+\t\tif trace && i > 0 {\n+\t\t\tp.tracef(\"\\n\")\n+\t\t}\n+\n+\t\tp.pos(m)\n+\t\tname := m.Name()\n+\t\tp.string(name)\n+\t\tif !exported(name) {\n+\t\t\tp.pkg(m.Pkg(), false)\n+\t\t}\n+\n+\t\tsig := m.Type().(*types.Signature)\n+\t\tp.paramList(types.NewTuple(sig.Recv()), false)\n+\t\tp.paramList(sig.Params(), sig.Variadic())\n+\t\tp.paramList(sig.Results(), false)\n+\t\tp.int(0) // dummy value for go:nointerface pragma - ignored by importer\n+\t}\n+\n+\tif trace && methods != nil {\n+\t\tp.tracef(\"<\\n} \")\n+\t}\n+}\n+\n+type methodsByName []*types.Func\n+\n+func (x methodsByName) Len() int           { return len(x) }\n+func (x methodsByName) Swap(i, j int)      { x[i], x[j] = x[j], x[i] }\n+func (x methodsByName) Less(i, j int) bool { return x[i].Name() < x[j].Name() }\n+\n+func (p *exporter) fieldList(t *types.Struct) {\n+\tif trace && t.NumFields() > 0 {\n+\t\tp.tracef(\"fields {>\\n\")\n+\t\tdefer p.tracef(\"<\\n} \")\n+\t}\n+\n+\tp.int(t.NumFields())\n+\tfor i := 0; i < t.NumFields(); i++ {\n+\t\tif trace && i > 0 {\n+\t\t\tp.tracef(\"\\n\")\n+\t\t}\n+\t\tp.field(t.Field(i))\n+\t\tp.string(t.Tag(i))\n+\t}\n+}\n+\n+func (p *exporter) field(f *types.Var) {\n+\tif !f.IsField() {\n+\t\tpanic(internalError(\"field expected\"))\n+\t}\n+\n+\tp.pos(f)\n+\tp.fieldName(f)\n+\tp.typ(f.Type())\n+}\n+\n+func (p *exporter) iface(t *types.Interface) {\n+\t// TODO(gri): enable importer to load embedded interfaces,\n+\t// then emit Embeddeds and ExplicitMethods separately here.\n+\tp.int(0)\n+\n+\tn := t.NumMethods()\n+\tif trace && n > 0 {\n+\t\tp.tracef(\"methods {>\\n\")\n+\t\tdefer p.tracef(\"<\\n} \")\n+\t}\n+\tp.int(n)\n+\tfor i := 0; i < n; i++ {\n+\t\tif trace && i > 0 {\n+\t\t\tp.tracef(\"\\n\")\n+\t\t}\n+\t\tp.method(t.Method(i))\n+\t}\n+}\n+\n+func (p *exporter) method(m *types.Func) {\n+\tsig := m.Type().(*types.Signature)\n+\tif sig.Recv() == nil {\n+\t\tpanic(internalError(\"method expected\"))\n+\t}\n+\n+\tp.pos(m)\n+\tp.string(m.Name())\n+\tif m.Name() != \"_\" && !ast.IsExported(m.Name()) {\n+\t\tp.pkg(m.Pkg(), false)\n+\t}\n+\n+\t// interface method; no need to encode receiver.\n+\tp.paramList(sig.Params(), sig.Variadic())\n+\tp.paramList(sig.Results(), false)\n+}\n+\n+func (p *exporter) fieldName(f *types.Var) {\n+\tname := f.Name()\n+\n+\tif f.Anonymous() {\n+\t\t// anonymous field - we distinguish between 3 cases:\n+\t\t// 1) field name matches base type name and is exported\n+\t\t// 2) field name matches base type name and is not exported\n+\t\t// 3) field name doesn't match base type name (alias name)\n+\t\tbname := basetypeName(f.Type())\n+\t\tif name == bname {\n+\t\t\tif ast.IsExported(name) {\n+\t\t\t\tname = \"\" // 1) we don't need to know the field name or package\n+\t\t\t} else {\n+\t\t\t\tname = \"?\" // 2) use unexported name \"?\" to force package export\n+\t\t\t}\n+\t\t} else {\n+\t\t\t// 3) indicate alias and export name as is\n+\t\t\t// (this requires an extra \"@\" but this is a rare case)\n+\t\t\tp.string(\"@\")\n+\t\t}\n+\t}\n+\n+\tp.string(name)\n+\tif name != \"\" && !ast.IsExported(name) {\n+\t\tp.pkg(f.Pkg(), false)\n+\t}\n+}\n+\n+func basetypeName(typ types.Type) string {\n+\tswitch typ := deref(typ).(type) {\n+\tcase *types.Basic:\n+\t\treturn typ.Name()\n+\tcase *types.Named:\n+\t\treturn typ.Obj().Name()\n+\tdefault:\n+\t\treturn \"\" // unnamed type\n+\t}\n+}\n+\n+func (p *exporter) paramList(params *types.Tuple, variadic bool) {\n+\t// use negative length to indicate unnamed parameters\n+\t// (look at the first parameter only since either all\n+\t// names are present or all are absent)\n+\tn := params.Len()\n+\tif n > 0 && params.At(0).Name() == \"\" {\n+\t\tn = -n\n+\t}\n+\tp.int(n)\n+\tfor i := 0; i < params.Len(); i++ {\n+\t\tq := params.At(i)\n+\t\tt := q.Type()\n+\t\tif variadic && i == params.Len()-1 {\n+\t\t\tt = &dddSlice{t.(*types.Slice).Elem()}\n+\t\t}\n+\t\tp.typ(t)\n+\t\tif n > 0 {\n+\t\t\tname := q.Name()\n+\t\t\tp.string(name)\n+\t\t\tif name != \"_\" {\n+\t\t\t\tp.pkg(q.Pkg(), false)\n+\t\t\t}\n+\t\t}\n+\t\tp.string(\"\") // no compiler-specific info\n+\t}\n+}\n+\n+func (p *exporter) value(x constant.Value) {\n+\tif trace {\n+\t\tp.tracef(\"= \")\n+\t}\n+\n+\tswitch x.Kind() {\n+\tcase constant.Bool:\n+\t\ttag := falseTag\n+\t\tif constant.BoolVal(x) {\n+\t\t\ttag = trueTag\n+\t\t}\n+\t\tp.tag(tag)\n+\n+\tcase constant.Int:\n+\t\tif v, exact := constant.Int64Val(x); exact {\n+\t\t\t// common case: x fits into an int64 - use compact encoding\n+\t\t\tp.tag(int64Tag)\n+\t\t\tp.int64(v)\n+\t\t\treturn\n+\t\t}\n+\t\t// uncommon case: large x - use float encoding\n+\t\t// (powers of 2 will be encoded efficiently with exponent)\n+\t\tp.tag(floatTag)\n+\t\tp.float(constant.ToFloat(x))\n+\n+\tcase constant.Float:\n+\t\tp.tag(floatTag)\n+\t\tp.float(x)\n+\n+\tcase constant.Complex:\n+\t\tp.tag(complexTag)\n+\t\tp.float(constant.Real(x))\n+\t\tp.float(constant.Imag(x))\n+\n+\tcase constant.String:\n+\t\tp.tag(stringTag)\n+\t\tp.string(constant.StringVal(x))\n+\n+\tcase constant.Unknown:\n+\t\t// package contains type errors\n+\t\tp.tag(unknownTag)\n+\n+\tdefault:\n+\t\tpanic(internalErrorf(\"unexpected value %v (%T)\", x, x))\n+\t}\n+}\n+\n+func (p *exporter) float(x constant.Value) {\n+\tif x.Kind() != constant.Float {\n+\t\tpanic(internalErrorf(\"unexpected constant %v, want float\", x))\n+\t}\n+\t// extract sign (there is no -0)\n+\tsign := constant.Sign(x)\n+\tif sign == 0 {\n+\t\t// x == 0\n+\t\tp.int(0)\n+\t\treturn\n+\t}\n+\t// x != 0\n+\n+\tvar f big.Float\n+\tif v, exact := constant.Float64Val(x); exact {\n+\t\t// float64\n+\t\tf.SetFloat64(v)\n+\t} else if num, denom := constant.Num(x), constant.Denom(x); num.Kind() == constant.Int {\n+\t\t// TODO(gri): add big.Rat accessor to constant.Value.\n+\t\tr := valueToRat(num)\n+\t\tf.SetRat(r.Quo(r, valueToRat(denom)))\n+\t} else {\n+\t\t// Value too large to represent as a fraction => inaccessible.\n+\t\t// TODO(gri): add big.Float accessor to constant.Value.\n+\t\tf.SetFloat64(math.MaxFloat64) // FIXME\n+\t}\n+\n+\t// extract exponent such that 0.5 <= m < 1.0\n+\tvar m big.Float\n+\texp := f.MantExp(&m)\n+\n+\t// extract mantissa as *big.Int\n+\t// - set exponent large enough so mant satisfies mant.IsInt()\n+\t// - get *big.Int from mant\n+\tm.SetMantExp(&m, int(m.MinPrec()))\n+\tmant, acc := m.Int(nil)\n+\tif acc != big.Exact {\n+\t\tpanic(internalError(\"internal error\"))\n+\t}\n+\n+\tp.int(sign)\n+\tp.int(exp)\n+\tp.string(string(mant.Bytes()))\n+}\n+\n+func valueToRat(x constant.Value) *big.Rat {\n+\t// Convert little-endian to big-endian.\n+\t// I can't believe this is necessary.\n+\tbytes := constant.Bytes(x)\n+\tfor i := 0; i < len(bytes)/2; i++ {\n+\t\tbytes[i], bytes[len(bytes)-1-i] = bytes[len(bytes)-1-i], bytes[i]\n+\t}\n+\treturn new(big.Rat).SetInt(new(big.Int).SetBytes(bytes))\n+}\n+\n+func (p *exporter) bool(b bool) bool {\n+\tif trace {\n+\t\tp.tracef(\"[\")\n+\t\tdefer p.tracef(\"= %v] \", b)\n+\t}\n+\n+\tx := 0\n+\tif b {\n+\t\tx = 1\n+\t}\n+\tp.int(x)\n+\treturn b\n+}\n+\n+// ----------------------------------------------------------------------------\n+// Low-level encoders\n+\n+func (p *exporter) index(marker byte, index int) {\n+\tif index < 0 {\n+\t\tpanic(internalError(\"invalid index < 0\"))\n+\t}\n+\tif debugFormat {\n+\t\tp.marker('t')\n+\t}\n+\tif trace {\n+\t\tp.tracef(\"%c%d \", marker, index)\n+\t}\n+\tp.rawInt64(int64(index))\n+}\n+\n+func (p *exporter) tag(tag int) {\n+\tif tag >= 0 {\n+\t\tpanic(internalError(\"invalid tag >= 0\"))\n+\t}\n+\tif debugFormat {\n+\t\tp.marker('t')\n+\t}\n+\tif trace {\n+\t\tp.tracef(\"%s \", tagString[-tag])\n+\t}\n+\tp.rawInt64(int64(tag))\n+}\n+\n+func (p *exporter) int(x int) {\n+\tp.int64(int64(x))\n+}\n+\n+func (p *exporter) int64(x int64) {\n+\tif debugFormat {\n+\t\tp.marker('i')\n+\t}\n+\tif trace {\n+\t\tp.tracef(\"%d \", x)\n+\t}\n+\tp.rawInt64(x)\n+}\n+\n+func (p *exporter) string(s string) {\n+\tif debugFormat {\n+\t\tp.marker('s')\n+\t}\n+\tif trace {\n+\t\tp.tracef(\"%q \", s)\n+\t}\n+\t// if we saw the string before, write its index (>= 0)\n+\t// (the empty string is mapped to 0)\n+\tif i, ok := p.strIndex[s]; ok {\n+\t\tp.rawInt64(int64(i))\n+\t\treturn\n+\t}\n+\t// otherwise, remember string and write its negative length and bytes\n+\tp.strIndex[s] = len(p.strIndex)\n+\tp.rawInt64(-int64(len(s)))\n+\tfor i := 0; i < len(s); i++ {\n+\t\tp.rawByte(s[i])\n+\t}\n+}\n+\n+// marker emits a marker byte and position information which makes\n+// it easy for a reader to detect if it is \"out of sync\". Used for\n+// debugFormat format only.\n+func (p *exporter) marker(m byte) {\n+\tp.rawByte(m)\n+\t// Enable this for help tracking down the location\n+\t// of an incorrect marker when running in debugFormat.\n+\tif false && trace {\n+\t\tp.tracef(\"#%d \", p.written)\n+\t}\n+\tp.rawInt64(int64(p.written))\n+}\n+\n+// rawInt64 should only be used by low-level encoders.\n+func (p *exporter) rawInt64(x int64) {\n+\tvar tmp [binary.MaxVarintLen64]byte\n+\tn := binary.PutVarint(tmp[:], x)\n+\tfor i := 0; i < n; i++ {\n+\t\tp.rawByte(tmp[i])\n+\t}\n+}\n+\n+// rawStringln should only be used to emit the initial version string.\n+func (p *exporter) rawStringln(s string) {\n+\tfor i := 0; i < len(s); i++ {\n+\t\tp.rawByte(s[i])\n+\t}\n+\tp.rawByte('\\n')\n+}\n+\n+// rawByte is the bottleneck interface to write to p.out.\n+// rawByte escapes b as follows (any encoding does that\n+// hides '$'):\n+//\n+//\t'$'  => '|' 'S'\n+//\t'|'  => '|' '|'\n+//\n+// Necessary so other tools can find the end of the\n+// export data by searching for \"$$\".\n+// rawByte should only be used by low-level encoders.\n+func (p *exporter) rawByte(b byte) {\n+\tswitch b {\n+\tcase '$':\n+\t\t// write '$' as '|' 'S'\n+\t\tb = 'S'\n+\t\tfallthrough\n+\tcase '|':\n+\t\t// write '|' as '|' '|'\n+\t\tp.out.WriteByte('|')\n+\t\tp.written++\n+\t}\n+\tp.out.WriteByte(b)\n+\tp.written++\n+}\n+\n+// tracef is like fmt.Printf but it rewrites the format string\n+// to take care of indentation.\n+func (p *exporter) tracef(format string, args ...interface{}) {\n+\tif strings.ContainsAny(format, \"<>\\n\") {\n+\t\tvar buf bytes.Buffer\n+\t\tfor i := 0; i < len(format); i++ {\n+\t\t\t// no need to deal with runes\n+\t\t\tch := format[i]\n+\t\t\tswitch ch {\n+\t\t\tcase '>':\n+\t\t\t\tp.indent++\n+\t\t\t\tcontinue\n+\t\t\tcase '<':\n+\t\t\t\tp.indent--\n+\t\t\t\tcontinue\n+\t\t\t}\n+\t\t\tbuf.WriteByte(ch)\n+\t\t\tif ch == '\\n' {\n+\t\t\t\tfor j := p.indent; j > 0; j-- {\n+\t\t\t\t\tbuf.WriteString(\".  \")\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t\tformat = buf.String()\n+\t}\n+\tfmt.Printf(format, args...)\n+}\n+\n+// Debugging support.\n+// (tagString is only used when tracing is enabled)\n+var tagString = [...]string{\n+\t// Packages\n+\t-packageTag: \"package\",\n+\n+\t// Types\n+\t-namedTag:     \"named type\",\n+\t-arrayTag:     \"array\",\n+\t-sliceTag:     \"slice\",\n+\t-dddTag:       \"ddd\",\n+\t-structTag:    \"struct\",\n+\t-pointerTag:   \"pointer\",\n+\t-signatureTag: \"signature\",\n+\t-interfaceTag: \"interface\",\n+\t-mapTag:       \"map\",\n+\t-chanTag:      \"chan\",\n+\n+\t// Values\n+\t-falseTag:    \"false\",\n+\t-trueTag:     \"true\",\n+\t-int64Tag:    \"int64\",\n+\t-floatTag:    \"float\",\n+\t-fractionTag: \"fraction\",\n+\t-complexTag:  \"complex\",\n+\t-stringTag:   \"string\",\n+\t-unknownTag:  \"unknown\",\n+\n+\t// Type aliases\n+\t-aliasTag: \"alias\",\n+}"
    },
    {
      "sha": "e9f73d14a182e18631a4968d3942672cab5462db",
      "filename": "backend/vendor/golang.org/x/tools/go/internal/gcimporter/bimport.go",
      "status": "added",
      "additions": 1039,
      "deletions": 0,
      "changes": 1039,
      "blob_url": "https://github.com/umputun/remark42/blob/80f4862c9c2905475f3484f20ca41f7bdf103de9/backend/vendor/golang.org/x/tools/go/internal/gcimporter/bimport.go",
      "raw_url": "https://github.com/umputun/remark42/raw/80f4862c9c2905475f3484f20ca41f7bdf103de9/backend/vendor/golang.org/x/tools/go/internal/gcimporter/bimport.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/golang.org/x/tools/go/internal/gcimporter/bimport.go?ref=80f4862c9c2905475f3484f20ca41f7bdf103de9",
      "patch": "@@ -0,0 +1,1039 @@\n+// Copyright 2015 The Go Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style\n+// license that can be found in the LICENSE file.\n+\n+// This file is a copy of $GOROOT/src/go/internal/gcimporter/bimport.go.\n+\n+package gcimporter\n+\n+import (\n+\t\"encoding/binary\"\n+\t\"fmt\"\n+\t\"go/constant\"\n+\t\"go/token\"\n+\t\"go/types\"\n+\t\"sort\"\n+\t\"strconv\"\n+\t\"strings\"\n+\t\"sync\"\n+\t\"unicode\"\n+\t\"unicode/utf8\"\n+)\n+\n+type importer struct {\n+\timports    map[string]*types.Package\n+\tdata       []byte\n+\timportpath string\n+\tbuf        []byte // for reading strings\n+\tversion    int    // export format version\n+\n+\t// object lists\n+\tstrList       []string           // in order of appearance\n+\tpathList      []string           // in order of appearance\n+\tpkgList       []*types.Package   // in order of appearance\n+\ttypList       []types.Type       // in order of appearance\n+\tinterfaceList []*types.Interface // for delayed completion only\n+\ttrackAllTypes bool\n+\n+\t// position encoding\n+\tposInfoFormat bool\n+\tprevFile      string\n+\tprevLine      int\n+\tfake          fakeFileSet\n+\n+\t// debugging support\n+\tdebugFormat bool\n+\tread        int // bytes read\n+}\n+\n+// BImportData imports a package from the serialized package data\n+// and returns the number of bytes consumed and a reference to the package.\n+// If the export data version is not recognized or the format is otherwise\n+// compromised, an error is returned.\n+func BImportData(fset *token.FileSet, imports map[string]*types.Package, data []byte, path string) (_ int, pkg *types.Package, err error) {\n+\t// catch panics and return them as errors\n+\tconst currentVersion = 6\n+\tversion := -1 // unknown version\n+\tdefer func() {\n+\t\tif e := recover(); e != nil {\n+\t\t\t// Return a (possibly nil or incomplete) package unchanged (see #16088).\n+\t\t\tif version > currentVersion {\n+\t\t\t\terr = fmt.Errorf(\"cannot import %q (%v), export data is newer version - update tool\", path, e)\n+\t\t\t} else {\n+\t\t\t\terr = fmt.Errorf(\"cannot import %q (%v), possibly version skew - reinstall package\", path, e)\n+\t\t\t}\n+\t\t}\n+\t}()\n+\n+\tp := importer{\n+\t\timports:    imports,\n+\t\tdata:       data,\n+\t\timportpath: path,\n+\t\tversion:    version,\n+\t\tstrList:    []string{\"\"}, // empty string is mapped to 0\n+\t\tpathList:   []string{\"\"}, // empty string is mapped to 0\n+\t\tfake: fakeFileSet{\n+\t\t\tfset:  fset,\n+\t\t\tfiles: make(map[string]*token.File),\n+\t\t},\n+\t}\n+\n+\t// read version info\n+\tvar versionstr string\n+\tif b := p.rawByte(); b == 'c' || b == 'd' {\n+\t\t// Go1.7 encoding; first byte encodes low-level\n+\t\t// encoding format (compact vs debug).\n+\t\t// For backward-compatibility only (avoid problems with\n+\t\t// old installed packages). Newly compiled packages use\n+\t\t// the extensible format string.\n+\t\t// TODO(gri) Remove this support eventually; after Go1.8.\n+\t\tif b == 'd' {\n+\t\t\tp.debugFormat = true\n+\t\t}\n+\t\tp.trackAllTypes = p.rawByte() == 'a'\n+\t\tp.posInfoFormat = p.int() != 0\n+\t\tversionstr = p.string()\n+\t\tif versionstr == \"v1\" {\n+\t\t\tversion = 0\n+\t\t}\n+\t} else {\n+\t\t// Go1.8 extensible encoding\n+\t\t// read version string and extract version number (ignore anything after the version number)\n+\t\tversionstr = p.rawStringln(b)\n+\t\tif s := strings.SplitN(versionstr, \" \", 3); len(s) >= 2 && s[0] == \"version\" {\n+\t\t\tif v, err := strconv.Atoi(s[1]); err == nil && v > 0 {\n+\t\t\t\tversion = v\n+\t\t\t}\n+\t\t}\n+\t}\n+\tp.version = version\n+\n+\t// read version specific flags - extend as necessary\n+\tswitch p.version {\n+\t// case currentVersion:\n+\t// \t...\n+\t//\tfallthrough\n+\tcase currentVersion, 5, 4, 3, 2, 1:\n+\t\tp.debugFormat = p.rawStringln(p.rawByte()) == \"debug\"\n+\t\tp.trackAllTypes = p.int() != 0\n+\t\tp.posInfoFormat = p.int() != 0\n+\tcase 0:\n+\t\t// Go1.7 encoding format - nothing to do here\n+\tdefault:\n+\t\terrorf(\"unknown bexport format version %d (%q)\", p.version, versionstr)\n+\t}\n+\n+\t// --- generic export data ---\n+\n+\t// populate typList with predeclared \"known\" types\n+\tp.typList = append(p.typList, predeclared()...)\n+\n+\t// read package data\n+\tpkg = p.pkg()\n+\n+\t// read objects of phase 1 only (see cmd/compile/internal/gc/bexport.go)\n+\tobjcount := 0\n+\tfor {\n+\t\ttag := p.tagOrIndex()\n+\t\tif tag == endTag {\n+\t\t\tbreak\n+\t\t}\n+\t\tp.obj(tag)\n+\t\tobjcount++\n+\t}\n+\n+\t// self-verification\n+\tif count := p.int(); count != objcount {\n+\t\terrorf(\"got %d objects; want %d\", objcount, count)\n+\t}\n+\n+\t// ignore compiler-specific import data\n+\n+\t// complete interfaces\n+\t// TODO(gri) re-investigate if we still need to do this in a delayed fashion\n+\tfor _, typ := range p.interfaceList {\n+\t\ttyp.Complete()\n+\t}\n+\n+\t// record all referenced packages as imports\n+\tlist := append(([]*types.Package)(nil), p.pkgList[1:]...)\n+\tsort.Sort(byPath(list))\n+\tpkg.SetImports(list)\n+\n+\t// package was imported completely and without errors\n+\tpkg.MarkComplete()\n+\n+\treturn p.read, pkg, nil\n+}\n+\n+func errorf(format string, args ...interface{}) {\n+\tpanic(fmt.Sprintf(format, args...))\n+}\n+\n+func (p *importer) pkg() *types.Package {\n+\t// if the package was seen before, i is its index (>= 0)\n+\ti := p.tagOrIndex()\n+\tif i >= 0 {\n+\t\treturn p.pkgList[i]\n+\t}\n+\n+\t// otherwise, i is the package tag (< 0)\n+\tif i != packageTag {\n+\t\terrorf(\"unexpected package tag %d version %d\", i, p.version)\n+\t}\n+\n+\t// read package data\n+\tname := p.string()\n+\tvar path string\n+\tif p.version >= 5 {\n+\t\tpath = p.path()\n+\t} else {\n+\t\tpath = p.string()\n+\t}\n+\tif p.version >= 6 {\n+\t\tp.int() // package height; unused by go/types\n+\t}\n+\n+\t// we should never see an empty package name\n+\tif name == \"\" {\n+\t\terrorf(\"empty package name in import\")\n+\t}\n+\n+\t// an empty path denotes the package we are currently importing;\n+\t// it must be the first package we see\n+\tif (path == \"\") != (len(p.pkgList) == 0) {\n+\t\terrorf(\"package path %q for pkg index %d\", path, len(p.pkgList))\n+\t}\n+\n+\t// if the package was imported before, use that one; otherwise create a new one\n+\tif path == \"\" {\n+\t\tpath = p.importpath\n+\t}\n+\tpkg := p.imports[path]\n+\tif pkg == nil {\n+\t\tpkg = types.NewPackage(path, name)\n+\t\tp.imports[path] = pkg\n+\t} else if pkg.Name() != name {\n+\t\terrorf(\"conflicting names %s and %s for package %q\", pkg.Name(), name, path)\n+\t}\n+\tp.pkgList = append(p.pkgList, pkg)\n+\n+\treturn pkg\n+}\n+\n+// objTag returns the tag value for each object kind.\n+func objTag(obj types.Object) int {\n+\tswitch obj.(type) {\n+\tcase *types.Const:\n+\t\treturn constTag\n+\tcase *types.TypeName:\n+\t\treturn typeTag\n+\tcase *types.Var:\n+\t\treturn varTag\n+\tcase *types.Func:\n+\t\treturn funcTag\n+\tdefault:\n+\t\terrorf(\"unexpected object: %v (%T)\", obj, obj) // panics\n+\t\tpanic(\"unreachable\")\n+\t}\n+}\n+\n+func sameObj(a, b types.Object) bool {\n+\t// Because unnamed types are not canonicalized, we cannot simply compare types for\n+\t// (pointer) identity.\n+\t// Ideally we'd check equality of constant values as well, but this is good enough.\n+\treturn objTag(a) == objTag(b) && types.Identical(a.Type(), b.Type())\n+}\n+\n+func (p *importer) declare(obj types.Object) {\n+\tpkg := obj.Pkg()\n+\tif alt := pkg.Scope().Insert(obj); alt != nil {\n+\t\t// This can only trigger if we import a (non-type) object a second time.\n+\t\t// Excluding type aliases, this cannot happen because 1) we only import a package\n+\t\t// once; and b) we ignore compiler-specific export data which may contain\n+\t\t// functions whose inlined function bodies refer to other functions that\n+\t\t// were already imported.\n+\t\t// However, type aliases require reexporting the original type, so we need\n+\t\t// to allow it (see also the comment in cmd/compile/internal/gc/bimport.go,\n+\t\t// method importer.obj, switch case importing functions).\n+\t\t// TODO(gri) review/update this comment once the gc compiler handles type aliases.\n+\t\tif !sameObj(obj, alt) {\n+\t\t\terrorf(\"inconsistent import:\\n\\t%v\\npreviously imported as:\\n\\t%v\\n\", obj, alt)\n+\t\t}\n+\t}\n+}\n+\n+func (p *importer) obj(tag int) {\n+\tswitch tag {\n+\tcase constTag:\n+\t\tpos := p.pos()\n+\t\tpkg, name := p.qualifiedName()\n+\t\ttyp := p.typ(nil, nil)\n+\t\tval := p.value()\n+\t\tp.declare(types.NewConst(pos, pkg, name, typ, val))\n+\n+\tcase aliasTag:\n+\t\t// TODO(gri) verify type alias hookup is correct\n+\t\tpos := p.pos()\n+\t\tpkg, name := p.qualifiedName()\n+\t\ttyp := p.typ(nil, nil)\n+\t\tp.declare(types.NewTypeName(pos, pkg, name, typ))\n+\n+\tcase typeTag:\n+\t\tp.typ(nil, nil)\n+\n+\tcase varTag:\n+\t\tpos := p.pos()\n+\t\tpkg, name := p.qualifiedName()\n+\t\ttyp := p.typ(nil, nil)\n+\t\tp.declare(types.NewVar(pos, pkg, name, typ))\n+\n+\tcase funcTag:\n+\t\tpos := p.pos()\n+\t\tpkg, name := p.qualifiedName()\n+\t\tparams, isddd := p.paramList()\n+\t\tresult, _ := p.paramList()\n+\t\tsig := types.NewSignature(nil, params, result, isddd)\n+\t\tp.declare(types.NewFunc(pos, pkg, name, sig))\n+\n+\tdefault:\n+\t\terrorf(\"unexpected object tag %d\", tag)\n+\t}\n+}\n+\n+const deltaNewFile = -64 // see cmd/compile/internal/gc/bexport.go\n+\n+func (p *importer) pos() token.Pos {\n+\tif !p.posInfoFormat {\n+\t\treturn token.NoPos\n+\t}\n+\n+\tfile := p.prevFile\n+\tline := p.prevLine\n+\tdelta := p.int()\n+\tline += delta\n+\tif p.version >= 5 {\n+\t\tif delta == deltaNewFile {\n+\t\t\tif n := p.int(); n >= 0 {\n+\t\t\t\t// file changed\n+\t\t\t\tfile = p.path()\n+\t\t\t\tline = n\n+\t\t\t}\n+\t\t}\n+\t} else {\n+\t\tif delta == 0 {\n+\t\t\tif n := p.int(); n >= 0 {\n+\t\t\t\t// file changed\n+\t\t\t\tfile = p.prevFile[:n] + p.string()\n+\t\t\t\tline = p.int()\n+\t\t\t}\n+\t\t}\n+\t}\n+\tp.prevFile = file\n+\tp.prevLine = line\n+\n+\treturn p.fake.pos(file, line, 0)\n+}\n+\n+// Synthesize a token.Pos\n+type fakeFileSet struct {\n+\tfset  *token.FileSet\n+\tfiles map[string]*token.File\n+}\n+\n+func (s *fakeFileSet) pos(file string, line, column int) token.Pos {\n+\t// TODO(mdempsky): Make use of column.\n+\n+\t// Since we don't know the set of needed file positions, we\n+\t// reserve maxlines positions per file.\n+\tconst maxlines = 64 * 1024\n+\tf := s.files[file]\n+\tif f == nil {\n+\t\tf = s.fset.AddFile(file, -1, maxlines)\n+\t\ts.files[file] = f\n+\t\t// Allocate the fake linebreak indices on first use.\n+\t\t// TODO(adonovan): opt: save ~512KB using a more complex scheme?\n+\t\tfakeLinesOnce.Do(func() {\n+\t\t\tfakeLines = make([]int, maxlines)\n+\t\t\tfor i := range fakeLines {\n+\t\t\t\tfakeLines[i] = i\n+\t\t\t}\n+\t\t})\n+\t\tf.SetLines(fakeLines)\n+\t}\n+\n+\tif line > maxlines {\n+\t\tline = 1\n+\t}\n+\n+\t// Treat the file as if it contained only newlines\n+\t// and column=1: use the line number as the offset.\n+\treturn f.Pos(line - 1)\n+}\n+\n+var (\n+\tfakeLines     []int\n+\tfakeLinesOnce sync.Once\n+)\n+\n+func (p *importer) qualifiedName() (pkg *types.Package, name string) {\n+\tname = p.string()\n+\tpkg = p.pkg()\n+\treturn\n+}\n+\n+func (p *importer) record(t types.Type) {\n+\tp.typList = append(p.typList, t)\n+}\n+\n+// A dddSlice is a types.Type representing ...T parameters.\n+// It only appears for parameter types and does not escape\n+// the importer.\n+type dddSlice struct {\n+\telem types.Type\n+}\n+\n+func (t *dddSlice) Underlying() types.Type { return t }\n+func (t *dddSlice) String() string         { return \"...\" + t.elem.String() }\n+\n+// parent is the package which declared the type; parent == nil means\n+// the package currently imported. The parent package is needed for\n+// exported struct fields and interface methods which don't contain\n+// explicit package information in the export data.\n+//\n+// A non-nil tname is used as the \"owner\" of the result type; i.e.,\n+// the result type is the underlying type of tname. tname is used\n+// to give interface methods a named receiver type where possible.\n+func (p *importer) typ(parent *types.Package, tname *types.Named) types.Type {\n+\t// if the type was seen before, i is its index (>= 0)\n+\ti := p.tagOrIndex()\n+\tif i >= 0 {\n+\t\treturn p.typList[i]\n+\t}\n+\n+\t// otherwise, i is the type tag (< 0)\n+\tswitch i {\n+\tcase namedTag:\n+\t\t// read type object\n+\t\tpos := p.pos()\n+\t\tparent, name := p.qualifiedName()\n+\t\tscope := parent.Scope()\n+\t\tobj := scope.Lookup(name)\n+\n+\t\t// if the object doesn't exist yet, create and insert it\n+\t\tif obj == nil {\n+\t\t\tobj = types.NewTypeName(pos, parent, name, nil)\n+\t\t\tscope.Insert(obj)\n+\t\t}\n+\n+\t\tif _, ok := obj.(*types.TypeName); !ok {\n+\t\t\terrorf(\"pkg = %s, name = %s => %s\", parent, name, obj)\n+\t\t}\n+\n+\t\t// associate new named type with obj if it doesn't exist yet\n+\t\tt0 := types.NewNamed(obj.(*types.TypeName), nil, nil)\n+\n+\t\t// but record the existing type, if any\n+\t\ttname := obj.Type().(*types.Named) // tname is either t0 or the existing type\n+\t\tp.record(tname)\n+\n+\t\t// read underlying type\n+\t\tt0.SetUnderlying(p.typ(parent, t0))\n+\n+\t\t// interfaces don't have associated methods\n+\t\tif types.IsInterface(t0) {\n+\t\t\treturn tname\n+\t\t}\n+\n+\t\t// read associated methods\n+\t\tfor i := p.int(); i > 0; i-- {\n+\t\t\t// TODO(gri) replace this with something closer to fieldName\n+\t\t\tpos := p.pos()\n+\t\t\tname := p.string()\n+\t\t\tif !exported(name) {\n+\t\t\t\tp.pkg()\n+\t\t\t}\n+\n+\t\t\trecv, _ := p.paramList() // TODO(gri) do we need a full param list for the receiver?\n+\t\t\tparams, isddd := p.paramList()\n+\t\t\tresult, _ := p.paramList()\n+\t\t\tp.int() // go:nointerface pragma - discarded\n+\n+\t\t\tsig := types.NewSignature(recv.At(0), params, result, isddd)\n+\t\t\tt0.AddMethod(types.NewFunc(pos, parent, name, sig))\n+\t\t}\n+\n+\t\treturn tname\n+\n+\tcase arrayTag:\n+\t\tt := new(types.Array)\n+\t\tif p.trackAllTypes {\n+\t\t\tp.record(t)\n+\t\t}\n+\n+\t\tn := p.int64()\n+\t\t*t = *types.NewArray(p.typ(parent, nil), n)\n+\t\treturn t\n+\n+\tcase sliceTag:\n+\t\tt := new(types.Slice)\n+\t\tif p.trackAllTypes {\n+\t\t\tp.record(t)\n+\t\t}\n+\n+\t\t*t = *types.NewSlice(p.typ(parent, nil))\n+\t\treturn t\n+\n+\tcase dddTag:\n+\t\tt := new(dddSlice)\n+\t\tif p.trackAllTypes {\n+\t\t\tp.record(t)\n+\t\t}\n+\n+\t\tt.elem = p.typ(parent, nil)\n+\t\treturn t\n+\n+\tcase structTag:\n+\t\tt := new(types.Struct)\n+\t\tif p.trackAllTypes {\n+\t\t\tp.record(t)\n+\t\t}\n+\n+\t\t*t = *types.NewStruct(p.fieldList(parent))\n+\t\treturn t\n+\n+\tcase pointerTag:\n+\t\tt := new(types.Pointer)\n+\t\tif p.trackAllTypes {\n+\t\t\tp.record(t)\n+\t\t}\n+\n+\t\t*t = *types.NewPointer(p.typ(parent, nil))\n+\t\treturn t\n+\n+\tcase signatureTag:\n+\t\tt := new(types.Signature)\n+\t\tif p.trackAllTypes {\n+\t\t\tp.record(t)\n+\t\t}\n+\n+\t\tparams, isddd := p.paramList()\n+\t\tresult, _ := p.paramList()\n+\t\t*t = *types.NewSignature(nil, params, result, isddd)\n+\t\treturn t\n+\n+\tcase interfaceTag:\n+\t\t// Create a dummy entry in the type list. This is safe because we\n+\t\t// cannot expect the interface type to appear in a cycle, as any\n+\t\t// such cycle must contain a named type which would have been\n+\t\t// first defined earlier.\n+\t\t// TODO(gri) Is this still true now that we have type aliases?\n+\t\t// See issue #23225.\n+\t\tn := len(p.typList)\n+\t\tif p.trackAllTypes {\n+\t\t\tp.record(nil)\n+\t\t}\n+\n+\t\tvar embeddeds []types.Type\n+\t\tfor n := p.int(); n > 0; n-- {\n+\t\t\tp.pos()\n+\t\t\tembeddeds = append(embeddeds, p.typ(parent, nil))\n+\t\t}\n+\n+\t\tt := newInterface(p.methodList(parent, tname), embeddeds)\n+\t\tp.interfaceList = append(p.interfaceList, t)\n+\t\tif p.trackAllTypes {\n+\t\t\tp.typList[n] = t\n+\t\t}\n+\t\treturn t\n+\n+\tcase mapTag:\n+\t\tt := new(types.Map)\n+\t\tif p.trackAllTypes {\n+\t\t\tp.record(t)\n+\t\t}\n+\n+\t\tkey := p.typ(parent, nil)\n+\t\tval := p.typ(parent, nil)\n+\t\t*t = *types.NewMap(key, val)\n+\t\treturn t\n+\n+\tcase chanTag:\n+\t\tt := new(types.Chan)\n+\t\tif p.trackAllTypes {\n+\t\t\tp.record(t)\n+\t\t}\n+\n+\t\tdir := chanDir(p.int())\n+\t\tval := p.typ(parent, nil)\n+\t\t*t = *types.NewChan(dir, val)\n+\t\treturn t\n+\n+\tdefault:\n+\t\terrorf(\"unexpected type tag %d\", i) // panics\n+\t\tpanic(\"unreachable\")\n+\t}\n+}\n+\n+func chanDir(d int) types.ChanDir {\n+\t// tag values must match the constants in cmd/compile/internal/gc/go.go\n+\tswitch d {\n+\tcase 1 /* Crecv */ :\n+\t\treturn types.RecvOnly\n+\tcase 2 /* Csend */ :\n+\t\treturn types.SendOnly\n+\tcase 3 /* Cboth */ :\n+\t\treturn types.SendRecv\n+\tdefault:\n+\t\terrorf(\"unexpected channel dir %d\", d)\n+\t\treturn 0\n+\t}\n+}\n+\n+func (p *importer) fieldList(parent *types.Package) (fields []*types.Var, tags []string) {\n+\tif n := p.int(); n > 0 {\n+\t\tfields = make([]*types.Var, n)\n+\t\ttags = make([]string, n)\n+\t\tfor i := range fields {\n+\t\t\tfields[i], tags[i] = p.field(parent)\n+\t\t}\n+\t}\n+\treturn\n+}\n+\n+func (p *importer) field(parent *types.Package) (*types.Var, string) {\n+\tpos := p.pos()\n+\tpkg, name, alias := p.fieldName(parent)\n+\ttyp := p.typ(parent, nil)\n+\ttag := p.string()\n+\n+\tanonymous := false\n+\tif name == \"\" {\n+\t\t// anonymous field - typ must be T or *T and T must be a type name\n+\t\tswitch typ := deref(typ).(type) {\n+\t\tcase *types.Basic: // basic types are named types\n+\t\t\tpkg = nil // // objects defined in Universe scope have no package\n+\t\t\tname = typ.Name()\n+\t\tcase *types.Named:\n+\t\t\tname = typ.Obj().Name()\n+\t\tdefault:\n+\t\t\terrorf(\"named base type expected\")\n+\t\t}\n+\t\tanonymous = true\n+\t} else if alias {\n+\t\t// anonymous field: we have an explicit name because it's an alias\n+\t\tanonymous = true\n+\t}\n+\n+\treturn types.NewField(pos, pkg, name, typ, anonymous), tag\n+}\n+\n+func (p *importer) methodList(parent *types.Package, baseType *types.Named) (methods []*types.Func) {\n+\tif n := p.int(); n > 0 {\n+\t\tmethods = make([]*types.Func, n)\n+\t\tfor i := range methods {\n+\t\t\tmethods[i] = p.method(parent, baseType)\n+\t\t}\n+\t}\n+\treturn\n+}\n+\n+func (p *importer) method(parent *types.Package, baseType *types.Named) *types.Func {\n+\tpos := p.pos()\n+\tpkg, name, _ := p.fieldName(parent)\n+\t// If we don't have a baseType, use a nil receiver.\n+\t// A receiver using the actual interface type (which\n+\t// we don't know yet) will be filled in when we call\n+\t// types.Interface.Complete.\n+\tvar recv *types.Var\n+\tif baseType != nil {\n+\t\trecv = types.NewVar(token.NoPos, parent, \"\", baseType)\n+\t}\n+\tparams, isddd := p.paramList()\n+\tresult, _ := p.paramList()\n+\tsig := types.NewSignature(recv, params, result, isddd)\n+\treturn types.NewFunc(pos, pkg, name, sig)\n+}\n+\n+func (p *importer) fieldName(parent *types.Package) (pkg *types.Package, name string, alias bool) {\n+\tname = p.string()\n+\tpkg = parent\n+\tif pkg == nil {\n+\t\t// use the imported package instead\n+\t\tpkg = p.pkgList[0]\n+\t}\n+\tif p.version == 0 && name == \"_\" {\n+\t\t// version 0 didn't export a package for _ fields\n+\t\treturn\n+\t}\n+\tswitch name {\n+\tcase \"\":\n+\t\t// 1) field name matches base type name and is exported: nothing to do\n+\tcase \"?\":\n+\t\t// 2) field name matches base type name and is not exported: need package\n+\t\tname = \"\"\n+\t\tpkg = p.pkg()\n+\tcase \"@\":\n+\t\t// 3) field name doesn't match type name (alias)\n+\t\tname = p.string()\n+\t\talias = true\n+\t\tfallthrough\n+\tdefault:\n+\t\tif !exported(name) {\n+\t\t\tpkg = p.pkg()\n+\t\t}\n+\t}\n+\treturn\n+}\n+\n+func (p *importer) paramList() (*types.Tuple, bool) {\n+\tn := p.int()\n+\tif n == 0 {\n+\t\treturn nil, false\n+\t}\n+\t// negative length indicates unnamed parameters\n+\tnamed := true\n+\tif n < 0 {\n+\t\tn = -n\n+\t\tnamed = false\n+\t}\n+\t// n > 0\n+\tparams := make([]*types.Var, n)\n+\tisddd := false\n+\tfor i := range params {\n+\t\tparams[i], isddd = p.param(named)\n+\t}\n+\treturn types.NewTuple(params...), isddd\n+}\n+\n+func (p *importer) param(named bool) (*types.Var, bool) {\n+\tt := p.typ(nil, nil)\n+\ttd, isddd := t.(*dddSlice)\n+\tif isddd {\n+\t\tt = types.NewSlice(td.elem)\n+\t}\n+\n+\tvar pkg *types.Package\n+\tvar name string\n+\tif named {\n+\t\tname = p.string()\n+\t\tif name == \"\" {\n+\t\t\terrorf(\"expected named parameter\")\n+\t\t}\n+\t\tif name != \"_\" {\n+\t\t\tpkg = p.pkg()\n+\t\t}\n+\t\tif i := strings.Index(name, \"·\"); i > 0 {\n+\t\t\tname = name[:i] // cut off gc-specific parameter numbering\n+\t\t}\n+\t}\n+\n+\t// read and discard compiler-specific info\n+\tp.string()\n+\n+\treturn types.NewVar(token.NoPos, pkg, name, t), isddd\n+}\n+\n+func exported(name string) bool {\n+\tch, _ := utf8.DecodeRuneInString(name)\n+\treturn unicode.IsUpper(ch)\n+}\n+\n+func (p *importer) value() constant.Value {\n+\tswitch tag := p.tagOrIndex(); tag {\n+\tcase falseTag:\n+\t\treturn constant.MakeBool(false)\n+\tcase trueTag:\n+\t\treturn constant.MakeBool(true)\n+\tcase int64Tag:\n+\t\treturn constant.MakeInt64(p.int64())\n+\tcase floatTag:\n+\t\treturn p.float()\n+\tcase complexTag:\n+\t\tre := p.float()\n+\t\tim := p.float()\n+\t\treturn constant.BinaryOp(re, token.ADD, constant.MakeImag(im))\n+\tcase stringTag:\n+\t\treturn constant.MakeString(p.string())\n+\tcase unknownTag:\n+\t\treturn constant.MakeUnknown()\n+\tdefault:\n+\t\terrorf(\"unexpected value tag %d\", tag) // panics\n+\t\tpanic(\"unreachable\")\n+\t}\n+}\n+\n+func (p *importer) float() constant.Value {\n+\tsign := p.int()\n+\tif sign == 0 {\n+\t\treturn constant.MakeInt64(0)\n+\t}\n+\n+\texp := p.int()\n+\tmant := []byte(p.string()) // big endian\n+\n+\t// remove leading 0's if any\n+\tfor len(mant) > 0 && mant[0] == 0 {\n+\t\tmant = mant[1:]\n+\t}\n+\n+\t// convert to little endian\n+\t// TODO(gri) go/constant should have a more direct conversion function\n+\t//           (e.g., once it supports a big.Float based implementation)\n+\tfor i, j := 0, len(mant)-1; i < j; i, j = i+1, j-1 {\n+\t\tmant[i], mant[j] = mant[j], mant[i]\n+\t}\n+\n+\t// adjust exponent (constant.MakeFromBytes creates an integer value,\n+\t// but mant represents the mantissa bits such that 0.5 <= mant < 1.0)\n+\texp -= len(mant) << 3\n+\tif len(mant) > 0 {\n+\t\tfor msd := mant[len(mant)-1]; msd&0x80 == 0; msd <<= 1 {\n+\t\t\texp++\n+\t\t}\n+\t}\n+\n+\tx := constant.MakeFromBytes(mant)\n+\tswitch {\n+\tcase exp < 0:\n+\t\td := constant.Shift(constant.MakeInt64(1), token.SHL, uint(-exp))\n+\t\tx = constant.BinaryOp(x, token.QUO, d)\n+\tcase exp > 0:\n+\t\tx = constant.Shift(x, token.SHL, uint(exp))\n+\t}\n+\n+\tif sign < 0 {\n+\t\tx = constant.UnaryOp(token.SUB, x, 0)\n+\t}\n+\treturn x\n+}\n+\n+// ----------------------------------------------------------------------------\n+// Low-level decoders\n+\n+func (p *importer) tagOrIndex() int {\n+\tif p.debugFormat {\n+\t\tp.marker('t')\n+\t}\n+\n+\treturn int(p.rawInt64())\n+}\n+\n+func (p *importer) int() int {\n+\tx := p.int64()\n+\tif int64(int(x)) != x {\n+\t\terrorf(\"exported integer too large\")\n+\t}\n+\treturn int(x)\n+}\n+\n+func (p *importer) int64() int64 {\n+\tif p.debugFormat {\n+\t\tp.marker('i')\n+\t}\n+\n+\treturn p.rawInt64()\n+}\n+\n+func (p *importer) path() string {\n+\tif p.debugFormat {\n+\t\tp.marker('p')\n+\t}\n+\t// if the path was seen before, i is its index (>= 0)\n+\t// (the empty string is at index 0)\n+\ti := p.rawInt64()\n+\tif i >= 0 {\n+\t\treturn p.pathList[i]\n+\t}\n+\t// otherwise, i is the negative path length (< 0)\n+\ta := make([]string, -i)\n+\tfor n := range a {\n+\t\ta[n] = p.string()\n+\t}\n+\ts := strings.Join(a, \"/\")\n+\tp.pathList = append(p.pathList, s)\n+\treturn s\n+}\n+\n+func (p *importer) string() string {\n+\tif p.debugFormat {\n+\t\tp.marker('s')\n+\t}\n+\t// if the string was seen before, i is its index (>= 0)\n+\t// (the empty string is at index 0)\n+\ti := p.rawInt64()\n+\tif i >= 0 {\n+\t\treturn p.strList[i]\n+\t}\n+\t// otherwise, i is the negative string length (< 0)\n+\tif n := int(-i); n <= cap(p.buf) {\n+\t\tp.buf = p.buf[:n]\n+\t} else {\n+\t\tp.buf = make([]byte, n)\n+\t}\n+\tfor i := range p.buf {\n+\t\tp.buf[i] = p.rawByte()\n+\t}\n+\ts := string(p.buf)\n+\tp.strList = append(p.strList, s)\n+\treturn s\n+}\n+\n+func (p *importer) marker(want byte) {\n+\tif got := p.rawByte(); got != want {\n+\t\terrorf(\"incorrect marker: got %c; want %c (pos = %d)\", got, want, p.read)\n+\t}\n+\n+\tpos := p.read\n+\tif n := int(p.rawInt64()); n != pos {\n+\t\terrorf(\"incorrect position: got %d; want %d\", n, pos)\n+\t}\n+}\n+\n+// rawInt64 should only be used by low-level decoders.\n+func (p *importer) rawInt64() int64 {\n+\ti, err := binary.ReadVarint(p)\n+\tif err != nil {\n+\t\terrorf(\"read error: %v\", err)\n+\t}\n+\treturn i\n+}\n+\n+// rawStringln should only be used to read the initial version string.\n+func (p *importer) rawStringln(b byte) string {\n+\tp.buf = p.buf[:0]\n+\tfor b != '\\n' {\n+\t\tp.buf = append(p.buf, b)\n+\t\tb = p.rawByte()\n+\t}\n+\treturn string(p.buf)\n+}\n+\n+// needed for binary.ReadVarint in rawInt64\n+func (p *importer) ReadByte() (byte, error) {\n+\treturn p.rawByte(), nil\n+}\n+\n+// byte is the bottleneck interface for reading p.data.\n+// It unescapes '|' 'S' to '$' and '|' '|' to '|'.\n+// rawByte should only be used by low-level decoders.\n+func (p *importer) rawByte() byte {\n+\tb := p.data[0]\n+\tr := 1\n+\tif b == '|' {\n+\t\tb = p.data[1]\n+\t\tr = 2\n+\t\tswitch b {\n+\t\tcase 'S':\n+\t\t\tb = '$'\n+\t\tcase '|':\n+\t\t\t// nothing to do\n+\t\tdefault:\n+\t\t\terrorf(\"unexpected escape sequence in export data\")\n+\t\t}\n+\t}\n+\tp.data = p.data[r:]\n+\tp.read += r\n+\treturn b\n+\n+}\n+\n+// ----------------------------------------------------------------------------\n+// Export format\n+\n+// Tags. Must be < 0.\n+const (\n+\t// Objects\n+\tpackageTag = -(iota + 1)\n+\tconstTag\n+\ttypeTag\n+\tvarTag\n+\tfuncTag\n+\tendTag\n+\n+\t// Types\n+\tnamedTag\n+\tarrayTag\n+\tsliceTag\n+\tdddTag\n+\tstructTag\n+\tpointerTag\n+\tsignatureTag\n+\tinterfaceTag\n+\tmapTag\n+\tchanTag\n+\n+\t// Values\n+\tfalseTag\n+\ttrueTag\n+\tint64Tag\n+\tfloatTag\n+\tfractionTag // not used by gc\n+\tcomplexTag\n+\tstringTag\n+\tnilTag     // only used by gc (appears in exported inlined function bodies)\n+\tunknownTag // not used by gc (only appears in packages with errors)\n+\n+\t// Type aliases\n+\taliasTag\n+)\n+\n+var predeclOnce sync.Once\n+var predecl []types.Type // initialized lazily\n+\n+func predeclared() []types.Type {\n+\tpredeclOnce.Do(func() {\n+\t\t// initialize lazily to be sure that all\n+\t\t// elements have been initialized before\n+\t\tpredecl = []types.Type{ // basic types\n+\t\t\ttypes.Typ[types.Bool],\n+\t\t\ttypes.Typ[types.Int],\n+\t\t\ttypes.Typ[types.Int8],\n+\t\t\ttypes.Typ[types.Int16],\n+\t\t\ttypes.Typ[types.Int32],\n+\t\t\ttypes.Typ[types.Int64],\n+\t\t\ttypes.Typ[types.Uint],\n+\t\t\ttypes.Typ[types.Uint8],\n+\t\t\ttypes.Typ[types.Uint16],\n+\t\t\ttypes.Typ[types.Uint32],\n+\t\t\ttypes.Typ[types.Uint64],\n+\t\t\ttypes.Typ[types.Uintptr],\n+\t\t\ttypes.Typ[types.Float32],\n+\t\t\ttypes.Typ[types.Float64],\n+\t\t\ttypes.Typ[types.Complex64],\n+\t\t\ttypes.Typ[types.Complex128],\n+\t\t\ttypes.Typ[types.String],\n+\n+\t\t\t// basic type aliases\n+\t\t\ttypes.Universe.Lookup(\"byte\").Type(),\n+\t\t\ttypes.Universe.Lookup(\"rune\").Type(),\n+\n+\t\t\t// error\n+\t\t\ttypes.Universe.Lookup(\"error\").Type(),\n+\n+\t\t\t// untyped types\n+\t\t\ttypes.Typ[types.UntypedBool],\n+\t\t\ttypes.Typ[types.UntypedInt],\n+\t\t\ttypes.Typ[types.UntypedRune],\n+\t\t\ttypes.Typ[types.UntypedFloat],\n+\t\t\ttypes.Typ[types.UntypedComplex],\n+\t\t\ttypes.Typ[types.UntypedString],\n+\t\t\ttypes.Typ[types.UntypedNil],\n+\n+\t\t\t// package unsafe\n+\t\t\ttypes.Typ[types.UnsafePointer],\n+\n+\t\t\t// invalid type\n+\t\t\ttypes.Typ[types.Invalid], // only appears in packages with errors\n+\n+\t\t\t// used internally by gc; never used by this package or in .a files\n+\t\t\tanyType{},\n+\t\t}\n+\t})\n+\treturn predecl\n+}\n+\n+type anyType struct{}\n+\n+func (t anyType) Underlying() types.Type { return t }\n+func (t anyType) String() string         { return \"any\" }"
    },
    {
      "sha": "f33dc5613e712eb0dc9454864f772d1edcf8f425",
      "filename": "backend/vendor/golang.org/x/tools/go/internal/gcimporter/exportdata.go",
      "status": "added",
      "additions": 93,
      "deletions": 0,
      "changes": 93,
      "blob_url": "https://github.com/umputun/remark42/blob/80f4862c9c2905475f3484f20ca41f7bdf103de9/backend/vendor/golang.org/x/tools/go/internal/gcimporter/exportdata.go",
      "raw_url": "https://github.com/umputun/remark42/raw/80f4862c9c2905475f3484f20ca41f7bdf103de9/backend/vendor/golang.org/x/tools/go/internal/gcimporter/exportdata.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/golang.org/x/tools/go/internal/gcimporter/exportdata.go?ref=80f4862c9c2905475f3484f20ca41f7bdf103de9",
      "patch": "@@ -0,0 +1,93 @@\n+// Copyright 2011 The Go Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style\n+// license that can be found in the LICENSE file.\n+\n+// This file is a copy of $GOROOT/src/go/internal/gcimporter/exportdata.go.\n+\n+// This file implements FindExportData.\n+\n+package gcimporter\n+\n+import (\n+\t\"bufio\"\n+\t\"fmt\"\n+\t\"io\"\n+\t\"strconv\"\n+\t\"strings\"\n+)\n+\n+func readGopackHeader(r *bufio.Reader) (name string, size int, err error) {\n+\t// See $GOROOT/include/ar.h.\n+\thdr := make([]byte, 16+12+6+6+8+10+2)\n+\t_, err = io.ReadFull(r, hdr)\n+\tif err != nil {\n+\t\treturn\n+\t}\n+\t// leave for debugging\n+\tif false {\n+\t\tfmt.Printf(\"header: %s\", hdr)\n+\t}\n+\ts := strings.TrimSpace(string(hdr[16+12+6+6+8:][:10]))\n+\tsize, err = strconv.Atoi(s)\n+\tif err != nil || hdr[len(hdr)-2] != '`' || hdr[len(hdr)-1] != '\\n' {\n+\t\terr = fmt.Errorf(\"invalid archive header\")\n+\t\treturn\n+\t}\n+\tname = strings.TrimSpace(string(hdr[:16]))\n+\treturn\n+}\n+\n+// FindExportData positions the reader r at the beginning of the\n+// export data section of an underlying GC-created object/archive\n+// file by reading from it. The reader must be positioned at the\n+// start of the file before calling this function. The hdr result\n+// is the string before the export data, either \"$$\" or \"$$B\".\n+//\n+func FindExportData(r *bufio.Reader) (hdr string, err error) {\n+\t// Read first line to make sure this is an object file.\n+\tline, err := r.ReadSlice('\\n')\n+\tif err != nil {\n+\t\terr = fmt.Errorf(\"can't find export data (%v)\", err)\n+\t\treturn\n+\t}\n+\n+\tif string(line) == \"!<arch>\\n\" {\n+\t\t// Archive file. Scan to __.PKGDEF.\n+\t\tvar name string\n+\t\tif name, _, err = readGopackHeader(r); err != nil {\n+\t\t\treturn\n+\t\t}\n+\n+\t\t// First entry should be __.PKGDEF.\n+\t\tif name != \"__.PKGDEF\" {\n+\t\t\terr = fmt.Errorf(\"go archive is missing __.PKGDEF\")\n+\t\t\treturn\n+\t\t}\n+\n+\t\t// Read first line of __.PKGDEF data, so that line\n+\t\t// is once again the first line of the input.\n+\t\tif line, err = r.ReadSlice('\\n'); err != nil {\n+\t\t\terr = fmt.Errorf(\"can't find export data (%v)\", err)\n+\t\t\treturn\n+\t\t}\n+\t}\n+\n+\t// Now at __.PKGDEF in archive or still at beginning of file.\n+\t// Either way, line should begin with \"go object \".\n+\tif !strings.HasPrefix(string(line), \"go object \") {\n+\t\terr = fmt.Errorf(\"not a Go object file\")\n+\t\treturn\n+\t}\n+\n+\t// Skip over object header to export data.\n+\t// Begins after first line starting with $$.\n+\tfor line[0] != '$' {\n+\t\tif line, err = r.ReadSlice('\\n'); err != nil {\n+\t\t\terr = fmt.Errorf(\"can't find export data (%v)\", err)\n+\t\t\treturn\n+\t\t}\n+\t}\n+\thdr = string(line)\n+\n+\treturn\n+}"
    },
    {
      "sha": "9cf186605f6ebb90ab870d9130efcbf1d73b3e60",
      "filename": "backend/vendor/golang.org/x/tools/go/internal/gcimporter/gcimporter.go",
      "status": "added",
      "additions": 1078,
      "deletions": 0,
      "changes": 1078,
      "blob_url": "https://github.com/umputun/remark42/blob/80f4862c9c2905475f3484f20ca41f7bdf103de9/backend/vendor/golang.org/x/tools/go/internal/gcimporter/gcimporter.go",
      "raw_url": "https://github.com/umputun/remark42/raw/80f4862c9c2905475f3484f20ca41f7bdf103de9/backend/vendor/golang.org/x/tools/go/internal/gcimporter/gcimporter.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/golang.org/x/tools/go/internal/gcimporter/gcimporter.go?ref=80f4862c9c2905475f3484f20ca41f7bdf103de9",
      "patch": "@@ -0,0 +1,1078 @@\n+// Copyright 2011 The Go Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style\n+// license that can be found in the LICENSE file.\n+\n+// This file is a modified copy of $GOROOT/src/go/internal/gcimporter/gcimporter.go,\n+// but it also contains the original source-based importer code for Go1.6.\n+// Once we stop supporting 1.6, we can remove that code.\n+\n+// Package gcimporter provides various functions for reading\n+// gc-generated object files that can be used to implement the\n+// Importer interface defined by the Go 1.5 standard library package.\n+package gcimporter // import \"golang.org/x/tools/go/internal/gcimporter\"\n+\n+import (\n+\t\"bufio\"\n+\t\"errors\"\n+\t\"fmt\"\n+\t\"go/build\"\n+\t\"go/constant\"\n+\t\"go/token\"\n+\t\"go/types\"\n+\t\"io\"\n+\t\"io/ioutil\"\n+\t\"os\"\n+\t\"path/filepath\"\n+\t\"sort\"\n+\t\"strconv\"\n+\t\"strings\"\n+\t\"text/scanner\"\n+)\n+\n+// debugging/development support\n+const debug = false\n+\n+var pkgExts = [...]string{\".a\", \".o\"}\n+\n+// FindPkg returns the filename and unique package id for an import\n+// path based on package information provided by build.Import (using\n+// the build.Default build.Context). A relative srcDir is interpreted\n+// relative to the current working directory.\n+// If no file was found, an empty filename is returned.\n+//\n+func FindPkg(path, srcDir string) (filename, id string) {\n+\tif path == \"\" {\n+\t\treturn\n+\t}\n+\n+\tvar noext string\n+\tswitch {\n+\tdefault:\n+\t\t// \"x\" -> \"$GOPATH/pkg/$GOOS_$GOARCH/x.ext\", \"x\"\n+\t\t// Don't require the source files to be present.\n+\t\tif abs, err := filepath.Abs(srcDir); err == nil { // see issue 14282\n+\t\t\tsrcDir = abs\n+\t\t}\n+\t\tbp, _ := build.Import(path, srcDir, build.FindOnly|build.AllowBinary)\n+\t\tif bp.PkgObj == \"\" {\n+\t\t\tid = path // make sure we have an id to print in error message\n+\t\t\treturn\n+\t\t}\n+\t\tnoext = strings.TrimSuffix(bp.PkgObj, \".a\")\n+\t\tid = bp.ImportPath\n+\n+\tcase build.IsLocalImport(path):\n+\t\t// \"./x\" -> \"/this/directory/x.ext\", \"/this/directory/x\"\n+\t\tnoext = filepath.Join(srcDir, path)\n+\t\tid = noext\n+\n+\tcase filepath.IsAbs(path):\n+\t\t// for completeness only - go/build.Import\n+\t\t// does not support absolute imports\n+\t\t// \"/x\" -> \"/x.ext\", \"/x\"\n+\t\tnoext = path\n+\t\tid = path\n+\t}\n+\n+\tif false { // for debugging\n+\t\tif path != id {\n+\t\t\tfmt.Printf(\"%s -> %s\\n\", path, id)\n+\t\t}\n+\t}\n+\n+\t// try extensions\n+\tfor _, ext := range pkgExts {\n+\t\tfilename = noext + ext\n+\t\tif f, err := os.Stat(filename); err == nil && !f.IsDir() {\n+\t\t\treturn\n+\t\t}\n+\t}\n+\n+\tfilename = \"\" // not found\n+\treturn\n+}\n+\n+// ImportData imports a package by reading the gc-generated export data,\n+// adds the corresponding package object to the packages map indexed by id,\n+// and returns the object.\n+//\n+// The packages map must contains all packages already imported. The data\n+// reader position must be the beginning of the export data section. The\n+// filename is only used in error messages.\n+//\n+// If packages[id] contains the completely imported package, that package\n+// can be used directly, and there is no need to call this function (but\n+// there is also no harm but for extra time used).\n+//\n+func ImportData(packages map[string]*types.Package, filename, id string, data io.Reader) (pkg *types.Package, err error) {\n+\t// support for parser error handling\n+\tdefer func() {\n+\t\tswitch r := recover().(type) {\n+\t\tcase nil:\n+\t\t\t// nothing to do\n+\t\tcase importError:\n+\t\t\terr = r\n+\t\tdefault:\n+\t\t\tpanic(r) // internal error\n+\t\t}\n+\t}()\n+\n+\tvar p parser\n+\tp.init(filename, id, data, packages)\n+\tpkg = p.parseExport()\n+\n+\treturn\n+}\n+\n+// Import imports a gc-generated package given its import path and srcDir, adds\n+// the corresponding package object to the packages map, and returns the object.\n+// The packages map must contain all packages already imported.\n+//\n+func Import(packages map[string]*types.Package, path, srcDir string, lookup func(path string) (io.ReadCloser, error)) (pkg *types.Package, err error) {\n+\tvar rc io.ReadCloser\n+\tvar filename, id string\n+\tif lookup != nil {\n+\t\t// With custom lookup specified, assume that caller has\n+\t\t// converted path to a canonical import path for use in the map.\n+\t\tif path == \"unsafe\" {\n+\t\t\treturn types.Unsafe, nil\n+\t\t}\n+\t\tid = path\n+\n+\t\t// No need to re-import if the package was imported completely before.\n+\t\tif pkg = packages[id]; pkg != nil && pkg.Complete() {\n+\t\t\treturn\n+\t\t}\n+\t\tf, err := lookup(path)\n+\t\tif err != nil {\n+\t\t\treturn nil, err\n+\t\t}\n+\t\trc = f\n+\t} else {\n+\t\tfilename, id = FindPkg(path, srcDir)\n+\t\tif filename == \"\" {\n+\t\t\tif path == \"unsafe\" {\n+\t\t\t\treturn types.Unsafe, nil\n+\t\t\t}\n+\t\t\treturn nil, fmt.Errorf(\"can't find import: %q\", id)\n+\t\t}\n+\n+\t\t// no need to re-import if the package was imported completely before\n+\t\tif pkg = packages[id]; pkg != nil && pkg.Complete() {\n+\t\t\treturn\n+\t\t}\n+\n+\t\t// open file\n+\t\tf, err := os.Open(filename)\n+\t\tif err != nil {\n+\t\t\treturn nil, err\n+\t\t}\n+\t\tdefer func() {\n+\t\t\tif err != nil {\n+\t\t\t\t// add file name to error\n+\t\t\t\terr = fmt.Errorf(\"%s: %v\", filename, err)\n+\t\t\t}\n+\t\t}()\n+\t\trc = f\n+\t}\n+\tdefer rc.Close()\n+\n+\tvar hdr string\n+\tbuf := bufio.NewReader(rc)\n+\tif hdr, err = FindExportData(buf); err != nil {\n+\t\treturn\n+\t}\n+\n+\tswitch hdr {\n+\tcase \"$$\\n\":\n+\t\t// Work-around if we don't have a filename; happens only if lookup != nil.\n+\t\t// Either way, the filename is only needed for importer error messages, so\n+\t\t// this is fine.\n+\t\tif filename == \"\" {\n+\t\t\tfilename = path\n+\t\t}\n+\t\treturn ImportData(packages, filename, id, buf)\n+\n+\tcase \"$$B\\n\":\n+\t\tvar data []byte\n+\t\tdata, err = ioutil.ReadAll(buf)\n+\t\tif err != nil {\n+\t\t\tbreak\n+\t\t}\n+\n+\t\t// TODO(gri): allow clients of go/importer to provide a FileSet.\n+\t\t// Or, define a new standard go/types/gcexportdata package.\n+\t\tfset := token.NewFileSet()\n+\n+\t\t// The indexed export format starts with an 'i'; the older\n+\t\t// binary export format starts with a 'c', 'd', or 'v'\n+\t\t// (from \"version\"). Select appropriate importer.\n+\t\tif len(data) > 0 && data[0] == 'i' {\n+\t\t\t_, pkg, err = IImportData(fset, packages, data[1:], id)\n+\t\t} else {\n+\t\t\t_, pkg, err = BImportData(fset, packages, data, id)\n+\t\t}\n+\n+\tdefault:\n+\t\terr = fmt.Errorf(\"unknown export data header: %q\", hdr)\n+\t}\n+\n+\treturn\n+}\n+\n+// ----------------------------------------------------------------------------\n+// Parser\n+\n+// TODO(gri) Imported objects don't have position information.\n+//           Ideally use the debug table line info; alternatively\n+//           create some fake position (or the position of the\n+//           import). That way error messages referring to imported\n+//           objects can print meaningful information.\n+\n+// parser parses the exports inside a gc compiler-produced\n+// object/archive file and populates its scope with the results.\n+type parser struct {\n+\tscanner    scanner.Scanner\n+\ttok        rune                      // current token\n+\tlit        string                    // literal string; only valid for Ident, Int, String tokens\n+\tid         string                    // package id of imported package\n+\tsharedPkgs map[string]*types.Package // package id -> package object (across importer)\n+\tlocalPkgs  map[string]*types.Package // package id -> package object (just this package)\n+}\n+\n+func (p *parser) init(filename, id string, src io.Reader, packages map[string]*types.Package) {\n+\tp.scanner.Init(src)\n+\tp.scanner.Error = func(_ *scanner.Scanner, msg string) { p.error(msg) }\n+\tp.scanner.Mode = scanner.ScanIdents | scanner.ScanInts | scanner.ScanChars | scanner.ScanStrings | scanner.ScanComments | scanner.SkipComments\n+\tp.scanner.Whitespace = 1<<'\\t' | 1<<' '\n+\tp.scanner.Filename = filename // for good error messages\n+\tp.next()\n+\tp.id = id\n+\tp.sharedPkgs = packages\n+\tif debug {\n+\t\t// check consistency of packages map\n+\t\tfor _, pkg := range packages {\n+\t\t\tif pkg.Name() == \"\" {\n+\t\t\t\tfmt.Printf(\"no package name for %s\\n\", pkg.Path())\n+\t\t\t}\n+\t\t}\n+\t}\n+}\n+\n+func (p *parser) next() {\n+\tp.tok = p.scanner.Scan()\n+\tswitch p.tok {\n+\tcase scanner.Ident, scanner.Int, scanner.Char, scanner.String, '·':\n+\t\tp.lit = p.scanner.TokenText()\n+\tdefault:\n+\t\tp.lit = \"\"\n+\t}\n+\tif debug {\n+\t\tfmt.Printf(\"%s: %q -> %q\\n\", scanner.TokenString(p.tok), p.scanner.TokenText(), p.lit)\n+\t}\n+}\n+\n+func declTypeName(pkg *types.Package, name string) *types.TypeName {\n+\tscope := pkg.Scope()\n+\tif obj := scope.Lookup(name); obj != nil {\n+\t\treturn obj.(*types.TypeName)\n+\t}\n+\tobj := types.NewTypeName(token.NoPos, pkg, name, nil)\n+\t// a named type may be referred to before the underlying type\n+\t// is known - set it up\n+\ttypes.NewNamed(obj, nil, nil)\n+\tscope.Insert(obj)\n+\treturn obj\n+}\n+\n+// ----------------------------------------------------------------------------\n+// Error handling\n+\n+// Internal errors are boxed as importErrors.\n+type importError struct {\n+\tpos scanner.Position\n+\terr error\n+}\n+\n+func (e importError) Error() string {\n+\treturn fmt.Sprintf(\"import error %s (byte offset = %d): %s\", e.pos, e.pos.Offset, e.err)\n+}\n+\n+func (p *parser) error(err interface{}) {\n+\tif s, ok := err.(string); ok {\n+\t\terr = errors.New(s)\n+\t}\n+\t// panic with a runtime.Error if err is not an error\n+\tpanic(importError{p.scanner.Pos(), err.(error)})\n+}\n+\n+func (p *parser) errorf(format string, args ...interface{}) {\n+\tp.error(fmt.Sprintf(format, args...))\n+}\n+\n+func (p *parser) expect(tok rune) string {\n+\tlit := p.lit\n+\tif p.tok != tok {\n+\t\tp.errorf(\"expected %s, got %s (%s)\", scanner.TokenString(tok), scanner.TokenString(p.tok), lit)\n+\t}\n+\tp.next()\n+\treturn lit\n+}\n+\n+func (p *parser) expectSpecial(tok string) {\n+\tsep := 'x' // not white space\n+\ti := 0\n+\tfor i < len(tok) && p.tok == rune(tok[i]) && sep > ' ' {\n+\t\tsep = p.scanner.Peek() // if sep <= ' ', there is white space before the next token\n+\t\tp.next()\n+\t\ti++\n+\t}\n+\tif i < len(tok) {\n+\t\tp.errorf(\"expected %q, got %q\", tok, tok[0:i])\n+\t}\n+}\n+\n+func (p *parser) expectKeyword(keyword string) {\n+\tlit := p.expect(scanner.Ident)\n+\tif lit != keyword {\n+\t\tp.errorf(\"expected keyword %s, got %q\", keyword, lit)\n+\t}\n+}\n+\n+// ----------------------------------------------------------------------------\n+// Qualified and unqualified names\n+\n+// PackageId = string_lit .\n+//\n+func (p *parser) parsePackageId() string {\n+\tid, err := strconv.Unquote(p.expect(scanner.String))\n+\tif err != nil {\n+\t\tp.error(err)\n+\t}\n+\t// id == \"\" stands for the imported package id\n+\t// (only known at time of package installation)\n+\tif id == \"\" {\n+\t\tid = p.id\n+\t}\n+\treturn id\n+}\n+\n+// PackageName = ident .\n+//\n+func (p *parser) parsePackageName() string {\n+\treturn p.expect(scanner.Ident)\n+}\n+\n+// dotIdentifier = ( ident | '·' ) { ident | int | '·' } .\n+func (p *parser) parseDotIdent() string {\n+\tident := \"\"\n+\tif p.tok != scanner.Int {\n+\t\tsep := 'x' // not white space\n+\t\tfor (p.tok == scanner.Ident || p.tok == scanner.Int || p.tok == '·') && sep > ' ' {\n+\t\t\tident += p.lit\n+\t\t\tsep = p.scanner.Peek() // if sep <= ' ', there is white space before the next token\n+\t\t\tp.next()\n+\t\t}\n+\t}\n+\tif ident == \"\" {\n+\t\tp.expect(scanner.Ident) // use expect() for error handling\n+\t}\n+\treturn ident\n+}\n+\n+// QualifiedName = \"@\" PackageId \".\" ( \"?\" | dotIdentifier ) .\n+//\n+func (p *parser) parseQualifiedName() (id, name string) {\n+\tp.expect('@')\n+\tid = p.parsePackageId()\n+\tp.expect('.')\n+\t// Per rev f280b8a485fd (10/2/2013), qualified names may be used for anonymous fields.\n+\tif p.tok == '?' {\n+\t\tp.next()\n+\t} else {\n+\t\tname = p.parseDotIdent()\n+\t}\n+\treturn\n+}\n+\n+// getPkg returns the package for a given id. If the package is\n+// not found, create the package and add it to the p.localPkgs\n+// and p.sharedPkgs maps. name is the (expected) name of the\n+// package. If name == \"\", the package name is expected to be\n+// set later via an import clause in the export data.\n+//\n+// id identifies a package, usually by a canonical package path like\n+// \"encoding/json\" but possibly by a non-canonical import path like\n+// \"./json\".\n+//\n+func (p *parser) getPkg(id, name string) *types.Package {\n+\t// package unsafe is not in the packages maps - handle explicitly\n+\tif id == \"unsafe\" {\n+\t\treturn types.Unsafe\n+\t}\n+\n+\tpkg := p.localPkgs[id]\n+\tif pkg == nil {\n+\t\t// first import of id from this package\n+\t\tpkg = p.sharedPkgs[id]\n+\t\tif pkg == nil {\n+\t\t\t// first import of id by this importer;\n+\t\t\t// add (possibly unnamed) pkg to shared packages\n+\t\t\tpkg = types.NewPackage(id, name)\n+\t\t\tp.sharedPkgs[id] = pkg\n+\t\t}\n+\t\t// add (possibly unnamed) pkg to local packages\n+\t\tif p.localPkgs == nil {\n+\t\t\tp.localPkgs = make(map[string]*types.Package)\n+\t\t}\n+\t\tp.localPkgs[id] = pkg\n+\t} else if name != \"\" {\n+\t\t// package exists already and we have an expected package name;\n+\t\t// make sure names match or set package name if necessary\n+\t\tif pname := pkg.Name(); pname == \"\" {\n+\t\t\tpkg.SetName(name)\n+\t\t} else if pname != name {\n+\t\t\tp.errorf(\"%s package name mismatch: %s (given) vs %s (expected)\", id, pname, name)\n+\t\t}\n+\t}\n+\treturn pkg\n+}\n+\n+// parseExportedName is like parseQualifiedName, but\n+// the package id is resolved to an imported *types.Package.\n+//\n+func (p *parser) parseExportedName() (pkg *types.Package, name string) {\n+\tid, name := p.parseQualifiedName()\n+\tpkg = p.getPkg(id, \"\")\n+\treturn\n+}\n+\n+// ----------------------------------------------------------------------------\n+// Types\n+\n+// BasicType = identifier .\n+//\n+func (p *parser) parseBasicType() types.Type {\n+\tid := p.expect(scanner.Ident)\n+\tobj := types.Universe.Lookup(id)\n+\tif obj, ok := obj.(*types.TypeName); ok {\n+\t\treturn obj.Type()\n+\t}\n+\tp.errorf(\"not a basic type: %s\", id)\n+\treturn nil\n+}\n+\n+// ArrayType = \"[\" int_lit \"]\" Type .\n+//\n+func (p *parser) parseArrayType(parent *types.Package) types.Type {\n+\t// \"[\" already consumed and lookahead known not to be \"]\"\n+\tlit := p.expect(scanner.Int)\n+\tp.expect(']')\n+\telem := p.parseType(parent)\n+\tn, err := strconv.ParseInt(lit, 10, 64)\n+\tif err != nil {\n+\t\tp.error(err)\n+\t}\n+\treturn types.NewArray(elem, n)\n+}\n+\n+// MapType = \"map\" \"[\" Type \"]\" Type .\n+//\n+func (p *parser) parseMapType(parent *types.Package) types.Type {\n+\tp.expectKeyword(\"map\")\n+\tp.expect('[')\n+\tkey := p.parseType(parent)\n+\tp.expect(']')\n+\telem := p.parseType(parent)\n+\treturn types.NewMap(key, elem)\n+}\n+\n+// Name = identifier | \"?\" | QualifiedName .\n+//\n+// For unqualified and anonymous names, the returned package is the parent\n+// package unless parent == nil, in which case the returned package is the\n+// package being imported. (The parent package is not nil if the the name\n+// is an unqualified struct field or interface method name belonging to a\n+// type declared in another package.)\n+//\n+// For qualified names, the returned package is nil (and not created if\n+// it doesn't exist yet) unless materializePkg is set (which creates an\n+// unnamed package with valid package path). In the latter case, a\n+// subsequent import clause is expected to provide a name for the package.\n+//\n+func (p *parser) parseName(parent *types.Package, materializePkg bool) (pkg *types.Package, name string) {\n+\tpkg = parent\n+\tif pkg == nil {\n+\t\tpkg = p.sharedPkgs[p.id]\n+\t}\n+\tswitch p.tok {\n+\tcase scanner.Ident:\n+\t\tname = p.lit\n+\t\tp.next()\n+\tcase '?':\n+\t\t// anonymous\n+\t\tp.next()\n+\tcase '@':\n+\t\t// exported name prefixed with package path\n+\t\tpkg = nil\n+\t\tvar id string\n+\t\tid, name = p.parseQualifiedName()\n+\t\tif materializePkg {\n+\t\t\tpkg = p.getPkg(id, \"\")\n+\t\t}\n+\tdefault:\n+\t\tp.error(\"name expected\")\n+\t}\n+\treturn\n+}\n+\n+func deref(typ types.Type) types.Type {\n+\tif p, _ := typ.(*types.Pointer); p != nil {\n+\t\treturn p.Elem()\n+\t}\n+\treturn typ\n+}\n+\n+// Field = Name Type [ string_lit ] .\n+//\n+func (p *parser) parseField(parent *types.Package) (*types.Var, string) {\n+\tpkg, name := p.parseName(parent, true)\n+\n+\tif name == \"_\" {\n+\t\t// Blank fields should be package-qualified because they\n+\t\t// are unexported identifiers, but gc does not qualify them.\n+\t\t// Assuming that the ident belongs to the current package\n+\t\t// causes types to change during re-exporting, leading\n+\t\t// to spurious \"can't assign A to B\" errors from go/types.\n+\t\t// As a workaround, pretend all blank fields belong\n+\t\t// to the same unique dummy package.\n+\t\tconst blankpkg = \"<_>\"\n+\t\tpkg = p.getPkg(blankpkg, blankpkg)\n+\t}\n+\n+\ttyp := p.parseType(parent)\n+\tanonymous := false\n+\tif name == \"\" {\n+\t\t// anonymous field - typ must be T or *T and T must be a type name\n+\t\tswitch typ := deref(typ).(type) {\n+\t\tcase *types.Basic: // basic types are named types\n+\t\t\tpkg = nil // objects defined in Universe scope have no package\n+\t\t\tname = typ.Name()\n+\t\tcase *types.Named:\n+\t\t\tname = typ.Obj().Name()\n+\t\tdefault:\n+\t\t\tp.errorf(\"anonymous field expected\")\n+\t\t}\n+\t\tanonymous = true\n+\t}\n+\ttag := \"\"\n+\tif p.tok == scanner.String {\n+\t\ts := p.expect(scanner.String)\n+\t\tvar err error\n+\t\ttag, err = strconv.Unquote(s)\n+\t\tif err != nil {\n+\t\t\tp.errorf(\"invalid struct tag %s: %s\", s, err)\n+\t\t}\n+\t}\n+\treturn types.NewField(token.NoPos, pkg, name, typ, anonymous), tag\n+}\n+\n+// StructType = \"struct\" \"{\" [ FieldList ] \"}\" .\n+// FieldList  = Field { \";\" Field } .\n+//\n+func (p *parser) parseStructType(parent *types.Package) types.Type {\n+\tvar fields []*types.Var\n+\tvar tags []string\n+\n+\tp.expectKeyword(\"struct\")\n+\tp.expect('{')\n+\tfor i := 0; p.tok != '}' && p.tok != scanner.EOF; i++ {\n+\t\tif i > 0 {\n+\t\t\tp.expect(';')\n+\t\t}\n+\t\tfld, tag := p.parseField(parent)\n+\t\tif tag != \"\" && tags == nil {\n+\t\t\ttags = make([]string, i)\n+\t\t}\n+\t\tif tags != nil {\n+\t\t\ttags = append(tags, tag)\n+\t\t}\n+\t\tfields = append(fields, fld)\n+\t}\n+\tp.expect('}')\n+\n+\treturn types.NewStruct(fields, tags)\n+}\n+\n+// Parameter = ( identifier | \"?\" ) [ \"...\" ] Type [ string_lit ] .\n+//\n+func (p *parser) parseParameter() (par *types.Var, isVariadic bool) {\n+\t_, name := p.parseName(nil, false)\n+\t// remove gc-specific parameter numbering\n+\tif i := strings.Index(name, \"·\"); i >= 0 {\n+\t\tname = name[:i]\n+\t}\n+\tif p.tok == '.' {\n+\t\tp.expectSpecial(\"...\")\n+\t\tisVariadic = true\n+\t}\n+\ttyp := p.parseType(nil)\n+\tif isVariadic {\n+\t\ttyp = types.NewSlice(typ)\n+\t}\n+\t// ignore argument tag (e.g. \"noescape\")\n+\tif p.tok == scanner.String {\n+\t\tp.next()\n+\t}\n+\t// TODO(gri) should we provide a package?\n+\tpar = types.NewVar(token.NoPos, nil, name, typ)\n+\treturn\n+}\n+\n+// Parameters    = \"(\" [ ParameterList ] \")\" .\n+// ParameterList = { Parameter \",\" } Parameter .\n+//\n+func (p *parser) parseParameters() (list []*types.Var, isVariadic bool) {\n+\tp.expect('(')\n+\tfor p.tok != ')' && p.tok != scanner.EOF {\n+\t\tif len(list) > 0 {\n+\t\t\tp.expect(',')\n+\t\t}\n+\t\tpar, variadic := p.parseParameter()\n+\t\tlist = append(list, par)\n+\t\tif variadic {\n+\t\t\tif isVariadic {\n+\t\t\t\tp.error(\"... not on final argument\")\n+\t\t\t}\n+\t\t\tisVariadic = true\n+\t\t}\n+\t}\n+\tp.expect(')')\n+\n+\treturn\n+}\n+\n+// Signature = Parameters [ Result ] .\n+// Result    = Type | Parameters .\n+//\n+func (p *parser) parseSignature(recv *types.Var) *types.Signature {\n+\tparams, isVariadic := p.parseParameters()\n+\n+\t// optional result type\n+\tvar results []*types.Var\n+\tif p.tok == '(' {\n+\t\tvar variadic bool\n+\t\tresults, variadic = p.parseParameters()\n+\t\tif variadic {\n+\t\t\tp.error(\"... not permitted on result type\")\n+\t\t}\n+\t}\n+\n+\treturn types.NewSignature(recv, types.NewTuple(params...), types.NewTuple(results...), isVariadic)\n+}\n+\n+// InterfaceType = \"interface\" \"{\" [ MethodList ] \"}\" .\n+// MethodList    = Method { \";\" Method } .\n+// Method        = Name Signature .\n+//\n+// The methods of embedded interfaces are always \"inlined\"\n+// by the compiler and thus embedded interfaces are never\n+// visible in the export data.\n+//\n+func (p *parser) parseInterfaceType(parent *types.Package) types.Type {\n+\tvar methods []*types.Func\n+\n+\tp.expectKeyword(\"interface\")\n+\tp.expect('{')\n+\tfor i := 0; p.tok != '}' && p.tok != scanner.EOF; i++ {\n+\t\tif i > 0 {\n+\t\t\tp.expect(';')\n+\t\t}\n+\t\tpkg, name := p.parseName(parent, true)\n+\t\tsig := p.parseSignature(nil)\n+\t\tmethods = append(methods, types.NewFunc(token.NoPos, pkg, name, sig))\n+\t}\n+\tp.expect('}')\n+\n+\t// Complete requires the type's embedded interfaces to be fully defined,\n+\t// but we do not define any\n+\treturn types.NewInterface(methods, nil).Complete()\n+}\n+\n+// ChanType = ( \"chan\" [ \"<-\" ] | \"<-\" \"chan\" ) Type .\n+//\n+func (p *parser) parseChanType(parent *types.Package) types.Type {\n+\tdir := types.SendRecv\n+\tif p.tok == scanner.Ident {\n+\t\tp.expectKeyword(\"chan\")\n+\t\tif p.tok == '<' {\n+\t\t\tp.expectSpecial(\"<-\")\n+\t\t\tdir = types.SendOnly\n+\t\t}\n+\t} else {\n+\t\tp.expectSpecial(\"<-\")\n+\t\tp.expectKeyword(\"chan\")\n+\t\tdir = types.RecvOnly\n+\t}\n+\telem := p.parseType(parent)\n+\treturn types.NewChan(dir, elem)\n+}\n+\n+// Type =\n+//\tBasicType | TypeName | ArrayType | SliceType | StructType |\n+//      PointerType | FuncType | InterfaceType | MapType | ChanType |\n+//      \"(\" Type \")\" .\n+//\n+// BasicType   = ident .\n+// TypeName    = ExportedName .\n+// SliceType   = \"[\" \"]\" Type .\n+// PointerType = \"*\" Type .\n+// FuncType    = \"func\" Signature .\n+//\n+func (p *parser) parseType(parent *types.Package) types.Type {\n+\tswitch p.tok {\n+\tcase scanner.Ident:\n+\t\tswitch p.lit {\n+\t\tdefault:\n+\t\t\treturn p.parseBasicType()\n+\t\tcase \"struct\":\n+\t\t\treturn p.parseStructType(parent)\n+\t\tcase \"func\":\n+\t\t\t// FuncType\n+\t\t\tp.next()\n+\t\t\treturn p.parseSignature(nil)\n+\t\tcase \"interface\":\n+\t\t\treturn p.parseInterfaceType(parent)\n+\t\tcase \"map\":\n+\t\t\treturn p.parseMapType(parent)\n+\t\tcase \"chan\":\n+\t\t\treturn p.parseChanType(parent)\n+\t\t}\n+\tcase '@':\n+\t\t// TypeName\n+\t\tpkg, name := p.parseExportedName()\n+\t\treturn declTypeName(pkg, name).Type()\n+\tcase '[':\n+\t\tp.next() // look ahead\n+\t\tif p.tok == ']' {\n+\t\t\t// SliceType\n+\t\t\tp.next()\n+\t\t\treturn types.NewSlice(p.parseType(parent))\n+\t\t}\n+\t\treturn p.parseArrayType(parent)\n+\tcase '*':\n+\t\t// PointerType\n+\t\tp.next()\n+\t\treturn types.NewPointer(p.parseType(parent))\n+\tcase '<':\n+\t\treturn p.parseChanType(parent)\n+\tcase '(':\n+\t\t// \"(\" Type \")\"\n+\t\tp.next()\n+\t\ttyp := p.parseType(parent)\n+\t\tp.expect(')')\n+\t\treturn typ\n+\t}\n+\tp.errorf(\"expected type, got %s (%q)\", scanner.TokenString(p.tok), p.lit)\n+\treturn nil\n+}\n+\n+// ----------------------------------------------------------------------------\n+// Declarations\n+\n+// ImportDecl = \"import\" PackageName PackageId .\n+//\n+func (p *parser) parseImportDecl() {\n+\tp.expectKeyword(\"import\")\n+\tname := p.parsePackageName()\n+\tp.getPkg(p.parsePackageId(), name)\n+}\n+\n+// int_lit = [ \"+\" | \"-\" ] { \"0\" ... \"9\" } .\n+//\n+func (p *parser) parseInt() string {\n+\ts := \"\"\n+\tswitch p.tok {\n+\tcase '-':\n+\t\ts = \"-\"\n+\t\tp.next()\n+\tcase '+':\n+\t\tp.next()\n+\t}\n+\treturn s + p.expect(scanner.Int)\n+}\n+\n+// number = int_lit [ \"p\" int_lit ] .\n+//\n+func (p *parser) parseNumber() (typ *types.Basic, val constant.Value) {\n+\t// mantissa\n+\tmant := constant.MakeFromLiteral(p.parseInt(), token.INT, 0)\n+\tif mant == nil {\n+\t\tpanic(\"invalid mantissa\")\n+\t}\n+\n+\tif p.lit == \"p\" {\n+\t\t// exponent (base 2)\n+\t\tp.next()\n+\t\texp, err := strconv.ParseInt(p.parseInt(), 10, 0)\n+\t\tif err != nil {\n+\t\t\tp.error(err)\n+\t\t}\n+\t\tif exp < 0 {\n+\t\t\tdenom := constant.MakeInt64(1)\n+\t\t\tdenom = constant.Shift(denom, token.SHL, uint(-exp))\n+\t\t\ttyp = types.Typ[types.UntypedFloat]\n+\t\t\tval = constant.BinaryOp(mant, token.QUO, denom)\n+\t\t\treturn\n+\t\t}\n+\t\tif exp > 0 {\n+\t\t\tmant = constant.Shift(mant, token.SHL, uint(exp))\n+\t\t}\n+\t\ttyp = types.Typ[types.UntypedFloat]\n+\t\tval = mant\n+\t\treturn\n+\t}\n+\n+\ttyp = types.Typ[types.UntypedInt]\n+\tval = mant\n+\treturn\n+}\n+\n+// ConstDecl   = \"const\" ExportedName [ Type ] \"=\" Literal .\n+// Literal     = bool_lit | int_lit | float_lit | complex_lit | rune_lit | string_lit .\n+// bool_lit    = \"true\" | \"false\" .\n+// complex_lit = \"(\" float_lit \"+\" float_lit \"i\" \")\" .\n+// rune_lit    = \"(\" int_lit \"+\" int_lit \")\" .\n+// string_lit  = `\"` { unicode_char } `\"` .\n+//\n+func (p *parser) parseConstDecl() {\n+\tp.expectKeyword(\"const\")\n+\tpkg, name := p.parseExportedName()\n+\n+\tvar typ0 types.Type\n+\tif p.tok != '=' {\n+\t\t// constant types are never structured - no need for parent type\n+\t\ttyp0 = p.parseType(nil)\n+\t}\n+\n+\tp.expect('=')\n+\tvar typ types.Type\n+\tvar val constant.Value\n+\tswitch p.tok {\n+\tcase scanner.Ident:\n+\t\t// bool_lit\n+\t\tif p.lit != \"true\" && p.lit != \"false\" {\n+\t\t\tp.error(\"expected true or false\")\n+\t\t}\n+\t\ttyp = types.Typ[types.UntypedBool]\n+\t\tval = constant.MakeBool(p.lit == \"true\")\n+\t\tp.next()\n+\n+\tcase '-', scanner.Int:\n+\t\t// int_lit\n+\t\ttyp, val = p.parseNumber()\n+\n+\tcase '(':\n+\t\t// complex_lit or rune_lit\n+\t\tp.next()\n+\t\tif p.tok == scanner.Char {\n+\t\t\tp.next()\n+\t\t\tp.expect('+')\n+\t\t\ttyp = types.Typ[types.UntypedRune]\n+\t\t\t_, val = p.parseNumber()\n+\t\t\tp.expect(')')\n+\t\t\tbreak\n+\t\t}\n+\t\t_, re := p.parseNumber()\n+\t\tp.expect('+')\n+\t\t_, im := p.parseNumber()\n+\t\tp.expectKeyword(\"i\")\n+\t\tp.expect(')')\n+\t\ttyp = types.Typ[types.UntypedComplex]\n+\t\tval = constant.BinaryOp(re, token.ADD, constant.MakeImag(im))\n+\n+\tcase scanner.Char:\n+\t\t// rune_lit\n+\t\ttyp = types.Typ[types.UntypedRune]\n+\t\tval = constant.MakeFromLiteral(p.lit, token.CHAR, 0)\n+\t\tp.next()\n+\n+\tcase scanner.String:\n+\t\t// string_lit\n+\t\ttyp = types.Typ[types.UntypedString]\n+\t\tval = constant.MakeFromLiteral(p.lit, token.STRING, 0)\n+\t\tp.next()\n+\n+\tdefault:\n+\t\tp.errorf(\"expected literal got %s\", scanner.TokenString(p.tok))\n+\t}\n+\n+\tif typ0 == nil {\n+\t\ttyp0 = typ\n+\t}\n+\n+\tpkg.Scope().Insert(types.NewConst(token.NoPos, pkg, name, typ0, val))\n+}\n+\n+// TypeDecl = \"type\" ExportedName Type .\n+//\n+func (p *parser) parseTypeDecl() {\n+\tp.expectKeyword(\"type\")\n+\tpkg, name := p.parseExportedName()\n+\tobj := declTypeName(pkg, name)\n+\n+\t// The type object may have been imported before and thus already\n+\t// have a type associated with it. We still need to parse the type\n+\t// structure, but throw it away if the object already has a type.\n+\t// This ensures that all imports refer to the same type object for\n+\t// a given type declaration.\n+\ttyp := p.parseType(pkg)\n+\n+\tif name := obj.Type().(*types.Named); name.Underlying() == nil {\n+\t\tname.SetUnderlying(typ)\n+\t}\n+}\n+\n+// VarDecl = \"var\" ExportedName Type .\n+//\n+func (p *parser) parseVarDecl() {\n+\tp.expectKeyword(\"var\")\n+\tpkg, name := p.parseExportedName()\n+\ttyp := p.parseType(pkg)\n+\tpkg.Scope().Insert(types.NewVar(token.NoPos, pkg, name, typ))\n+}\n+\n+// Func = Signature [ Body ] .\n+// Body = \"{\" ... \"}\" .\n+//\n+func (p *parser) parseFunc(recv *types.Var) *types.Signature {\n+\tsig := p.parseSignature(recv)\n+\tif p.tok == '{' {\n+\t\tp.next()\n+\t\tfor i := 1; i > 0; p.next() {\n+\t\t\tswitch p.tok {\n+\t\t\tcase '{':\n+\t\t\t\ti++\n+\t\t\tcase '}':\n+\t\t\t\ti--\n+\t\t\t}\n+\t\t}\n+\t}\n+\treturn sig\n+}\n+\n+// MethodDecl = \"func\" Receiver Name Func .\n+// Receiver   = \"(\" ( identifier | \"?\" ) [ \"*\" ] ExportedName \")\" .\n+//\n+func (p *parser) parseMethodDecl() {\n+\t// \"func\" already consumed\n+\tp.expect('(')\n+\trecv, _ := p.parseParameter() // receiver\n+\tp.expect(')')\n+\n+\t// determine receiver base type object\n+\tbase := deref(recv.Type()).(*types.Named)\n+\n+\t// parse method name, signature, and possibly inlined body\n+\t_, name := p.parseName(nil, false)\n+\tsig := p.parseFunc(recv)\n+\n+\t// methods always belong to the same package as the base type object\n+\tpkg := base.Obj().Pkg()\n+\n+\t// add method to type unless type was imported before\n+\t// and method exists already\n+\t// TODO(gri) This leads to a quadratic algorithm - ok for now because method counts are small.\n+\tbase.AddMethod(types.NewFunc(token.NoPos, pkg, name, sig))\n+}\n+\n+// FuncDecl = \"func\" ExportedName Func .\n+//\n+func (p *parser) parseFuncDecl() {\n+\t// \"func\" already consumed\n+\tpkg, name := p.parseExportedName()\n+\ttyp := p.parseFunc(nil)\n+\tpkg.Scope().Insert(types.NewFunc(token.NoPos, pkg, name, typ))\n+}\n+\n+// Decl = [ ImportDecl | ConstDecl | TypeDecl | VarDecl | FuncDecl | MethodDecl ] \"\\n\" .\n+//\n+func (p *parser) parseDecl() {\n+\tif p.tok == scanner.Ident {\n+\t\tswitch p.lit {\n+\t\tcase \"import\":\n+\t\t\tp.parseImportDecl()\n+\t\tcase \"const\":\n+\t\t\tp.parseConstDecl()\n+\t\tcase \"type\":\n+\t\t\tp.parseTypeDecl()\n+\t\tcase \"var\":\n+\t\t\tp.parseVarDecl()\n+\t\tcase \"func\":\n+\t\t\tp.next() // look ahead\n+\t\t\tif p.tok == '(' {\n+\t\t\t\tp.parseMethodDecl()\n+\t\t\t} else {\n+\t\t\t\tp.parseFuncDecl()\n+\t\t\t}\n+\t\t}\n+\t}\n+\tp.expect('\\n')\n+}\n+\n+// ----------------------------------------------------------------------------\n+// Export\n+\n+// Export        = \"PackageClause { Decl } \"$$\" .\n+// PackageClause = \"package\" PackageName [ \"safe\" ] \"\\n\" .\n+//\n+func (p *parser) parseExport() *types.Package {\n+\tp.expectKeyword(\"package\")\n+\tname := p.parsePackageName()\n+\tif p.tok == scanner.Ident && p.lit == \"safe\" {\n+\t\t// package was compiled with -u option - ignore\n+\t\tp.next()\n+\t}\n+\tp.expect('\\n')\n+\n+\tpkg := p.getPkg(p.id, name)\n+\n+\tfor p.tok != '$' && p.tok != scanner.EOF {\n+\t\tp.parseDecl()\n+\t}\n+\n+\tif ch := p.scanner.Peek(); p.tok != '$' || ch != '$' {\n+\t\t// don't call next()/expect() since reading past the\n+\t\t// export data may cause scanner errors (e.g. NUL chars)\n+\t\tp.errorf(\"expected '$$', got %s %c\", scanner.TokenString(p.tok), ch)\n+\t}\n+\n+\tif n := p.scanner.ErrorCount; n != 0 {\n+\t\tp.errorf(\"expected no scanner errors, got %d\", n)\n+\t}\n+\n+\t// Record all locally referenced packages as imports.\n+\tvar imports []*types.Package\n+\tfor id, pkg2 := range p.localPkgs {\n+\t\tif pkg2.Name() == \"\" {\n+\t\t\tp.errorf(\"%s package has no name\", id)\n+\t\t}\n+\t\tif id == p.id {\n+\t\t\tcontinue // avoid self-edge\n+\t\t}\n+\t\timports = append(imports, pkg2)\n+\t}\n+\tsort.Sort(byPath(imports))\n+\tpkg.SetImports(imports)\n+\n+\t// package was imported completely and without errors\n+\tpkg.MarkComplete()\n+\n+\treturn pkg\n+}\n+\n+type byPath []*types.Package\n+\n+func (a byPath) Len() int           { return len(a) }\n+func (a byPath) Swap(i, j int)      { a[i], a[j] = a[j], a[i] }\n+func (a byPath) Less(i, j int) bool { return a[i].Path() < a[j].Path() }"
    },
    {
      "sha": "4be32a2e55fe086634dfe61ddf4c57f112813519",
      "filename": "backend/vendor/golang.org/x/tools/go/internal/gcimporter/iexport.go",
      "status": "added",
      "additions": 739,
      "deletions": 0,
      "changes": 739,
      "blob_url": "https://github.com/umputun/remark42/blob/80f4862c9c2905475f3484f20ca41f7bdf103de9/backend/vendor/golang.org/x/tools/go/internal/gcimporter/iexport.go",
      "raw_url": "https://github.com/umputun/remark42/raw/80f4862c9c2905475f3484f20ca41f7bdf103de9/backend/vendor/golang.org/x/tools/go/internal/gcimporter/iexport.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/golang.org/x/tools/go/internal/gcimporter/iexport.go?ref=80f4862c9c2905475f3484f20ca41f7bdf103de9",
      "patch": "@@ -0,0 +1,739 @@\n+// Copyright 2019 The Go Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style\n+// license that can be found in the LICENSE file.\n+\n+// Indexed binary package export.\n+// This file was derived from $GOROOT/src/cmd/compile/internal/gc/iexport.go;\n+// see that file for specification of the format.\n+\n+package gcimporter\n+\n+import (\n+\t\"bytes\"\n+\t\"encoding/binary\"\n+\t\"go/ast\"\n+\t\"go/constant\"\n+\t\"go/token\"\n+\t\"go/types\"\n+\t\"io\"\n+\t\"math/big\"\n+\t\"reflect\"\n+\t\"sort\"\n+)\n+\n+// Current indexed export format version. Increase with each format change.\n+// 0: Go1.11 encoding\n+const iexportVersion = 0\n+\n+// IExportData returns the binary export data for pkg.\n+//\n+// If no file set is provided, position info will be missing.\n+// The package path of the top-level package will not be recorded,\n+// so that calls to IImportData can override with a provided package path.\n+func IExportData(fset *token.FileSet, pkg *types.Package) (b []byte, err error) {\n+\tdefer func() {\n+\t\tif e := recover(); e != nil {\n+\t\t\tif ierr, ok := e.(internalError); ok {\n+\t\t\t\terr = ierr\n+\t\t\t\treturn\n+\t\t\t}\n+\t\t\t// Not an internal error; panic again.\n+\t\t\tpanic(e)\n+\t\t}\n+\t}()\n+\n+\tp := iexporter{\n+\t\tout:         bytes.NewBuffer(nil),\n+\t\tfset:        fset,\n+\t\tallPkgs:     map[*types.Package]bool{},\n+\t\tstringIndex: map[string]uint64{},\n+\t\tdeclIndex:   map[types.Object]uint64{},\n+\t\ttypIndex:    map[types.Type]uint64{},\n+\t\tlocalpkg:    pkg,\n+\t}\n+\n+\tfor i, pt := range predeclared() {\n+\t\tp.typIndex[pt] = uint64(i)\n+\t}\n+\tif len(p.typIndex) > predeclReserved {\n+\t\tpanic(internalErrorf(\"too many predeclared types: %d > %d\", len(p.typIndex), predeclReserved))\n+\t}\n+\n+\t// Initialize work queue with exported declarations.\n+\tscope := pkg.Scope()\n+\tfor _, name := range scope.Names() {\n+\t\tif ast.IsExported(name) {\n+\t\t\tp.pushDecl(scope.Lookup(name))\n+\t\t}\n+\t}\n+\n+\t// Loop until no more work.\n+\tfor !p.declTodo.empty() {\n+\t\tp.doDecl(p.declTodo.popHead())\n+\t}\n+\n+\t// Append indices to data0 section.\n+\tdataLen := uint64(p.data0.Len())\n+\tw := p.newWriter()\n+\tw.writeIndex(p.declIndex)\n+\tw.flush()\n+\n+\t// Assemble header.\n+\tvar hdr intWriter\n+\thdr.WriteByte('i')\n+\thdr.uint64(iexportVersion)\n+\thdr.uint64(uint64(p.strings.Len()))\n+\thdr.uint64(dataLen)\n+\n+\t// Flush output.\n+\tio.Copy(p.out, &hdr)\n+\tio.Copy(p.out, &p.strings)\n+\tio.Copy(p.out, &p.data0)\n+\n+\treturn p.out.Bytes(), nil\n+}\n+\n+// writeIndex writes out an object index. mainIndex indicates whether\n+// we're writing out the main index, which is also read by\n+// non-compiler tools and includes a complete package description\n+// (i.e., name and height).\n+func (w *exportWriter) writeIndex(index map[types.Object]uint64) {\n+\t// Build a map from packages to objects from that package.\n+\tpkgObjs := map[*types.Package][]types.Object{}\n+\n+\t// For the main index, make sure to include every package that\n+\t// we reference, even if we're not exporting (or reexporting)\n+\t// any symbols from it.\n+\tpkgObjs[w.p.localpkg] = nil\n+\tfor pkg := range w.p.allPkgs {\n+\t\tpkgObjs[pkg] = nil\n+\t}\n+\n+\tfor obj := range index {\n+\t\tpkgObjs[obj.Pkg()] = append(pkgObjs[obj.Pkg()], obj)\n+\t}\n+\n+\tvar pkgs []*types.Package\n+\tfor pkg, objs := range pkgObjs {\n+\t\tpkgs = append(pkgs, pkg)\n+\n+\t\tsort.Slice(objs, func(i, j int) bool {\n+\t\t\treturn objs[i].Name() < objs[j].Name()\n+\t\t})\n+\t}\n+\n+\tsort.Slice(pkgs, func(i, j int) bool {\n+\t\treturn w.exportPath(pkgs[i]) < w.exportPath(pkgs[j])\n+\t})\n+\n+\tw.uint64(uint64(len(pkgs)))\n+\tfor _, pkg := range pkgs {\n+\t\tw.string(w.exportPath(pkg))\n+\t\tw.string(pkg.Name())\n+\t\tw.uint64(uint64(0)) // package height is not needed for go/types\n+\n+\t\tobjs := pkgObjs[pkg]\n+\t\tw.uint64(uint64(len(objs)))\n+\t\tfor _, obj := range objs {\n+\t\t\tw.string(obj.Name())\n+\t\t\tw.uint64(index[obj])\n+\t\t}\n+\t}\n+}\n+\n+type iexporter struct {\n+\tfset *token.FileSet\n+\tout  *bytes.Buffer\n+\n+\tlocalpkg *types.Package\n+\n+\t// allPkgs tracks all packages that have been referenced by\n+\t// the export data, so we can ensure to include them in the\n+\t// main index.\n+\tallPkgs map[*types.Package]bool\n+\n+\tdeclTodo objQueue\n+\n+\tstrings     intWriter\n+\tstringIndex map[string]uint64\n+\n+\tdata0     intWriter\n+\tdeclIndex map[types.Object]uint64\n+\ttypIndex  map[types.Type]uint64\n+}\n+\n+// stringOff returns the offset of s within the string section.\n+// If not already present, it's added to the end.\n+func (p *iexporter) stringOff(s string) uint64 {\n+\toff, ok := p.stringIndex[s]\n+\tif !ok {\n+\t\toff = uint64(p.strings.Len())\n+\t\tp.stringIndex[s] = off\n+\n+\t\tp.strings.uint64(uint64(len(s)))\n+\t\tp.strings.WriteString(s)\n+\t}\n+\treturn off\n+}\n+\n+// pushDecl adds n to the declaration work queue, if not already present.\n+func (p *iexporter) pushDecl(obj types.Object) {\n+\t// Package unsafe is known to the compiler and predeclared.\n+\tassert(obj.Pkg() != types.Unsafe)\n+\n+\tif _, ok := p.declIndex[obj]; ok {\n+\t\treturn\n+\t}\n+\n+\tp.declIndex[obj] = ^uint64(0) // mark n present in work queue\n+\tp.declTodo.pushTail(obj)\n+}\n+\n+// exportWriter handles writing out individual data section chunks.\n+type exportWriter struct {\n+\tp *iexporter\n+\n+\tdata     intWriter\n+\tcurrPkg  *types.Package\n+\tprevFile string\n+\tprevLine int64\n+}\n+\n+func (w *exportWriter) exportPath(pkg *types.Package) string {\n+\tif pkg == w.p.localpkg {\n+\t\treturn \"\"\n+\t}\n+\treturn pkg.Path()\n+}\n+\n+func (p *iexporter) doDecl(obj types.Object) {\n+\tw := p.newWriter()\n+\tw.setPkg(obj.Pkg(), false)\n+\n+\tswitch obj := obj.(type) {\n+\tcase *types.Var:\n+\t\tw.tag('V')\n+\t\tw.pos(obj.Pos())\n+\t\tw.typ(obj.Type(), obj.Pkg())\n+\n+\tcase *types.Func:\n+\t\tsig, _ := obj.Type().(*types.Signature)\n+\t\tif sig.Recv() != nil {\n+\t\t\tpanic(internalErrorf(\"unexpected method: %v\", sig))\n+\t\t}\n+\t\tw.tag('F')\n+\t\tw.pos(obj.Pos())\n+\t\tw.signature(sig)\n+\n+\tcase *types.Const:\n+\t\tw.tag('C')\n+\t\tw.pos(obj.Pos())\n+\t\tw.value(obj.Type(), obj.Val())\n+\n+\tcase *types.TypeName:\n+\t\tif obj.IsAlias() {\n+\t\t\tw.tag('A')\n+\t\t\tw.pos(obj.Pos())\n+\t\t\tw.typ(obj.Type(), obj.Pkg())\n+\t\t\tbreak\n+\t\t}\n+\n+\t\t// Defined type.\n+\t\tw.tag('T')\n+\t\tw.pos(obj.Pos())\n+\n+\t\tunderlying := obj.Type().Underlying()\n+\t\tw.typ(underlying, obj.Pkg())\n+\n+\t\tt := obj.Type()\n+\t\tif types.IsInterface(t) {\n+\t\t\tbreak\n+\t\t}\n+\n+\t\tnamed, ok := t.(*types.Named)\n+\t\tif !ok {\n+\t\t\tpanic(internalErrorf(\"%s is not a defined type\", t))\n+\t\t}\n+\n+\t\tn := named.NumMethods()\n+\t\tw.uint64(uint64(n))\n+\t\tfor i := 0; i < n; i++ {\n+\t\t\tm := named.Method(i)\n+\t\t\tw.pos(m.Pos())\n+\t\t\tw.string(m.Name())\n+\t\t\tsig, _ := m.Type().(*types.Signature)\n+\t\t\tw.param(sig.Recv())\n+\t\t\tw.signature(sig)\n+\t\t}\n+\n+\tdefault:\n+\t\tpanic(internalErrorf(\"unexpected object: %v\", obj))\n+\t}\n+\n+\tp.declIndex[obj] = w.flush()\n+}\n+\n+func (w *exportWriter) tag(tag byte) {\n+\tw.data.WriteByte(tag)\n+}\n+\n+func (w *exportWriter) pos(pos token.Pos) {\n+\tif w.p.fset == nil {\n+\t\tw.int64(0)\n+\t\treturn\n+\t}\n+\n+\tp := w.p.fset.Position(pos)\n+\tfile := p.Filename\n+\tline := int64(p.Line)\n+\n+\t// When file is the same as the last position (common case),\n+\t// we can save a few bytes by delta encoding just the line\n+\t// number.\n+\t//\n+\t// Note: Because data objects may be read out of order (or not\n+\t// at all), we can only apply delta encoding within a single\n+\t// object. This is handled implicitly by tracking prevFile and\n+\t// prevLine as fields of exportWriter.\n+\n+\tif file == w.prevFile {\n+\t\tdelta := line - w.prevLine\n+\t\tw.int64(delta)\n+\t\tif delta == deltaNewFile {\n+\t\t\tw.int64(-1)\n+\t\t}\n+\t} else {\n+\t\tw.int64(deltaNewFile)\n+\t\tw.int64(line) // line >= 0\n+\t\tw.string(file)\n+\t\tw.prevFile = file\n+\t}\n+\tw.prevLine = line\n+}\n+\n+func (w *exportWriter) pkg(pkg *types.Package) {\n+\t// Ensure any referenced packages are declared in the main index.\n+\tw.p.allPkgs[pkg] = true\n+\n+\tw.string(w.exportPath(pkg))\n+}\n+\n+func (w *exportWriter) qualifiedIdent(obj types.Object) {\n+\t// Ensure any referenced declarations are written out too.\n+\tw.p.pushDecl(obj)\n+\n+\tw.string(obj.Name())\n+\tw.pkg(obj.Pkg())\n+}\n+\n+func (w *exportWriter) typ(t types.Type, pkg *types.Package) {\n+\tw.data.uint64(w.p.typOff(t, pkg))\n+}\n+\n+func (p *iexporter) newWriter() *exportWriter {\n+\treturn &exportWriter{p: p}\n+}\n+\n+func (w *exportWriter) flush() uint64 {\n+\toff := uint64(w.p.data0.Len())\n+\tio.Copy(&w.p.data0, &w.data)\n+\treturn off\n+}\n+\n+func (p *iexporter) typOff(t types.Type, pkg *types.Package) uint64 {\n+\toff, ok := p.typIndex[t]\n+\tif !ok {\n+\t\tw := p.newWriter()\n+\t\tw.doTyp(t, pkg)\n+\t\toff = predeclReserved + w.flush()\n+\t\tp.typIndex[t] = off\n+\t}\n+\treturn off\n+}\n+\n+func (w *exportWriter) startType(k itag) {\n+\tw.data.uint64(uint64(k))\n+}\n+\n+func (w *exportWriter) doTyp(t types.Type, pkg *types.Package) {\n+\tswitch t := t.(type) {\n+\tcase *types.Named:\n+\t\tw.startType(definedType)\n+\t\tw.qualifiedIdent(t.Obj())\n+\n+\tcase *types.Pointer:\n+\t\tw.startType(pointerType)\n+\t\tw.typ(t.Elem(), pkg)\n+\n+\tcase *types.Slice:\n+\t\tw.startType(sliceType)\n+\t\tw.typ(t.Elem(), pkg)\n+\n+\tcase *types.Array:\n+\t\tw.startType(arrayType)\n+\t\tw.uint64(uint64(t.Len()))\n+\t\tw.typ(t.Elem(), pkg)\n+\n+\tcase *types.Chan:\n+\t\tw.startType(chanType)\n+\t\t// 1 RecvOnly; 2 SendOnly; 3 SendRecv\n+\t\tvar dir uint64\n+\t\tswitch t.Dir() {\n+\t\tcase types.RecvOnly:\n+\t\t\tdir = 1\n+\t\tcase types.SendOnly:\n+\t\t\tdir = 2\n+\t\tcase types.SendRecv:\n+\t\t\tdir = 3\n+\t\t}\n+\t\tw.uint64(dir)\n+\t\tw.typ(t.Elem(), pkg)\n+\n+\tcase *types.Map:\n+\t\tw.startType(mapType)\n+\t\tw.typ(t.Key(), pkg)\n+\t\tw.typ(t.Elem(), pkg)\n+\n+\tcase *types.Signature:\n+\t\tw.startType(signatureType)\n+\t\tw.setPkg(pkg, true)\n+\t\tw.signature(t)\n+\n+\tcase *types.Struct:\n+\t\tw.startType(structType)\n+\t\tw.setPkg(pkg, true)\n+\n+\t\tn := t.NumFields()\n+\t\tw.uint64(uint64(n))\n+\t\tfor i := 0; i < n; i++ {\n+\t\t\tf := t.Field(i)\n+\t\t\tw.pos(f.Pos())\n+\t\t\tw.string(f.Name())\n+\t\t\tw.typ(f.Type(), pkg)\n+\t\t\tw.bool(f.Anonymous())\n+\t\t\tw.string(t.Tag(i)) // note (or tag)\n+\t\t}\n+\n+\tcase *types.Interface:\n+\t\tw.startType(interfaceType)\n+\t\tw.setPkg(pkg, true)\n+\n+\t\tn := t.NumEmbeddeds()\n+\t\tw.uint64(uint64(n))\n+\t\tfor i := 0; i < n; i++ {\n+\t\t\tf := t.Embedded(i)\n+\t\t\tw.pos(f.Obj().Pos())\n+\t\t\tw.typ(f.Obj().Type(), f.Obj().Pkg())\n+\t\t}\n+\n+\t\tn = t.NumExplicitMethods()\n+\t\tw.uint64(uint64(n))\n+\t\tfor i := 0; i < n; i++ {\n+\t\t\tm := t.ExplicitMethod(i)\n+\t\t\tw.pos(m.Pos())\n+\t\t\tw.string(m.Name())\n+\t\t\tsig, _ := m.Type().(*types.Signature)\n+\t\t\tw.signature(sig)\n+\t\t}\n+\n+\tdefault:\n+\t\tpanic(internalErrorf(\"unexpected type: %v, %v\", t, reflect.TypeOf(t)))\n+\t}\n+}\n+\n+func (w *exportWriter) setPkg(pkg *types.Package, write bool) {\n+\tif write {\n+\t\tw.pkg(pkg)\n+\t}\n+\n+\tw.currPkg = pkg\n+}\n+\n+func (w *exportWriter) signature(sig *types.Signature) {\n+\tw.paramList(sig.Params())\n+\tw.paramList(sig.Results())\n+\tif sig.Params().Len() > 0 {\n+\t\tw.bool(sig.Variadic())\n+\t}\n+}\n+\n+func (w *exportWriter) paramList(tup *types.Tuple) {\n+\tn := tup.Len()\n+\tw.uint64(uint64(n))\n+\tfor i := 0; i < n; i++ {\n+\t\tw.param(tup.At(i))\n+\t}\n+}\n+\n+func (w *exportWriter) param(obj types.Object) {\n+\tw.pos(obj.Pos())\n+\tw.localIdent(obj)\n+\tw.typ(obj.Type(), obj.Pkg())\n+}\n+\n+func (w *exportWriter) value(typ types.Type, v constant.Value) {\n+\tw.typ(typ, nil)\n+\n+\tswitch v.Kind() {\n+\tcase constant.Bool:\n+\t\tw.bool(constant.BoolVal(v))\n+\tcase constant.Int:\n+\t\tvar i big.Int\n+\t\tif i64, exact := constant.Int64Val(v); exact {\n+\t\t\ti.SetInt64(i64)\n+\t\t} else if ui64, exact := constant.Uint64Val(v); exact {\n+\t\t\ti.SetUint64(ui64)\n+\t\t} else {\n+\t\t\ti.SetString(v.ExactString(), 10)\n+\t\t}\n+\t\tw.mpint(&i, typ)\n+\tcase constant.Float:\n+\t\tf := constantToFloat(v)\n+\t\tw.mpfloat(f, typ)\n+\tcase constant.Complex:\n+\t\tw.mpfloat(constantToFloat(constant.Real(v)), typ)\n+\t\tw.mpfloat(constantToFloat(constant.Imag(v)), typ)\n+\tcase constant.String:\n+\t\tw.string(constant.StringVal(v))\n+\tcase constant.Unknown:\n+\t\t// package contains type errors\n+\tdefault:\n+\t\tpanic(internalErrorf(\"unexpected value %v (%T)\", v, v))\n+\t}\n+}\n+\n+// constantToFloat converts a constant.Value with kind constant.Float to a\n+// big.Float.\n+func constantToFloat(x constant.Value) *big.Float {\n+\tassert(x.Kind() == constant.Float)\n+\t// Use the same floating-point precision (512) as cmd/compile\n+\t// (see Mpprec in cmd/compile/internal/gc/mpfloat.go).\n+\tconst mpprec = 512\n+\tvar f big.Float\n+\tf.SetPrec(mpprec)\n+\tif v, exact := constant.Float64Val(x); exact {\n+\t\t// float64\n+\t\tf.SetFloat64(v)\n+\t} else if num, denom := constant.Num(x), constant.Denom(x); num.Kind() == constant.Int {\n+\t\t// TODO(gri): add big.Rat accessor to constant.Value.\n+\t\tn := valueToRat(num)\n+\t\td := valueToRat(denom)\n+\t\tf.SetRat(n.Quo(n, d))\n+\t} else {\n+\t\t// Value too large to represent as a fraction => inaccessible.\n+\t\t// TODO(gri): add big.Float accessor to constant.Value.\n+\t\t_, ok := f.SetString(x.ExactString())\n+\t\tassert(ok)\n+\t}\n+\treturn &f\n+}\n+\n+// mpint exports a multi-precision integer.\n+//\n+// For unsigned types, small values are written out as a single\n+// byte. Larger values are written out as a length-prefixed big-endian\n+// byte string, where the length prefix is encoded as its complement.\n+// For example, bytes 0, 1, and 2 directly represent the integer\n+// values 0, 1, and 2; while bytes 255, 254, and 253 indicate a 1-,\n+// 2-, and 3-byte big-endian string follow.\n+//\n+// Encoding for signed types use the same general approach as for\n+// unsigned types, except small values use zig-zag encoding and the\n+// bottom bit of length prefix byte for large values is reserved as a\n+// sign bit.\n+//\n+// The exact boundary between small and large encodings varies\n+// according to the maximum number of bytes needed to encode a value\n+// of type typ. As a special case, 8-bit types are always encoded as a\n+// single byte.\n+//\n+// TODO(mdempsky): Is this level of complexity really worthwhile?\n+func (w *exportWriter) mpint(x *big.Int, typ types.Type) {\n+\tbasic, ok := typ.Underlying().(*types.Basic)\n+\tif !ok {\n+\t\tpanic(internalErrorf(\"unexpected type %v (%T)\", typ.Underlying(), typ.Underlying()))\n+\t}\n+\n+\tsigned, maxBytes := intSize(basic)\n+\n+\tnegative := x.Sign() < 0\n+\tif !signed && negative {\n+\t\tpanic(internalErrorf(\"negative unsigned integer; type %v, value %v\", typ, x))\n+\t}\n+\n+\tb := x.Bytes()\n+\tif len(b) > 0 && b[0] == 0 {\n+\t\tpanic(internalErrorf(\"leading zeros\"))\n+\t}\n+\tif uint(len(b)) > maxBytes {\n+\t\tpanic(internalErrorf(\"bad mpint length: %d > %d (type %v, value %v)\", len(b), maxBytes, typ, x))\n+\t}\n+\n+\tmaxSmall := 256 - maxBytes\n+\tif signed {\n+\t\tmaxSmall = 256 - 2*maxBytes\n+\t}\n+\tif maxBytes == 1 {\n+\t\tmaxSmall = 256\n+\t}\n+\n+\t// Check if x can use small value encoding.\n+\tif len(b) <= 1 {\n+\t\tvar ux uint\n+\t\tif len(b) == 1 {\n+\t\t\tux = uint(b[0])\n+\t\t}\n+\t\tif signed {\n+\t\t\tux <<= 1\n+\t\t\tif negative {\n+\t\t\t\tux--\n+\t\t\t}\n+\t\t}\n+\t\tif ux < maxSmall {\n+\t\t\tw.data.WriteByte(byte(ux))\n+\t\t\treturn\n+\t\t}\n+\t}\n+\n+\tn := 256 - uint(len(b))\n+\tif signed {\n+\t\tn = 256 - 2*uint(len(b))\n+\t\tif negative {\n+\t\t\tn |= 1\n+\t\t}\n+\t}\n+\tif n < maxSmall || n >= 256 {\n+\t\tpanic(internalErrorf(\"encoding mistake: %d, %v, %v => %d\", len(b), signed, negative, n))\n+\t}\n+\n+\tw.data.WriteByte(byte(n))\n+\tw.data.Write(b)\n+}\n+\n+// mpfloat exports a multi-precision floating point number.\n+//\n+// The number's value is decomposed into mantissa × 2**exponent, where\n+// mantissa is an integer. The value is written out as mantissa (as a\n+// multi-precision integer) and then the exponent, except exponent is\n+// omitted if mantissa is zero.\n+func (w *exportWriter) mpfloat(f *big.Float, typ types.Type) {\n+\tif f.IsInf() {\n+\t\tpanic(\"infinite constant\")\n+\t}\n+\n+\t// Break into f = mant × 2**exp, with 0.5 <= mant < 1.\n+\tvar mant big.Float\n+\texp := int64(f.MantExp(&mant))\n+\n+\t// Scale so that mant is an integer.\n+\tprec := mant.MinPrec()\n+\tmant.SetMantExp(&mant, int(prec))\n+\texp -= int64(prec)\n+\n+\tmanti, acc := mant.Int(nil)\n+\tif acc != big.Exact {\n+\t\tpanic(internalErrorf(\"mantissa scaling failed for %f (%s)\", f, acc))\n+\t}\n+\tw.mpint(manti, typ)\n+\tif manti.Sign() != 0 {\n+\t\tw.int64(exp)\n+\t}\n+}\n+\n+func (w *exportWriter) bool(b bool) bool {\n+\tvar x uint64\n+\tif b {\n+\t\tx = 1\n+\t}\n+\tw.uint64(x)\n+\treturn b\n+}\n+\n+func (w *exportWriter) int64(x int64)   { w.data.int64(x) }\n+func (w *exportWriter) uint64(x uint64) { w.data.uint64(x) }\n+func (w *exportWriter) string(s string) { w.uint64(w.p.stringOff(s)) }\n+\n+func (w *exportWriter) localIdent(obj types.Object) {\n+\t// Anonymous parameters.\n+\tif obj == nil {\n+\t\tw.string(\"\")\n+\t\treturn\n+\t}\n+\n+\tname := obj.Name()\n+\tif name == \"_\" {\n+\t\tw.string(\"_\")\n+\t\treturn\n+\t}\n+\n+\tw.string(name)\n+}\n+\n+type intWriter struct {\n+\tbytes.Buffer\n+}\n+\n+func (w *intWriter) int64(x int64) {\n+\tvar buf [binary.MaxVarintLen64]byte\n+\tn := binary.PutVarint(buf[:], x)\n+\tw.Write(buf[:n])\n+}\n+\n+func (w *intWriter) uint64(x uint64) {\n+\tvar buf [binary.MaxVarintLen64]byte\n+\tn := binary.PutUvarint(buf[:], x)\n+\tw.Write(buf[:n])\n+}\n+\n+func assert(cond bool) {\n+\tif !cond {\n+\t\tpanic(\"internal error: assertion failed\")\n+\t}\n+}\n+\n+// The below is copied from go/src/cmd/compile/internal/gc/syntax.go.\n+\n+// objQueue is a FIFO queue of types.Object. The zero value of objQueue is\n+// a ready-to-use empty queue.\n+type objQueue struct {\n+\tring       []types.Object\n+\thead, tail int\n+}\n+\n+// empty returns true if q contains no Nodes.\n+func (q *objQueue) empty() bool {\n+\treturn q.head == q.tail\n+}\n+\n+// pushTail appends n to the tail of the queue.\n+func (q *objQueue) pushTail(obj types.Object) {\n+\tif len(q.ring) == 0 {\n+\t\tq.ring = make([]types.Object, 16)\n+\t} else if q.head+len(q.ring) == q.tail {\n+\t\t// Grow the ring.\n+\t\tnring := make([]types.Object, len(q.ring)*2)\n+\t\t// Copy the old elements.\n+\t\tpart := q.ring[q.head%len(q.ring):]\n+\t\tif q.tail-q.head <= len(part) {\n+\t\t\tpart = part[:q.tail-q.head]\n+\t\t\tcopy(nring, part)\n+\t\t} else {\n+\t\t\tpos := copy(nring, part)\n+\t\t\tcopy(nring[pos:], q.ring[:q.tail%len(q.ring)])\n+\t\t}\n+\t\tq.ring, q.head, q.tail = nring, 0, q.tail-q.head\n+\t}\n+\n+\tq.ring[q.tail%len(q.ring)] = obj\n+\tq.tail++\n+}\n+\n+// popHead pops a node from the head of the queue. It panics if q is empty.\n+func (q *objQueue) popHead() types.Object {\n+\tif q.empty() {\n+\t\tpanic(\"dequeue empty\")\n+\t}\n+\tobj := q.ring[q.head%len(q.ring)]\n+\tq.head++\n+\treturn obj\n+}"
    },
    {
      "sha": "a31a880263e123683154599025fda8f5e29f1685",
      "filename": "backend/vendor/golang.org/x/tools/go/internal/gcimporter/iimport.go",
      "status": "added",
      "additions": 630,
      "deletions": 0,
      "changes": 630,
      "blob_url": "https://github.com/umputun/remark42/blob/80f4862c9c2905475f3484f20ca41f7bdf103de9/backend/vendor/golang.org/x/tools/go/internal/gcimporter/iimport.go",
      "raw_url": "https://github.com/umputun/remark42/raw/80f4862c9c2905475f3484f20ca41f7bdf103de9/backend/vendor/golang.org/x/tools/go/internal/gcimporter/iimport.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/golang.org/x/tools/go/internal/gcimporter/iimport.go?ref=80f4862c9c2905475f3484f20ca41f7bdf103de9",
      "patch": "@@ -0,0 +1,630 @@\n+// Copyright 2018 The Go Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style\n+// license that can be found in the LICENSE file.\n+\n+// Indexed package import.\n+// See cmd/compile/internal/gc/iexport.go for the export data format.\n+\n+// This file is a copy of $GOROOT/src/go/internal/gcimporter/iimport.go.\n+\n+package gcimporter\n+\n+import (\n+\t\"bytes\"\n+\t\"encoding/binary\"\n+\t\"fmt\"\n+\t\"go/constant\"\n+\t\"go/token\"\n+\t\"go/types\"\n+\t\"io\"\n+\t\"sort\"\n+)\n+\n+type intReader struct {\n+\t*bytes.Reader\n+\tpath string\n+}\n+\n+func (r *intReader) int64() int64 {\n+\ti, err := binary.ReadVarint(r.Reader)\n+\tif err != nil {\n+\t\terrorf(\"import %q: read varint error: %v\", r.path, err)\n+\t}\n+\treturn i\n+}\n+\n+func (r *intReader) uint64() uint64 {\n+\ti, err := binary.ReadUvarint(r.Reader)\n+\tif err != nil {\n+\t\terrorf(\"import %q: read varint error: %v\", r.path, err)\n+\t}\n+\treturn i\n+}\n+\n+const predeclReserved = 32\n+\n+type itag uint64\n+\n+const (\n+\t// Types\n+\tdefinedType itag = iota\n+\tpointerType\n+\tsliceType\n+\tarrayType\n+\tchanType\n+\tmapType\n+\tsignatureType\n+\tstructType\n+\tinterfaceType\n+)\n+\n+// IImportData imports a package from the serialized package data\n+// and returns the number of bytes consumed and a reference to the package.\n+// If the export data version is not recognized or the format is otherwise\n+// compromised, an error is returned.\n+func IImportData(fset *token.FileSet, imports map[string]*types.Package, data []byte, path string) (_ int, pkg *types.Package, err error) {\n+\tconst currentVersion = 1\n+\tversion := int64(-1)\n+\tdefer func() {\n+\t\tif e := recover(); e != nil {\n+\t\t\tif version > currentVersion {\n+\t\t\t\terr = fmt.Errorf(\"cannot import %q (%v), export data is newer version - update tool\", path, e)\n+\t\t\t} else {\n+\t\t\t\terr = fmt.Errorf(\"cannot import %q (%v), possibly version skew - reinstall package\", path, e)\n+\t\t\t}\n+\t\t}\n+\t}()\n+\n+\tr := &intReader{bytes.NewReader(data), path}\n+\n+\tversion = int64(r.uint64())\n+\tswitch version {\n+\tcase currentVersion, 0:\n+\tdefault:\n+\t\terrorf(\"unknown iexport format version %d\", version)\n+\t}\n+\n+\tsLen := int64(r.uint64())\n+\tdLen := int64(r.uint64())\n+\n+\twhence, _ := r.Seek(0, io.SeekCurrent)\n+\tstringData := data[whence : whence+sLen]\n+\tdeclData := data[whence+sLen : whence+sLen+dLen]\n+\tr.Seek(sLen+dLen, io.SeekCurrent)\n+\n+\tp := iimporter{\n+\t\tipath:   path,\n+\t\tversion: int(version),\n+\n+\t\tstringData:  stringData,\n+\t\tstringCache: make(map[uint64]string),\n+\t\tpkgCache:    make(map[uint64]*types.Package),\n+\n+\t\tdeclData: declData,\n+\t\tpkgIndex: make(map[*types.Package]map[string]uint64),\n+\t\ttypCache: make(map[uint64]types.Type),\n+\n+\t\tfake: fakeFileSet{\n+\t\t\tfset:  fset,\n+\t\t\tfiles: make(map[string]*token.File),\n+\t\t},\n+\t}\n+\n+\tfor i, pt := range predeclared() {\n+\t\tp.typCache[uint64(i)] = pt\n+\t}\n+\n+\tpkgList := make([]*types.Package, r.uint64())\n+\tfor i := range pkgList {\n+\t\tpkgPathOff := r.uint64()\n+\t\tpkgPath := p.stringAt(pkgPathOff)\n+\t\tpkgName := p.stringAt(r.uint64())\n+\t\t_ = r.uint64() // package height; unused by go/types\n+\n+\t\tif pkgPath == \"\" {\n+\t\t\tpkgPath = path\n+\t\t}\n+\t\tpkg := imports[pkgPath]\n+\t\tif pkg == nil {\n+\t\t\tpkg = types.NewPackage(pkgPath, pkgName)\n+\t\t\timports[pkgPath] = pkg\n+\t\t} else if pkg.Name() != pkgName {\n+\t\t\terrorf(\"conflicting names %s and %s for package %q\", pkg.Name(), pkgName, path)\n+\t\t}\n+\n+\t\tp.pkgCache[pkgPathOff] = pkg\n+\n+\t\tnameIndex := make(map[string]uint64)\n+\t\tfor nSyms := r.uint64(); nSyms > 0; nSyms-- {\n+\t\t\tname := p.stringAt(r.uint64())\n+\t\t\tnameIndex[name] = r.uint64()\n+\t\t}\n+\n+\t\tp.pkgIndex[pkg] = nameIndex\n+\t\tpkgList[i] = pkg\n+\t}\n+\tif len(pkgList) == 0 {\n+\t\terrorf(\"no packages found for %s\", path)\n+\t\tpanic(\"unreachable\")\n+\t}\n+\tp.ipkg = pkgList[0]\n+\tnames := make([]string, 0, len(p.pkgIndex[p.ipkg]))\n+\tfor name := range p.pkgIndex[p.ipkg] {\n+\t\tnames = append(names, name)\n+\t}\n+\tsort.Strings(names)\n+\tfor _, name := range names {\n+\t\tp.doDecl(p.ipkg, name)\n+\t}\n+\n+\tfor _, typ := range p.interfaceList {\n+\t\ttyp.Complete()\n+\t}\n+\n+\t// record all referenced packages as imports\n+\tlist := append(([]*types.Package)(nil), pkgList[1:]...)\n+\tsort.Sort(byPath(list))\n+\tp.ipkg.SetImports(list)\n+\n+\t// package was imported completely and without errors\n+\tp.ipkg.MarkComplete()\n+\n+\tconsumed, _ := r.Seek(0, io.SeekCurrent)\n+\treturn int(consumed), p.ipkg, nil\n+}\n+\n+type iimporter struct {\n+\tipath   string\n+\tipkg    *types.Package\n+\tversion int\n+\n+\tstringData  []byte\n+\tstringCache map[uint64]string\n+\tpkgCache    map[uint64]*types.Package\n+\n+\tdeclData []byte\n+\tpkgIndex map[*types.Package]map[string]uint64\n+\ttypCache map[uint64]types.Type\n+\n+\tfake          fakeFileSet\n+\tinterfaceList []*types.Interface\n+}\n+\n+func (p *iimporter) doDecl(pkg *types.Package, name string) {\n+\t// See if we've already imported this declaration.\n+\tif obj := pkg.Scope().Lookup(name); obj != nil {\n+\t\treturn\n+\t}\n+\n+\toff, ok := p.pkgIndex[pkg][name]\n+\tif !ok {\n+\t\terrorf(\"%v.%v not in index\", pkg, name)\n+\t}\n+\n+\tr := &importReader{p: p, currPkg: pkg}\n+\tr.declReader.Reset(p.declData[off:])\n+\n+\tr.obj(name)\n+}\n+\n+func (p *iimporter) stringAt(off uint64) string {\n+\tif s, ok := p.stringCache[off]; ok {\n+\t\treturn s\n+\t}\n+\n+\tslen, n := binary.Uvarint(p.stringData[off:])\n+\tif n <= 0 {\n+\t\terrorf(\"varint failed\")\n+\t}\n+\tspos := off + uint64(n)\n+\ts := string(p.stringData[spos : spos+slen])\n+\tp.stringCache[off] = s\n+\treturn s\n+}\n+\n+func (p *iimporter) pkgAt(off uint64) *types.Package {\n+\tif pkg, ok := p.pkgCache[off]; ok {\n+\t\treturn pkg\n+\t}\n+\tpath := p.stringAt(off)\n+\tif path == p.ipath {\n+\t\treturn p.ipkg\n+\t}\n+\terrorf(\"missing package %q in %q\", path, p.ipath)\n+\treturn nil\n+}\n+\n+func (p *iimporter) typAt(off uint64, base *types.Named) types.Type {\n+\tif t, ok := p.typCache[off]; ok && (base == nil || !isInterface(t)) {\n+\t\treturn t\n+\t}\n+\n+\tif off < predeclReserved {\n+\t\terrorf(\"predeclared type missing from cache: %v\", off)\n+\t}\n+\n+\tr := &importReader{p: p}\n+\tr.declReader.Reset(p.declData[off-predeclReserved:])\n+\tt := r.doType(base)\n+\n+\tif base == nil || !isInterface(t) {\n+\t\tp.typCache[off] = t\n+\t}\n+\treturn t\n+}\n+\n+type importReader struct {\n+\tp          *iimporter\n+\tdeclReader bytes.Reader\n+\tcurrPkg    *types.Package\n+\tprevFile   string\n+\tprevLine   int64\n+\tprevColumn int64\n+}\n+\n+func (r *importReader) obj(name string) {\n+\ttag := r.byte()\n+\tpos := r.pos()\n+\n+\tswitch tag {\n+\tcase 'A':\n+\t\ttyp := r.typ()\n+\n+\t\tr.declare(types.NewTypeName(pos, r.currPkg, name, typ))\n+\n+\tcase 'C':\n+\t\ttyp, val := r.value()\n+\n+\t\tr.declare(types.NewConst(pos, r.currPkg, name, typ, val))\n+\n+\tcase 'F':\n+\t\tsig := r.signature(nil)\n+\n+\t\tr.declare(types.NewFunc(pos, r.currPkg, name, sig))\n+\n+\tcase 'T':\n+\t\t// Types can be recursive. We need to setup a stub\n+\t\t// declaration before recursing.\n+\t\tobj := types.NewTypeName(pos, r.currPkg, name, nil)\n+\t\tnamed := types.NewNamed(obj, nil, nil)\n+\t\tr.declare(obj)\n+\n+\t\tunderlying := r.p.typAt(r.uint64(), named).Underlying()\n+\t\tnamed.SetUnderlying(underlying)\n+\n+\t\tif !isInterface(underlying) {\n+\t\t\tfor n := r.uint64(); n > 0; n-- {\n+\t\t\t\tmpos := r.pos()\n+\t\t\t\tmname := r.ident()\n+\t\t\t\trecv := r.param()\n+\t\t\t\tmsig := r.signature(recv)\n+\n+\t\t\t\tnamed.AddMethod(types.NewFunc(mpos, r.currPkg, mname, msig))\n+\t\t\t}\n+\t\t}\n+\n+\tcase 'V':\n+\t\ttyp := r.typ()\n+\n+\t\tr.declare(types.NewVar(pos, r.currPkg, name, typ))\n+\n+\tdefault:\n+\t\terrorf(\"unexpected tag: %v\", tag)\n+\t}\n+}\n+\n+func (r *importReader) declare(obj types.Object) {\n+\tobj.Pkg().Scope().Insert(obj)\n+}\n+\n+func (r *importReader) value() (typ types.Type, val constant.Value) {\n+\ttyp = r.typ()\n+\n+\tswitch b := typ.Underlying().(*types.Basic); b.Info() & types.IsConstType {\n+\tcase types.IsBoolean:\n+\t\tval = constant.MakeBool(r.bool())\n+\n+\tcase types.IsString:\n+\t\tval = constant.MakeString(r.string())\n+\n+\tcase types.IsInteger:\n+\t\tval = r.mpint(b)\n+\n+\tcase types.IsFloat:\n+\t\tval = r.mpfloat(b)\n+\n+\tcase types.IsComplex:\n+\t\tre := r.mpfloat(b)\n+\t\tim := r.mpfloat(b)\n+\t\tval = constant.BinaryOp(re, token.ADD, constant.MakeImag(im))\n+\n+\tdefault:\n+\t\tif b.Kind() == types.Invalid {\n+\t\t\tval = constant.MakeUnknown()\n+\t\t\treturn\n+\t\t}\n+\t\terrorf(\"unexpected type %v\", typ) // panics\n+\t\tpanic(\"unreachable\")\n+\t}\n+\n+\treturn\n+}\n+\n+func intSize(b *types.Basic) (signed bool, maxBytes uint) {\n+\tif (b.Info() & types.IsUntyped) != 0 {\n+\t\treturn true, 64\n+\t}\n+\n+\tswitch b.Kind() {\n+\tcase types.Float32, types.Complex64:\n+\t\treturn true, 3\n+\tcase types.Float64, types.Complex128:\n+\t\treturn true, 7\n+\t}\n+\n+\tsigned = (b.Info() & types.IsUnsigned) == 0\n+\tswitch b.Kind() {\n+\tcase types.Int8, types.Uint8:\n+\t\tmaxBytes = 1\n+\tcase types.Int16, types.Uint16:\n+\t\tmaxBytes = 2\n+\tcase types.Int32, types.Uint32:\n+\t\tmaxBytes = 4\n+\tdefault:\n+\t\tmaxBytes = 8\n+\t}\n+\n+\treturn\n+}\n+\n+func (r *importReader) mpint(b *types.Basic) constant.Value {\n+\tsigned, maxBytes := intSize(b)\n+\n+\tmaxSmall := 256 - maxBytes\n+\tif signed {\n+\t\tmaxSmall = 256 - 2*maxBytes\n+\t}\n+\tif maxBytes == 1 {\n+\t\tmaxSmall = 256\n+\t}\n+\n+\tn, _ := r.declReader.ReadByte()\n+\tif uint(n) < maxSmall {\n+\t\tv := int64(n)\n+\t\tif signed {\n+\t\t\tv >>= 1\n+\t\t\tif n&1 != 0 {\n+\t\t\t\tv = ^v\n+\t\t\t}\n+\t\t}\n+\t\treturn constant.MakeInt64(v)\n+\t}\n+\n+\tv := -n\n+\tif signed {\n+\t\tv = -(n &^ 1) >> 1\n+\t}\n+\tif v < 1 || uint(v) > maxBytes {\n+\t\terrorf(\"weird decoding: %v, %v => %v\", n, signed, v)\n+\t}\n+\n+\tbuf := make([]byte, v)\n+\tio.ReadFull(&r.declReader, buf)\n+\n+\t// convert to little endian\n+\t// TODO(gri) go/constant should have a more direct conversion function\n+\t//           (e.g., once it supports a big.Float based implementation)\n+\tfor i, j := 0, len(buf)-1; i < j; i, j = i+1, j-1 {\n+\t\tbuf[i], buf[j] = buf[j], buf[i]\n+\t}\n+\n+\tx := constant.MakeFromBytes(buf)\n+\tif signed && n&1 != 0 {\n+\t\tx = constant.UnaryOp(token.SUB, x, 0)\n+\t}\n+\treturn x\n+}\n+\n+func (r *importReader) mpfloat(b *types.Basic) constant.Value {\n+\tx := r.mpint(b)\n+\tif constant.Sign(x) == 0 {\n+\t\treturn x\n+\t}\n+\n+\texp := r.int64()\n+\tswitch {\n+\tcase exp > 0:\n+\t\tx = constant.Shift(x, token.SHL, uint(exp))\n+\tcase exp < 0:\n+\t\td := constant.Shift(constant.MakeInt64(1), token.SHL, uint(-exp))\n+\t\tx = constant.BinaryOp(x, token.QUO, d)\n+\t}\n+\treturn x\n+}\n+\n+func (r *importReader) ident() string {\n+\treturn r.string()\n+}\n+\n+func (r *importReader) qualifiedIdent() (*types.Package, string) {\n+\tname := r.string()\n+\tpkg := r.pkg()\n+\treturn pkg, name\n+}\n+\n+func (r *importReader) pos() token.Pos {\n+\tif r.p.version >= 1 {\n+\t\tr.posv1()\n+\t} else {\n+\t\tr.posv0()\n+\t}\n+\n+\tif r.prevFile == \"\" && r.prevLine == 0 && r.prevColumn == 0 {\n+\t\treturn token.NoPos\n+\t}\n+\treturn r.p.fake.pos(r.prevFile, int(r.prevLine), int(r.prevColumn))\n+}\n+\n+func (r *importReader) posv0() {\n+\tdelta := r.int64()\n+\tif delta != deltaNewFile {\n+\t\tr.prevLine += delta\n+\t} else if l := r.int64(); l == -1 {\n+\t\tr.prevLine += deltaNewFile\n+\t} else {\n+\t\tr.prevFile = r.string()\n+\t\tr.prevLine = l\n+\t}\n+}\n+\n+func (r *importReader) posv1() {\n+\tdelta := r.int64()\n+\tr.prevColumn += delta >> 1\n+\tif delta&1 != 0 {\n+\t\tdelta = r.int64()\n+\t\tr.prevLine += delta >> 1\n+\t\tif delta&1 != 0 {\n+\t\t\tr.prevFile = r.string()\n+\t\t}\n+\t}\n+}\n+\n+func (r *importReader) typ() types.Type {\n+\treturn r.p.typAt(r.uint64(), nil)\n+}\n+\n+func isInterface(t types.Type) bool {\n+\t_, ok := t.(*types.Interface)\n+\treturn ok\n+}\n+\n+func (r *importReader) pkg() *types.Package { return r.p.pkgAt(r.uint64()) }\n+func (r *importReader) string() string      { return r.p.stringAt(r.uint64()) }\n+\n+func (r *importReader) doType(base *types.Named) types.Type {\n+\tswitch k := r.kind(); k {\n+\tdefault:\n+\t\terrorf(\"unexpected kind tag in %q: %v\", r.p.ipath, k)\n+\t\treturn nil\n+\n+\tcase definedType:\n+\t\tpkg, name := r.qualifiedIdent()\n+\t\tr.p.doDecl(pkg, name)\n+\t\treturn pkg.Scope().Lookup(name).(*types.TypeName).Type()\n+\tcase pointerType:\n+\t\treturn types.NewPointer(r.typ())\n+\tcase sliceType:\n+\t\treturn types.NewSlice(r.typ())\n+\tcase arrayType:\n+\t\tn := r.uint64()\n+\t\treturn types.NewArray(r.typ(), int64(n))\n+\tcase chanType:\n+\t\tdir := chanDir(int(r.uint64()))\n+\t\treturn types.NewChan(dir, r.typ())\n+\tcase mapType:\n+\t\treturn types.NewMap(r.typ(), r.typ())\n+\tcase signatureType:\n+\t\tr.currPkg = r.pkg()\n+\t\treturn r.signature(nil)\n+\n+\tcase structType:\n+\t\tr.currPkg = r.pkg()\n+\n+\t\tfields := make([]*types.Var, r.uint64())\n+\t\ttags := make([]string, len(fields))\n+\t\tfor i := range fields {\n+\t\t\tfpos := r.pos()\n+\t\t\tfname := r.ident()\n+\t\t\tftyp := r.typ()\n+\t\t\temb := r.bool()\n+\t\t\ttag := r.string()\n+\n+\t\t\tfields[i] = types.NewField(fpos, r.currPkg, fname, ftyp, emb)\n+\t\t\ttags[i] = tag\n+\t\t}\n+\t\treturn types.NewStruct(fields, tags)\n+\n+\tcase interfaceType:\n+\t\tr.currPkg = r.pkg()\n+\n+\t\tembeddeds := make([]types.Type, r.uint64())\n+\t\tfor i := range embeddeds {\n+\t\t\t_ = r.pos()\n+\t\t\tembeddeds[i] = r.typ()\n+\t\t}\n+\n+\t\tmethods := make([]*types.Func, r.uint64())\n+\t\tfor i := range methods {\n+\t\t\tmpos := r.pos()\n+\t\t\tmname := r.ident()\n+\n+\t\t\t// TODO(mdempsky): Matches bimport.go, but I\n+\t\t\t// don't agree with this.\n+\t\t\tvar recv *types.Var\n+\t\t\tif base != nil {\n+\t\t\t\trecv = types.NewVar(token.NoPos, r.currPkg, \"\", base)\n+\t\t\t}\n+\n+\t\t\tmsig := r.signature(recv)\n+\t\t\tmethods[i] = types.NewFunc(mpos, r.currPkg, mname, msig)\n+\t\t}\n+\n+\t\ttyp := newInterface(methods, embeddeds)\n+\t\tr.p.interfaceList = append(r.p.interfaceList, typ)\n+\t\treturn typ\n+\t}\n+}\n+\n+func (r *importReader) kind() itag {\n+\treturn itag(r.uint64())\n+}\n+\n+func (r *importReader) signature(recv *types.Var) *types.Signature {\n+\tparams := r.paramList()\n+\tresults := r.paramList()\n+\tvariadic := params.Len() > 0 && r.bool()\n+\treturn types.NewSignature(recv, params, results, variadic)\n+}\n+\n+func (r *importReader) paramList() *types.Tuple {\n+\txs := make([]*types.Var, r.uint64())\n+\tfor i := range xs {\n+\t\txs[i] = r.param()\n+\t}\n+\treturn types.NewTuple(xs...)\n+}\n+\n+func (r *importReader) param() *types.Var {\n+\tpos := r.pos()\n+\tname := r.ident()\n+\ttyp := r.typ()\n+\treturn types.NewParam(pos, r.currPkg, name, typ)\n+}\n+\n+func (r *importReader) bool() bool {\n+\treturn r.uint64() != 0\n+}\n+\n+func (r *importReader) int64() int64 {\n+\tn, err := binary.ReadVarint(&r.declReader)\n+\tif err != nil {\n+\t\terrorf(\"readVarint: %v\", err)\n+\t}\n+\treturn n\n+}\n+\n+func (r *importReader) uint64() uint64 {\n+\tn, err := binary.ReadUvarint(&r.declReader)\n+\tif err != nil {\n+\t\terrorf(\"readUvarint: %v\", err)\n+\t}\n+\treturn n\n+}\n+\n+func (r *importReader) byte() byte {\n+\tx, err := r.declReader.ReadByte()\n+\tif err != nil {\n+\t\terrorf(\"declReader.ReadByte: %v\", err)\n+\t}\n+\treturn x\n+}"
    },
    {
      "sha": "463f2522714638323cb37cd9523a6be99d3bdac8",
      "filename": "backend/vendor/golang.org/x/tools/go/internal/gcimporter/newInterface10.go",
      "status": "added",
      "additions": 21,
      "deletions": 0,
      "changes": 21,
      "blob_url": "https://github.com/umputun/remark42/blob/80f4862c9c2905475f3484f20ca41f7bdf103de9/backend/vendor/golang.org/x/tools/go/internal/gcimporter/newInterface10.go",
      "raw_url": "https://github.com/umputun/remark42/raw/80f4862c9c2905475f3484f20ca41f7bdf103de9/backend/vendor/golang.org/x/tools/go/internal/gcimporter/newInterface10.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/golang.org/x/tools/go/internal/gcimporter/newInterface10.go?ref=80f4862c9c2905475f3484f20ca41f7bdf103de9",
      "patch": "@@ -0,0 +1,21 @@\n+// Copyright 2018 The Go Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style\n+// license that can be found in the LICENSE file.\n+\n+// +build !go1.11\n+\n+package gcimporter\n+\n+import \"go/types\"\n+\n+func newInterface(methods []*types.Func, embeddeds []types.Type) *types.Interface {\n+\tnamed := make([]*types.Named, len(embeddeds))\n+\tfor i, e := range embeddeds {\n+\t\tvar ok bool\n+\t\tnamed[i], ok = e.(*types.Named)\n+\t\tif !ok {\n+\t\t\tpanic(\"embedding of non-defined interfaces in interfaces is not supported before Go 1.11\")\n+\t\t}\n+\t}\n+\treturn types.NewInterface(methods, named)\n+}"
    },
    {
      "sha": "ab28b95cbb84f5dd327ab613498d261d6364c917",
      "filename": "backend/vendor/golang.org/x/tools/go/internal/gcimporter/newInterface11.go",
      "status": "added",
      "additions": 13,
      "deletions": 0,
      "changes": 13,
      "blob_url": "https://github.com/umputun/remark42/blob/80f4862c9c2905475f3484f20ca41f7bdf103de9/backend/vendor/golang.org/x/tools/go/internal/gcimporter/newInterface11.go",
      "raw_url": "https://github.com/umputun/remark42/raw/80f4862c9c2905475f3484f20ca41f7bdf103de9/backend/vendor/golang.org/x/tools/go/internal/gcimporter/newInterface11.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/golang.org/x/tools/go/internal/gcimporter/newInterface11.go?ref=80f4862c9c2905475f3484f20ca41f7bdf103de9",
      "patch": "@@ -0,0 +1,13 @@\n+// Copyright 2018 The Go Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style\n+// license that can be found in the LICENSE file.\n+\n+// +build go1.11\n+\n+package gcimporter\n+\n+import \"go/types\"\n+\n+func newInterface(methods []*types.Func, embeddeds []types.Type) *types.Interface {\n+\treturn types.NewInterfaceType(methods, embeddeds)\n+}"
    },
    {
      "sha": "af22a2fc4a7a07f2aad4bee3f5364aa1276d451d",
      "filename": "backend/vendor/modules.txt",
      "status": "modified",
      "additions": 11,
      "deletions": 0,
      "changes": 11,
      "blob_url": "https://github.com/umputun/remark42/blob/80f4862c9c2905475f3484f20ca41f7bdf103de9/backend/vendor/modules.txt",
      "raw_url": "https://github.com/umputun/remark42/raw/80f4862c9c2905475f3484f20ca41f7bdf103de9/backend/vendor/modules.txt",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend/vendor/modules.txt?ref=80f4862c9c2905475f3484f20ca41f7bdf103de9",
      "patch": "@@ -212,6 +212,10 @@ go.mongodb.org/mongo-driver/x/mongo/driver/session\n go.mongodb.org/mongo-driver/x/mongo/driver/topology\n go.mongodb.org/mongo-driver/x/mongo/driver/uuid\n go.mongodb.org/mongo-driver/x/mongo/driver/wiremessage\n+# go.uber.org/goleak v1.0.0\n+## explicit\n+go.uber.org/goleak\n+go.uber.org/goleak/internal/stack\n # golang.org/x/crypto v0.0.0-20200406173513-056763e48d71\n ## explicit\n golang.org/x/crypto/acme\n@@ -221,6 +225,9 @@ golang.org/x/crypto/pbkdf2\n ## explicit\n golang.org/x/image/draw\n golang.org/x/image/math/f64\n+# golang.org/x/lint v0.0.0-20190930215403-16217165b5de\n+golang.org/x/lint\n+golang.org/x/lint/golint\n # golang.org/x/net v0.0.0-20200520182314-0ba52f642ac2\n ## explicit\n golang.org/x/net/context\n@@ -248,6 +255,10 @@ golang.org/x/text/unicode/bidi\n golang.org/x/text/unicode/norm\n # golang.org/x/time v0.0.0-20200416051211-89c76fbcd5d1\n golang.org/x/time/rate\n+# golang.org/x/tools v0.0.0-20191108193012-7d206e10da11\n+golang.org/x/tools/go/ast/astutil\n+golang.org/x/tools/go/gcexportdata\n+golang.org/x/tools/go/internal/gcimporter\n # google.golang.org/appengine v1.4.0\n google.golang.org/appengine\n google.golang.org/appengine/internal"
    }
  ]
}
